{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "layout: post\n",
        "title: Variational Inference for MRP with Reliable Posterior Distributions\n",
        "subtitle: Part 5- Normalizing Flows\n",
        "date: 2023-06-11\n",
        "draft: False\n",
        "image: images/32_layer.gif\n",
        "categories:\n",
        "  - MRP\n",
        "  - Variational Inference\n",
        "  - Normalizing Flows\n",
        "---\n",
        "   \n",
        "This is section 5 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality. \n",
        "\n",
        "The general structure for this post and the posts around it will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in these four posts will go a long way towards more robust variational inference. I'll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.\n",
        "\n",
        "In the [last post](https://andytimm.github.io/posts/Variational%20MRP%20Pt4/variational_mrp_4.html) we saw a variety of different ways importance sampling can be used to improve VI and make it more robust, from defining a tighter bound to optimize in the importance weighted ELBO, to weighting $q(x)$ samples together efficiently to look more like $p(x)$, to combining entirely different variational approximations together to cover different parts of the posterior with multiple importance sampling.\n",
        "\n",
        "In this post, we'll tackle the problem of how to define a deeply flexible variational\n",
        "family $\\mathscr{Q}$ that can adapt to each problem while still being easy to sample from.\n",
        "To do this, we'll draw on normalizing flows, a technique for defining a composition\n",
        "of invertible transformations on top of a simple base distribution like a normal\n",
        "distribution. We'll build our way up to using increasingly complex neural networks\n",
        "to define those transformations, allowing for for truly complex variational\n",
        "families that are problem adaptive, training as we train our variational model.\n",
        "\n",
        "The rough plan for the series is as follows:\n",
        "\n",
        "1.  Introducing the Problem- Why is VI useful, why VI can produce spherical cows\n",
        "2.  How far does iteration on classic VI algorithms like mean-field and full-rank get us?\n",
        "3.  Problem 1: KL-D prefers exclusive solutions; are there alternatives?\n",
        "4.  Problem 2: Not all VI samples are of equal utility; can we weight them cleverly?\n",
        "5.  **(This post)** Problem 3: How can we get deeply flexible variational approximations; are Normalizing Flows the answer?\n",
        "6. Problem 4: How can we know when VI is wrong? Are there useful error bounds?\n",
        "7. Better grounded diagnostics and workflow\n",
        "\n",
        "# A problem adaptive variational family with less tinkering?\n",
        "\n",
        "![](images/flows_stairs_meme.png){fig-alt=\"Something about NNs makes me meme more\"}\n",
        "\n",
        "Jumping from mean-field or full-rank Gaussians and similar distributions\n",
        "to neural networks feels a little... dramatic[^1],  so I want to spend\n",
        "some time justifying why this is a good idea.\n",
        "\n",
        "For VI to work well, we need something that's still simple to sample from, but capable\n",
        "of, in aggregate, representing a posterior that is probably pretty complex. Certainly,\n",
        "some problems are amenable to the simple variational families $\\mathscr{Q}$ we've tried so far,\n",
        "but it's worth re-emphasizing that we're probably trying to represent something complex,\n",
        "and even moderate success at that using a composition of normals should be\n",
        "a little surprising, not the expected outcome.\n",
        "\n",
        "If we need $\\mathscr{Q}$ to be more complex, aren't there choices between what\n",
        "we've seen and a neural network? There's a whole literature of them- from using\n",
        "mixture distributions as variational distributions to inducing some additional\n",
        "structure into a mean-field type solution if you have some specific knowledge\n",
        "about your target posterior you can use. By and large though, this type of\n",
        "class of solutions has been surpassed by normalizing flows in much of modern\n",
        "use for more complex posteriors.\n",
        "\n",
        "Why? A first reason is described in the paper that started the normalizing flows\n",
        "for VI literature, Rezende and Mohamed's [**Variational Inference with Normalizing Flows\n",
        "**](https://arxiv.org/pdf/1505.05770.pdf): making our base variational distribution\n",
        "more complex adds a variety of different computational costs, which add up quickly.\n",
        "This isn't the most face-valid argument when I'm claiming a neural network\n",
        "is a good alternative, but it gets more plausible when you think through\n",
        "how poorly it'd scale to keep making your mixture distribution more and more\n",
        "complex as your posteriors get harder to handle. So this is a *scalability*\n",
        "argument- it might sound extreme to bring in a neural net, but as problems\n",
        "get bigger, scaling matters.\n",
        "\n",
        "The other point I'd raise is that all these other tools aren't very black box at\n",
        "all- if we can make things work with a problem-adapted version of mean-field with\n",
        "some structure based on the knowledge of a specific problem we have, that sounds\n",
        "like it gets time consuming fast. If I'm going to have\n",
        "to find a particular, problem-specific solution each time I want to use variational\n",
        "inference, that feels fragile and fiddly.\n",
        "\n",
        "The novel idea with normalizing flows is that we'll start with a simple base\n",
        "density like a normal distribution that is easy to sample from, but instead of only optimizing the parameters\n",
        "of that normal distribution, we'll also use the training on our ELBO or\n",
        "other objective to learn a transformation that reshapes that normal distribution to\n",
        "look like our posterior. By having that transforming component be partially\n",
        "composed of a neural network,\n",
        "we give ourselves access to an incredibly expressive, problem adaptive,\n",
        "and heavily scalable variant of variational inference that is quite\n",
        "widely used.\n",
        "\n",
        "And if the approximation isn't expressive enough? Deep learning researchers have\n",
        "an unfussy, general purpose innovation for that: MORE LAYERS![^2]\n",
        "\n",
        "![](images/more_layers.png){fig-alt=\"Wow such estimator, very deep\"}\n",
        "\n",
        "# What is a normalizing flow?\n",
        "\n",
        "A normalizing flow transforms a simple base density into a complex one through\n",
        "a sequence of invertible transformations. By stacking more and more of these\n",
        "invertible transformations (having the density \"flow\" through them), we can create\n",
        "arbitrarily complex distributions that remain valid probability distributions. Since\n",
        "it isn't universal in the flows literature, let me be explicit that I'll consider\n",
        "\"forward\" to be the direction flowing from the base density to the posterior, and\n",
        "the \"backward\" or \"normalizing\" direction as towards the base density.\n",
        "\n",
        "![Image Credit to [Simon Boehm](https://siboehm.com/articles/19/normalizing-flow-network) here](images/normalizing-flow.png)\n",
        "\n",
        "If we have a random variable $x$, with distribution $q(x)$, some function $f$ with an inverse\n",
        "$f^{-1} = g, g \\circ f(x) = x$, then the distribution of the result of\n",
        "one iteration of x through, $q^\\prime(x)$ is:\n",
        "\n",
        "$$\n",
        "q\\prime(x) = q(x) \\lvert det \\frac{\\partial f^{-1}}{\\partial x^\\prime} \\rvert = q(x) \\lvert \\frac{\\partial f}{\\partial x} \\rvert^{-1}\n",
        "$$\n",
        "I won't derive this identity[^3], but it follows from the chain rule and the\n",
        " properties of Jacobians of invertible functions.\n",
        "\n",
        "The real power comes in here when we see that these transformations stack. If\n",
        "we've got a chain of transformations (e.g. $f_K(...(f_2(f_1(x))))$:\n",
        "\n",
        "$$\n",
        "x_K = f(x) \\circ ... \\circ f_2 \\circ f_1(x_0)\n",
        "$$\n",
        "\n",
        "then the resulting density $q_K(x)$ looks like:\n",
        "\n",
        "$$\n",
        "ln q_K (x_K) = lnq_0(x_0) - \\sum \\limits_{K = 1}\\limits^{K} ln  \\lvert \\frac{\\partial f_k}{\\partial x_{k-1}} \\rvert^{-1}\n",
        "$$\n",
        "\n",
        "Neat, and surprisingly simple! If the terms above are all easy to calculate,\n",
        "we can very efficiently stack a bunch of these transformations and make\n",
        "an expressive model.\n",
        "\n",
        "## Normalizing flows for variational inference versus other applications\n",
        "\n",
        "One source of confusion when I was learning about normalizing flows for\n",
        "variational inference was that variational inference makes up a fairly\n",
        "small proportion of use cases, and thus the academic\n",
        "literature and online discussion. More common applications include density estimation, image generation,\n",
        "representation learning, and reinforcement learning. In addition to making specifically applicable\n",
        "discussions harder to find, often resources will make strong claims about properties of a given\n",
        "flow structure, that really only holding in some subset of the above applications[^4].\n",
        "\n",
        "By taking a second to explain this crisply and compare different application's needs,\n",
        "hopefully I can save you some confusion and make engaging with the broader literature easier.\n",
        "\n",
        "To start, consider the relevant operations we've introduced so far:\n",
        "\n",
        "1. computing $f$, that is pushing a sample through the transformations\n",
        "2. computing $g$, $f$'s inverse which undoes the manipulations\n",
        "3. computing the (log) determinant of the Jacobian\n",
        " \n",
        "1 and 3 definitely need to be efficient for our use case, since we need to be\n",
        "able to sample and push through using the formula above efficiently to calculate\n",
        "an ELBO and train our model. 2 is where things get\n",
        "more subtle: we definitely need $f$ to be invertible, since our formulas above\n",
        "are dependent on a property of Jacobians of invertible functions. But we don't\n",
        "actually really need to explicitly compute $g$ for variational inference. Even knowing the inverse\n",
        "exists but not having a formula might be fine for us!\n",
        "\n",
        "Contrast\n",
        "this with density estimation, where the goal would not to sample from the distribution,\n",
        "but instead to estimate the density. In this case, most of the time would be\n",
        "spent going in the opposite direction, so that they can evaluate the log-likliehood\n",
        "of the data, and maximize it to improve the model[^5]. The need for an expressive\n",
        "transformation of densities unite these two cases, but the goal is quite different!\n",
        "\n",
        "This level of goal disagreement also shows it face in what direction papers\n",
        "choose to call forward: Most papers outside of variational inference applications consider forward to be the opposite of what I do here. For them, \"forward\" is the direction towards\n",
        "the base density, the normalizing direction. \n",
        "\n",
        "For our use, hopefully this short digression has clarified which operations we need to be\n",
        "fast versus just exist. If you dive deeper into\n",
        "further work on normalizing flows, hopefully recognizing there are two\n",
        "different ways to consider forward helps you more quickly orient yourself\n",
        "to how other work describes flows.\n",
        "\n",
        "# How to train your neural net\n",
        "\n",
        "Now, let's turn to how we actually fit a normalizing flow. Since this would be a bit\n",
        "hard to grok a code presentation if I took advantage of the full flexibility and abstraction that\n",
        "something like [`vistan`](https://github.com/abhiagwl/vistan/tree/master) provides, before\n",
        "heading into general purpose tools I'll talk through a bit more explicit implementation\n",
        "of a simpler flow called a planar flow in `PyTorch` for illustration. Rather than\n",
        "reinventing the wheel, I'll leverage Edvard Hulten's great implementation [here](https://github.com/e-hulten/planar-flows).\n",
        "\n",
        "In this section,\n",
        "I'll define conceptually how we're fitting the model, and build out a fun\n",
        "target distribution and loss- since I expect many people reading\n",
        "this may moderately new to PyTorch, I'll explain in detail\n",
        "than normal what each operation is doing and why we need it.\n"
      ],
      "id": "e2ebaebf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Uniform\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "from typing import Tuple\n",
        "from typing import Callable\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.set_default_device('cuda')\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "id": "51c77c3c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's first make a fun target posterior distribution from an image to model. I\n",
        "think it'd be a fun preview gif for the post to watch this be fit from a normal,\n",
        "so let's use this ring shaped density.\n",
        "\n",
        "![](images/ring_true_density.png){fig-alt=\"Wow such estimator, very deep\"}\n",
        "\n",
        "This is a solid starting example in that this'd be quite\n",
        "hard to fit with a mean-field normal variational family, but it's pretty easy to define in PyTorch as well:"
      ],
      "id": "6870222f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# https://github.com/e-hulten/planar-flows/blob/master/target_distribution.py\n",
        "def ring_density(z):\n",
        "                exp1 = torch.exp(-0.5 * ((z[:, 0] - 2) / 0.8) ** 2)\n",
        "                exp2 = torch.exp(-0.5 * ((z[:, 0] + 2) / 0.8) ** 2)\n",
        "                u = 0.5 * ((torch.norm(z, 2, dim=1) - 4) / 0.4) ** 2\n",
        "                u = u - torch.log(exp1 + exp2 + 1e-6)\n",
        "                return u"
      ],
      "id": "d3a284b0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's define our loss for training, which will just be a slight\n",
        "reformulation of our ELBO:\n",
        "\n",
        "$$\n",
        " \\mathbb{E}[logp(z,x)] - \\mathbb{E}[logq(z)]\n",
        "$$\n",
        "\n",
        "To do this, we'll define a class for the loss.\n",
        "\n",
        "First, we pick a simple base distribution to push through our flow, here a \n",
        "2-D Normal distribution called `base_distr`. We'll also include the interesting\n",
        "target we just made above, `distr`.\n",
        "\n",
        "Next, the forward pass structure. The `forward` method is the is the core of the computational graph structure in PyTorch. It defines operations that are applied to the input tensors to compute the output, and \n",
        "gives PyTorch the needed information for automatic differentiation, which allows smooth calculation\n",
        "and backpropagation of loss through the model to train it. This `VariationalLoss`\n",
        "module will run at the end of the forward pass to calculate the loss and allow us\n",
        "to pass it back through the graph for training.\n",
        "\n",
        "Keeping with the structure above of numbering successive stages of the flow,\n",
        "`z0` here is our base distribution, and `z` will be the learned approximation\n",
        "to the target. In addition to the terms you'd expect in the ELBO, we're also\n",
        "tracking and making use of the sum of the log determinant of the Jacobians to\n",
        "a handle on the distortion of the base density the flows apply."
      ],
      "id": "71b0e7a2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: False\n",
        "\n",
        "class TargetDistribution:\n",
        "    def __init__(self, name: str):\n",
        "        \"\"\"Define target distribution. \n",
        "\n",
        "        Args:\n",
        "            name: The name of the target density to use. \n",
        "                  Valid choices: [\"U_1\", \"U_2\", \"U_3\", \"U_4\", \"ring\"].\n",
        "        \"\"\"\n",
        "        self.func = self.get_target_distribution(name)\n",
        "\n",
        "    def __call__(self, z: Tensor) -> Tensor:\n",
        "        return self.func(z)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_target_distribution(name: str) -> Callable[[Tensor], Tensor]:\n",
        "        w1 = lambda z: torch.sin(2 * np.pi * z[:, 0] / 4)\n",
        "        w2 = lambda z: 3 * torch.exp(-0.5 * ((z[:, 0] - 1) / 0.6) ** 2)\n",
        "        w3 = lambda z: 3 * torch.sigmoid((z[:, 0] - 1) / 0.3)\n",
        "\n",
        "        if name == \"U_1\":\n",
        "\n",
        "            def U_1(z):\n",
        "                u = 0.5 * ((torch.norm(z, 2, dim=1) - 2) / 0.4) ** 2\n",
        "                u = u - torch.log(\n",
        "                    torch.exp(-0.5 * ((z[:, 0] - 2) / 0.6) ** 2)\n",
        "                    + torch.exp(-0.5 * ((z[:, 0] + 2) / 0.6) ** 2)\n",
        "                )\n",
        "                return u\n",
        "\n",
        "            return U_1\n",
        "        elif name == \"U_2\":\n",
        "\n",
        "            def U_2(z):\n",
        "                u = 0.5 * ((z[:, 1] - w1(z)) / 0.4) ** 2\n",
        "                return u\n",
        "\n",
        "            return U_2\n",
        "        elif name == \"U_3\":\n",
        "\n",
        "            def U_3(z):\n",
        "                u = -torch.log(\n",
        "                    torch.exp(-0.5 * ((z[:, 1] - w1(z)) / 0.35) ** 2)\n",
        "                    + torch.exp(-0.5 * ((z[:, 1] - w1(z) + w2(z)) / 0.35) ** 2)\n",
        "                    + 1e-6\n",
        "                )\n",
        "                return u\n",
        "\n",
        "            return U_3\n",
        "        elif name == \"U_4\":\n",
        "\n",
        "            def U_4(z):\n",
        "                u = -torch.log(\n",
        "                    torch.exp(-0.5 * ((z[:, 1] - w1(z)) / 0.4) ** 2)\n",
        "                    + torch.exp(-0.5 * ((z[:, 1] - w1(z) + w3(z)) / 0.35) ** 2)\n",
        "                    + 1e-6\n",
        "                )\n",
        "                return u\n",
        "\n",
        "            return U_4\n",
        "        elif name == \"ring\":\n",
        "\n",
        "            def ring_density(z):\n",
        "                exp1 = torch.exp(-0.5 * ((z[:, 0] - 2) / 0.8) ** 2)\n",
        "                exp2 = torch.exp(-0.5 * ((z[:, 0] + 2) / 0.8) ** 2)\n",
        "                u = 0.5 * ((torch.norm(z, 2, dim=1) - 4) / 0.4) ** 2\n",
        "                u = u - torch.log(exp1 + exp2 + 1e-6)\n",
        "                return u\n",
        "\n",
        "            return ring_density\n",
        "          \n",
        "        elif name == \"hi\":\n",
        "\n",
        "          def hi_density(z):\n",
        "              return interpolate_tensor(torch_posterior,z)\n",
        "          \n",
        "          return hi_density\n",
        "        \n",
        "        elif name == \"moons\":\n",
        "          \n",
        "          def two_moons_density(z):\n",
        "            x = z[:, 0]\n",
        "            y = z[:, 1]\n",
        "            d = torch.sqrt(x**2 + y**2)\n",
        "            density = torch.exp(-0.2 * d) * torch.cos(4 * np.pi * d)\n",
        "            return density\n",
        "          \n",
        "          return two_moons_density"
      ],
      "id": "b28c825e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# https://github.com/e-hulten/planar-flows/blob/master/loss.py\n",
        "class VariationalLoss(nn.Module):\n",
        "  def __init__(self,distribution):\n",
        "      super().__init__()\n",
        "      self.distr = distribution\n",
        "      self.base_distr = MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
        "\n",
        "  def forward(self, z0: Tensor, z: Tensor, sum_log_det_J: float) -> float:\n",
        "      base_log_prob = self.base_distr.log_prob(z0)\n",
        "      target_density_log_prob = -self.distr(z)\n",
        "      return (base_log_prob - target_density_log_prob - sum_log_det_J).mean()"
      ],
      "id": "7045c791",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A basic flow\n",
        "\n",
        "Next, let's define the structure of the actual flow. To do this, we'll first\n",
        "describe a single layer of the flow, then we'll show structure to stack\n",
        "the flow in layers.\n",
        "\n",
        "Our first flow we look at will be the **planar flow** from the original\n",
        "Normalizing Flows for Variational Inference paper mentioned above. The name\n",
        "comes from how the function defines a (hyper)plane, and compresses or expand\n",
        "the density around it:\n",
        "\n",
        "$$\n",
        "f(x) = x + u*tanh(w^Tx + b), w, u \\in \t\\mathbb{R}^d, b \\in\t\\mathbb{R} \n",
        "$$\n",
        "\n",
        "$w$ and $b$ define the hyperplane and u specifies the direction and strength\n",
        "of the expansion. I'll show a visualization of just one layer of that below.\n",
        "\n",
        "If you're more used to working with neural nets, you might wonder why we\n",
        "choose the non-linearity $tanh$ here, which generally isn't as popular as something\n",
        "like $ReLU$ or its variants in more recent years due to it's more unstable\n",
        "gradient flows. As the authors show in appendix $A.1$, functions like the\n",
        "above aren't actually always invertible, and choosing $tanh$ allows them\n",
        "to impose some constraints that make things reliably invertible. See the appendix\n",
        "for more details about how that works, or take a careful look at Edvard's\n",
        "implementation of the single function below. Realize this sort of constitutes\n",
        "a workaround; ideally we wouldn't have to force constraints like this for\n",
        "our flow to work.\n",
        "\n",
        "There isn't that much that's new conceptually in this PyTorch code; we're\n",
        "defining the layer as a stackable module, which provides torch what it\n",
        "needs to calculate both the forward and backward pass of an arbitrary\n",
        "number of layers."
      ],
      "id": "d4278d35"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# From https://github.com/e-hulten/planar-flows/blob/master/planar_transform.py\n",
        "\n",
        "class PlanarTransform(nn.Module):\n",
        "  \"\"\"Implementation of the invertible transformation used in planar flow:\n",
        "      f(z) = z + u * h(dot(w.T, z) + b)\n",
        "  See Section 4.1 in https://arxiv.org/pdf/1505.05770.pdf. \n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, dim: int = 2):\n",
        "      \"\"\"Initialise weights and bias.\n",
        "      \n",
        "      Args:\n",
        "          dim: Dimensionality of the distribution to be estimated.\n",
        "      \"\"\"\n",
        "      super().__init__()\n",
        "      self.w = nn.Parameter(torch.randn(1, dim).normal_(0, 0.1))\n",
        "      self.b = nn.Parameter(torch.randn(1).normal_(0, 0.1))\n",
        "      self.u = nn.Parameter(torch.randn(1, dim).normal_(0, 0.1))\n",
        "\n",
        "  def forward(self, z: Tensor) -> Tensor:\n",
        "      if torch.mm(self.u, self.w.T) < -1:\n",
        "          self.get_u_hat()\n",
        "\n",
        "      return z + self.u * nn.Tanh()(torch.mm(z, self.w.T) + self.b)\n",
        "\n",
        "  def log_det_J(self, z: Tensor) -> Tensor:\n",
        "      if torch.mm(self.u, self.w.T) < -1:\n",
        "          self.get_u_hat()\n",
        "      a = torch.mm(z, self.w.T) + self.b\n",
        "      psi = (1 - nn.Tanh()(a) ** 2) * self.w\n",
        "      abs_det = (1 + torch.mm(self.u, psi.T)).abs()\n",
        "      log_det = torch.log(1e-4 + abs_det)\n",
        "\n",
        "      return log_det\n",
        "\n",
        "  def get_u_hat(self) -> None:\n",
        "      \"\"\"Enforce w^T u >= -1. When using h(.) = tanh(.), this is a sufficient condition \n",
        "      for invertibility of the transformation f(z). See Appendix A.1.\n",
        "      \"\"\"\n",
        "      wtu = torch.mm(self.u, self.w.T)\n",
        "      m_wtu = -1 + torch.log(1 + torch.exp(wtu))\n",
        "      self.u.data = (\n",
        "          self.u + (m_wtu - wtu) * self.w / torch.norm(self.w, p=2, dim=1) ** 2\n",
        "      )"
      ],
      "id": "dba18419",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Where things will start to get exciting is multiple layers of the flow; here's\n",
        "how we can make an abstraction that allows us to stack up $K$ layers\n",
        "of the flow to control the flexibility of our approximation."
      ],
      "id": "fbaff434"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class PlanarFlow(nn.Module):\n",
        "    def __init__(self, dim: int = 2, K: int = 6):\n",
        "        \"\"\"Make a planar flow by stacking planar transformations in sequence.\n",
        "\n",
        "        Args:\n",
        "            dim: Dimensionality of the distribution to be estimated.\n",
        "            K: Number of transformations in the flow. \n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.layers = [PlanarTransform(dim) for _ in range(K)]\n",
        "        self.model = nn.Sequential(*self.layers)\n",
        "\n",
        "    def forward(self, z: Tensor) -> Tuple[Tensor, float]:\n",
        "        log_det_J = 0\n",
        "\n",
        "        for layer in self.layers:\n",
        "            log_det_J += layer.log_det_J(z)\n",
        "            z = layer(z)\n",
        "\n",
        "        return z, log_det_J"
      ],
      "id": "4ed63e80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run this for a single layer to introduce the training loop, and build some\n",
        "intuition on the planar flow. \n",
        "\n",
        "The hyperparameter names here should be fairly intuitive, but it's worth pointing\n",
        "out that the batch size, learning rate (`lr`), and choice of Adam as an optimizer are\n",
        "all pretty basic reasonable first tries, but something you'd want to consider tuning\n",
        "in a more complicated context- we inherit that level of fiddly-ness when we choose\n",
        "to approach VI using normalizing flows. Also, note that I'm hiding setting up the plot code\n",
        "since it doesn't add anything to the intuition here."
      ],
      "id": "40559cfa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: False\n",
        "\n",
        "# https://github.com/e-hulten/planar-flows/blob/master/utils/plot.py\n",
        "def plot_density(density, xlim=4, ylim=4, ax=None, cmap=\"Blues\"):\n",
        "    x = y = np.linspace(-xlim, xlim, 300)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    shape = X.shape\n",
        "    X_flatten, Y_flatten = np.reshape(X, (-1, 1)), np.reshape(Y, (-1, 1))\n",
        "    Z = torch.from_numpy(np.concatenate([X_flatten, Y_flatten], 1))\n",
        "    U = torch.exp(-density(Z))\n",
        "    U = U.reshape(shape)\n",
        "    if ax is None:\n",
        "        fig = plt.figure(figsize=(7, 7))\n",
        "        ax = fig.add_subplot(111)\n",
        "\n",
        "    ax.set_xlim(-xlim, xlim)\n",
        "    ax.set_ylim(-xlim, xlim)\n",
        "    ax.set_aspect(1)\n",
        "\n",
        "    ax.pcolormesh(X, Y, U, cmap=cmap, rasterized=True)\n",
        "    ax.tick_params(\n",
        "        axis=\"both\",\n",
        "        left=False,\n",
        "        top=False,\n",
        "        right=False,\n",
        "        bottom=False,\n",
        "        labelleft=False,\n",
        "        labeltop=False,\n",
        "        labelright=False,\n",
        "        labelbottom=False,\n",
        "    )\n",
        "    return ax\n",
        "\n",
        "\n",
        "def plot_samples(z):\n",
        "    nbins = 250\n",
        "    lim = 4\n",
        "    # z = np.exp(-z)\n",
        "    k = gaussian_kde([z[:, 0], z[:, 1]])\n",
        "    xi, yi = np.mgrid[-lim : lim : nbins * 1j, -lim : lim : nbins * 1j]\n",
        "    zi = k(np.vstack([xi.flatten(), yi.flatten()]))\n",
        "\n",
        "    fig = plt.figure(figsize=[7, 7])\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.set_xlim(-5, 5)\n",
        "    ax.set_aspect(1)\n",
        "    plt.pcolormesh(xi, yi, zi.reshape(xi.shape), cmap=\"Purples\", rasterized=True)\n",
        "    return ax\n",
        "\n",
        "\n",
        "def plot_transformation(model, n=500, xlim=4, ylim=4, ax=None, cmap=\"Purples\"):\n",
        "    base_distr = torch.distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
        "    x = torch.linspace(-xlim, xlim, n)\n",
        "    xx, yy = torch.meshgrid(x, x)\n",
        "    zz = torch.stack((xx.flatten(), yy.flatten()), dim=-1).squeeze()\n",
        "\n",
        "    zk, sum_log_jacobians = model(zz)\n",
        "    \n",
        "\n",
        "    base_log_prob = base_distr.log_prob(zz)\n",
        "    final_log_prob = base_log_prob - sum_log_jacobians\n",
        "    qk = torch.exp(final_log_prob)\n",
        "    \n",
        "    if ax is None:\n",
        "        fig = plt.figure(figsize=[7, 7])\n",
        "        ax = fig.add_subplot(111)\n",
        "    ax.set_xlim(-xlim, xlim)\n",
        "    ax.set_ylim(-ylim, ylim)\n",
        "    ax.set_aspect(1)\n",
        "    \n",
        "    ax.pcolormesh(\n",
        "        zk[:, 0].detach().cpu().data.reshape(n, n),\n",
        "        zk[:, 1].detach().cpu().data.reshape(n, n),\n",
        "        qk.detach().cpu().data.reshape(n, n),\n",
        "        cmap=cmap,\n",
        "        rasterized=True,\n",
        "    )\n",
        "    \n",
        "\n",
        "    plt.tick_params(\n",
        "        axis=\"both\",\n",
        "        left=False,\n",
        "        top=False,\n",
        "        right=False,\n",
        "        bottom=False,\n",
        "        labelleft=False,\n",
        "        labeltop=False,\n",
        "        labelright=False,\n",
        "        labelbottom=False,\n",
        "    )\n",
        "    if cmap == \"Purples\":\n",
        "        ax.set_facecolor(plt.cm.Purples(0.0))\n",
        "    elif cmap == \"Reds\":\n",
        "        ax.set_facecolor(plt.cm.Reds(0.0))\n",
        "\n",
        "    return ax\n",
        "\n",
        "\n",
        "def plot_training(model, flow_length, batch_num, lr, axlim):\n",
        "    ax = plot_transformation(model, xlim=axlim, ylim=axlim)\n",
        "    ax.text(\n",
        "        0,\n",
        "        axlim - 2,\n",
        "        \"Flow length: {}\\nDensity of one batch, iteration #{:06d}\\nLearning rate: {}\".format(\n",
        "            flow_length, batch_num, lr\n",
        "        ),\n",
        "        horizontalalignment=\"center\",\n",
        "    )\n",
        "    plt.savefig(\n",
        "        f\"training_plots/iteration_{batch_num:06d}.png\",\n",
        "        bbox_inches=\"tight\",\n",
        "        pad_inches=0.5,\n",
        "    )\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_comparison(model, target_distr, flow_length, dpi=400):\n",
        "    xlim = ylim = 7 if target_distr == \"ring\" else 5\n",
        "    fig, axes = plt.subplots(\n",
        "        ncols=2, nrows=1, sharex=True, sharey=True, figsize=[10, 5], dpi=dpi\n",
        "    )\n",
        "    axes[0].tick_params(\n",
        "        axis=\"both\",\n",
        "        left=False,\n",
        "        top=False,\n",
        "        right=False,\n",
        "        bottom=False,\n",
        "        labelleft=False,\n",
        "        labeltop=False,\n",
        "        labelright=False,\n",
        "        labelbottom=False,\n",
        "    )\n",
        "    # Plot true density.\n",
        "    density = TargetDistribution(target_distr)\n",
        "    plot_density(density, xlim=xlim, ylim=ylim, ax=axes[0])\n",
        "    axes[0].text(\n",
        "        0,\n",
        "        ylim - 1,\n",
        "        \"True density $\\exp(-{})$\".format(target_distr),\n",
        "        size=14,\n",
        "        horizontalalignment=\"center\",\n",
        "    )\n",
        "\n",
        "    # Plot estimated density.\n",
        "    batch = torch.zeros(500, 2).normal_(mean=0, std=1)\n",
        "    z = model(batch)[0].detach().numpy()\n",
        "    axes[1] = plot_transformation(model, xlim=xlim, ylim=ylim, ax=axes[1], cmap=\"Reds\")\n",
        "    axes[1].text(\n",
        "        0,\n",
        "        ylim - 1,\n",
        "        \"Estimated density $\\exp(-{})$\".format(target_distr),\n",
        "        size=14,\n",
        "        horizontalalignment=\"center\",\n",
        "    )\n",
        "    fig.savefig(\n",
        "        \"results/\" + target_distr + \"_K\" + str(flow_length) + \"_comparison.pdf\",\n",
        "        bbox_inches=\"tight\",\n",
        "        pad_inches=0.1,\n",
        "    )\n",
        "\n",
        "\n",
        "def plot_available_distributions():\n",
        "    target_distributions = [\"U_1\", \"U_2\", \"U_3\", \"U_4\", \"ring\"]\n",
        "    cmaps = [\"Reds\", \"Purples\", \"Oranges\", \"Greens\", \"Blues\"]\n",
        "    fig, axes = plt.subplots(1, len(target_distributions), figsize=(20, 5))\n",
        "    for i, distr in enumerate(target_distributions):\n",
        "        axlim = 7 if distr == \"ring\" else 5\n",
        "        density = TargetDistribution(distr)\n",
        "        plot_density(density, xlim=axlim, ylim=axlim, ax=axes[i], cmap=cmaps[i])\n",
        "        axes[i].set_title(f\"Name: '{distr}'\", size=16)\n",
        "        plt.setp(axes, xticks=[], yticks=[])\n",
        "    plt.show()"
      ],
      "id": "ef515b1c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: False\n",
        "#From https://github.com/e-hulten/planar-flows/blob/master/train.py\n",
        "target_distr = \"ring\"  # U_1, U_2, U_3, U_4, ring\n",
        "flow_length = 1\n",
        "dim = 2\n",
        "num_batches = 20000\n",
        "batch_size = 128\n",
        "lr = 6e-4\n",
        "axlim = xlim = ylim = 7  # 5 for U_1 to U_4, 7 for ring\n",
        "# ------------------------------------\n",
        "\n",
        "density = TargetDistribution(target_distr)\n",
        "model = PlanarFlow(dim, K=flow_length)\n",
        "bound = VariationalLoss(density)\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Train model.\n",
        "for batch_num in range(1, num_batches + 1):\n",
        "    # Get batch from N(0,I).\n",
        "    batch = torch.zeros(size=(batch_size, 2)).normal_(mean=0, std=1)\n",
        "    # Pass batch through flow.\n",
        "    zk, log_jacobians = model(batch)\n",
        "    \n",
        "    # Compute loss under target distribution.\n",
        "    loss = bound(batch, zk, log_jacobians)\n",
        "\n",
        "    optimiser.zero_grad()\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    if batch_num % 100 == 0:\n",
        "        print(f\"(batch_num {batch_num:05d}/{num_batches}) loss: {loss}\")\n",
        "        \n",
        "\n",
        "    if batch_num == 1 or batch_num % 100 == 0:\n",
        "        # Save plots during training. Plots are saved to the 'train_plots' folder.\n",
        "        plot_training(model, flow_length, batch_num, lr, axlim)"
      ],
      "id": "fa0fe3a5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At each iteration, we pass the normal draws thorugh the flow, calculate the loss,\n",
        "and propagate that loss backward through the flow to train it using gradient\n",
        "descent.\n",
        "\n",
        "Here's a gif[^6] of what that looks like over the course of training. With just a single\n",
        "layer of planar flow of course, this isn't expressive enough to capture the full density, but we can see why\n",
        "this approach has some promise- it's learning to cover the target density, rather\n",
        "than us having to get creative in specifying a base density that does this.\n",
        "\n",
        "![](images/1_layer.gif){fig-alt=\"Maximally engineered two biomodal\"}\n",
        "\n",
        "Let's try a more serious attempt, with a depth of 32:"
      ],
      "id": "3456581c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: False\n",
        "#From https://github.com/e-hulten/planar-flows/blob/master/train.py\n",
        "target_distr = \"ring\"  # U_1, U_2, U_3, U_4, ring\n",
        "flow_length = 32\n",
        "dim = 2\n",
        "num_batches = 20000\n",
        "batch_size = 128\n",
        "lr = 6e-4\n",
        "axlim = xlim = ylim = 7  # 5 for U_1 to U_4, 7 for ring\n",
        "# ------------------------------------\n",
        "\n",
        "density = TargetDistribution(target_distr)\n",
        "model = PlanarFlow(dim, K=flow_length)\n",
        "bound = VariationalLoss(density)\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Train model.\n",
        "for batch_num in range(1, num_batches + 1):\n",
        "    # Get batch from N(0,I).\n",
        "    batch = torch.zeros(size=(batch_size, 2)).normal_(mean=0, std=1)\n",
        "    # Pass batch through flow.\n",
        "    zk, log_jacobians = model(batch)\n",
        "    \n",
        "    # Compute loss under target distribution.\n",
        "    loss = bound(batch, zk, log_jacobians)\n",
        "\n",
        "    optimiser.zero_grad()\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    if batch_num % 100 == 0:\n",
        "        print(f\"(batch_num {batch_num:05d}/{num_batches}) loss: {loss}\")\n",
        "        #print(log_jacobians)\n",
        "\n",
        "    if batch_num == 1 or batch_num % 100 == 0:\n",
        "        # Save plots during training. Plots are saved to the 'train_plots' folder.\n",
        "        plot_training(model, flow_length, batch_num, lr, axlim)"
      ],
      "id": "0ede6650",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](images/32_layer.gif){fig-alt=\"An actually useful normalizing flow\"}\n",
        "\n",
        "Now we've got it! With this planar flow, we've transformed our base normal\n",
        "into a pretty complex (for 2-D) distribution, cool!\n",
        "\n",
        "This took about 20 minutes to train, so this is adding some considerable time\n",
        "to our VI workflow, but on the other hand, we're not spending the human time\n",
        "needed to figure out what weird base density could be fitted to look like this, which\n",
        "is a win. Also worth pointing out here is that we started with a simple example\n",
        "for illustration purposes, so if we did have something much more complicated\n",
        "or high dimensional to fit, we'd start to see the scalability of normalizing\n",
        "flows start to shine more.\n",
        "\n",
        "Planar flows are a great learning tool, but in reality they aren't a great\n",
        "choice once we get outside relatively low-dimensional examples. See [Kong and Chadhuri, (2020)](https://arxiv.org/abs/2006.00392) if you want more mathematical rigor, but intuitively,\n",
        "expansion or compression around a hyperplane doesn't scale to high dimensions well\n",
        "given the operation is pretty simple. We can get around this partially by\n",
        "making the flow deeper, but that introduces its own problems. Very deep planar\n",
        "flows can struggle to fit given the somewhat artificial constraints imposed in computing the log determinant of the Jacobian (implemented in `get_u_hat` above) to ensure invertability. Finally,\n",
        "there are flows developed since the original normalizing flows paper\n",
        "that both are more expressive and have cheaper to compute transformations\n",
        "and log determinants of the Jacobian- let's turn to those now.\n",
        "\n",
        "# What more complicated Flows look like\n",
        "\n",
        "More complex flows are an active area of research, and I won't attempt to talk\n",
        "through the whole zoo- I'd recommend either Lilian Weng's [blog post](https://lilianweng.github.io/posts/2018-10-13-flow-models/), or [Kobyzev et\n",
        "al. (2020)](https://arxiv.org/abs/1908.09257) as good starting points for seeing\n",
        "the full range of available flows.\n",
        "\n",
        "Instead, I'll introduce just a single more complex flow, RealNVP, introduced in\n",
        "[Dinh et al. (2017)](https://arxiv.org/abs/1605.08803). This is a good example\n",
        "both because the flow is shown to be robustly good for high dimensional variational inference tasks\n",
        "in review papers like [Dhaka et al. (2021)](https://arxiv.org/abs/2103.01085) and [Agrawal et al. (2020)](https://arxiv.org/abs/2006.10343), and because it illustrates some generalizable\n",
        "ideas about flow design.\n",
        "\n",
        "Dinh et al. start the RealNVP paper by noting some goals: they want a Jacobian\n",
        "that is *triangular*, because this makes computing the determinant incredibly cheap (it's just\n",
        "the product of the diagonal terms). Second, they want a transformation that's\n",
        "simple to invert, but complex via inducing interdependencies between different\n",
        "parts of the data.\n",
        "\n",
        "To do both of these at once, the key insight the authors come to is the idea\n",
        "of a *coupling layer*, where if the layer is $D$-dimensional, the first half of the dimensions $1:d$\n",
        "remain unchanged, and $d+1:D$ are transformed as complex function of the first half:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "y_{1:d} &= x_{1:d}\\\\\n",
        "y_{d+1:D} &= x_{1:d} \\odot exp(s(x_{1:d})) + t(x_{1:d})\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Where s and t are scaling and transformation functions from $\\mathbb{R}^d \\rightarrow \\mathbb{R}^{D-d}$,\n",
        "and $\\odot$ is the Hadamard (element-wise) product. Visually, at each layer:\n",
        "\n",
        "![Figure credit due to [Eric Jang](https://blog.evjang.com/2018/01/nf2.html); he uses the notation $\\alpha$ and $\\mu$ instead of $s$ and $t$](images/real_nvp_illustration.png){fig-alt=\"transforming half of the dimensions as a function of the other half\"}\n",
        "\n",
        "This has a lot of really appealing properties. First, this has a triangular Jacobian:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x^T}=\\left[\\begin{array}{cc}\\mathbb{I}_d & 0 \\\\ \\frac{\\partial y_{d+1: D}}{\\partial x_{1: d}^T} & \\operatorname{diag}\\left(\\exp \\left[s\\left(x_{1: d}\\right)\\right]\\right)\\end{array}\\right]\n",
        "$$\n",
        "\n",
        "which means that we can really efficiently compute the determinant as $\\exp \\left[\\sum_j s\\left(x_{1: d}\\right)_j\\right]$. For a sense of scale, with no specific structure to exploit,\n",
        "calculating the determinant is roughly $\\mathcal{O}(n^3)$ or a little better[^7],\n",
        "but for triangular matrices the same operation takes just $\\mathcal{O}(n)$; that's\n",
        "a massive speedup!\n",
        "\n",
        "Another nice characteristic here is that we don't need to compute the Jacobian\n",
        "of $s$ or $t$ in computing the determinant of the above Jacobian, so $s$ and $t$\n",
        "are much easier to make quite complex. Contrast this with the planar flow, where\n",
        "we needed to use a specific (tanh) non-linearity, and impose somewhat arbitrary\n",
        "constraints to ensure invertability at all, let alone easy, fast invertability. With\n",
        "a realNVP flow constructed out of many such coupling layers, it's easy to throw\n",
        "in a lot of improvements that make training large neural networks much more reliable,\n",
        "like batch normalization, weight normalization, and architectures like residual\n",
        "connections.\n",
        "\n",
        "As a last appealing property here, realize this can be really expressive: by varying at each\n",
        "layer which dimensions $d$ are held constant and which are transformed, we can\n",
        "build up quite complex interrelationships between different dimensions over the flow. This\n",
        "can be done simply at random, or perhaps even using structure of the problem to\n",
        "decide how to partition the dimensions. For example, Dinh et al. provide an\n",
        "example on image data where a checkerboard pattern is used to structure the partitions. Kingma and Dhariwal take this further with [Glow (2018)](https://arxiv.org/abs/1807.03039), a flow using 1x1 convolutions. Again, it's really nice we don't need the Jacobian of $s$ and $t$; they can have arbitrarily complex structure and we\n",
        "don't need pay the computational cost of computing their Jacobians.\n",
        "\n",
        "It doesn't add that much intuition to see another flow in code, so I'll hold\n",
        "off on showing off the implementation of RealNVP for another post or two when\n",
        "I return to fitting our MRP model better using all the tools we've built up.\n",
        "\n",
        "Like I said at the start of this section, there are tons and tons of possible flow\n",
        "structures that get more computationally complex in exchange for expressiveness. RealNVP\n",
        "is a great start though, and for many variational inference problems provides\n",
        "the amount of expressiveness we need. It also\n",
        "illustrates a lot of the core strategy for building flow structures well:\n",
        "\n",
        "1. Make the log determinant of the Jacobian fast to calculate.\n",
        "2. Impose structure such that calculating the log determinant of the Jacobian\n",
        "isn't entangled with your source of learnable complexity; this allows expressiveness\n",
        "not fitting restrictions to guide what's implemented.\n",
        "3. Leverage tools for scalable, stable neural networks, from batch norm to\n",
        "architecture choices like residual connections to GPU computing speed ups.\n",
        "\n",
        "# Conclusion \n",
        "\n",
        "Let's take stock of how normalizing flows continue our project of extending vanilla variational inference.\n",
        "Normalizing flows allow us to learn the variational family rather than iterating\n",
        "through a bunch of base densities by hand until one works, and can do so for much\n",
        "more complex posteriors than any of the simple choices like a mean-field or\n",
        "full-rank gaussian we've seen so far. This is both a gain of functionality (we\n",
        "can now fit posteriors with VI that we absolutely couldn't before),\n",
        "and a gain of convenience (the workflow for \"make my neural network expressive\" is much, much\n",
        "more convenient than the one where the analyst tries to find or make increasingly weird distributions themselves).\n",
        "\n",
        "Of course, this adds compute time, and a requirement to start understanding neural network\n",
        "implementation choices reasonably well. This isn't a free lunch- even the simple planar flow\n",
        "on a toy example above took about 20 minutes to fit on my laptop, and having to\n",
        "understand neural nets well to fit a Bayesian model feels kind of silly. Still though,\n",
        "in the telling of review papers like [Dhaka et al. (2021)](https://arxiv.org/abs/2103.01085) and [Agrawal et al. (2020)](https://arxiv.org/abs/2006.10343), a basic RealNVP flow is a serious\n",
        "improvement for many complex posterior distributions at fairly palatable run times.\n",
        "This is a pretty good tradeoff for many realistic models, and it's for that reason that normalizing\n",
        "flows are an increasingly popular part of the variational inference toolbox.\n",
        "\n",
        "Like with alternative optimization objectives or the various uses of (Pareto smoothed)\n",
        "importance sampling from the last post, normalizing flows give us tools to\n",
        "fit a wider range of models with variational inference, and do so more robustly\n",
        "and conveniently. This can come with it's own problems, but these trades are\n",
        "often worth it. In the next post, we'll add a final set of tools to our VI\n",
        "toolbox: robust diagnostics to know if our approximation is good or not.\n",
        "\n",
        "Thanks for reading. The code for this post can be found [here](https://github.com/andytimm/andytimm.github.io/blob/main/posts/Variational%20MRP%20Pt5/variational_mrp_pt5.qmd).\n",
        "\n",
        "[^1]: It also almost has a bit of \"no brain no pain\" ML guy energy, in the sense\n",
        "that we're really pulling out the biggest algorithm possible. It really is a funny\n",
        "trajectory to me to go from \"I'd like to still be Bayesian, but avoid MCMC because it's slow\"\n",
        "to \"screw subtle design, let's throw a NN at it\".\n",
        "[^2]: This is mostly a joke, but it really is a tremendous convenience that\n",
        "there's such a straight forward knob to turn for \"expressivity\" in this context.\n",
        "We'll get into the ways that isn't completely true soon, but NNs provide fantastic\n",
        "convenience in terms of workflow for improving model flexibility.\n",
        "[^3]: You can see it in the original Normalizing Flows paper linked above, or\n",
        "combined with a nice matrix calc review by [Lilian Weng](https://lilianweng.github.io/posts/2018-10-13-flow-models/). As a more general note, since this is a common topic on a few different talented\n",
        "people's blogs, I'll try to focus on covering material I think I can provide\n",
        "more intuition for, or that are most relevant for variational inference.\n",
        "[^4]: A great example of this is Lilian Weng's [NF walkthrough](https://lilianweng.github.io/posts/2018-10-13-flow-models/) which\n",
        "I recommended above- It\n",
        "has a fantastic review of the needed linear algebra and covers a lot of different\n",
        "flow types, but is a bit overly general about what properties are most desirable\n",
        "in a flow, and is therefore initially a bit fuzzy on the value different flows\n",
        "have.\n",
        "[^5]: Deriving precisely how this works would take us too far afield, but see [Kobyzev et\n",
        "al. (2020)](https://arxiv.org/abs/1908.09257) if you're interested. It's a great review paper that does a lot of work to recognize there are multiple different possible applications of normalizing flows, and thus\n",
        "different notations and framings that they very successfully bridge.\n",
        "[^6]: Depending on your browser settings you may need to refresh the page here to watch it run.\n",
        "[^7]: Ok fine, you probably get that down to $\\mathcal{O}(n^{2.8...})$ using\n",
        "[Strassen](https://en.wikipedia.org/wiki/Strassen_algorithm) which is implemented\n",
        "essentially everywhere that matters."
      ],
      "id": "224297eb"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/atimm/anaconda3/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}