---
layout: post
title: Variational Inference for MRP with Reliable Posterior Distributions
subtitle: Part 2
date: 2022-10-28
draft: True
categories:
- MRP
- BART
- Variational Inference
---

This is the second post in my series on using Variational Inference to speed up relatively complex Bayesian like Multilevel Regression and Poststratification models without the approximation being of disastrously poor quality. In the last post, I laid out why such reformulating the Bayesian inference problem as optimization might be desirable, but previewed why this might be quite hard to find high quality approximations amenable to optimization. I then introduced our running example (predicting national/sub-national opinion on an abortion question from the CCES using MRP), and gave an initial introduction to a version of Variational Inference where we maximize the Evidence Lower Bound (ELBO) as an objective, and do so using a mean-field Gaussian approximation. We saw that with 60k examples, this took about 8 hours to fit with MCMC, but 144 seconds (!) with VI.

In this post, we'll explore the shortcomings of this initial approximation, and take a first pass at trying to better with a more complex (full rank) variational approximation. The goal is to get a better feel for what failing models could look like, at least in this relatively simple case.

The rough plan for the series is as follows:

1.  [Introducing the Problem- Why is VI useful, why VI can produce spherical cows](https://andytimm.github.io/posts/Variational%20MRP%20Pt1/variational_mrp_pt1.html)
2.  **(This post)** How far does iteration on classic VI algorithms like mean-field and full-rank get us?
3.  Some theory on why posterior approximation with VI can be so poor
4.  Seeing if some more sophisticated techniques like normalizing flows help

```{r, echo= FALSE, warning=FALSE, output = FALSE}
library(tidyverse)
library(rstanarm)
library(tidybayes)
library(tictoc)

options(dplyr.summarise.inform = FALSE)

# As noted in the text of the first post, I'm starting from and working off of
# the wonderful example developed in MRP Case Studies book
# (https://bookdown.org/jl5522/MRP-case-studies/introduction-to-mrp.html)


set.seed(605)

options(mc.cores = parallel::detectCores(logical = FALSE))

# The US census and CCES data use FIPS codes to identify states. For better
# interpretability, we label these FIPS codes with their corresponding abbreviation.
# Note that the FIPS codes include the district of Columbia and US territories which
# are not considered in this study, creating some gaps in the numbering system.
state_abb <- datasets::state.abb
state_fips <- c(1,2,4,5,6,8,9,10,12,13,15,16,17,18,19,20,21,22,23,24,
                25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,
                44,45,46,47,48,49,50,51,53,54,55,56)
recode_fips <- function(column) {
  factor(column, levels = state_fips, labels = state_abb)
}

cces_all_df <- read_csv("data/cces18_common_vv.csv.gz")

clean_cces <- function(df, remove_nas = TRUE){
  
  ## Abortion -- dichotomous (0 - Oppose / 1 - Support)
  df$abortion <- abs(df$CC18_321d-2)
  
  ## State -- factor
  df$state <- recode_fips(df$inputstate)
  
  ## Gender -- dichotomous (coded as -0.5 Female, +0.5 Male)
  df$male <- abs(df$gender-2)-0.5
  
  ## ethnicity -- factor
  df$eth <- factor(df$race,
                   levels = 1:8,
                   labels = c("White", "Black", "Hispanic", "Asian", "Native American", "Mixed", "Other", "Middle Eastern"))
  df$eth <- fct_collapse(df$eth, "Other" = c("Asian", "Other", "Middle Eastern", "Mixed", "Native American"))
  
  ## Age -- cut into factor
  df$age <- 2018 - df$birthyr
  df$age <- cut(as.integer(df$age), breaks = c(0, 29, 39, 49, 59, 69, 120), 
                labels = c("18-29","30-39","40-49","50-59","60-69","70+"),
                ordered_result = TRUE)
  
  ## Education -- factor
  df$educ <- factor(as.integer(df$educ), 
                    levels = 1:6, 
                    labels = c("No HS", "HS", "Some college", "Associates", "4-Year College", "Post-grad"), ordered = TRUE)
  df$educ <- fct_collapse(df$educ, "Some college" = c("Some college", "Associates"))  
  
  # Filter out unnecessary columns and remove NAs
  df <- df %>% select(abortion, state, eth, male, age, educ) 
  if (remove_nas){
    df <- df %>% drop_na()
  }
  
  return(df)
  
}

cces_all_df_cleaned <-  clean_cces(cces_all_df, remove_nas = TRUE)

# Load data frame created in the appendix. The data frame that contains the poststratification
# table is called poststrat_df
poststrat_df <- read_csv("data/poststrat_df.csv")

statelevel_predictors_df <- read_csv('data/statelevel_predictors.csv')

poststrat_df_60k <- left_join(poststrat_df, statelevel_predictors_df, by = "state")

cces_all_df <- left_join(cces_all_df_cleaned, statelevel_predictors_df, by = "state")
```

# The disclaimer

One sort of obvious objections to how I've set up this series is "Why not talk about theory on why VI approximations can be poor before trying stuff?". While in practice I did read a lot of the papers for the next post before writing this one, I think there's a lot of value is looking at failed solutions to a problem to build up intuition about what our failure mode looks like, and what it might require to get it right. <Example or two to tease this once I have them>.

# Toplines

```{r, output = F}
meanfield_60k <- readRDS("fit_60k_meanfield.rds")
mcmc_60k <- readRDS("fit_60k_mcmc.rds")

# Meanfield 
epred_mat_mf <- posterior_epred(meanfield_60k, newdata = poststrat_df_60k, draws = 1000)
mrp_estimates_vector_mf <- epred_mat_mf %*% poststrat_df_60k$n / 
                                              sum(poststrat_df_60k$n)
mrp_estimate_mf <- c(mean = mean(mrp_estimates_vector_mf),
                     sd = sd(mrp_estimates_vector_mf))


# MCMC 
epred_mat_mcmc <- posterior_epred(mcmc_60k, newdata = poststrat_df_60k, draws = 1000)
mrp_estimates_vector_mcmc <- epred_mat_mcmc %*% poststrat_df_60k$n /
                                                  sum(poststrat_df_60k$n)
mrp_estimate_mcmc <- c(mean = mean(mrp_estimates_vector_mcmc),
                       sd = sd(mrp_estimates_vector_mcmc))

cat("Meanfield MRP estimate mean, sd: ", round(mrp_estimate_mf, 3))
cat("MCMC MRP estimate mean, sd: ", round(mrp_estimate_mcmc, 3))
```

|   | Mean  | SD  |
|---|---|---|
| MCMC  | 43.9%  |  .2% |
| mean-field VI  |  43.7% | .2%  |

Starting with basics, the toplines are pretty much identical, which is a good start. The minor difference here could easily be simulation error- from a few quick re-runs these
often end up having matching means to 3 decimals.

# State Level Estimates

What happens if we produce state level estimates, similar to the plot last post
comparing MRP to a simple weighted estimate? Note that I'll steer away from
the MRP Case Study example here in a few ways. I'll use `tidybayes` for
working with the draws (more elegant than their loop based approach),
and I'll use more draws (helps with simulation error in smaller states).

```{r}
mcmc_state_level <- poststrat_df_60k %>% add_epred_draws(mcmc_60k, ndraws = 1000)
mfvi_state_level <- poststrat_df_60k %>% add_epred_draws(meanfield_60k, ndraws = 1000)

mcmc_state_level %>% glimpse()
```

If you haven't worked with `tidybayes` before, the glimpse above should help
give some intuition about the new shape of the data- we've take the 12,000 row
`poststrat_df_60k`, and added a row per observation per draw, with the prediction
(.epred) and related metadata. This gives
12,000 x 1,000 = 12,000,000 million rows. This really isn't the most space
efficient storage, but it allows for very elegant `dplyr` style manipulation of
results and quick exploration.

Let's now plot and compare the 50 and 95% credible intervals by state between
the two models.

```{r, fig.width=8,fig.height=10}

mcmc_state_summary <- mcmc_state_level %>% 
                        # multiply each draw by it's cell's proportion of state N
                        # this is the P in MRP
                        group_by(state,.draw) %>%
                        mutate(postrat_draw = sum(.epred*(n/sum(n)))) %>%
                        group_by(state) %>%
                        median_qi(postrat_draw, .width = c(.5,.95)) %>%
                        mutate(model = "mcmc")
mfvi_state_summary <- mfvi_state_level %>% 
                        group_by(state,.draw) %>%
                        mutate(postrat_draw = sum(.epred*(n/sum(n)))) %>%
                        group_by(state) %>%
                        median_qi(postrat_draw, .width = c(.5,.95)) %>%
                        mutate(model = "MF-VI")

combined_summary <- bind_rows(mcmc_state_summary,mfvi_state_summary)

combined_summary %>%
  mutate(ordered_state = fct_reorder(combined_summary$state,
                                     combined_summary$postrat_draw)) %>%
  ggplot(aes(y = ordered_state,
             x = postrat_draw,
             xmin = .lower,
             xmax = .upper,
             color = model)) +
  geom_pointinterval(position = position_dodge(1)) +
  xlim(.25,.75) +
  theme(legend.position="top")
```
... That looks concerning. What's happened here?

What might you get wrong if you used the VI approximation for inference here? If you
only cared about the median estimate primarily, you might be ok with this effort.
If you care about uncertainty though, here's a non-exaustive list of concerns here:

1.  Probably unimodal, smooth
posterior distributions from MCMC have gone off-course to the point where the
Median/50/95% presentation no longer seems up to expressing the posterior shape
(more on this in a second).
2.   The MF-VI posteriors are often narrower in 50% or 95% CI- we'd on average
underestimate various types of uncertainty here.
3.   Worse[^1], the MF-VI posterior's CIs aren't **consistently** narrower, either in
the sense they are always narrower, or that they tend to consistently distort the
same way. Sometimes both the 50% and 95% are just a small amount narrower than
MCMC- the Michigan posterior attempt looks passable. Sometimes things are worse,
with 50% MFVI CIs almost as wide as the MCMC 95% interval- Wyoming shows such a 
distortion. Sometimes the probability mass between 50% and 95% is confined
to such a minuscule range it looks like I forgot to plot it. 

That last point is
particularly important because it suggests there's no easy rule of thumb for
mechanically correcting these intervals, or deciding which could be plausible
approximations without the MCMC plot alongside to guide that process. We can't
use VI to save a ton of time, infer the intervals consistently need to x% be wider,
and call it a day- we need to reckon more precisely with why they're distorted.

Let's return now to the point about how the shape has gone wrong. Below is
a dot plot ([Kay et al., 2016](https://dl.acm.org/doi/10.1145/2858036.2858558))- each
point here represents about 1% of the probability mass. I enjoy this approach
to posterior visualization when things are getting weird, as this clarifies a
lot about the full shape of the posterior
distribution, making fewer smoothing assumptions like a density or eye plot might.


```{r, fig.width=8,fig.height=10, warning=FALSE}
mcmc_state_points <- mcmc_state_level %>% 
                        # multiply each draw by it's cell's proportion of state N
                        # this is the P in MRP
                        group_by(state,.draw) %>%
                        summarize(postrat_draw = sum(.epred*(n/sum(n)))) %>%
                        mutate(model = "mcmc")
mfvi_state_points <- mfvi_state_level %>% 
                        group_by(state,.draw) %>%
                        summarize(postrat_draw = sum(.epred*(n/sum(n)))) %>%
                        mutate(model = "MF-VI")

combined_points <- mcmc_state_points %>%
                      bind_rows(mfvi_state_points) %>%
                      ungroup()

combined_points %>%
  mutate(ordered_state = fct_reorder(combined_points$state,
                                     combined_points$postrat_draw)) %>%
  ggplot(aes(y = ordered_state,
             x = postrat_draw,
             color = model)) +
     stat_dots(quantiles = 100) +
     facet_wrap(~model) +
     theme(legend.position="none")

```
Eek. The closer to the individual draws we get, the less these two models seem to be
producing comparable estimates. This isn't me expressing an aesthetic preference
for smooth, unimodal distributions- the MFVI plots in this view imply beliefs
like "support for this policy in Wyoming is overwhelmingly likely to fall in 1 of
3 narrow ranges, all other values are unlikely"[^2]. Other similar humorous claims
are easy to find.

Stepping back for a second, if our use-case for this model takes pretty
much any form of interest in quantifying uncertainty accurately, this is not
an acceptable approximation. I could poke more holes, but I can more profitably
do that after I've explored some theory of why VI models struggle, and brought
in some more sophisticated diagnostic tools than looking with our eyeballs[^3];
so let's hold off on that.

# Do more basic fixes solve anything?

So I've been billing this simple mean-field model as a first pass- I fit it on
more or less default `rstanarm` parameters. I think it's worth taking a moment
to show that getting this approximation problem right isn't going to be solved
with low hanging fruit ideas, since that will motivate our need for better
diagnostics and more expressive approximations.


## Lowering the tolerance

So we managed to structure our Bayesian inference problem as an optimization
problem. Can't we just optimize better? Maybe with more training the result
will be less bad?

the `tol_rel_obj` parameter control's the convergence tolerance on the relative
norm of the objective. In other words, it controls what Evidence Lower Bound
value we consider accurate enough to stop at. The default is 0.01, which feels
a bit opaque, but let's try setting it way down to 1e-8 (1Mx lower). Then we can
plot it alongside the MCMC estimates and original MF-VI attempt.

```{r, warnings = FALSE, fig.width=8,fig.height=10}
tic()
fit_60k_1e8 <- stan_glmer(abortion ~ (1 | state) + (1 | eth) + (1 | educ) +
                                      male + (1 | male:eth) + (1 | educ:age) +
                                      (1 | educ:eth) + repvote + factor(region),
  family = binomial(link = "logit"),
  data = cces_all_df,
  prior = normal(0, 1, autoscale = TRUE),
  prior_covariance = decov(scale = 0.50),
  adapt_delta = 0.99,
  refresh = 0,
  tol_rel_obj = 1e-8,
  algorithm = "meanfield",
  seed = 605)
toc()

lower_tol_draws <- poststrat_df_60k %>% add_epred_draws(fit_60k_1e8, ndraws = 1000)

mfvi_lower_tol_points <- lower_tol_draws %>% 
                        group_by(state,.draw) %>%
                        summarize(postrat_draw = sum(.epred*(n/sum(n)))) %>%
                        mutate(model = "MF-VI 1e-8")

combined_points_w_lower_tol <- combined_points %>%
                      bind_rows(mfvi_lower_tol_points) %>%
                      ungroup()

combined_points_w_lower_tol %>%
  mutate(ordered_state = fct_reorder(combined_points_w_lower_tol$state,
                                     combined_points_w_lower_tol$postrat_draw)) %>%
  ggplot(aes(y = ordered_state,
             x = postrat_draw,
             color = model)) +
     stat_dots(quantiles = 100) +
     facet_wrap(~model) +
     theme(legend.position="none")
```
... That certainly looks different, but I don't really think I'd say it looks
meaningfully better. It doesn't seem like further optimization is remaining
coupled with better looking estimates. 

## Full-Rank Approximation

So if we can lower our ELBO, but improvements don't result, maybe the next
option is ask whether we need something more sophisticated than a mean-field
approximation. Instead of 

$$q(z) = \prod_{j=1}^{m} q_j(z_j)$$

let's now try the full-rank approximation. Gather than each $z_j$ getting it's own
independent Gaussian, this uses a single multivariate normal distribution-
so we can now (roughly) learn correlation structure, fancy.

$$q(z) = \mathcal{N}(z|\mu,\Sigma)$$
```{r, warnings = FALSE, message=FALSE, fig.width=8,fig.height=10}
tic()
fit_60k_fullrank <- stan_glmer(abortion ~ (1 | state) + (1 | eth) + (1 | educ) +
                                      male + (1 | male:eth) + (1 | educ:age) +
                                      (1 | educ:eth) + repvote + factor(region),
  family = binomial(link = "logit"),
  data = cces_all_df,
  prior = normal(0, 1, autoscale = TRUE),
  prior_covariance = decov(scale = 0.50),
  adapt_delta = 0.99,
  refresh = 0,
  #tol_rel_obj = 1e-4,
  #init = 0,
  algorithm = "fullrank",
  QR = TRUE,
  seed = 605)
toc()

full_rank_draws <- poststrat_df_60k %>% add_epred_draws(fit_60k_fullrank,
                                                        ndraws = 1000)

frvi_points <- full_rank_draws %>% 
                        group_by(state,.draw) %>%
                        summarize(postrat_draw = sum(.epred*(n/sum(n)))) %>%
                        mutate(model = "FR-VI")

combined_points_w_frvi <- combined_points_w_lower_tol %>%
                      bind_rows(frvi_points) %>%
                      ungroup()

combined_points_w_frvi %>%
  mutate(ordered_state = fct_reorder(combined_points_w_frvi$state,
                                     combined_points_w_frvi$postrat_draw)) %>%
  ggplot(aes(y = ordered_state,
             x = postrat_draw,
             color = model)) +
     stat_dots(quantiles = 100) +
     facet_wrap(~model) +
     theme(legend.position="none")
```
The first thing to note here is that unlike the mean-field approximation, fitting
this model wasn't as simple as 

# Where to from here? (Why is it like this!?)

[^1]: Really, the worst type of wrong, completely unpredictable wrong. If you spend
time staring to try to infer a causal pattern of which states we can't estimate well,
you're likely just going to end up confused.
[^2]: Some of these MFVI distributions are bad enough that you might reasonably
wonder if some of the badness is just plotting weirdness. That was my intuition at
first. Of course though, this is sufficient granularity to make the MCMC results
look reasonable. But even if you zoom in on 1 or two states and add way more
points, the improbably sharp spikes remain.
[^3]: Phrase due to Richard McElreath. The magic of good visualizations like
Kay et Al.'s is that makes it trivial to let pattern recognition go to work,
and be able to go "oh, I don't believe that".