---
layout: post
title: Variational Inference for MRP with Reliable Posterior Distributions
subtitle: Part 3- Some theory on why VI is hard
date: 2022-12-03
draft: True
categories:
- MRP
- Variational Inference
---

This is section 3 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.

In the [last post](https://andytimm.github.io/posts/Variational%20MRP%20Pt2/Variational_MRP_pt2.html) we threw caution to the wind, and tried out some simple variational inference implementations, to build up some intuition about what bad VI might look like. Just pulling a simple variational inference implementation off the shelf and whacking run perhaps unsurprisingly produced dubious models, so in this post we'll bring in long overdue theory to understand why VI is so difficult, and what we can do about it.

The general structure for this post will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. I also have a grab bag of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below at the end.

The rough plan for the series is as follows:

1.  Introducing the Problem- Why is VI useful, why VI can produce spherical cows
2.  How far does iteration on classic VI algorithms like mean-field and full-rank get us?
3.  **(This post)** Some theory on why posterior approximation with VI can be so poor
4.  Better grounded diagnostics and workflow
5.  Seeing if some more sophisticated techniques like normalizing flows help

Draft problems/solutions:

# Inclusive versus Exclusive KL-divergence

Like I mentioned in the first post in the series, the Evidence Lower Bound (ELBO), our optimization objective we're working with is a tractable approximation of the Kullback-Leibler Divergence between our choice of approximating distribution $q(z)$ to our true posterior $p(z)$.

The KL divergence is asymmetric: in general, $KL(p||q) \neq KL(q||p)$. Previously,
we saw that this asymmetry mattered quite a bit for our ELBO idea:

$$argmin_{q(z) \in \mathscr{Q}}(q(z)||\frac{p(z,x)}{\bf p(x)}) = \mathbb{E}[logq(z)] - \mathbb{E}[logp(z,x)] + {\bf logp(x)}$$
We can't calculate the bolded term $logp(x)$; if we could we wouldn't be finding
this inference thing so hard in the first place. The way we sidestepped that with
the ELBO is to note that the term is constant with respect to $q$; so we can
go on our merry way minimizing the above without it. 

If we flip the divergence around though, we've got an issue. That term would
then be a $logq(x)$ ... which we can't write off in the same way- it varies as we
optimize. So if we're
doing this ELBO minimizing version of variational inference, we're obligated to
use this "reverse" KL divergence, the second option below. 


$$
\begin{align}
KL(p||q) = \sum_{x \in X}{p(x)}log[\frac{p(x)}{q(x)}]  \\
KL(q||p) = \sum_{x \in X}{q(x)}log[\frac{q(x)}{p(x)}] 
\end{align}
$$

Unfortunately, this choice to optimize the "reverse" KL divergence bakes in preference
for a certain type of solution[^1]. 

I found I built better intuition for this encoded preference after seeing it presented many
different ways, so here are a few of my favorites.

One way to see the difference is through a variety of labels for each direction. One could call Forward KL (1) vs. Reverse KL (2):

1. Inclusive vs. Exclusive (my favorite, and so what I'm using for the section header)
2. Mean Seeking vs. Mode Seeking
3. Zero Avoiding vs. Zero Forcing

let's quickly illustrate this in the case of a simple mixture of normals:
```{r, message=F}
library(ggplot2)
library(gridExtra)
library(tidyverse)

mixture <- data.frame(normals = c(rnorm(1000,3,1),rnorm(1000,15,2)),
                      mode_seeking_kl = rnorm(2000,3.5,2),
                      mean_seeking_kl = rnorm(2000,9,4))

rkl_plot <- mixture %>% ggplot(aes(x = normals)) +
  geom_density(aes(x = normals), color = "red") +
  geom_density(aes(x = mode_seeking_kl), color = "green") + ggtitle("Exclusive KL") +
  xlab("")

fkl_plot <- mixture %>% ggplot(aes(x = normals)) +
  geom_density(aes(x = normals), color = "red") +
  geom_density(aes(x = mean_seeking_kl), color = "green") + ggtitle("Inclusive KL") +
  xlab("")

grid.arrange(rkl_plot,fkl_plot)
```

To approximate the same exact red distribution $p(x)$, Inclusive KL (1) and Exclusive KL (2)
optmize the green $q(p)$ in quite different manner. 

To spell out the ways to describe this above: Inclusive KL will try to cover all the probability mass in $p(x)$, even if it means a peak
at a unfortunate middle ground. Exclusive KL, on the other hand, will try to concentrate
it's mass on the largest mode, even if it means missing much of the mixture of normals. Alternatively,
we could describe the top graph as mode seeking, and the bottom as mean seeking. Finally,
we could say the top graph shows "Zero Forcing" behavior- it will heavily favor
putting zero mass on some parts of the graph to avoid any weight where $p(x)$ has no
mass, even if it means missing an entire mode. Conversely, Inclusive KL will
aim to cover all the mass of $p(x)$ in full even if the result is an odd solution, in order
to avoid having zero mass where $p(x)$ has some.

How does this follow from the form of the divergence?

To start with, notice that for inclusive KL we could think of the $log(\frac{p(x)}{q(x)})$ part of the term being weighted by $p(x)$- if in some range of $x$ $p(x)$ is 0, we don't pay a penalty if $q(x)$ puts mass. The reverse is not true however- if our $q(x)$ is zero where there should be mass
in our true distribution, our Inclusive KL divergence is infinite[^2].

$$
\begin{align}
KL(p||q) = \sum_{x \in X}{p(x)}log[\frac{p(x)}{q(x)}]  \\
KL(q||p) = \sum_{x \in X}{q(x)}log[\frac{q(x)}{p(x)}] 
\end{align}
$$

And if we change the direction of the divergence, the opposite zeros and infinities
show up, enforcing strong preferences for a specific type of solution.

When the example is a simple mix of two gaussians approximated with a single
gaussian, it's fairly easy to intuit how the choice of KL divergence will influence
the optimization solution. This all gets a bit more opaque on harder problems-
like we saw with the example last post, ELBO based VI will tend to underestimate
the support of $p(x)$ but whether the solution is narrow but overall reasonable, or pretty much degenerate
is hard to predict. However, this exploration of how the form of the divergence
influences the results still gives a rough intuition for why our ELBO optimized
posteriors might collapse.

If we want to try the opposite direction of KL divergence, it isn't immediately obvious there's a
global objective we can choose to perhaps seek out a optimization problem
that favors overdispersed solutions. Like I mentioned above, if we try to make
an ELBO-esque target but reverse the KL divergence, the $logp(x)$ which is constant with respect
to the $q(x)$ we're optimizing becomes a $logq(x)$ which we can't so easily work around.

Let's look first at a solution in the spirit of VI[^3] to the above problem which requires us to pick up a new divergence, the $\chi^{2}$-divergence, and optimizes a new bound. Let's take a look at it.

## $\chi^{2}$ Variational Inference (CHIVI) and the CUBO bound

The $\chi^{2}$-divergence has form:

$$
D_{\chi^2}(p||q) = \mathbb{E}_{q(z;\lambda)}[(\frac{p(z|x)}{q(z;\lambda)})^2 -1]
$$
For simplicity and comparability, I'm switching here to using Dieng et Al. (2017)'s notation here- they use $q(z;\lambda)$ to refer to the variational family we're using, indexed by parameters $\lambda$.

This divergence has the properties we wanted when we tried to use Inclusive KL Divergence- it tends to be mean seeking instead of mode seeking.

Like with the ELBO, we need to show that we have a bound here independent of $logp(x)$, and that we have a way to estimate that bound efficiently.

Let's first move around a few pieces of the first term above:

$$
\begin{align}
\mathbb{E}_{q(z;\lambda)}[(\frac{p(z|x)}{q(z;\lambda)})^2& = 1 + D_{\chi^2}(p(z|x)|q(z;\lambda)) \\
&= p(x)^2[1 + D_{\chi^2}(p(z|x)|q(z;\lambda))]
\end{align}
$$
Then we can take the log of both the right hand side equations, which gives us:

$$
\frac{1}{2}log(1 + D_{\chi^2}(p(z|x)|q(z;\lambda))) = -logp(x) + \frac{1}{2}log\mathbb{E}_{q(z;\lambda)}[(\frac{p(z|x)}{q(z;\lambda)})^2]  
$$
...and this is starting to feel a lot like the ELBO derivation. Log is monotonic, and logp(x) is constant as we optimize $q$, so we've found something that we're close to able to minimize:

$$
CUBO_{2}(\lambda) = \frac{1}{2}log\mathbb{E}_{q(z;\lambda)}[(\frac{p(z|x)}{q(z;\lambda)})^2]
$$
Since this new divergence is non-negative as well, this is a upper bound of the model evidence. This is thus named $\chi$ upper bound (CUBO)[^4].

## ... But can we estimate it?

One other issue here: how do we estimate this? The CUBO objective got rid of the $logp(x)$ we were worried about, but it seems like that expectation is going to be difficult to estimate in general. 

If you're a good Bayesian your first idea might be to Monte Carlo (not MCMC) estimate it roughly like this:

$$
CUBO_2(\lambda) = \frac{1}{2}log\frac{1}{S}\sum_{s=1}^{S}[(\frac{p(x,z^{s})}{q(z^{s};\lambda)})^2]
$$
Unfortunately, the $log$ transform here means our Monte Carlo estimator will be biased: we can see this by applying Jensen's inequality to the above. To make this stably act as an upper bound, we can apply a clever transformation:

$$
\bf{L} = exp(n* CUBO_2(\lambda))
$$

Since exp is monotonic, this has the same objective as the CUBO, but we can Monte Carlo estimate it unbiasedly. Is that the last problem to solve?

## ... But can we calculate gradients efficiently?

Wait, wait no. Sorry to keep saying there's one more step here, but there's a lot that goes into making a full, convenient, general use algorithm here. The last step (for real this time) is that we need to figure out how to get gradients for the estimate of $\bf{L}$ above, $\bf{\hat{L}}$. The issue is that we don't have any guarantee that a unbiased Monte Carlo estimator of $\bf{\hat{L}}$ gets us a Monte Carlo way to estimate $\nabla_\lambda\bf{\hat{L}}$- we can't guarantee that the gradient of the expectation is equal to the expectation of the gradient.

For this, we need to pull out a trick from the variational autoencoder literature. This is usually referred to as the "Reparameterization Trick"[^5], but the original CHIVI paper refers to them as "reparmeterization gradients". We will assume we can rewrite the generative process of our model as $z = g(\lambda,\epsilon)$, where $\epsilon \sim p(\epsilon)$ and g being a deterministic function. Then we have a new estimator for both $\bf{\hat{L}}$ and it's gradient:

$$
\begin{align}
\bf{\hat{L}} &= \frac{1}{B}\sum_{b=1}^B(\frac{p(x,g(\lambda,\epsilon^{(b)}))}{q(g(\lambda,\epsilon^{(b)};\lambda))})^{2} \\ 
\nabla_\lambda\bf{\hat{L}}  &= \frac{2}{B}\sum_{b=1}^B(\frac{p(x,g(\lambda,\epsilon^{(b)}))}{q(g(\lambda,\epsilon^{(b)};\lambda))})^2 \nabla_\lambda log(\frac{p(x,g(\lambda,\epsilon^{(b)}))}{q(g(\lambda,\epsilon^{(b)};\lambda))})
\end{align}
$$
There are one or two more neat computational tricks in the paper I won't explain here (essentially: how do we extend this to work in minibatch fashion, and how do we avoid numerical underflow issues), but this is now essentially functional. The whole algorithm, which they dubbed CHIVI is below:

![](images/CHIVI_algorithm.png)

Stepping back, let's talk about some practical properties of the algorithm we've been stepping through. 

First, and probably most exciting given what we saw in the last post, CHIVI's objective has the property that it is inclusive, unlike the ELBO we were using earlier. This won't be the right choice for all variational inference problems, but given our prior issues with very narrow posteriors this will be exciting to test[^6]. And as we'll see in the next section, this overdispersion tendency in the posterior will often have a beneficial interaction with importance sampling which can improve our estimates further.

Another nice thing here is that if we were to estimate both the ELBO and CUBO for a given problem, we'd get both a upper and lower bound on the model evidence. This is theoretically convenient in that we now have a sandwich estimator, which actually obtains reasonably tight bounds. We'll even be able use the fact we have both later to get some bounds on practical bounds on quantities we tend to report in practice like means and covariances!

A final neat benefit here is that to the extent we are willing to consider ensembling models (again, more on that soon), this CHIVI framework will produce estimates that succeed (and fail) in less similar ways that the the ELBO based estimators we looked at last post. Expanding our available set of tools is always good, but it's even better when we're ensembling because we can leave more heavily on each model for the tasks it succeeds on.

One potential downside here is that we introduced a solution that partially relies on a Monte Carlo estimator. That said, this is pretty cheap in practice; if we're using VI as a drop in for MCMC, this is still going to be much much faster than MCMC for any big problem. We'll need to think about a reasonable number of samples in a given case, but realistically this isn't going to be a driving factor in determining compute time.

Another final problem is that the estimator we built out for the CUBO that we could actually estimate tends to end up having pretty high variance. Exponentiating the objective isn't free in that sense; but this problem of variance reduction in estimators is something that feels like a tractable problem to iterate on.

# Not all samples are equally good

So we've made an approximation q(x) that's cheap to sample from, and is somewhat
close to p(x), our true posterior. The way to improve the approximation we've
focused on so far is to just go back to the start and make q(x) better; for example,
through changing up the variational family, or to switching to a different
optimization objective like the CUBO. That's one solution that's often necessary,
but can we work with a particular q(x) we have and somehow make it look more like
p(x)?

... Phased this way, this sounds a lot like importance sampling. If you haven't seen them before,
an importance sampling estimator allows us to take draws from a (preferably) easy to sample
from distribution[^7] and reweight the samples to look more like our true target
distribution. The weight $w_i$ for each sample $i$ take form:

$$
w_i = \frac{p(x_i)}{q(x_i)}
$$
Before you get worried that we don't have $p(x_i)$ because of the normalizing constant like every time we talk about having $p(x)$ in this series, there's a clever estimator that
"self-normalizes" such that this can be a reasonable strategy. Intuitively, we're
just placing more weight on samples in the support of $p(x)$.

This footenote[^8] has a selection of some of my favorite resources for learning more or refreshing your memory about importance sampling, but for the main discussion let me pull out some
particularly important sub-problems to solve, which all take the forms of managing
the variance of the estimator since it's pretty easy to see it's unbiased. 

First, our choice of the "proposal" distribution we're reweighting to be more like
$p(x)$ matters for making this process practically feasible. We need the proposal
distribution to be close enough to $p(x)$ that a realistic number of the draws
get non-negligible weights.
It might be true that we could draw proposals from a big $N$ dimensional uniform
distribution for every problem, but if we want to be done sampling enough this century
we need to at least get fairly close with our initial $q(x)$.

A second, but related problem is that it's quite common for the unmodified
importance sampling estimator to have some weights which are orders and orders
of magnitude higher than the average weight, blowing up the variance of the estimator.
Dan Simpson's slides I linked above has an instructive example with not too
weird $p(x)$ and $q(x)$'s that has a max weight ~1.4 million (!) times the average.
If that happens, our estimator will essentially ignore most samples without gigantic
weights, and it'll take ages for that estimator to tell us anything remotely
reliable. 

So with those points we need to address, here are the next topics in this section:

1. Importance Weighted Variational Inference
2. Robust importance sampling with built in diagnostics via PSIS
3. Combining multiple proposal distributions via Multiple Importance Sampling


## Importance Weighted Variational Inference

The actual use of Importance Weighting for VI in it's simplest form is
pretty intuitive (draw samples from $q(x)$, weight them...), but let's derive the
new Importance Weighted Variational Inference (IWVI) estimator since some nice
intuition will come with it.

We'll aim to show that we can get a tighter ELBO by using importance weights. This
type of tighter ELBO was first shown by [Burda et Al. (2015)](https://arxiv.org/abs/1509.00519)
in the context of Variational Autoencoders after which is was fairly clear this could apply
to variational inference, but [Domke and Sheldon (2018)](https://arxiv.org/abs/1808.09034)
fleshed out some details of that extension- I'll be explaining some of their main
results first.

To start, imagine a random variable $R$, such that $\mathbb{E}{R} = p(x)$, which
we'll think of as a estimator of p(x). Then by Jensen's Inequality:

$$
logp(x) = \mathbb{E}logR + \mathbb{E}log\frac{p(x)}{R}
$$

The first term is the bound, which will be tighter if $R$ is highly concentrated. 

This is a more general form of the ELBO; we can make it quite familiar looking by
having our R above be:

$$
R = \frac{p(z,x)}{q(z)}, z \sim q
$$

The reason pointing out this fairly simple generalization is helpful is that it
frames how to tighten our ELBO bound on $logp(x)$ via alternative estimators $R$.

By drawing $M$ samples and averaging them as in importance sampling, we get:

$$
R_M = \frac{1}{M}\sum_{m=1}^{M}\frac{p(z_m,x)}{q(z_m)}, z_m \sim q
$$
From there, we can derive a tighter bound on $logp(x)$, referred to as the IW-ELBO:

$$
IW-ELBO_M[q(z)||p(z,x)] := \mathbb{E}_{q(z_{1:M})}log\frac{1}{M} \sum_{m=1}^{M}\frac{p(z_m,x)}{q(z_m)}
$$
Where we're using the $1:M$ as a shorthand for eg $q(z_{1:M}) = q(z_1)...q(z_M)$.


It's worth noting that the last few lines don't specify a particular form of importance
sampling- we're getting the tighter theoretical bounding behavior from the averaging of samples
from $q$. We'll see a particularly good form of importance sampling with desirable
practical properties in a moment.

### How does IW-ELBO change the VI problem conceptually?

The tighter bound is nice, but importance sampling also has the side effect (done right, side benefit)
of modifying our incentives in choosing a variational family. To see what I mean,
we can re-use the plots above for KL Divergence, where red was the true distribution,
and green were our potential approximations. If we're not going to draw multiple samples and weight them, it makes sense to choose something like the first plot below. Every draw in the middle of the two target modes
is expensive per our ELBO objective, so better to choose a mode.

```{r}
rkl_plot <- mixture %>% ggplot(aes(x = normals)) +
  geom_density(aes(x = normals), color = "red") +
  geom_density(aes(x = mode_seeking_kl), color = "green") + ggtitle("Without weighting, we prefer to capture a mode") +
  xlab("")

fkl_plot <- mixture %>% ggplot(aes(x = normals)) +
  geom_density(aes(x = normals), color = "red") +
  geom_density(aes(x = mean_seeking_kl), color = "green") + ggtitle("With importance sampling, weights allow us to prefer coverage") +
  xlab("")

grid.arrange(rkl_plot,fkl_plot)
```

If we can use importance samples though, quite the opposite is be true! Note that we're
still using the ELBO, a reverse-KL based metric- that hasn't changed. What has changed is our
ability to mitigate the objective costs of those samples between the two extremes.
Points outside the two target modes will get lower importance weights, and points within
the modes will get higher ones, so as long as we're covering the modes with some reasonable amount of probability mass, and drawing enough samples we can actually
do better with the distribution centered between the modes. 

We can also imagine how varying the number of samples might effect the point
from the last paragraph. Between $M=1$ and "enough draws to get all the benefits of IS",
we can imagine there's a slow transition from "just stick with 1 mode" and "go with IS".
So it seems like we should be worried about getting the number of samples right, but
fortunately as we'll see in the next section there are great rules of thumb in
some variants of IS. We'll still need to bear the cost of sampling (which gets higher as $q(x)$ becomes "further" from $p(x)$, as we'll need more samples to weight into a good approximation), but
the cost of sampling for most VI implementations will often be pretty manageable
if our proposal distribution is somewhat close to $p(x)$.

Another way to think about how importance sampling changes our task with variational
inference is to think about what sorts of distributions make sense to have as
our variational family, and even which objective might be better given IS. On choice
of a variational family, if we're aiming for coverage, moving towards thicker-
tailed distributions like t distributions makes a lot of sense. While we explored
the IW-ELBO above to build intuition, there's no reason not to apply VI to the CUBO
 and thus CHIVI- this also naturally produces nicely overdispersed distributions which can
 be importance sampled closer to the true $p(x)$. This idea of aiming for a
 wide proposal to sample from is referred to in the importance sampling literature (eg [Owen, 2013](https://artowen.su.domains/mc/)) as "defensive sampling", with [Domke and Sheldon (2018)](https://arxiv.org/abs/1808.09034) exploring the VI connection more fully. For intuition, by ensuring most of p(x) is covered by some reasonable mass makes it easier to efficiently
 get draws that can be weighted into a final posterior, even if the "unweighted"
 posterior might be too wide.

## Solving our IS problems with Pareto-Smoothed Importance Sampling

As we've been talking about importance sampling, we've been leaving some of the
messier details aside (how many samples to draw, how to deal with the cases when some
of the weights get huge, how to know when our proposal distribution is "close" enough).

While the Importance Sampling Literature is huge and there are a lot of possible
variants here, I'll next introduce [Vehtari et Al. (2015)](https://arxiv.org/abs/1507.02646)'s Pareto-Smoothed Importance Sampling.  

## Multiple Proposal Distributions with Multiple Importance Sampling

# Can we bound error in terms of ELBO or CUBO?

## Wasserstein Bounds

# Conclusions + Bonus Context

[^1]: In truth, both KL divergences encode structural preferences for the type
of optimization solution they admit- neither will be the right choice for every
problem and  variational family combination. But as we'll see, being able to
choose will give us more options to fit models we believe.

[^2]: This is the footnote for those of you that are annoyed because you tried
to write out how this would happen, and got something like $p(x) log \frac{p(x)}{0}$,
which should be undefined if we're following normal math rules. But this is information
theory, and in this strange land we say $p(x) log \frac{p(x)}{0} = \infty$. I don't
have a strong intuition for why this is the best solution, but a information encoding perspective
makes it make more sense at least: if we know the distribution of $p$,
we can construct a code for it with average description length $H(x)$. One way to understand
the KL divergence is as what happens when we try to use the code for a distribution $q$ to describe $p$,
we'd need $H(p) + KL(p||q)$ bits on average to describe $p$. In the code for $q$ has no way to represent some element of $p$, then requiring... infinite bits feels like the right way to describe the breakdown of meaning? All this to say this condition is something our optimizer will try hard to avoid.
[^3]: I'll mention an alternative approach, Expectation Propagation, that takes a different (not global objective based) approach further down.
[^4]: This approach actually defines a family of $n$ new divergences, where you replace the $\frac{1}{2}$ with $\frac{1}{n}$ and similarly replace the square an exponent with n. Fully stepping through why this is neat wasn't worth how far afield it'd take us, but the original $CHIVI$ paper has some cool derivations based on this, one of which I'll discuss on the next section on importance sampling. 
[^5]: I thought about a section to explain the reparameterization trick, but there are enough good explanations online of the trick. If you're interested in learning more about why this is important for optimization through stocastic models, I'd recommend starting with Gregory Gundersen's explanation [here](https://gregorygundersen.com/blog/2018/04/29/reparameterization/) and then move on to the original Kingma & Welling, 2013 paper. As general advice on understanding it better though, I'll echo Greg's point that some of the online explanations I've seen are a bit loose- the key is that we want to express a gradient of an expectation (can't MC estimate for sure) as an expectation of a gradient (which we can MC estimate provided our convenient deterministic function $g$ is differentiable). 
[^6]: In a few posts, we're theoryposting for a bit.
[^7]: we'll call it q(x) here to make the application super clear, but often
I see the "proposal" distribution called f(x) and the the distribution we want
to approximate called g(x). 
[^8]: If you're looking to learn about importance sampling for the first time,
a great place to start is Ben Lambert's video introductions to the basic idea:
[video 1](https://www.youtube.com/watch?v=V8f8ueBc9sY), and [video 2](https://www.youtube.com/watch?v=F5PdIQxMA28). For building more intuition about
why we need all these variance reducing modifications to general IS, Dan Simpson
has some great [slides](https://dpsimpson.github.io/pages/talks/Importance_sampling_unsw_2019.pdf) which have a side benefit of being hilarious. Those slides will mention a lot of the books/papers
I find most instructive, but it's worth calling out especially Vehtari et Al's Pareto Smoothed
Importance Sampling [paper](https://arxiv.org/abs/1507.02646) as particularly
well written and paradigm shaping. Finally, Elvira et Al's (2019) Multiple Importance Sampling
[paper](https://projecteuclid.org/journals/statistical-science/volume-34/issue-1/Generalized-Multiple-Importance-Sampling/10.1214/18-STS668.full) is the most thorough I know, but isn't particularly approachable. Instead, for MIS I'd recommend starting with the first few minutes of [this talk](https://www.youtube.com/watch?v=dxFSwplfdpk) (although the main topic of their talk is less relevant, the visualizations are super helpful), and the first ~8 pages of [this paper](https://arxiv.org/pdf/2102.05407.pdf), also by Elvira et Al. (2021) (I especially like
that it spends a bit more time on notation; since multiple importance sampling comes from/comes up
in computer graphics, the notational choices sometimes feel a bit annoying to me). Finally,
the [original MIS paper itself](https://dl.acm.org/doi/10.1145/218380.218498), Veach & Guibas (1995) is quite readable, but requires
a bit of reading around or reading into computer graphics to grok their examples
and notational choices.