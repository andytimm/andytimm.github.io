---
layout: post
title: Variational Inference for MRP with Reliable Posterior Distributions
subtitle: Part 3- Some theory on why VI is hard
date: 2022-12-03
draft: True
categories:
- MRP
- Variational Inference
---

This is section 3 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.

In the [last post](https://andytimm.github.io/posts/Variational%20MRP%20Pt2/Variational_MRP_pt2.html) we threw caution to the wind, and tried out some simple variational inference implementations, to build up some intuition about what bad VI might look like. Just pulling a simple variational inference implementation off the shelf and whacking run perhaps unsurprisingly produced dubious models, so in this post we'll bring in long overdue theory to understand why VI is so difficult, and what we can do about it.

The general structure for this post will be to describe a problem with VI, and then describe how that problem can be fixed to some degree.

The rough plan for the series is as follows:

1.  Introducing the Problem- Why is VI useful, why VI can produce spherical cows
2.  How far does iteration on classic VI algorithms like mean-field and full-rank get us?
3.  **(This post)** Some theory on why posterior approximation with VI can be so poor
4.  Better grounded diagnostics and workflow
5.  Seeing if some more sophisticated techniques like normalizing flows help

Draft problems/solutions:

# Inclusive versus Exclusive KL-divergence

Like I mentioned in the first post in the series, the Evidence Lower Bound (ELBO), our optimization objective we're working with is a tractable approximation of the Kullback-Leibler Divergence between our choice of approximating distribution $q(z)$ to our true posterior $p(z)$.

The KL divergence is asymmetric: in general, $KL(p||q) \neq KL(q||p)$. Previously,
we saw that this asymmetry mattered quite a bit for our ELBO idea:

$$argmin_{q(z) \in \mathscr{Q}}(q(z)||\frac{p(z,x)}{\bf p(x)}) = \mathbb{E}[logq(z)] - \mathbb{E}[logp(z,x)] + {\bf logp(x)}$$
We can't calculate the bolded term $logp(x)$; if we could we wouldn't be finding
this inference thing so hard in the first place. The way we sidestepped that with
the ELBO is to note that the term is constant with respect to $q$; so we can
go on our merry way minimizing the above without it.

If we flip the divergence around though, we've got an issue. That term would
then be a $logq(x)$ ... which we can't write off in the same way- it varies as we
optimize. So if we're
doing this ELBO minimizing version of variational inference, we're obligated to
use this "reverse" KL divergence, the second option below. 


$$
\begin{align}
KL(p||q) = \sum_{x \in X}{p(x)}log[\frac{p(x)}{q(x)}]  \\
KL(q||p) = \sum_{x \in X}{q(x)}log[\frac{q(x)}{p(x)}] 
\end{align}
$$

This turns out to matter 

## CHIVI bound optimization

# Not all points are equal

## PSIS-Variational Inference

# Can we bound error in terms of ELBO or CHIVI?

## Wasserstein Bounds

# Conclusions
