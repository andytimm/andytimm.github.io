---
layout: post
title: Variational Inference for MRP with Reliable Posterior Distributions
subtitle: Introductions- things to do, places to be
date: 2022-09-28
categories:
- MRP
- BART
- Variational Inference
draft: true
---

This post introduces a series of posts I intend to write, exploring using [Variational Inference](https://en.wikipedia.org/wiki/Variational_Bayesian_methods) to massively speed up running complex survey estimation models like variants of [MRP](https://en.wikipedia.org/wiki/Multilevel_regression_with_poststratification) while aiming to keep approximation error from completely ruining the model.

The rough plan for the series is as follows:

1. **(This post)** Introducing the Problem- Why is VI useful, why VI can produce spherical cows
2. How far does iteration on classic VI algorithms like mean-field and full rank get us?
3. Some theory on why posterior approximation with VI can be so poor
4. Seeing if some more sophisticated techniques like normalizing flows help

# Motivation for series

I learn well by explaining things to others, and I've been particularly excited to learn about variational inference and ways to improve it over the past few months. There are lots of Bayesian models I would like to fit, especially my political work, that I would categorize as being incredibly useful, but on the edge of practically acceptable run times. For example, the somewhat but not particularly complex model I'll use as a running example for the series **takes ~6 hours to fix on 60k observations**. 

Having a model run overnight or for a full work day can be fine sometimes, but what if there is a more urgent need for the results? What if we need to  iterate to find the "right" model? What if the predictions from this model need to feed into a later one? How constrained do we feel about adding just a little bit more complexity to the model, or increasing our N size just a bit more?

If we can get VI to fit well, we can make complex Bayesian models a lot more practical to use in a wider variety of scenarios, and maybe even extend the complexity of what we can build given time and resource constraints.


# Spherical Cow Sadness
##### I've got that...

::: {layout="[25,-2,10]" layout-valign="top"}
![](rstanarm_disclaimer.png)

![](blei_vi_spherical.png)
:::

If VI can make Bayesian inference much faster, what's the catch? The above two images encapsulate the problem pretty well. First, as the left screenshot from [rstanarm's documentation](https://mc-stan.org/rstanarm/reference/rstanarm-package.html#estimation-algorithms) shows, variational inference requires a (bold text warning requiring) set of approximating distribution choices in order to be tractable to optimize. On the right, in their survey paper on VI, [Blei et al., 2018](https://arxiv.org/pdf/1601.00670.pdf) are showing one of the potential posterior distorting consequences of our choice to approximate.

So stepping back for a second, we've taken a problem for which there's usually no closed form solution (Bayesian inference), where even the best approximation algorithm we can usually use (MCMC) isn't always enough for valid inference without very careful validation and tinkering. Then we decided our approximation could do with being more approximate.

That was perhaps an overly bleak description, but it should give some intuition why this is a hard problem. We want to choose some method of approximating our posterior such that it is amenable to optimization-based solving instead of requiring sampling, but not trade away our ability to correctly understand the full complexity of the posterior distribution[^1].

# Introducing MRP and our running example

# Introducing Variational Inference

# A first try at VI on this dataset

[^1]: If I were that type of Bayesian, this is where I'd complain that if we screw this up badly enough, we might as well be frequentists or worse, machine learning folk.