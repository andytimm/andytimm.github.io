---
layout: post
title: Taking Synthetic Respondents Seriously
subtitle: Part 1: Mere prediction is sufficient
date: 2025-07-12
draft: True
categories:
  - Synthetic Respondents
---

AAPOR this year was wonderful- a chance to learn from and connect with dozens of pollsters and survey researchers
who are nerdy precisely in many of the ways I am. That said, I generally found myself disappointed with the rigor and *vibes* of discussion around synthetic respondents at the conference[^1]. The night before the conference started, Grow Progress (where I work) co-hosted a happy hour. In a discussion with a few other progressive pollsters, one hope we all expressed for the conference was for someone to rigorously steelman the case for synthetic sample.

Broadly, I left AAPOR not having heard such a case for integration of synthetic sample into polling and opinion research, so I want to provide my own attempt at that steelman here. I take issue with both extremes of this debdate- both the breathless hype of market research folks who discuss present day, workable drop-in total replacement of human respondents, and the equally exasperating methodological conservatives who seem content to point to the (real) limitations and biases of current LLMs as justification for writing off the **possibility** of LLMs utility to their research.

Instead, I'm most excited about a research agenda focused on mixing human and LLM responses with estimators that provably increase precision of estimates where LLMs track ground truth human respondents, but guards against bias where LLMs do not. The basic contours of my argument here are:
1. The language of 'Synthetic Respondents' feels conceptually unhelpful- instead, we're better off thinking of LLMs as producing **conditional predictions**, which are, of course, imperfect proxies for human responses
2. Using imperfect predictions/proxies of outcomes to improve estimates is an exceedingly well studied statistical problem- we're quite capable of reliably mitigating bias when leveraging these predictions, whilst still gaining huge amounts of precision when appropriate.
3. Broader improvements to LLMs and techniques for specializing LLMs for survey research will likely continue to improve the baseline quality of such predictions- suggesting that benefits from these 'mixed subjects designs' will continue to increase over time, and grow in applicability.

# The Plan

This post

[^1]: To state the obvious, AAPOR is a huge conference! Apologies in advance if there was a session where the discussion felt significantly better than what I've outlined here; I'd love to see materials for any sessions I missed. To be clear, there were also lots of really fantastic sessions on other uses from AI in survey research from folks like Mas  @ Applecart, and Josh Learner from NORC; I'm specifically taking issue with the discussion of synthetic sample, where folks felt distinctly uncalibrated in their takes.