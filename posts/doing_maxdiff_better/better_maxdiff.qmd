---
layout: post
title: Better discrete choice modeling through rank ordered logits
subtitle: Or- a mathmatically correct model of a psychologically coherent concept
date: 2024-06-09
draft: True
image: imgs/gumby_distribution.webp
categories:
  - priors
  - stupid Bayesian stuff
editor: 
  markdown: 
    wrap: 72
---

![](imgs/jimmy_dunk.png){fig-align="center"}

The wonderful Jim Savage calls the MaxDiff model of discrete choice a
"mathematically incorrect model of a psychologically incoherent
concept"[^1].

[^1]: The smiley face is threatening: it's there to make sure MaxDiff stays
    down :-D

Despite this lovely dunk, and some wonderful notes explaining why
MaxDiff's not great, the model remains frequently used in market
research, most prominently as implemented in
[Sawtooth](https://sawtoothsoftware.com/). Why is this?

Putting aside the most obvious answers like inertia and a dim view of
statistical practice in marketing, it's also surprisingly hard to find a
fully fleshed out explanation of the alternatives online. In this post,
I remedy that, with sections for:

1.  Introducing MaxDiff and its blemishes
2.  Showing how the issues propagate into final model quality
3.  Alternatives- Rank-Ordered Logits with connected choice
    graphs, and Rank-Ordered Logits with Ties
4.  Steelmanning some common objections

# Introducing MaxDiff

If you're reading this post, you likely have some familiarity with the
basics of discrete choice models, but here's a brief refresher[^2].

[^2]: If you want more of an in-depth introduction, here are some
    resources I'd recommend. First, Jim Savage's [blog post
    series](https://khakieconomics.github.io/2019/03/17/Logit-models-of-discrete-choice.html)
    is great, and does a fantastic job of explaining how our assumptions
    about choicemaking map onto the math. I also benefited from
    reading Glasgow's [Interpreting Discrete Choice
    Models](https://www.cambridge.org/core/elements/abs/interpreting-discrete-choice-models/676EC2C0F19A3B6D932E12CC612872BC),
    which builds up the basics of choice models a bit more slowly.
    Finally, if you want something that goes much more in detail,
    Kenneth Train's [Discrete Choice Models with
    Simulation](https://eml.berkeley.edu/books/choice2.html) goes into
    great mathematical detail about the models.

We want to build a model of how people respond to the request to
make decisions among discrete options. This could be:

-   A pollster asking which candidate(s) or political parties each
    respondent favors
-   A marketing firm studying preferences amongst a variety of chocolate
    bars
-   A political scientist asking which message voters find most
    convincing

One reasonable model[^3] for this is to say that individual $i$ making
choices among $j$ options can be inferred to have some underlying
utility $\mu$ from the available choices, and that while they usually
choose their most preferred option according to their utility function,
there is some degree of randomness. The basic model is then

[^3]: I won't rehash more subtle implications of this basic model here;
    see the footnote above for references that explore the mind of this
    *Homo Economicus* more in depth. Instead, I'll mostly pull up
    assumptions as they become relevant for discussing MaxDiff.

$$
\mu_{ij} = \mu_{ij} + \epsilon_{ij}
$$ ,

where the $\mu_{ij}$ can have rich provenance (demographics and
other individual traits, the context in which the decision is made, the
other options available...) but is fixed, but there's some
$\epsilon_{ij}$ of randomness involved. To make this model easier to
estimate, we assume that $\epsilon_{ij}$ has a
[Gumbel](https://en.wikipedia.org/wiki/Gumbel_distribution)[^4]
distribution.

[^4]: Other options are possible, but less common, and would take us too
    far afield, so I'll skip explaining the choice of Gumbel versus
    other distributions here.

Under this model, the probability that individual $i$ chooses
alternative $j$ from a choice set $C_i$ is: $$
P_{ij} = \frac{\exp(\mu_{ij})}{\sum_{k \in C_i} \exp(\mu_{ik})}
$$ This is the standard multinomial logit.

Now, suppose we have a dataset where each individual $i$ has made a
choice $y_i$ from their choice set $C_i$. The likelihood of observing
this dataset under the multinomial logit model is: $$
L = \prod_i P_{iy_i} = \prod_i \frac{\exp(\mu_{iy_i})}{\sum_{k \in C_i} \exp(\mu_{ik})}
$$ The likelihood is the product of the choice probabilities for each
individual's observed choice $y_i$. We can estimate the parameters of
the utility function $\mu_{ij}$ by maximizing this likelihood function
(or, more commonly, the log-likelihood). This is how the single best
choice data is naturally incorporated into the likelihood function. Each
observation contributes a term to the likelihood based on the
probability of the chosen "best" alternative under the model.

While we can (and will) push the basic logit model of this further to
include respondent and choice level covariates, multilevel components,
and other improvements, let's think about the pressures of gathering
data here for a moment, since that'll motivate the desire for something
like MaxDiff.

To estimate this, we gather respondents and ask them to choose their
favorite option amongst a given choice set. As a way to control the
difficulty of making a choice while still gathering enough data, we can
limit the size of the set (choose the best of 10 items, instead of 20),
and repeat the choice task.

Getting respondents and getting them to stick through a bunch of choice
tasks is hard though, and it's only natural to wonder: can we extract
more with each choice set? One option would be to ask the respondents to
rank ALL the options at once, but if you have a large choice set that
sounds exhausting.

What if we asked people to choose their best and worst choices each
time? After all, people might not have strong preferences amongst the
middling 18 chocolate bars, but the best and worst seem more memorable,
and that's only 1 more choice.

And now, (stepping into the MaxDiff trap to show its allure, if you
will), what if we don't want to entirely change up our likelihood to
handle the new rich source of data?
Instead, what if we just treat the worsts as the opposite of the bests,
which makes a sort of sense, and simplifies the likelihood one hell of a
lot:

$$
U_{ij}^W = -U_{ij}^B = -(\mu_{ij} + \epsilon_{ij})
$$

Under this assumption, the probability of individual $i$ choosing
alternative $j$ as the worst is: $$
P_{ij}^W = \frac{\exp(-\mu_{ij})}{\sum_{k \in C_i} \exp(-\mu_{ik})}
$$

... Now we've stepped in it, and Jimmy is mad at us.

## Psycologically Incoherent

What I've introduced is the core of the MaxDiff formulation of discrete
choice, before bells and whistles are introduced. This has some deep
problems though; let's start with the "psychological" ones. Human
decisionmaking is an incredibly complex, not always logical process, and
we'll always be losing significant fidelity in boiling it down into a
model. Here though, I'll focus on explaining a handful of breakdowns in
the relationship between reality and the world of Maxdiff models that are
particularly harmful.

First, let's talk about **symmetry**. With the MaxDiff likelihood above,
we're treating the worst as equal and opposite to the best. For example,
though, I don't like Joe Biden as much as I dislike Trump[^5]. I really
really like the best pizza in Brookyln, but New York pizza is all New
York pizza, it only gets so bad. This might be a reasonable
simplification in some cases, but it's hard to argue that baking
this into our model faithfully mirrors reality.

[^5]: President Biden is great, it's just hard to compete with the
    comically malignant and incompetent by being solid and stabilizing.

Also, we're not only asking them to be symmetric, we're sort of
conjoining the best and worst choice, asking them to share the **same utility scale**.
In other words, the factors that make an alternative
more attractive for the best choice are assumed to make it equally less
attractive for the worst choice. An easy example is something like
health risks- "that sushi place gave me food poisoning" is very relevant
to my choice of worst restaurant, but the moment I have to think about
food safety, a restaurant isn't really anywhere relevant on the "best"
side of the spectrum for me. Again, you can probably think of a case
where this is a fine approximation, but in an ideal world, we won't
force ourselves to weld our notions of best to our notions of worst.

Finally, the **error variances** being treated as the same should feel
pretty strange. I'm much, much more consistent in my selection of
"bests" than "worsts"- why would I spent a bunch of time deciding which
opinion of 20 is the absolute worst and which is just 19th worst? People
tend to be much less consistent, and frankly much less engaged, with
their worst choices. Why would we bake this into our model?

Putting this all together, The simplification in estimation that comes
with MaxDiff also leaks into how the model "sees" the decision maker,
and it meaningfully distorts the map in a way that does not necessarily reflect the
territory.

## Mathmatically Incorrect

Beyond calling the model psychologically incoherent, Savage also says
the model is mathematically incorrect. This feels like a slightly
stronger insult, and indeed it is, in the sense that the model is
failing on its own terms.

How? Well, we've explicitly laid out a model with Gumby errors:

![](imgs/gumby_distribution.webp){fig-align="center"}

Wait, no sorry sorry[^6]. we've laid out a model with Gumbel errors
$\epsilon_{ij}$. As a reminder, the Gumbel distribution is chosen for its mathematical convenience, as it leads to a closed-form expression for the choice probabilities in the logit model. the PDF is plotted below:

[^6]: Unlike the blog post about left-handed kangaroos drinking fosters
    I'm working on, this post was not primarily motivated by the urge to
    ask DALL-E to create this.

![](imgs/Gumbel-Density.svg.png){fig-align="center"}

Here's where the trouble starts: like we discussed above, the MaxDiff
model forcibly inserts an element of symmetry into the error
distributions of the choices. However, as you probably already noticed,
the ~~Gumby~~ Gumbel distribution is not a symmetric distribution! This
is fine in the case where we're just reasoning about the best choice,
but even on it's own terms the MaxDiff formulation doesn't quite make
sense.

# The defects are not just theoretical or cosmetic: an illustration

So I've shown some ways I claim that the MaxDiff model of discrete
choice falls short, but how much do they really matter? As someone with
an appreciation for the [agnostic perspective on statistical
modeling](https://www.cambridge.org/core/books/foundations-of-agnostic-statistics/684756357E7E9B3DFF0A8157FB2DCECA)
, I think it's important to not only show that there are theoretical
senses in which a model might be flawed, but further prove that these
technical blemishes harm model performance on metrics we care about.

To do this, let's generate some synthetic data, and fit both the
best-choice and MaxDiff models to it. For this section, I'll essentially
be starting from Jim Savage's best choice model
[here](https://khakieconomics.github.io/2018/12/27/Ranked-random-coefficients-logit.html),
and then add in the "worst choice as negative best choice" logic
afterwards.

Let's start with the synthetic data. If the exact DGP isn't very
exciting to you, feel free to skim or skip this section at first:

<details> 

<summary>**Data Generation for Simulation Study**</summary>


```{R}
library(tidyverse)
library(rstan)
library(ibd)
library(crossdes)
library(combinat)
options(mc.cores = parallel::detectCores())

set.seed(604)

# Again, note that this code is just reproducing https://khakieconomics.github.io/2018/12/27/Ranked-random-coefficients-logit.html,
# although with a more real-world feeling number of respondents and
# and total choices. If this is taking uncomfortably long, feel free to scale
# down I, the results should reproduce just fine.

# Number of individuals
I <- 30
# Number of tasks per individual
Tasks <- 10
# Number of choices per task
J <- 5
# Dimension of covariate matrix
P <- 5
# Dimension of demographic matrix
P2 <- 6

# demographic matrix
W <- matrix(rnorm(I*P2), I, P2)
# Loading matrix
Gamma <- matrix(rnorm(P*P2), P, P2)

# Show W * t(Gamma) to make sure it looks right
W %*% t(Gamma)

# Correlation of decisionmaker random slopes
Omega <- cor(matrix(rnorm(P*(P+2)), P+2, P))

# Scale of decisionmaker random slopes
tau <- abs(rnorm(P, 0, .5))

# Covariance matrix of decisionmaker random slopes
Sigma <- diag(tau) %*% Omega %*% diag(tau)

# Centers of random slopes
beta <- rnorm(P)

# Individual slopes
beta_i <- MASS::mvrnorm(I, beta, Sigma) + W %*% t(Gamma)

# Again, quick plot to sanity check
plot(as.data.frame(beta_i))

# Create X -- let's make this a dummy matrix
X <- matrix(sample(0:1, Tasks*I*J*P, replace = T), Tasks*I*J, P)
# Each of the rows in this matrix correspond to a choice presented to a given individual
# in a given task

indexes <- crossing(individual = 1:I, task = 1:Tasks, option = 1:J) %>% 
  mutate(row = 1:n())

# Write a Gumbel random number generator using inverse CDF trick
rgumbel <- function(n, mu = 0, beta = 1) mu - beta * log(-log(runif(n)))
mean(rgumbel(1e6))

# Ok, now we need to simulate choices. Each person in each task compares each 
# choice according to X*beta_i + epsilon, where epsilon is gumbel distributed. 
# They return their rankings. 

ranked_options <- indexes %>% 
  group_by(individual, task) %>% 
  mutate(fixed_utility = as.numeric(X[row,] %*% as.numeric(beta_i[first(individual),])),
         plus_gumbel_error = fixed_utility + rgumbel(n()),
         rank = rank(-plus_gumbel_error),
         # We're going to use the order rather than the rank in the Stan part of the model
         order = order(rank),
         # And here we create a dummy vector for the best choice
         best_choice = as.numeric(1:n() == which.max(plus_gumbel_error)),
         worst_choice = as.numeric(1:n() == which.min(plus_gumbel_error))
  )


tt <- ranked_options %>% 
  group_by(individual, task) %>%
  summarise(start = min(row), 
            end = max(row)) %>% 
  ungroup %>%
  mutate(task_number = 1:n())
```


</details> 

While this simulation isn't exactly simple, let me pause to point
something out something I've not done. I have not made any choices here
that are designed to make MaxDiff look poor. Since we want to use a
model of realistic complexity, this has simulated structure such that
it'll benefit from the random coefficients formulation I'll use in a
moment. Similarly, this has load-bearing covariates at both the
individual and choice level, since we would almost always have those in
the real world. But the core utility function we've created for
respondents boils down to simulating data under the assumptions of the
logit choice model with Gumbel errors we've been discussing all along.

Now, let's specify the best choice model in Stan[^7]:

<details> 

<summary>**Best Choice Logit Model**</summary>

```{R}
best <- "// saved as mixed_conditional_individual_effects.stan
data {
  int N; // number of rows
  int T; // number of inidvidual-choice sets/task combinations
  int I; // number of Individuals
  int P; // number of covariates that vary by choice
  int P2; // number of covariates that vary by individual
  int K; // number of choices
  
  vector<lower = 0, upper = 1>[N] choice; // binary indicator for choice
  matrix[N, P] X; // choice attributes
  matrix[I, P2] X2; // individual attributes
  
  int task[T]; // index for tasks
  int task_individual[T]; // index for individual
  int start[T]; // the starting observation for each task
  int end[T]; // the ending observation for each task
}
parameters {
  vector[P] beta; // hypermeans of the part-worths
  matrix[P, P2] Gamma; // coefficient matrix on individual attributes
  vector<lower = 0>[P] tau; // diagonal of the part-worth covariance matrix
  matrix[I, P] z; // individual random effects (unscaled)
  cholesky_factor_corr[P] L_Omega; // the cholesky factor of the correlation matrix of tastes/part-worths
}
transformed parameters {
  // here we use the reparameterization discussed on slide 30
  matrix[I, P] beta_individual = rep_matrix(beta', I) + X2 * Gamma' + z*diag_pre_multiply(tau, L_Omega);
}
model {
  // create a temporary holding vector
  vector[N] log_prob;
  
  // priors on the parameters
  tau ~ normal(0, .5);
  beta ~ normal(0, .5);
  to_vector(z) ~ normal(0, 1);
  L_Omega ~ lkj_corr_cholesky(4);
  to_vector(Gamma) ~ normal(0, 1);
  
  // log probabilities of each choice in the dataset
  for(t in 1:T) {
    vector[K] utilities; // tmp vector holding the utilities for the task/individual combination
    // add utility from product attributes with individual part-worths/marginal utilities
    utilities = X[start[t]:end[t]]*beta_individual[task_individual[t]]';
    
    log_prob[start[t]:end[t]] = log_softmax(utilities);
  }
  
  // use the likelihood derivation on slide 29
  target += log_prob' * choice;
}"
```

</details> 

Much of this logic handles the multilevel component and covariates, but the real
key is the last few lines where the likelihood is built. That's where the problem
will arise when we extend the above model into MaxDiff:

<details> 

<summary>**MaxDiff Model**</summary>

```{R}
best_worst <- "// saved as mixed_conditional_individual_effects.stan
data {
  int N; // number of rows
  int T; // number of inidvidual-choice sets/task combinations
  int I; // number of Individuals
  int P; // number of covariates that vary by choice
  int P2; // number of covariates that vary by individual
  int K; // number of choices
  
  vector<lower = 0, upper = 1>[N] choice; // binary indicator for choice
  vector<lower = 0, upper = 1>[N] worst_choice; // binary indicator for worst choice
  matrix[N, P] X; // choice attributes
  matrix[I, P2] X2; // individual attributes
  
  int task[T]; // index for tasks
  int task_individual[T]; // index for individual
  int start[T]; // the starting observation for each task
  int end[T]; // the ending observation for each task
}
parameters {
  vector[P] beta; // hypermeans of the part-worths
  matrix[P, P2] Gamma; // coefficient matrix on individual attributes
  vector<lower = 0>[P] tau; // diagonal of the part-worth covariance matrix
  matrix[I, P] z; // individual random effects (unscaled)
  cholesky_factor_corr[P] L_Omega; // the cholesky factor of the correlation matrix of tastes/part-worths
}
transformed parameters {
  // here we use the reparameterization discussed on slide 30
  matrix[I, P] beta_individual = rep_matrix(beta', I) + X2 * Gamma' + z*diag_pre_multiply(tau, L_Omega);
}
model {
  // create a temporary holding vector
  vector[N] log_prob;
  vector[N] log_prob_worst;
  
  // priors on the parameters
  tau ~ normal(0, .5);
  beta ~ normal(0, .5);
  to_vector(z) ~ normal(0, 1);
  L_Omega ~ lkj_corr_cholesky(4);
  to_vector(Gamma) ~ normal(0, 1);
  
  // log probabilities of each choice in the dataset
  for(t in 1:T) {
    vector[K] utilities; // tmp vector holding the utilities for the task/individual combination
    // add utility from product attributes with individual part-worths/marginal utilities
    utilities = X[start[t]:end[t]]*beta_individual[task_individual[t]]';
    
    log_prob[start[t]:end[t]] = log_softmax(utilities);
    log_prob_worst[start[t]:end[t]] = log_softmax(-utilities);
  }
  
  // use the likelihood derivation on slide 29
  target += log_prob' * choice;
  target += log_prob_worst' * worst_choice;
}"
```

</details> 

Comparing the two, you can definitely see the implementation simplicity MaxDiff
provides; we're essentially just building out `log_prob_worst` and pulling it
into the likelihood. 

So how does this perform? Well, let's fit the two and find out. The next code
block has some plumbing to fit the models and extract results, so feel free
to jump to the results plot below:

<details> 

<summary>**Fit the Two Models, and Gather Predictions to Compare**</summary>

```{R}
data_list_best_choice <-list(N = nrow(X),
                             T = nrow(tt),
                             I = I, 
                             P = P, 
                             P2 = P2, 
                             K = J, 
                             # NOTE!! This is the tricky bit -- we use the order of the ranks (within task)
                             # Not the raw rank orderings. This is how we get the likelihood evaluation to be pretty quick
                             choice = ranked_options$best_choice,
                             X = X, 
                             X2 = W, 
                             task = tt$task_number, 
                             task_individual = tt$individual,
                             start = tt$start, 
                             end = tt$end) 

data_list_best_worst <-list(N = nrow(X),
                             T = nrow(tt),
                             I = I, 
                             P = P, 
                             P2 = P2, 
                             K = J,
                             choice = ranked_options$best_choice,
                             worst_choice = ranked_options$worst_choice,
                             X = X, 
                             X2 = W, 
                             task = tt$task_number, 
                             task_individual = tt$individual,
                             start = tt$start, 
                             end = tt$end) 

compiled_best_choice_model <- stan_model(model_code = best)
compiled_bw_choice_model <- stan_model(model_code = best_worst)

best_choice_fit <- sampling(compiled_best_choice_model, 
                            data = data_list_best_choice, 
                            iter = 800)

best__worst_choice_fit <- sampling(compiled_bw_choice_model, 
                            data = data_list_best_worst, 
                            iter = 800)


# Now make some predictions to compare
best_choice <- as.data.frame(best_choice_fit, pars = "beta_individual") %>%
  gather(Parameter, Value) %>%
  group_by(Parameter) %>%
  summarise(median = median(Value),
            lower = quantile(Value, .05),
            upper = quantile(Value, .95)) %>%
  mutate(individual = str_extract(Parameter, "[0-9]+(?=,)") %>% parse_number,
         column = str_extract(Parameter, ",[0-9]{1,2}") %>% parse_number) %>%
  arrange(individual, column) %>%
  mutate(`True value` = as.numeric(t(beta_i)),
         Dataset = "Best Choice")

best_worst_choice <- as.data.frame(best__worst_choice_fit, pars = "beta_individual") %>%
  gather(Parameter, Value) %>%
  group_by(Parameter) %>%
  summarise(median = median(Value),
            lower = quantile(Value, .05),
            upper = quantile(Value, .95)) %>%
  mutate(individual = str_extract(Parameter, "[0-9]+(?=,)") %>% parse_number,
         column = str_extract(Parameter, ",[0-9]{1,2}") %>% parse_number) %>%
  arrange(individual, column) %>%
  mutate(`True value` = as.numeric(t(beta_i)),
         Dataset = "Best-Worst Choice")

# Combine the datasets
combined_data <- bind_rows(best_choice, best_worst_choice)

```

</details> 

```{R}
# Plot results
ggplot(combined_data, aes(x = `True value`, y = median, color = Dataset)) +
  geom_linerange(aes(ymin = lower, ymax = upper), alpha = 0.3) +
  geom_point(aes(y = median), alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1) +
  labs(y = "Utility Estimates",
       title = "Utility Estimates from the Two Models",
       subtitle = "With interior 90% credibility intervals") +
  scale_color_manual(values = c("Best Choice" = "blue", "Best-Worst Choice" = "red")) +
  facet_wrap(~Dataset) +
  theme(legend.position="none")
```

To be clear, what's plotted here are respondent level utilities for each choice. Concerningly, it doesn't even seem clear that the best-worst choice model
performs better here despite the extra data. The fact that we have to do
this at all doesn't bode well, but let's compare the RMSE's of the predictions
versus the (simulated) truth to confirm:

```{R}
# Is the B/W model actually better?
combined_data %>%
  group_by(Dataset) %>%
  summarize(RMSE = sqrt(mean((`True value` - median)^2)))
```

To say this more emphatically, MaxDiff gets twice as much data from each
respondent, and it's not even consistently the case that the MaxDiff model
is better[^8]! And again, it's not like I chose some obscure DGP where MaxDiff
is uniquely poor, we're working with data that aligns precisely with
the model assumptions.

So to summarize, I've described some psychological and mathematical issues with MaxDiff.
Then, via a comparison to a best choice only model, I showed that these differences
are more than theoretical naggles: for pretty vanilla data, the MaxDiff model
can actually perform worse than the best choice formulation.

It's worth pointing out that we only get this type of precise comparison in a
simulation study. On real world survey data with a similar DGP, you'd just see
the models give fairly similar answers, with no reason to suspect the slight
differences might actually be bad tweaks on MaxDiff's part. Modeling only real
world data is how many market researchers approach their work, so it should be
no surprise that MaxDiff continues to be accepted in the field. With simulation,
however, we can look closer, and the flaws I've discussed become increasingly
apparent.

[^7]: I won't re-explain the details of the model here, since the original source already
does a fantastic job. If you haven't already read the original post series, it's
well worth taking the time to do so now, as there are a lot of moving pieces
here- setting sensible priors for this model, incorporating covariates logically,
and setting this up to run efficiently in Stan.

[^8]: I will note that if you're following along at home, sampling variability
means that for some realizations of this DGP + these models at lowe N, MaxDiff performs
just slightly better. But "barely better in a fraction of runs" isn't really a good outcome from
twice as much data.

# Alternative #1: rank-ordered logits with connected graphs of choices

Ok, so you get it. MaxDiff isn't great. But respondents are still expensive,
and you still have a choice modeling task to complete at work, and you really,
really want to do better than just using the best choices because your boss
thinks Sawtooth is fine since it's been used for a long time. What's the alternative?

In this section, I'll lay out what we'll call the rank-ordered logit with connected
choices. If this sounds complicated, don't worry; the model isn't actually going to get much
more complicated.

Instead, the plan is mostly to be cleverer about how we build our choice sets
in the first place.

## Setting up the Choice Sets

What we really want, I'd argue, is full choice ranking data, for each respondent.
But of course, people rapidly become unhappy and start really satisficing when
they are forced to rank 20 different largely similar options at the same time. How
can we get the simplicity from the respondent POV of MaxDiff, but the rich
ranking data we want?

The clever trick here is that by asking the right 3 item choice sets, and asking
the survey taker to pick best and worst, we can collect every choice needed to
construct a full ranking[^8]. We'll then be able to simply fit a rank-ordered
logit model to these data, which makes far fewer simplifying assumptions about
how the choices and their utilities interact.

There are some downsides I'll be transparent about here: for $K$ choices, you
need each respondent to do $K-1$ choice tasks. If you have 20 options, this
can get tiring fast, and you won't really be gathering "extra" opinions per
respondent anymore by doing the minimum $K-1$ rounds. Here's a hand worked
example if you'd like it for intuition:

<details> 

<summary>**Illustration: K-1 Comparisons are Needed**</summary>

Let's say we want to get someone's ranking over 2024 presidential candidate
choices, from the set {Biden,Trump,RFK,Stein}.

Choice Set 1:

Options: {Biden, Stein, RFK}
Best choice: Biden
Worst choice: RFK

Choice Set 2:

Options: {Stein, RFK, Trump}
Best choice: Stein
Worst choice: Trump

Choice Set 3:

Options: {Biden, Stein, Trump}
Best choice: Biden
Worst choice: Trump

Now, let's reconstruct the full ranking from these choice sets:

From Choice Set 1, we know that Biden > Stein > RFK.
From Choice Set 2, we know that Stein > RFK > Trump.
From Choice Set 3, we know that Biden > Stein > Trump.

Combining these partial rankings, we can infer the full ranking:
Biden > Stein > RFK > Trump

</details> 

You'll notice that you can't get the full ranking for e.g. 4 options in any less than 3 sets,
but you well could add more sets to provide some redundancy and statistical
efficiency if you so choose. Of course, in larger choice sets, you'll probably
end up closer to K-1 sets given respondent fatigue, but for smaller sets it's
a very helpful option to know exists.

Most of the innovation here was in the choice set design, but there's some in
the model too, so let's turn to that:

<details> 

<summary>**Model 3: Rank-Ordered Logit for Connected Graphs**</summary>

```{R}
ranked <- "// saved as ranked_rcl.stan
functions {
  real rank_logit_lpmf(int[] rank_order, vector delta) {
    // We reorder the raw utilities so that the first rank is first, second rank second... 
    vector[rows(delta)] tmp = delta[rank_order];
    real out;
    // ... and sequentially take the log of the first element of the softmax applied to the remaining
    // unranked elements.
    for(i in 1:(rows(tmp) - 1)) {
      if(i == 1) {
        out = tmp[1] - log_sum_exp(tmp);
      } else {
        out += tmp[i] - log_sum_exp(tmp[i:]);
      }
    }
    // And return the log likelihood of observing that ranking
    return(out);
  }
}
data {
  int N; // number of rows
  int T; // number of inidvidual-choice sets/task combinations
  int I; // number of Individuals
  int P; // number of covariates that vary by choice
  int P2; // number of covariates that vary by individual
  int K; // number of choices
  
  int rank_order[N]; // The vector describing the index (within each task) of the first, second, third, ... choices. 
  // In R, this is order(-utility) within each task
  matrix[N, P] X; // choice attributes
  matrix[I, P2] X2; // individual attributes
  
  int task[T]; // index for tasks
  int task_individual[T]; // index for individual
  int start[T]; // the starting observation for each task
  int end[T]; // the ending observation for each task
}
parameters {
  vector[P] beta; // hypermeans of the part-worths
  matrix[P, P2] Gamma; // coefficient matrix on individual attributes
  vector<lower = 0>[P] tau; // diagonal of the part-worth covariance matrix
  matrix[I, P] z; // individual random effects (unscaled)
  cholesky_factor_corr[P] L_Omega; // the cholesky factor of the correlation matrix of tastes/part-worths
}
transformed parameters {
  // here we use the reparameterization discussed on slide 30
  matrix[I, P] beta_individual = rep_matrix(beta', I) + X2 * Gamma' + z * diag_pre_multiply(tau, L_Omega);
}
model {
  // priors on the parameters
  tau ~ normal(0, .5);
  beta ~ normal(0, 1);
  to_vector(z) ~ normal(0, 1);
  L_Omega ~ lkj_corr_cholesky(4);
  to_vector(Gamma) ~ normal(0, 1);
  
  // log probabilities of each choice in the dataset
  for(t in 1:T) {
    vector[K] utilities; // tmp vector holding the utilities for the task/individual combination
    // add utility from product attributes with individual part-worths/marginal utilities
    utilities = X[start[t]:end[t]]*beta_individual[task_individual[t]]';
    rank_order[start[t]:end[t]] ~ rank_logit(utilities);
  }
}"
```

</details> 

Take a second to reason through the above code; we're able to use all of the information
a full ranking provides to us, without having to force any interrelationship
between best and worst to exist that isn't truly there. Also, notice that there's
nothing here specifically for the "connected graph of choices" part of model- that's all
in the choice set generation, which we'll do next.

## Building the choice sets

Let's try making the choice sets the new way, and then compare models again.

To keep things simple, we'll generate our graph of options as a
[balanced incomplete block design](https://online.stat.psu.edu/stat503/lesson/4/4.7).
This is a quick way to get the connectedness property we find desirable along with
a reasonably statistically efficient design. It's worth noting though that the
literature on designing maximally efficient discrete choice experiments is
deep, and has some beautiful connections to the broader experimental design
literature; here's a footnote if you're the sort of person who finds that exciting[^9].

[^9]: A great place to start reading about efficient design of discrete choice experiments
is the literature review section of
[Traets et al, 2020](https://www.jstatsoft.org/article/view/v096i03). For more on the
desirable properties of designs like the BIBD I actually chose, Raghavarao's 1988 book
**Constructions and Combinatorial Problems in Design of Experiments** is wonderful.
Part of what I find appealing about this broader literature is that results not
only from the 80s, but stretching back a century or more remain relevant for
building good choice experiments. While the last decade or so has seen numerous new
techniques pop up that squeeze out additional bits of accuracy in the right conditions,
the core challenge remains interwoven with the combinatorics of
experimental design that has been recognized by the earliest statisticians.


<details>

<summary>**Updated Data Generation for Connected Graph Illustration**</summary>

```{R}
# Number of individuals
I <- 30
# Number of tasks per individual
Tasks <- 10
# Number of choices per task
J <- 3
# Dimension of covariate matrix
P <- 5
# Dimension of demographic matrix
P2 <- 6

# demographic matrix
W <- matrix(rnorm(I*P2), I, P2)
# Loading matrix
Gamma <- matrix(rnorm(P*P2), P, P2)

# Show W * t(Gamma) to make sure it looks right
W %*% t(Gamma)

# Correlation of decisionmaker random slopes
Omega <- cor(matrix(rnorm(P*(P+2)), P+2, P))

# Scale of decisionmaker random slopes
tau <- abs(rnorm(P, 0, .5))

# Covariance matrix of decisionmaker random slopes
Sigma <- diag(tau) %*% Omega %*% diag(tau)

# Centers of random slopes
beta <- rnorm(P)

# Individual slopes
beta_i <- MASS::mvrnorm(I, beta, Sigma) + W %*% t(Gamma)

# Again, quick plot to sanity check
plot(as.data.frame(beta_i))

bibd <- find.BIB(trt = Tasks, b = I*Tasks, k = J)

# Checks if the design is balanced wrt to both rows and columns.
# The above function is not guaranteed to produce a valid BIBD; it may produce
# a close but imperfect design, which is sufficient for our purposes. I will
# however confirm it's connected, since we'd start to lose quite a lot of efficiency
# if it were not. 

# Returns 1 if the design is connected
is.connected(bibd)


# Create a mapping from BIBD options to attribute levels; idea is to make
# identically shaped inputs to our previous test, but using the conditions
# from the BIBD instead of sampling randomly for X.
option_attributes <- matrix(sample(0:1, 10*P, replace = TRUE), nrow = 10, ncol = P)

# Function to convert BIBD task to attribute matrix
bibd_to_attributes <- function(bibd_row) {
  matrix(c(
    option_attributes[bibd_row[1], ],
    option_attributes[bibd_row[2], ],
    option_attributes[bibd_row[3], ]
  ), nrow = 3, byrow = TRUE)
}

# Generate X matrix based on BIBD
X <- do.call(rbind, lapply(1:nrow(bibd), function(i) bibd_to_attributes(bibd[i,])))

indexes <- crossing(individual = 1:I, task = 1:Tasks, option = 1:J) %>% 
  mutate(row = 1:n())

# Write a Gumbel random number generator using inverse CDF trick
rgumbel <- function(n, mu = 0, beta = 1) mu - beta * log(-log(runif(n)))
mean(rgumbel(1e6))

# Ok, now we need to simulate choices. Each person in each task compares each 
# choice according to X*beta_i + epsilon, where epsilon is gumbel distributed. 
# They return their rankings. 

ranked_options <- indexes %>% 
  group_by(individual, task) %>% 
  mutate(fixed_utility = as.numeric(X[row,] %*% as.numeric(beta_i[first(individual),])),
         plus_gumbel_error = fixed_utility + rgumbel(n()),
         rank = rank(-plus_gumbel_error),
         # We're going to use the order rather than the rank in the Stan part of the model
         order = order(rank),
         # And here we create a dummy vector for the best choice
         best_choice = as.numeric(1:n() == which.max(plus_gumbel_error)),
         worst_choice = as.numeric(1:n() == which.min(plus_gumbel_error))
  )


tt <- ranked_options %>% 
  group_by(individual, task) %>%
  summarise(start = min(row), 
            end = max(row)) %>% 
  ungroup %>%
  mutate(task_number = 1:n())
```
</details>

Now we have simulated data that represents a connected graph design with 10 total choices, and 15 choice sets per respondent. Each choice set contains 3 options, and each pair of choices appears together in at least one set across all respondents.

## Evaluating our new model

So now we've introduced the ranked choice logit model, and simulated data that
fits the connected graph structure. How do the three models perform?

Again, some data plumbing in the drop down below, so feel free to jump to the
results plots after it.

<details>

<summary>**Comparing ROL to Best Choice, and MaxDiff models**</summary>

```{R}
data_list_best_choice <-list(N = nrow(X),
                             T = nrow(tt),
                             I = I, 
                             P = P, 
                             P2 = P2, 
                             K = J, 
                             # NOTE!! This is the tricky bit -- we use the order of the ranks (within task)
                             # Not the raw rank orderings. This is how we get the likelihood evaluation to be pretty quick
                             choice = ranked_options$best_choice,
                             X = X, 
                             X2 = W, 
                             task = tt$task_number, 
                             task_individual = tt$individual,
                             start = tt$start, 
                             end = tt$end)

data_list_best_worst <-list(N = nrow(X),
                            T = nrow(tt),
                            I = I, 
                            P = P, 
                            P2 = P2, 
                            K = J,
                            choice = ranked_options$best_choice,
                            worst_choice = ranked_options$worst_choice,
                            X = X, 
                            X2 = W, 
                            task = tt$task_number, 
                            task_individual = tt$individual,
                            start = tt$start, 
                            end = tt$end)

data_list_ranked_rcl <- list(N = nrow(X),
                             T = nrow(tt),
                             I = I, 
                             P = P, 
                             P2 = P2, 
                             K = J, 
                             # NOTE!! This is the tricky bit -- we use the order of the ranks (within task)
                             # Not the raw rank orderings. This is how we get the likelihood evaluation to be pretty quick
                             rank_order = ranked_options$order,
                             X = X, 
                             X2 = W, 
                             task = tt$task_number, 
                             task_individual = tt$individual,
                             start = tt$start, 
                             end = tt$end)

compiled_best_choice_model <- stan_model(model_code = best)
best_choice_fit <- sampling(compiled_best_choice_model, 
                            data = data_list_best_choice, 
                            iter = 800)

# Compile and fit MaxDiff model
compiled_maxdiff_model <- stan_model(model_code = best_worst)
maxdiff_fit <- sampling(compiled_maxdiff_model,
                        data = data_list_best_worst,
                        iter = 800)

# Compile and fit ROL model
compiled_rol_model <- stan_model(model_code = ranked)
rol_fit <- sampling(compiled_rol_model,
                    data = data_list_ranked_rcl,
                    iter = 800)

best_choice <- as.data.frame(best_choice_fit, pars = "beta_individual") %>%
  gather(Parameter, Value) %>%
  group_by(Parameter) %>%
  summarise(median = median(Value),
            lower = quantile(Value, .05),
            upper = quantile(Value, .95)) %>%
  mutate(individual = str_extract(Parameter, "[0-9]+(?=,)") %>% parse_number,
         column = str_extract(Parameter, ",[0-9]{1,2}") %>% parse_number) %>%
  arrange(individual, column) %>%
  mutate(`True value` = as.numeric(t(beta_i)),
         Dataset = "Best Choice")

# Process MaxDiff results
maxdiff_choice <- as.data.frame(maxdiff_fit, pars = "beta_individual") %>%
  gather(Parameter, Value) %>%
  group_by(Parameter) %>%
  summarise(median = median(Value),
            lower = quantile(Value, .05),
            upper = quantile(Value, .95)) %>%
  mutate(individual = str_extract(Parameter, "[0-9]+(?=,)") %>% parse_number,
         column = str_extract(Parameter, ",[0-9]{1,2}") %>% parse_number) %>%
  arrange(individual, column) %>%
  mutate(`True value` = as.numeric(t(beta_i)),
         Dataset = "MaxDiff")

# Process ROL results
rol_choice <- as.data.frame(rol_fit, pars = "beta_individual") %>%
  gather(Parameter, Value) %>%
  group_by(Parameter) %>%
  summarise(median = median(Value),
            lower = quantile(Value, .05),
            upper = quantile(Value, .95)) %>%
  mutate(individual = str_extract(Parameter, "[0-9]+(?=,)") %>% parse_number,
         column = str_extract(Parameter, ",[0-9]{1,2}") %>% parse_number) %>%
  arrange(individual, column) %>%
  mutate(`True value` = as.numeric(t(beta_i)),
         Dataset = "Rank-Ordered Logit")

# Combine all results
all_results <- bind_rows(best_choice, maxdiff_choice, rol_choice)
```

</details>

```{R}
ggplot(all_results, aes(x = `True value`, y = median, color = Dataset)) +
  geom_linerange(aes(ymin = lower, ymax = upper), alpha = 0.3) +
  geom_point(aes(y = median), alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1) +
  labs(y = "Utility Estimates",
       title = "Utility Estimates from the Three Models",
       subtitle = "With interior 90% credibility intervals") +
  scale_color_manual(values = c("Best Choice" = "blue", "MaxDiff" = "red", "Rank-Ordered Logit" = "green")) +
  facet_wrap(~Dataset) +
  theme(legend.position="bottom")
```

At a squint, Rank-Ordered Logit > Best Choice > MaxDiff as I've argued. Notice though that
unlike the MaxDiff model, both Best Choice and Rank-Ordered Logit look unbiased;
the ROL model simply seems more efficient. Let's check the RMSE to confirm

```{R}
all_results %>%
  group_by(Dataset) %>%
  summarize(RMSE = sqrt(mean((`True value` - median)^2)))
```

The ordering is as we'd expect from the plots. 

If I have free choice over the design of a choice experiment, this sort of
connected graph, analyzed with the rank-ordered logit is what I'd generally
prefer to use. Designing the choice graphs to make this work well isn't
particularly hard with modern software, and the rank ordered logit model
is widely available, not just in a sophisticated Stan model like I cribbed
from Jim Savage here, but also in [Stata](https://www.stata.com/manuals13/rrologit.pdf),
and other commonly used packages.

Hopefully the theoretical discussion and simulation study above are persuasive:
you can and should aim to do better than the Maxdiff model of discrete choice.

# Alternative #2: a backup plan- rank-ordered logits with unknown middle preferences

The connected design I've advocated above isn't strictly necessary since I've used
a mixed logit, and the multilevel component can help estimate when cases are
disconnected. Of course though, having a completely connected graph within
individual respondents helps a lot for statistical efficiency.

That raises an interesting question: what to do if you already ran an experiment
designed without connectedness in mind, or your survey platform won't easily
allow you to make a connected design? 

It's not my preferred choice if I have control over design, but in this scenario
we can use a Rank-Ordered Logit with unknown middle preferences.

## Allowing ties in Rank-Ordered Logits

Let's reuse the example above where we ranked my preferences over 2024 presidential candidates. The complete ordering for me is:
$$
\textcolor{blue}{\text{Biden}} > \textcolor{green}{\text{Stein}} > \textcolor{yellow}{\text{RFK}} > \textcolor{red}{\text{Trump}}
$$
If we collected data from me though by only asking my best and worse choices, we'd have:
$$
\textcolor{blue}{\text{Biden}} > ??? > ??? > \textcolor{red}{\text{Trump}}
$$
To keep using a rank-ordered model here, we'd need to figure out how to reason about the middle two options being unknown. Allison and Christakis, 1994 discuss various ways to handle this, but the one that seems most promising and that we'll discuss operates as if the true ranking exists, but is unknown to the modeler. That seems pretty reasonable, since we're not saying the middle preferences have to be strong, just that people likely aren't totally ambivalent between the two in general.
To handle this, we'll first enumerate the possible ways the middle options could pan out. Here's there's just two:
$$
\textcolor{blue}{\text{Biden}} > \textcolor{green}{\text{Stein}} > \textcolor{yellow}{\text{RFK}} > \textcolor{red}{\text{Trump}} \linebreak
\textcolor{blue}{\text{Biden}} > \textcolor{yellow}{\text{RFK}} > \textcolor{green}{\text{Stein}} > \textcolor{red}{\text{Trump}}
$$
Then, to construct a likelihood, we'll marginalize over all the possible middle options. Since the two ways the middle options could shake out are mutually exclusive, that implies:
$$
P(\textcolor{blue}{B}>\textcolor{green}{S}>\textcolor{yellow}{R}>\textcolor{red}{T} \text{ or } \textcolor{blue}{B}>\textcolor{yellow}{R}>\textcolor{green}{S}>\textcolor{red}{T}) = P(\textcolor{blue}{B}>\textcolor{green}{S}>\textcolor{yellow}{R}>\textcolor{red}{T}) + P(\textcolor{blue}{B}>\textcolor{yellow}{R}>\textcolor{green}{S}>\textcolor{red}{T})
$$
Now, let's express this using the notation from Allison and Christakis (1994), since it'll make juggling all the subscripts a bit easier. For a given individual $i$, let $U_{ij}$ be the utility of alternative $j$. The probability of observing a particular complete ranking $r$ (where $r$ is one of the possible rankings consistent with the observed partial ranking) is:
$$
P(r) = \prod_{k=1}^{J-1} \frac{\exp(U_{ir_k})}{\sum_{l=k}^J \exp(U_{ir_l})}
$$
where $r_k$ is the alternative ranked $k$th in ranking $r$, and $J$ is the total number of alternatives.
For our example with unknown middle preferences, the likelihood becomes:
$$
L(\textcolor{blue}{B}>\textcolor{green}{S},\textcolor{yellow}{R}>\textcolor{red}{T}) = \frac{\exp(U_{i\textcolor{blue}{B}})}{\sum_{j \in {\textcolor{blue}{B},\textcolor{green}{S},\textcolor{yellow}{R},\textcolor{red}{T}}} \exp(U_{ij})} \cdot \left[ \frac{\exp(U_{i\textcolor{green}{S}})}{\exp(U_{i\textcolor{green}{S}}) + \exp(U_{i\textcolor{yellow}{R}}) + \exp(U_{i\textcolor{red}{T}})} + \frac{\exp(U_{i\textcolor{yellow}{R}})}{\exp(U_{i\textcolor{green}{S}}) + \exp(U_{i\textcolor{yellow}{R}}) + \exp(U_{i\textcolor{red}{T}})} \right] \cdot \frac{\exp(U_{i\textcolor{red}{T}})}{\exp(U_{i\textcolor{red}{T}})}
$$
The central terms are the key to the unknown ranking model[^11]. This lets
us fit a ranked model and avoid MaxDiff, even with unknown middle options.

[^11]: Just for fun, notice what happens if there's only 1 middle term. It's your
friendly neighborhood complete rank-ordered logit, since there's only 1 way
1 option can occur!

Of course, this approach isn't without issues. Obviously, with a bit more foresight
on design we could've learned more from the middle options while still asking respondents
to make two choices per task. Putting that aside, the number of unknown middle
options grows as $n_{unknown}!$, which makes calculating all the likelihood terms
a real, real pain in the ass. If we re-do our initial example with 10 total alternatives,
the middle 8 will be unknown, and there will be 40,320 permutations. I'll
also show an approximation for the unknown middle options in a moment, 

## Building the ROL with Unknown Middle Preferences in Stan

I'll present two versions of the ROL with unknown ties model in Stan. First,
the full marginal likelihood model above, and then the Efron approximation
you can use when the number of unknown middle options. We'll build towards
comparing MaxDiff, ROL with known middle options, and the two ROL with unknown
middle options all.

To make it easier to follow logic for many middle options, we'll use
$J = 5$[^10]. The initial block below just sets up our DGP again with that $J$,
so feel free to skip over it.

[^10]: To be transparent, note that J = 5 isn't a totally neutral comparison
ground between the full marginal likelihood model and the Efron approximation. The
Efron will get somewhat worse as the number of middle options grows, but of
course better an approximation that finishes in a useful amount of time than
a correct model that doesn't.

<details>

<summary>**Setting up Sim Study with J = 5**</summary>

```{R}
# Number of individuals
I <- 30
# Number of tasks per individual
Tasks <- 10
# Number of choices per task
J <- 5
# Dimension of covariate matrix
P <- 5
# Dimension of demographic matrix
P2 <- 6

# demographic matrix
W <- matrix(rnorm(I*P2), I, P2)
# Loading matrix
Gamma <- matrix(rnorm(P*P2), P, P2)

# Show W * t(Gamma) to make sure it looks right
W %*% t(Gamma)

# Correlation of decisionmaker random slopes
Omega <- cor(matrix(rnorm(P*(P+2)), P+2, P))

# Scale of decisionmaker random slopes
tau <- abs(rnorm(P, 0, .5))

# Covariance matrix of decisionmaker random slopes
Sigma <- diag(tau) %*% Omega %*% diag(tau)

# Centers of random slopes
beta <- rnorm(P)

# Individual slopes
beta_i <- MASS::mvrnorm(I, beta, Sigma) + W %*% t(Gamma)

# Again, quick plot to sanity check
plot(as.data.frame(beta_i))

# Create X -- let's make this a dummy matrix
X <- matrix(sample(0:1, Tasks*I*J*P, replace = T), Tasks*I*J, P)
# Each of the rows in this matrix correspond to a choice presented to a given individual
# in a given task

indexes <- crossing(individual = 1:I, task = 1:Tasks, option = 1:J) %>% 
  mutate(row = 1:n())

# Write a Gumbel random number generator using inverse CDF trick
rgumbel <- function(n, mu = 0, beta = 1) mu - beta * log(-log(runif(n)))
mean(rgumbel(1e6))

# Ok, now we need to simulate choices. Each person in each task compares each 
# choice according to X*beta_i + epsilon, where epsilon is gumbel distributed. 
# They return their rankings. 

# Modify the ranked_options data frame to include all necessary information
ranked_options <- indexes %>% 
  group_by(individual, task) %>% 
  mutate(
    fixed_utility = as.numeric(X[row,] %*% as.numeric(beta_i[first(individual),])),
    plus_gumbel_error = fixed_utility + rgumbel(n()),
    true_rank = rank(-plus_gumbel_error),
    true_order = order(true_rank),
    observed_order = case_when(
      true_order == J ~ J,  # Worst choice
      true_order == 1 ~ 1,  # Best choice
      TRUE ~ 3  # Tie all middle orders
    ),
    best_choice = as.numeric(true_order == 1),
    worst_choice = as.numeric(true_order == J)
  )

tt <- ranked_options %>% 
  group_by(individual, task) %>%
  summarise(start = min(row), 
            end = max(row)) %>% 
  ungroup %>%
  mutate(task_number = 1:n())
```

</details>

Ok, onto new material; we'll generate a "permutation matrix" of all the ways
the $J - 2$ middle options can occur, which we'll iterate over to calculate all
the possible likelihoods.

For example, here with $J = 5$, the permutation matrix will have the following
6 permutations:

```{R}
n_tied <- J - 2  
permutations <- permn(n_tied)
permutation_matrix <- do.call(rbind, permutations)
n_permutations <- nrow(permutation_matrix)

print(permutation_matrix)

```

Calculating these outside R is convenient in two senses. First, it's more efficient-
Stan isn't really designed or equipped to work with permutations like this as they get more
involved.
Second, it can be helpful for intuition to mentally step through this matrix of permutations
and consider the calculation needed for each entry, imagining
it growing rapidly with the number of unknown middle options.

Here's the stan input data and model:

<details>

<summary>**ROL with unknown middle preferences**</summary>



```{R}
stan_data_rol_ties <- list(
  N = nrow(X),
  T = nrow(tt),
  I = I, 
  P = P, 
  P2 = P2, 
  K = J, 
  rank_order = ranked_options$observed_order,
  X = X, 
  X2 = W, 
  task = tt$task_number, 
  task_individual = tt$individual,
  start = tt$start, 
  end = tt$end,
  n_tied = n_tied,
  permutations = permutation_matrix,
  n_permutations = n_permutations
)

rol_with_uknown_middle_options <- "
functions {
  real rank_logit_ties_lpmf(int[] y, vector delta, int[,] permutations, int n_permutations, int n_tied) {
    int K = rows(delta);
    vector[K] sorted_delta = delta[y];
    real out = 0;
    
    // Handle known best
    out += sorted_delta[1] - log_sum_exp(sorted_delta);
    
    // Compute normalizing factor once
    real normalizing_factor = log_sum_exp(sorted_delta[2:K]);
    
    // Handle tied middle ranks
    real perm_sum = 0;
    for (p in 1:n_permutations) {
      real perm_ll = 0;
      for (i in 1:n_tied) {
        int idx = permutations[p, i];
        perm_ll += sorted_delta[1+idx];
      }
      perm_sum += exp(perm_ll - n_tied * normalizing_factor);
    }
    out += log(perm_sum / n_permutations);
    
    return out;
  }
}

data {
  int N; // number of rows
  int T; // number of individual-choice sets/task combinations
  int I; // number of Individuals
  int P; // number of covariates that vary by choice
  int P2; // number of covariates that vary by individual
  int K; // number of choices
  
  int rank_order[N]; // The vector describing the index (within each task) of the first, second, third, ... choices. 
  matrix[N, P] X; // choice attributes
  matrix[I, P2] X2; // individual attributes
  
  int task[T]; // index for tasks
  int task_individual[T]; // index for individual
  int start[T]; // the starting observation for each task
  int end[T]; // the ending observation for each task
  
  int<lower=2> n_tied;
  int<lower=1> n_permutations;
  int permutations[n_permutations, n_tied];
}

parameters {
  vector[P] beta; // hypermeans of the part-worths
  matrix[P, P2] Gamma; // coefficient matrix on individual attributes
  vector<lower = 0>[P] tau; // diagonal of the part-worth covariance matrix
  matrix[I, P] z; // individual random effects (unscaled)
  cholesky_factor_corr[P] L_Omega; // the cholesky factor of the correlation matrix of tastes/part-worths
}

transformed parameters {
  matrix[I, P] beta_individual = rep_matrix(beta', I) + X2 * Gamma' + z * diag_pre_multiply(tau, L_Omega);
}

model {
  tau ~ normal(0, .5);
  beta ~ normal(0, 1);
  to_vector(z) ~ normal(0, 1);
  L_Omega ~ lkj_corr_cholesky(4);
  to_vector(Gamma) ~ normal(0, 1);

  for(t in 1:T) {
    vector[K] utilities;
    utilities = X[start[t]:end[t]] * beta_individual[task_individual[t]]';
    rank_order[start[t]:end[t]] ~ rank_logit_ties(utilities, permutations, n_permutations, n_tied);
  }
}
"
```

</details>

Summing up all the possible middle options should be fairly transparent, but if you're interested
in the `log_sum_exp` tricks I'm using to speed this up, see this footnote[^12].

[^12]: Woo optimization gang! Ok, so the idea here is similar to this block in 
the ROL without ties model, where at each choice we normalize by subtracting out ` out += tmp[i] - log_sum_exp(tmp[i:])`,
but it has to function differently given the unknowns. More specifically, the right
normalizing factor for all the middle $J -2$ options is `log_sum_exp(sorted_delta[2:K])` for
each possible *permutation*, not each possible middle option. So as the number
of permutations grows, we can save a bunch of compute mostly through pre-computing
the normalizing factor, but also by doing `n_tied * normalizing_factor` instead of
subtracting `n_tied` times per permutation.

To reinforce the intuition here, this is choosing to handles unknown middle options
with maximum precision given we know very little about them, at (usually) great computational cost. Before
we compare this to MaxDiff, and see if it is meaningfully more accurate with the same data,
I want to also introduce an approximation to this full marginal likelihood to throw
into the comparison as well.

## N! is pretty bad; is there an approximation?

As we've seen, calculating all possible orderings for unknown middle preferences can quickly become computationally intensive as the number of choices increases. Fortunately, there's a clever approximation that can help us out.
This approximation comes to us via a deep connection between discrete choice models and survival analysis. To gesture at the analogy here- tied times in survival models function like our unknown middle options, and the survival analysis literature has decades of experience handling this type of tie given how common they are in survival data [^11]. We'll look at my favorite of these, the Efron approximation, originally developed for tied times in Cox proportional-hazards models.

Mathematically, the Efron approximation for our rank-ordered logit with unknown middle preferences looks like this:

[^11]: See the Allison/Christakis paper above for a more fleshed out version of this comparison. It's very elegant,
but I don't think there's a way to do that analogy justice without a massive detour in this post. Similarly, I'd
love to walk through the reasoning that leads to the Efron approximation and how it compares to competitors, but this post is
already pretty long.

$$
L(\beta) = \prod_{i=1}^N \frac{\exp(\beta'X_{i(1)})}{\sum_{j \in C_i} \exp(\beta'X_j)} \cdot \prod_{k=2}^{J-2} \frac{\exp(\beta'X_{i(k)})}{\sum_{j \in C_i} \exp(\beta'X_j) - \frac{k-1}{J-2} \sum_{l \in M_i} \exp(\beta'X_l)}
$$

Where $C_i$ is the choice set at decision $i$, $T_i$ is the set of tied alternatives, and $t_i$ is the number of tied alternatives. I recognize this is super dense, and that grokking it will require a bit of reading that's outside the scope of this post; here's a footnote with some helpful references[^13].

[^13]: I really appreciate [this set of slides](https://myweb.uiowa.edu/pbreheny/7210/f15/notes/11-5.pdf) for pumping intuitions about different approaches to approximating the middle options. These [course notes](https://public.websites.umich.edu/~yili/lect4notes.pdf) are also super helpful, especially if you want to see the Cox proportional hazards model discussed more in depth before you try to understand the tie handling in their terms. Finally of course, the [original paper](https://www.jstor.org/stable/2286217) is insightful, but very very dense.

Intuitively, this approximation attempts to more continuously approximate the full marginal likelihood we just discussed without calculating every possible ranking. It does this by adjusting the denominator of the likelihood to account for the tied middle alternatives in a stepwise fashion. This approach recognizes that while we know the best and worst choices, the middle alternatives are essentially competing with each other in a way that's more nuanced than simply treating them as fully ranked or completely tied.

Here's the code implementation, which to me gives a lot more intuition than the equation above:

<details>

<summary>**ROL with Efron Approximation of Unknown Middle Options**</summary>

```{R}
efron_model <- "
functions {
  real rank_logit_efron_lpmf(int[] y, vector delta, int n_tied) {
    int K = rows(delta);
    vector[K] sorted_delta = delta[y];
    real out = 0;
    
    // Handle known best
    out += sorted_delta[1] - log_sum_exp(sorted_delta);
    
    // Handle tied middle ranks using Efron approximation
    real tied_sum = log_sum_exp(sorted_delta[2:(K-1)]);  // Changed to log_sum_exp
    real normalizing_factor = log_sum_exp(sorted_delta[2:K]);
    for (i in 1:n_tied) {
      real d = (i - 1.0) / n_tied;
      real adjusted_factor = log_diff_exp(normalizing_factor, log(d) + tied_sum);
      out += sorted_delta[1+i] - adjusted_factor;
    }
    
    // Handle known worst
    // out += sorted_delta[K] - log_sum_exp(sorted_delta[2:K]);
    
    return out;
  }
}

data {
  int N; // number of rows
  int T; // number of individual-choice sets/task combinations
  int I; // number of Individuals
  int P; // number of covariates that vary by choice
  int P2; // number of covariates that vary by individual
  int K; // number of choices
  
  int rank_order[N]; // The vector describing the index (within each task) of the first, second, third, ... choices. 
  matrix[N, P] X; // choice attributes
  matrix[I, P2] X2; // individual attributes
  
  int task[T]; // index for tasks
  int task_individual[T]; // index for individual
  int start[T]; // the starting observation for each task
  int end[T]; // the ending observation for each task
  
  int<lower=2> n_tied; // number of tied ranks
}

parameters {
  vector[P] beta; // hypermeans of the part-worths
  matrix[P, P2] Gamma; // coefficient matrix on individual attributes
  vector<lower = 0>[P] tau; // diagonal of the part-worth covariance matrix
  matrix[I, P] z; // individual random effects (unscaled)
  cholesky_factor_corr[P] L_Omega; // the cholesky factor of the correlation matrix of tastes/part-worths
}

transformed parameters {
  matrix[I, P] beta_individual = rep_matrix(beta', I) + X2 * Gamma' + z * diag_pre_multiply(tau, L_Omega);
}

model {
  tau ~ normal(0, .5);
  beta ~ normal(0, 1);
  to_vector(z) ~ normal(0, 1);
  L_Omega ~ lkj_corr_cholesky(4);
  to_vector(Gamma) ~ normal(0, 1);

  for(t in 1:T) {
    vector[K] utilities;
    utilities = X[start[t]:end[t]] * beta_individual[task_individual[t]]';
    rank_order[start[t]:end[t]] ~ rank_logit_efron(utilities, n_tied);
  }
}
"
```

</details>

The beauty of the Efron approximation is that it works remarkably well in practice compared to alternatives. As discussed in Hertz-Picciotto and Rockhill (1997) and elsewhere, the Efron method tends to outperform other approximations, particularly when dealing with moderate to heavy ties (read: unknown middle ranks).

## Evaluating ROL with Unknown Middle Preferences against MaxDiff and ROL

So now we've got 3 candidate models for best/worst data- MaxDiff, ROL with
ROL using the Efron approximation
for handling the middle options. I'll also add in the ROL without ties in the comparison
below to give a sense of the information lost when just collecting best and worst
choices, though of course my main goal is compare the quality of the 3 estimators
on best/worst data.

The next chunk just sets up the comparison between the four models on data with $J = 5$,
so feel free to jump to the summary plot after it.

<details>

<summary>**4 way Comparison Setup**</summary>

```{R}
# 1. Setup and compile models
compiled_maxdiff_model <- stan_model(model_code = best_worst)
compiled_rol_ties_model <- stan_model(model_code = rol_with_uknown_middle_options)
compiled_efron_model <- stan_model(model_code = efron_model)
compiled_rol_model <- stan_model(model_code = ranked)

# Prepare data for Stan models
stan_data_maxdiff <- list(
  N = nrow(X),
  T = nrow(tt),
  I = I,
  P = P,
  P2 = P2,
  K = J,
  choice = ranked_options$best_choice,
  worst_choice = ranked_options$worst_choice,
  X = X, X2 = W, 
  task = tt$task_number, 
  task_individual = tt$individual,
  start = tt$start, end = tt$end
)

stan_data_efron <- list(
  N = nrow(X),
  T = nrow(tt),
  I = I,
  P = P,
  P2 = P2,
  K = J,
  rank_order = ranked_options$observed_order,
  X = X, X2 = W, 
  task = tt$task_number, 
  task_individual = tt$individual,
  start = tt$start, end = tt$end,
  n_tied = J - 2  # number of tied ranks
)

stan_data_rol <- list(
  N = nrow(X),
  T = nrow(tt),
  I = I,
  P = P,
  P2 = P2,
  K = J,
  rank_order = ranked_options$true_order,  # Use true_order for full ranking
  X = X, X2 = W, 
  task = tt$task_number, 
  task_individual = tt$individual,
  start = tt$start, end = tt$end
)

# 2. Fit the models
fit_maxdiff <- sampling(compiled_maxdiff_model, data = stan_data_maxdiff, iter = 800)
fit_rol_ties <- sampling(compiled_rol_ties_model, data = stan_data_rol_ties, iter = 800)
fit_efron <- sampling(compiled_efron_model, data = stan_data_efron, iter = 800)
fit_rol <- sampling(compiled_rol_model, data = stan_data_rol, iter = 800)

# Function to normalize utilities across all respondents
normalize_utilities <- function(utilities) {
  (utilities - mean(utilities)) / sd(utilities)
}

# 3. Process results and create plot
process_results <- function(fit, dataset_name) {
  as.data.frame(fit, pars = "beta_individual") %>%
    gather(Parameter, Value) %>%
    mutate(
      individual = str_extract(Parameter, "[0-9]+(?=,)") %>% parse_number(),
      column = str_extract(Parameter, ",[0-9]{1,2}") %>% parse_number()
    ) %>%
    group_by(individual) %>%
    mutate(Value_normalized = normalize_utilities(Value)) %>%
    group_by(individual, column) %>%
    summarise(
      median = median(Value_normalized),
      lower = quantile(Value_normalized, 0.05),
      upper = quantile(Value_normalized, 0.95),
      .groups = 'drop'
    ) %>%
    ungroup() %>%
    mutate(
      True_value = as.numeric(t(beta_i)),
      Dataset = dataset_name
    ) %>%
    group_by(individual) %>%
    mutate(True_value_normalized = normalize_utilities(True_value)) %>%
    ungroup()
}

results_maxdiff <- process_results(fit_maxdiff, "MaxDiff")
results_rol_ties <- process_results(fit_rol_ties, "ROL with ties")
results_efron <- process_results(fit_efron, "Efron Approximation")
results_rol <- process_results(fit_rol, "ROL (no ties)")

all_results <- bind_rows(results_maxdiff, results_rol_ties, results_efron, results_rol)

# 4. Calculate and print RMSE
calculate_rmse <- function(data) {
  sqrt(mean((data$True_value_normalized - data$median)^2))
}

rmse_comparison <- data.frame(
  Model = c("MaxDiff", "ROL with ties", "Efron Approximation", "ROL (no ties)"),
  RMSE = c(
    calculate_rmse(results_maxdiff),
    calculate_rmse(results_rol_ties),
    calculate_rmse(results_efron),
    calculate_rmse(results_rol)
  )
) %>%
  arrange(desc(RMSE))


fourway_comparison_plot <- ggplot(all_results, aes(x = True_value_normalized, y = median, color = Dataset)) +
  geom_linerange(aes(ymin = lower, ymax = upper), alpha = 0.3) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  labs(x = "Normalized True Utility",
       y = "Normalized Estimated Utility",
       title = "Comparison of Model Performances",
       subtitle = "With 90% credibility intervals") +
  scale_color_manual(values = c("MaxDiff" = "blue", 
                                "ROL with ties" = "green",
                                "Efron Approximation" = "red",
                                "ROL (no ties)" = "purple")) +
  theme_minimal() +
  facet_wrap(~Dataset, ncol = 2) +
  theme(legend.position = "none",
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))

```

</details>

Before we show the comparison plot, here's a minor technical caveat. The natural
scale of the utilities the two unknown handling models produce is different than
that of the MaxDiff and ROL without ties models. To plot them usefully against
one another, I thus have to normalize the utilities. This makes it a bit harder
to compare these plots to the last few, but unfortunately its necessary for a
valid comparison.

```{R}
print(fourway_comparison_plot)
```

So roughly from the plot: ROL without unknowns > ROL with full marginal likelihood for unknowns > ROL with Efron Approximation for Unknowns > MaxDiff. Let's sanity check that by looking at RMSEs.

```{R}
print(rmse_comparison)
```

I flagged this briefly above, but to say it again: $J = 5$ has few unknown middle options, meaning
these are fairly favorable conditions for the Efron approximation. We'd expect
higher $J$ to widen the gap between the full marginal likelihood and the Efron
approximation.

So practically, the takeaway here is that you can do better than modeling with MaxDiff
even if you designed your discrete choice experiment for MaxDiff. If you have the
time and/or your number of alternatives isn't super large, you can do better with
the full marginal likelihood model. If you want a workable and fast approximation,
the Efron model still looks meaningfully better than MaxDiff.

Beyond giving a better option for existing MaxDiff data, I'll admit I had a secondary 
motive in wanting to explore these "unknown middle choices" style models. I've sometimes
seen MaxDiff discussed as an imperfect modeling strategy that becomes desirable
because it gives access to new and important data collection strategies.

To flesh out that claim, the idea is that MaxDiff has known flaws that aren't
ideal, but a study only needing to collect best and worst options
hits a sweet spot of respondent experience, simplicity to setup, and 
statistical efficiency (via being able to repeat many choice sets). Better a slightly
worse model on significantly more informative-dense data, since design trumps analysis. 

This is probably
the type of argument for MaxDiff I'm most sympathetic to, in the sense that I do genuinely 
do believe choice experiments place a lot of burden on respondents in a way that
may bias results; perhaps selecting only the best and worst option meaningfully
helps with that. Further, I even find it plausible that if you need to compare a larger
than ideal set of options, there might be value in asking a bunch of best/worst
of 10 choice sets instead of trying to do tons of best of threes to enable the
full ROL with ties option I mentioned above. 
Even if you think this way though, the existence of the ROL with unknown middle options
models I've just shown you are the more accurate model for the design you find significant
value in. **You can't justify
using MaxDiff via the designs it enables.**

# Conclusions

In summary:

1. There are some odd psychological and mathematical quirks entailed in using a
MaxDiff model of discrete choice.
2. These issues with the MaxDiff model are more than theoretical, and can cause
significant harm to the quality of model predictions.
3. If you have control over the design of the choice experiment, you can do much
better than MaxDiff by designing the choice sets in a connected graph, and then
using a Rank-Ordered Logit. This gets most of the benefit of ROLs more generally,
while preserving the relatively low burden respondent experience of just choosing
a best and worst choice.
4. If you don't have control over the design of the experiment, you can still do
better than MaxDiff via ROL models that correctly handle unknown middle choices.
Crucially, the existence of these options implies MaxDiff has no unique design
it enables us to use; we're free to use any design with Best/Worst choices only,
and we can still do better.

In writing this post, I've aimed to both give intuition and concrete examples of
models I would've benefited from seeing when I was first trying to find alternatives
to the MaxDiff formulation of discrete choice. My hope is gathering both the objections and alternatives
in one place can make it a bit easier for others to move away from MaxDiff to more the effective
models available. Thanks for reading!