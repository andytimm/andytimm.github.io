---
layout: post
title: Better discrete choice modeling through rank ordered logits
subtitle: Or- a mathmatically correct model of a psychologically coherent concept
date: 2024-06-09
draft: True
image: imgs/gumby_distribution.webp
categories:
  - priors
  - stupid Bayesian stuff
editor: 
  markdown: 
    wrap: 72
---

![](imgs/jimmy_dunk.png){fig-align="center"}

The wonderful Jim Savage calls the MaxDiff model of discrete choice a
"mathematically incorrect model of a psychologically incoherent
concept"[^1].

[^1]: The smiley face is a threat: it's there to make sure MaxDiff stays
    down :-D

Despite this lovely dunk, and some wonderful notes explaining why
MaxDiff's not great, the model remains frequently used in market
research, most prominently as implemented in
[Sawtooth](https://sawtoothsoftware.com/). Why is this?

Putting aside the most obvious answers like inertia and a dim view of
statistical practice in marketing, it's surprisingly hard to find a
fully fleshed out explanation of the alternatives online. In this post,
I remedy that, with sections for:

1.  Introducing MaxDiff and its blemishes
2.  Showing how the issues propagate into final model quality
3.  Alternatives- Rank-Ordered Logits with connected choice
    graphs, and Rank-Ordered Logits with Ties 

# Introducing MaxDiff

If you're reading this post, you likely have some familiarity with the
basics of discrete choice models, but here's a brief refresher[^2].

[^2]: If you want more of an in-depth introduction, here are some
    resources I'd recommend. First, Jim Savage's [blog post
    series](https://khakieconomics.github.io/2019/03/17/Logit-models-of-discrete-choice.html)
    is great, and does a fantastic job of explaining how our assumptions
    about choice making map onto the math. I also benefited from
    reading Glasgow's [Interpreting Discrete Choice
    Models](https://www.cambridge.org/core/elements/abs/interpreting-discrete-choice-models/676EC2C0F19A3B6D932E12CC612872BC),
    which builds up the basics of choice models a bit more slowly.
    Finally, if you want something that goes much more in detail,
    Kenneth Train's [Discrete Choice Models with
    Simulation](https://eml.berkeley.edu/books/choice2.html) goes into
    great mathematical detail about the models.

We want to build a model of how people respond to the request to
make decisions among discrete options. This could be:

-   A pollster asking which candidate(s) or political parties each
    respondent favors
-   A marketing firm studying preferences amongst a variety of chocolate
    bars
-   A political scientist asking which message voters find most
    convincing

One reasonable model[^3] for this is to say that individual $i$ making
choices among $j$ options can be inferred to have some underlying
utility $\mu$ from the available choices, and that while they usually
choose their most preferred option according to their utility function,
there is some degree of randomness. The basic model is then

[^3]: I won't rehash more subtle implications of this basic model here;
    see the footnote above for references that explore the mind of this
    *Homo Economicus* more in depth. Instead, I'll mostly pull up
    assumptions as they become relevant for discussing MaxDiff.

$$
\mu_{ij} = \mu_{ij} + \epsilon_{ij}
$$ ,

where the $\mu_{ij}$ can have rich provenance (demographics and
other individual traits, the context in which the decision is made, the
other options available...) but is fixed, but there's some
$\epsilon_{ij}$ of randomness involved. To make this model easier to
estimate, we assume that $\epsilon_{ij}$ has a
[Gumbel](https://en.wikipedia.org/wiki/Gumbel_distribution)[^4]
distribution.

[^4]: Other options are possible, but less common, and would take us too
    far afield, so I'll skip explaining the choice of Gumbel versus
    other distributions here.

Under this model, the probability that individual $i$ chooses
alternative $j$ from a choice set $C_i$ is: $$
P_{ij} = \frac{\exp(\mu_{ij})}{\sum_{k \in C_i} \exp(\mu_{ik})}
$$ This is the standard multinomial logit.

Now, suppose we have a dataset where each individual $i$ has made a
choice $y_i$ from their choice set $C_i$. The likelihood of observing
this dataset under the multinomial logit model is: $$
L = \prod_i P_{iy_i} = \prod_i \frac{\exp(\mu_{iy_i})}{\sum_{k \in C_i} \exp(\mu_{ik})}
$$ The likelihood is the product of the choice probabilities for each
individual's observed choice $y_i$. We can estimate the parameters of
the utility function $\mu_{ij}$ by maximizing this likelihood function
(or, more commonly, the log-likelihood). This is how the single best
choice data is naturally incorporated into the likelihood function. Each
observation contributes a term to the likelihood based on the
probability of the chosen "best" alternative under the model.

While we can (and will) push the basic logit model of this further to
include respondent and choice level covariates, multilevel components,
and other improvements, let's think about the pressures of gathering
data here for a moment, since that'll motivate the desire for something
like MaxDiff.

To estimate this, we gather respondents and ask them to choose their
favorite option amongst a given choice set. As a way to control the
difficulty of making a choice while still gathering enough data, we can
limit the size of the set (choose the best of 10 items, instead of 20),
and repeat the choice task.

Getting respondents and getting them to stick through a bunch of choice
tasks is hard though, and it's only natural to wonder: can we extract
more with each choice set? One option would be to ask the respondents to
rank ALL the options at once, but if you have a large choice set that
sounds exhausting.

What if we asked people to choose their best and worst choices each
time? After all, people might not have strong preferences amongst the
middling 18 chocolate bars, but the best and worst seem more memorable,
and that's only 1 more choice.

And now, (stepping into the MaxDiff trap to show its allure, if you
will), what if we don't want to entirely change up our likelihood to
handle the new rich source of data? What if handling ties sounds awful?
Instead, what if we just treat the worsts as the opposite of the bests,
which makes a sort of sense, and simplifies the likelihood one hell of a
lot:

$$
U_{ij}^W = -U_{ij}^B = -(\mu_{ij} + \epsilon_{ij})
$$

Under this assumption, the probability of individual $i$ choosing
alternative $j$ as the worst is: $$
P_{ij}^W = \frac{\exp(-\mu_{ij})}{\sum_{k \in C_i} \exp(-\mu_{ik})}
$$

... Now we've stepped in it, and Jimmy is mad at us.

## Psycologically Incoherent

What I've introduced is the core of the MaxDiff formulation of discrete
choice, before bells and whistles are introduced. This has some deep
problems though; let's start with the "psychological" ones. Human
decisionmaking is an incredibly complex, not always logical process, and
we'll always be losing significant fidelity in boiling it down into a
model. Here though, I'll focus on explaining a handful of breakdowns in
the relationship between reality and the world of our models that are
particularly harmful.

First, let's talk about **symmetry**. With the MaxDiff likelihood above,
we're treating the worst as equal and opposite to the best. For example,
though, I don't like Joe Biden as much as I dislike Trump[^5]. I really
really like the best pizza in Brookyln, but New York pizza is all New
York pizza, it only gets so bad. This might be a reasonable
simplification in some rare cases, but it's hard to argue that baking
this into our model faithfully mirrors reality.

[^5]: President Biden is great, it's just hard to compete with the
    comically malignant and incompetent by being solid and stabilizing.

Also, we're not only asking them to be symmetric, we're sort of
conjoining the best and worst choice, asking them to share the **same
utility scale**. In other words, the factors that make an alternative
more attractive for the best choice are assumed to make it equally less
attractive for the worst choice. An easy example is something like
health risks- "that sushi place gave me food poisoning" is very relevant
to my choice of worst restaurant, but the moment I have to think about
food safety, a restaurant isn't really anywhere relevant on the "best"
side of the spectrum for me. Again, you can probably think of a case
where this is a fine approximation, but in an ideal world, we won't
force ourselves to weld our notions of best to our notions of worst.

Finally, the **error variances** being treated as the same should feel
pretty strange. I'm much, much more consistent in my selection of
"bests" than "worsts"- why would I spent a bunch of time deciding which
opinion of 20 is the absolute worst and which is just 19th worst? People
tend to be much less consistent, and frankly much less engaged, with
their worst choices. Why would we bake this into our model?

Putting this all together, The simplification in estimation that comes
with MaxDiff also leaks into how the model "sees" the decision maker,
and it meaningfully distorts the map in a way that does not reflect the
territory.

## Mathmatically Incorrect

Beyond calling the model psychologically incoherent, Savage also says
the model is mathematically incorrect. This feels like a slightly
stronger insult, and indeed it is, in the sense that the model is
failing on its own terms.

How? Well, we've explicitly laid out a model with Gumby errors:

![](imgs/gumby_distribution.webp){fig-align="center"}

Wait, no sorry sorry[^6]. we've laid out a model with Gumbel errors
$\epsilon_{ij}$. As a reminder, the Gumbel distribution is chosen for its mathematical convenience, as it leads to a closed-form expression for the choice probabilities in the logit model. the PDF is plotted below:

[^6]: Unlike the blog post about left-handed kangaroos drinking fosters
    I'm working on, this post was not primarily motivated by the urge to
    ask DALL-E to create this.

![](imgs/Gumbel-Density.svg.png){fig-align="center"}

Here's where the trouble starts: like we discussed above, the MaxDiff
model forcibly inserts an element of symmetry into the error
distributions of the choices. However, as you probably already noticed,
the ~~Gumby~~ Gumbel distribution is not a symmetric distribution! This
is fine in the case where we're just reasoning about the best choice,
but even on it's own terms the MaxDiff formulation doesn't quite make
sense.

# The defects are not just theoretical or cosmetic: an illustration

So I've shown some ways I claim that the MaxDiff model of discrete
choice falls short, but how much do they really matter? As someone with
an appreciation for the [agnostic perspective on statistical
modeling](https://www.cambridge.org/core/books/foundations-of-agnostic-statistics/684756357E7E9B3DFF0A8157FB2DCECA)
, I think it's important to not only show that there are theoretical
senses in which a model might be flawed, but further prove that these
technical blemishes harm model performance on metrics we care about.

To do this, let's generate some synthetic data, and fit both the
best-choice and MaxDiff models to it. For this section, I'll essentially
be starting from Jim Savage's best choice model
[here](https://khakieconomics.github.io/2018/12/27/Ranked-random-coefficients-logit.html),
and then add in the "worst choice as negative best choice" logic
afterwards.

Let's start with the synthetic data. If the exact DGP isn't very
exciting to you, feel free to skim or skip this section at first:

<details> 

<summary>**Data Generation for Simulation Study**</summary>


```{R}
library(tidyverse)
library(rstan)
library(ibd)
options(mc.cores = parallel::detectCores())

set.seed(604)

# Again, note that this code is just reproducing https://khakieconomics.github.io/2018/12/27/Ranked-random-coefficients-logit.html,
# although with a more real-world feeling number of respondents and
# and total choices. If this is taking uncomfortably long, feel free to scale
# down I, the results should reproduce just fine.

# Number of individuals
I <- 30
# Number of tasks per individual
Tasks <- 10
# Number of choices per task
J <- 5
# Dimension of covariate matrix
P <- 5
# Dimension of demographic matrix
P2 <- 6

# demographic matrix
W <- matrix(rnorm(I*P2), I, P2)
# Loading matrix
Gamma <- matrix(rnorm(P*P2), P, P2)

# Show W * t(Gamma) to make sure it looks right
W %*% t(Gamma)

# Correlation of decisionmaker random slopes
Omega <- cor(matrix(rnorm(P*(P+2)), P+2, P))

# Scale of decisionmaker random slopes
tau <- abs(rnorm(P, 0, .5))

# Covariance matrix of decisionmaker random slopes
Sigma <- diag(tau) %*% Omega %*% diag(tau)

# Centers of random slopes
beta <- rnorm(P)

# Individual slopes
beta_i <- MASS::mvrnorm(I, beta, Sigma) + W %*% t(Gamma)

# Again, quick plot to sanity check
plot(as.data.frame(beta_i))

# Create X -- let's make this a dummy matrix
X <- matrix(sample(0:1, Tasks*I*J*P, replace = T), Tasks*I*J, P)
# Each of the rows in this matrix correspond to a choice presented to a given individual
# in a given task

indexes <- crossing(individual = 1:I, task = 1:Tasks, option = 1:J) %>% 
  mutate(row = 1:n())

# Write a Gumbel random number generator using inverse CDF trick
rgumbel <- function(n, mu = 0, beta = 1) mu - beta * log(-log(runif(n)))
mean(rgumbel(1e6))

# Ok, now we need to simulate choices. Each person in each task compares each 
# choice according to X*beta_i + epsilon, where epsilon is gumbel distributed. 
# They return their rankings. 

ranked_options <- indexes %>% 
  group_by(individual, task) %>% 
  mutate(fixed_utility = as.numeric(X[row,] %*% as.numeric(beta_i[first(individual),])),
         plus_gumbel_error = fixed_utility + rgumbel(n()),
         rank = rank(-plus_gumbel_error),
         # We're going to use the order rather than the rank in the Stan part of the model
         order = order(rank),
         # And here we create a dummy vector for the best choice
         best_choice = as.numeric(1:n() == which.max(plus_gumbel_error)),
         worst_choice = as.numeric(1:n() == which.min(plus_gumbel_error))
  )


tt <- ranked_options %>% 
  group_by(individual, task) %>%
  summarise(start = min(row), 
            end = max(row)) %>% 
  ungroup %>%
  mutate(task_number = 1:n())
```


</details> 

While this simulation isn't exactly simple, let me pause to point
something out something I've not done. I have not made any choices here
that are designed to make MaxDiff look poor. Since we want to use a
model of realistic complexity, this has simulated structure such that
it'll benefit from the random coefficients formulation I'll use in a
moment. Similarly, this has load-bearing covariates at both the
individual and choice level, since we would almost always have those in
the real world. But the core utility function we've created for
respondents boils down to simulating data under the assumptions of the
logit choice model with Gumbel we've been discussing all along.

Now, let's specify the best choice model in Stan[^7]:

<details> 

<summary>**Best Choice Logit Model**</summary>

```{R}
best <- "// saved as mixed_conditional_individual_effects.stan
data {
  int N; // number of rows
  int T; // number of inidvidual-choice sets/task combinations
  int I; // number of Individuals
  int P; // number of covariates that vary by choice
  int P2; // number of covariates that vary by individual
  int K; // number of choices
  
  vector<lower = 0, upper = 1>[N] choice; // binary indicator for choice
  matrix[N, P] X; // choice attributes
  matrix[I, P2] X2; // individual attributes
  
  int task[T]; // index for tasks
  int task_individual[T]; // index for individual
  int start[T]; // the starting observation for each task
  int end[T]; // the ending observation for each task
}
parameters {
  vector[P] beta; // hypermeans of the part-worths
  matrix[P, P2] Gamma; // coefficient matrix on individual attributes
  vector<lower = 0>[P] tau; // diagonal of the part-worth covariance matrix
  matrix[I, P] z; // individual random effects (unscaled)
  cholesky_factor_corr[P] L_Omega; // the cholesky factor of the correlation matrix of tastes/part-worths
}
transformed parameters {
  // here we use the reparameterization discussed on slide 30
  matrix[I, P] beta_individual = rep_matrix(beta', I) + X2 * Gamma' + z*diag_pre_multiply(tau, L_Omega);
}
model {
  // create a temporary holding vector
  vector[N] log_prob;
  
  // priors on the parameters
  tau ~ normal(0, .5);
  beta ~ normal(0, .5);
  to_vector(z) ~ normal(0, 1);
  L_Omega ~ lkj_corr_cholesky(4);
  to_vector(Gamma) ~ normal(0, 1);
  
  // log probabilities of each choice in the dataset
  for(t in 1:T) {
    vector[K] utilities; // tmp vector holding the utilities for the task/individual combination
    // add utility from product attributes with individual part-worths/marginal utilities
    utilities = X[start[t]:end[t]]*beta_individual[task_individual[t]]';
    
    log_prob[start[t]:end[t]] = log_softmax(utilities);
  }
  
  // use the likelihood derivation on slide 29
  target += log_prob' * choice;
}"

ranked <- "// saved as ranked_rcl.stan
functions {
  real rank_logit_lpmf(int[] rank_order, vector delta) {
    // We reorder the raw utilities so that the first rank is first, second rank second... 
    vector[rows(delta)] tmp = delta[rank_order];
    real out;
    // ... and sequentially take the log of the first element of the softmax applied to the remaining
    // unranked elements.
    for(i in 1:(rows(tmp) - 1)) {
      if(i == 1) {
        out = tmp[1] - log_sum_exp(tmp);
      } else {
        out += tmp[i] - log_sum_exp(tmp[i:]);
      }
    }
    // And return the log likelihood of observing that ranking
    return(out);
  }
}
data {
  int N; // number of rows
  int T; // number of inidvidual-choice sets/task combinations
  int I; // number of Individuals
  int P; // number of covariates that vary by choice
  int P2; // number of covariates that vary by individual
  int K; // number of choices
  
  int rank_order[N]; // The vector describing the index (within each task) of the first, second, third, ... choices. 
  // In R, this is order(-utility) within each task
  matrix[N, P] X; // choice attributes
  matrix[I, P2] X2; // individual attributes
  
  int task[T]; // index for tasks
  int task_individual[T]; // index for individual
  int start[T]; // the starting observation for each task
  int end[T]; // the ending observation for each task
}
parameters {
  vector[P] beta; // hypermeans of the part-worths
  matrix[P, P2] Gamma; // coefficient matrix on individual attributes
  vector<lower = 0>[P] tau; // diagonal of the part-worth covariance matrix
  matrix[I, P] z; // individual random effects (unscaled)
  cholesky_factor_corr[P] L_Omega; // the cholesky factor of the correlation matrix of tastes/part-worths
}
transformed parameters {
  // here we use the reparameterization discussed on slide 30
  matrix[I, P] beta_individual = rep_matrix(beta', I) + X2 * Gamma' + z * diag_pre_multiply(tau, L_Omega);
}
model {
  // priors on the parameters
  tau ~ normal(0, .5);
  beta ~ normal(0, 1);
  to_vector(z) ~ normal(0, 1);
  L_Omega ~ lkj_corr_cholesky(4);
  to_vector(Gamma) ~ normal(0, 1);
  
  // log probabilities of each choice in the dataset
  for(t in 1:T) {
    vector[K] utilities; // tmp vector holding the utilities for the task/individual combination
    // add utility from product attributes with individual part-worths/marginal utilities
    utilities = X[start[t]:end[t]]*beta_individual[task_individual[t]]';
    rank_order[start[t]:end[t]] ~ rank_logit(utilities);
  }
}"
```

</details> 

Much of this logic handles the multilevel component and covariates, but the real
key is the last few lines where the likelihood is built. That's where the problem
will arise when we extend the above model into MaxDiff:

<details> 

<summary>**MaxDiff Model**</summary>

```{R}
best_worst <- "// saved as mixed_conditional_individual_effects.stan
data {
  int N; // number of rows
  int T; // number of inidvidual-choice sets/task combinations
  int I; // number of Individuals
  int P; // number of covariates that vary by choice
  int P2; // number of covariates that vary by individual
  int K; // number of choices
  
  vector<lower = 0, upper = 1>[N] choice; // binary indicator for choice
  vector<lower = 0, upper = 1>[N] worst_choice; // binary indicator for worst choice
  matrix[N, P] X; // choice attributes
  matrix[I, P2] X2; // individual attributes
  
  int task[T]; // index for tasks
  int task_individual[T]; // index for individual
  int start[T]; // the starting observation for each task
  int end[T]; // the ending observation for each task
}
parameters {
  vector[P] beta; // hypermeans of the part-worths
  matrix[P, P2] Gamma; // coefficient matrix on individual attributes
  vector<lower = 0>[P] tau; // diagonal of the part-worth covariance matrix
  matrix[I, P] z; // individual random effects (unscaled)
  cholesky_factor_corr[P] L_Omega; // the cholesky factor of the correlation matrix of tastes/part-worths
}
transformed parameters {
  // here we use the reparameterization discussed on slide 30
  matrix[I, P] beta_individual = rep_matrix(beta', I) + X2 * Gamma' + z*diag_pre_multiply(tau, L_Omega);
}
model {
  // create a temporary holding vector
  vector[N] log_prob;
  vector[N] log_prob_worst;
  
  // priors on the parameters
  tau ~ normal(0, .5);
  beta ~ normal(0, .5);
  to_vector(z) ~ normal(0, 1);
  L_Omega ~ lkj_corr_cholesky(4);
  to_vector(Gamma) ~ normal(0, 1);
  
  // log probabilities of each choice in the dataset
  for(t in 1:T) {
    vector[K] utilities; // tmp vector holding the utilities for the task/individual combination
    // add utility from product attributes with individual part-worths/marginal utilities
    utilities = X[start[t]:end[t]]*beta_individual[task_individual[t]]';
    
    log_prob[start[t]:end[t]] = log_softmax(utilities);
    log_prob_worst[start[t]:end[t]] = log_softmax(-utilities);
  }
  
  // use the likelihood derivation on slide 29
  target += log_prob' * choice;
  target += log_prob_worst' * worst_choice;
}"
```

</details> 

Comparing the two, you can definitely see the implementation simplicity MaxDiff
provides; we're essentially just building out `log_prob_worst` and pulling it
into the likelihood. 

So how does this perform? Well, let's fit the two and find out. The next code
block has some plumbing to fit the models and extract results, so feel free
to jump to the results plot below:

<details> 

<summary>**Fit the Two Models, and Gather Predictions to Compare**</summary>

```{R}
data_list_best_choice <-list(N = nrow(X),
                             T = nrow(tt),
                             I = I, 
                             P = P, 
                             P2 = P2, 
                             K = J, 
                             # NOTE!! This is the tricky bit -- we use the order of the ranks (within task)
                             # Not the raw rank orderings. This is how we get the likelihood evaluation to be pretty quick
                             choice = ranked_options$best_choice,
                             X = X, 
                             X2 = W, 
                             task = tt$task_number, 
                             task_individual = tt$individual,
                             start = tt$start, 
                             end = tt$end) 

data_list_best_worst <-list(N = nrow(X),
                             T = nrow(tt),
                             I = I, 
                             P = P, 
                             P2 = P2, 
                             K = J,
                             choice = ranked_options$best_choice,
                             worst_choice = ranked_options$worst_choice,
                             X = X, 
                             X2 = W, 
                             task = tt$task_number, 
                             task_individual = tt$individual,
                             start = tt$start, 
                             end = tt$end) 

compiled_best_choice_model <- stan_model(model_code = best)
compiled_bw_choice_model <- stan_model(model_code = best_worst)

best_choice_fit <- sampling(compiled_best_choice_model, 
                            data = data_list_best_choice, 
                            iter = 800)

best__worst_choice_fit <- sampling(compiled_bw_choice_model, 
                            data = data_list_best_worst, 
                            iter = 800)


# Now make some predictions to compare
best_choice <- as.data.frame(best_choice_fit, pars = "beta_individual") %>%
  gather(Parameter, Value) %>%
  group_by(Parameter) %>%
  summarise(median = median(Value),
            lower = quantile(Value, .05),
            upper = quantile(Value, .95)) %>%
  mutate(individual = str_extract(Parameter, "[0-9]+(?=,)") %>% parse_number,
         column = str_extract(Parameter, ",[0-9]{1,2}") %>% parse_number) %>%
  arrange(individual, column) %>%
  mutate(`True value` = as.numeric(t(beta_i)),
         Dataset = "Best Choice")

best_worst_choice <- as.data.frame(best__worst_choice_fit, pars = "beta_individual") %>%
  gather(Parameter, Value) %>%
  group_by(Parameter) %>%
  summarise(median = median(Value),
            lower = quantile(Value, .05),
            upper = quantile(Value, .95)) %>%
  mutate(individual = str_extract(Parameter, "[0-9]+(?=,)") %>% parse_number,
         column = str_extract(Parameter, ",[0-9]{1,2}") %>% parse_number) %>%
  arrange(individual, column) %>%
  mutate(`True value` = as.numeric(t(beta_i)),
         Dataset = "Best-Worst Choice")

# Combine the datasets
combined_data <- bind_rows(best_choice, best_worst_choice)

```

</details> 

```{R}
# Plot results
ggplot(combined_data, aes(x = `True value`, y = median, color = Dataset)) +
  geom_linerange(aes(ymin = lower, ymax = upper), alpha = 0.3) +
  geom_point(aes(y = median), alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1) +
  labs(y = "Utility Estimates",
       title = "Utility Estimates from the Two Models",
       subtitle = "With interior 90% credibility intervals") +
  scale_color_manual(values = c("Best Choice" = "blue", "Best-Worst Choice" = "red")) +
  facet_wrap(~Dataset) +
  theme(legend.position="none")
```

To be clear, what's plotted here are respondent level utilities for each choice. Concerningly, it doesn't even seem clear that the best-worst choice model
performs better here despite the extra data. The fact that we have to do
this at all doesn't bode well, but let's compare the RMSE's of the predictions
versus the (simulated) truth to confirm:

```{R}
# Is the B/W model actually better?
combined_data %>%
  group_by(Dataset) %>%
  summarize(RMSE = sqrt(mean((`True value` - median)^2)))
```

To say this more emphatically, MaxDiff gets twice as much data from each
respondent, and it's not even consistently the case that the MaxDiff model
is better[^8]! And again, it's not like I chose some obscure DGP where MaxDiff
is uniquely poor, we're working with data that align pretty precisely with
the model assumptions.

So to summarize, I've described some psychological and mathematical issues with MaxDiff.
Then, via a comparison to a best choice only model, I showed that these differences
are more than theoretical naggles: for pretty vanilla data, the MaxDiff model
can actually perform worse than the best choice formulation.

It's worth pointing out that we only get this type of precise comparison in a
simulation study. On real world survey data with a similar DGP, you'd just see
the models give fairly similar answers, with no reason to suspect the slight
differences might actually be bad tweaks on MaxDiff's part. Modeling only real
world data is how many market researchers approach their work, so it should be
no surprise that MaxDiff continues to be accepted in the field. With simulation,
however, we can look closer, and the flaws I've discussed become increasingly
apparent.

[^7]: I won't re-explain the details of the model here, since the original source already
does a fantastic job. If you haven't already read the original post series, it's
well worth taking the time to do so now, as there are a lot of moving pieces
here- setting sensible priors for this model, incorporating covariates logically,
and setting this up to run efficiently in Stan.

[^8]: I will note that if you're following along at home, sampling variability
means that for some realizations of this DGP + these models, MaxDiff performs
just slightly better. But "barely better in a fraction of runs" isn't really a good outcome from
twice as much data.

# Alternative #1: rank-ordered logits with connected graphs of choices

Ok, so you get it. MaxDiff isn't great. But respondents are still expensive,
and you still have a choice modeling task to complete at work, and you really,
really want to do better than just using the best choices because your boss
thinks Sawtooth is fine since it's been fine for a long time. What's the alternative?

In this section, I'll lay out what we'll call the rank-ordered logit with connected
graphs. If this sounds complicated, the model isn't actually going to get much
more complicated.

Instead, the plan is mostly to be cleverer about how we build our choice sets
in the first place.

## Setting up the Choice Sets

What we really want, I'd argue, is full choice ranking data, for each respondent.
But of course, people rapidly become unhappy and start really satisficing when
they are forced to rank 20 different largely similar options at the same time. How
can we get the simplicity from the respondent POV of MaxDiff, but the rich
ranking data we want?

The clever trick here is that by asking the right 3 item choice sets, and asking
the survey taker to pick best and worst, we can collect every choice needed to
construct a full ranking[^8]. We'll then be able to simply fit a rank-ordered
logit model to these data, which makes far fewer simplifying assumptions about
how the choices and their utilities interact.

There are some downsides I'll be transparent about here: for $K$ choices, you
need each respondent to do $K-1$ choice tasks. If you have 20 options, this
can get tiring fast, and you won't really be gathering "extra" opinions per
respondent anymore by doing the minimum $K-1$ rounds. Here's a hand worked
example if you'd like it for intuition:

<details> 

<summary>**Illustration: K-1 Comparisons are Needed**</summary>

Let's say we want to get someone's ranking over 2024 presidential candidate
choices, from the set {Biden,Trump,RFK,Stein}.

Choice Set 1:

Options: {Biden, Stein, RFK}
Best choice: Biden
Worst choice: RFK

Choice Set 2:

Options: {Stein, RFK, Trump}
Best choice: Stein
Worst choice: Trump

Choice Set 3:

Options: {Biden, Stein, Trump}
Best choice: Biden
Worst choice: Trump

Now, let's reconstruct the full ranking from these choice sets:

From Choice Set 1, we know that Biden > Stein > RFK.
From Choice Set 2, we know that Stein > RFK > Trump.
From Choice Set 3, we know that Biden > Stein > Trump.

Combining these partial rankings, we can infer the full ranking:
Biden > Stein > RFK > Trump

</details> 

You'll notice that you can't get the full ranking for e.g. 4 options in any less than 3 sets,
but you well could add more sets to provide some redundancy and statistical
efficiency if you so choose. Of course, in larger choice sets, you'll probably
end up closer to K-1 sets given respondent fatigue, but for smaller sets it's
a very helpful option to know exists.

Most of the innovation here was in the choice set design, but there's some in
the model too, so let's turn to that:

<details> 

<summary>**Model 3: Rank-Ordered Logit for Connected Graphs**</summary>

```{R}
ranked <- "// saved as ranked_rcl.stan
functions {
  real rank_logit_lpmf(int[] rank_order, vector delta) {
    // We reorder the raw utilities so that the first rank is first, second rank second... 
    vector[rows(delta)] tmp = delta[rank_order];
    real out;
    // ... and sequentially take the log of the first element of the softmax applied to the remaining
    // unranked elements.
    for(i in 1:(rows(tmp) - 1)) {
      if(i == 1) {
        out = tmp[1] - log_sum_exp(tmp);
      } else {
        out += tmp[i] - log_sum_exp(tmp[i:]);
      }
    }
    // And return the log likelihood of observing that ranking
    return(out);
  }
}
data {
  int N; // number of rows
  int T; // number of inidvidual-choice sets/task combinations
  int I; // number of Individuals
  int P; // number of covariates that vary by choice
  int P2; // number of covariates that vary by individual
  int K; // number of choices
  
  int rank_order[N]; // The vector describing the index (within each task) of the first, second, third, ... choices. 
  // In R, this is order(-utility) within each task
  matrix[N, P] X; // choice attributes
  matrix[I, P2] X2; // individual attributes
  
  int task[T]; // index for tasks
  int task_individual[T]; // index for individual
  int start[T]; // the starting observation for each task
  int end[T]; // the ending observation for each task
}
parameters {
  vector[P] beta; // hypermeans of the part-worths
  matrix[P, P2] Gamma; // coefficient matrix on individual attributes
  vector<lower = 0>[P] tau; // diagonal of the part-worth covariance matrix
  matrix[I, P] z; // individual random effects (unscaled)
  cholesky_factor_corr[P] L_Omega; // the cholesky factor of the correlation matrix of tastes/part-worths
}
transformed parameters {
  // here we use the reparameterization discussed on slide 30
  matrix[I, P] beta_individual = rep_matrix(beta', I) + X2 * Gamma' + z * diag_pre_multiply(tau, L_Omega);
}
model {
  // priors on the parameters
  tau ~ normal(0, .5);
  beta ~ normal(0, 1);
  to_vector(z) ~ normal(0, 1);
  L_Omega ~ lkj_corr_cholesky(4);
  to_vector(Gamma) ~ normal(0, 1);
  
  // log probabilities of each choice in the dataset
  for(t in 1:T) {
    vector[K] utilities; // tmp vector holding the utilities for the task/individual combination
    // add utility from product attributes with individual part-worths/marginal utilities
    utilities = X[start[t]:end[t]]*beta_individual[task_individual[t]]';
    rank_order[start[t]:end[t]] ~ rank_logit(utilities);
  }
}"
```

</details> 

Take a second to reason through the above code; we're able to use all of the information
a full ranking provides to us, without having to force any interrelationship
between best and worst to exist that isn't truly there. Also, notice that there's
nothing here specifically for the "connected graph" part of model- that's all
in the choice set generation, which we'll do next.

[^8]: A fun little exercise: think through how these form a graph of connected
choices! What's the structure of the graph look like? May not be applicable for all definitions of fun.

## Building the model and model comparisons

Let's try making the choice sets the new way, and then compare models.

<details>

<summary>**Updated Data Generation for Connected Graph Illustration**</summary>

```{R}
# Number of individuals
I <- 30
# Total number of choices
J <- 10
# Number of choices per task
K <- 3
# Number of tasks per individual (calculated to ensure connectivity)
Tasks <- choose(J, 2) / choose(K, 2)
# Dimension of covariate matrix
P <- 5
# Dimension of demographic matrix
P2 <- 6

# The rest of the parameter generation remains the same
W <- matrix(rnorm(I*P2), I, P2)
Gamma <- matrix(rnorm(P*P2), P, P2)
Omega <- cor(matrix(rnorm(P*(P+2)), P+2, P))
tau <- abs(rnorm(P, 0, .5))
Sigma <- diag(tau) %*% Omega %*% diag(tau)
beta <- rnorm(P)
beta_i <- MASS::mvrnorm(I, beta, Sigma) + W %*% t(Gamma)

# Create X -- now for all possible choices
X <- matrix(sample(0:1, J*P, replace = TRUE), J, P)

# Generate connected choice sets
choice_sets <- combn(J, K)
choice_sets <- choice_sets[, sample(ncol(choice_sets), Tasks, replace = FALSE)]

# Generate choices
rgumbel <- function(n, mu = 0, beta = 1) mu - beta * log(-log(runif(n)))

ranked_options <- list()
for (i in 1:I) {
  for (t in 1:Tasks) {
    set_choices <- choice_sets[, t]
    set_utilities <- X[set_choices, ] %*% beta_i[i, ] + rgumbel(K)
    ranking <- order(set_utilities, decreasing = TRUE)
    
    ranked_options[[length(ranked_options) + 1]] <- data.frame(
      individual = i,
      task = t,
      option = set_choices,
      utility = set_utilities,
      rank = ranking,
      best_choice = as.integer(ranking == 1),
      worst_choice = as.integer(ranking == K)
    )
  }
}

ranked_options <- bind_rows(ranked_options)

# Create tt dataframe
tt <- ranked_options %>% 
  group_by(individual, task) %>%
  summarise(start = min(which(individual == first(individual) & task == first(task))),
            end = max(which(individual == first(individual) & task == first(task)))) %>% 
  ungroup() %>%
  mutate(task_number = 1:n())
```
</details>

Now, let's design the connected graph choice sets. We'll use a balanced incomplete block design (BIBD) to ensure that each pair of choices appears together in at least one set across all respondents. 
<details>
<summary>**Designing Connected Graph Choice Sets**</summary>

```{R}  
data_list_best_choice <- list(
  N = nrow(best_choice_data),
  T = Tasks * I,  # Total number of choice tasks across all individuals
  I = I,
  P = P,
  P2 = P2,
  K = K,
  choice = rep(1, nrow(best_choice_data)),  # Always 1 as we're only keeping the best choice
  X = X_best,
  X2 = W,
  task = best_choice_data$task + (best_choice_data$individual - 1) * Tasks,  # Unique task identifier
  task_individual = best_choice_data$individual,
  start = 1:nrow(best_choice_data),
  end = 1:nrow(best_choice_data)
)

best_choice_fit <- sampling(compiled_best_choice_model, 
                            data = data_list_best_choice, 
                            iter = 800)

best_choice <- as.data.frame(best_choice_fit, pars = "beta_individual") %>%
  gather(Parameter, Value) %>%
  group_by(Parameter) %>%
  summarise(median = median(Value),
            lower = quantile(Value, .05),
            upper = quantile(Value, .95)) %>%
  mutate(individual = str_extract(Parameter, "[0-9]+(?=,)") %>% parse_number,
         column = str_extract(Parameter, ",[0-9]{1,2}") %>% parse_number) %>%
  arrange(individual, column) %>%
  mutate(`True value` = as.numeric(t(beta_i)),
         Dataset = "Best Choice")

# Plot results
ggplot(best_choice, aes(x = `True value`, y = median, color = Dataset)) +
  geom_linerange(aes(ymin = lower, ymax = upper), alpha = 0.3) +
  geom_point(aes(y = median), alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1) +
  labs(y = "Utility Estimates",
       title = "Utility Estimates from the Two Models",
       subtitle = "With interior 90% credibility intervals") +
  scale_color_manual(values = c("Best Choice" = "blue")) +
  facet_wrap(~Dataset) +
  theme(legend.position="none")


```
</details>

Now we have simulated data that represents a connected graph design with 10 total choices, and 15 choice sets per respondent. Each choice set contains 3 options, and each pair of choices appears together in at least one set across all respondents.

How does this change performance?

```{R}
# Modify the data preparation
best_choice_data <- simulated_df %>%
  filter(ranking == 1) %>%
  select(individual, task, choices, utilities)

# Recreate X matrix for best choices only
X_best <- matrix(0, nrow = nrow(best_choice_data), ncol = P)
for (i in 1:nrow(best_choice_data)) {
  X_best[i,] <- X[best_choice_data$choices[i],]
}

# Calculate total number of tasks
total_tasks <- nrow(best_choice_data)

# 2. Create the Stan data list
data_list_best_choice <- list(
  N = nrow(best_choice_data),
  T = total_tasks,
  I = I,
  P = P,
  P2 = P2,
  K = 1,  # We only have one choice per task in the best choice model
  choice = rep(1, nrow(best_choice_data)),  # Always 1 as we're only keeping the best choice
  X = X_best,
  X2 = W,
  task = 1:total_tasks,
  task_individual = best_choice_data$individual,
  start = 1:total_tasks,
  end = 1:total_tasks
)

# 3. Modify the Stan model
best_choice_model <- "
data {
  int<lower=1> N;
  int<lower=1> T;
  int<lower=1> I;
  int<lower=1> P;
  int<lower=1> P2;
  int<lower=1> K;
  vector[N] choice;
  matrix[N, P] X;
  matrix[I, P2] X2;
  int task[T];
  int task_individual[T];
  int start[T];
  int end[T];
}

parameters {
  vector[P] beta;
  matrix[P, P2] Gamma;
  vector<lower=0>[P] tau;
  matrix[I, P] z;
  cholesky_factor_corr[P] L_Omega;
}

transformed parameters {
  matrix[I, P] beta_individual = rep_matrix(beta', I) + X2 * Gamma' + z * diag_pre_multiply(tau, L_Omega);
}

model {
  vector[N] log_prob;
  
  tau ~ normal(0, .5);
  beta ~ normal(0, .5);
  to_vector(z) ~ normal(0, 1);
  L_Omega ~ lkj_corr_cholesky(4);
  to_vector(Gamma) ~ normal(0, 1);
  
  for(t in 1:T) {
    log_prob[t] = X[t] * beta_individual[task_individual[t]]';
  }
  
  target += log_prob' * choice;
}
"

# Now try to fit the model again
fit_best_choice <- stan(model_code = best_choice_model, 
                        data = data_list_best_choice, 
                        iter = 2000, 
                        chains = 4)

# 4. Extract and examine the results
summary_best_choice <- summary(fit_best_choice, pars = c("beta", "tau"))$summary
print(summary_best_choice)

# Extract individual-level parameters
beta_individual_summary <- summary(fit_best_choice, pars = "beta_individual")$summary

# Compare estimated vs true individual-level parameters
comparison <- data.frame(
  true_value = as.vector(t(beta_i)),
  estimated_mean = beta_individual_summary[,"mean"],
  estimated_lower = beta_individual_summary[,"2.5%"],
  estimated_upper = beta_individual_summary[,"97.5%"]
)

# Plot comparison
ggplot(comparison, aes(x = true_value, y = estimated_mean)) +
  geom_point() +
  geom_errorbar(aes(ymin = estimated_lower, ymax = estimated_upper), alpha = 0.3) +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(x = "True Value", y = "Estimated Value",
       title = "Comparison of True vs Estimated Individual-Level Parameters",
       subtitle = "Best Choice Model") +
  theme_minimal()
```

# Alternative #2: a backup plan- rank-ordered logits with ties

# Stepping Back: the utility of discrete choice more broadly
