---
layout: post
title: Variational Inference for MRP with Reliable Posterior Distributions
subtitle: Part 6- Diagnostics
date: 2023-06-17
draft: True
categories:
  - MRP
  - Variational Inference
  - Diagnostics
---
   
   
This is section 6 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality. 

The general structure for this post and the posts around it will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in these four posts will go a long way towards more robust variational inference. I'll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.

In the [last post](https://andytimm.github.io/posts/Variational%20MRP%20Pt5/variational_mrp_5.html) we looked at normalizing flows, a way to leverage neural networks to learn significantly
more expressive variational families in a way that adapt to specific problems.

In this post, we'll explore different diagnostics for variational inference,
ranging from simple statistics that are easy to calculate as we fit our approximation to solving the problem
in parallel with MCMC to compare and contrast. Some recurring themes will be 
aiming to be precise about what constitutes failure under each diagnostic tool,
and providing intuition building examples where each diagnostic will fail to
do anything useful. While no single diagnostic provides strong guarantees of
variational inference's correctness on their own, taken together the tools
in this post broaden our ability to know when our models fall short.

The rough plan for the series is as follows:

1.  Introducing the Problem- Why is VI useful, why VI can produce spherical cows
2.  How far does iteration on classic VI algorithms like mean-field and full-rank get us?
3.  Problem 1: KL-D prefers exclusive solutions; are there alternatives?
4.  Problem 2: Not all VI samples are of equal utility; can we weight them cleverly?
5.  Problem 3: How can we get deeply flexible variational approximations; are Normalizing Flows the answer?
6.  **(This post)** Problem 4: How can we know when VI is wrong? Are there useful error bounds?
7.  Putting the workflow all together

# Looking at our loss function

One logical place to start with diagnostics is to discuss what we can and can't
infer from our optimization objectives like an ELBO or CUBO.

In training a model with variational inference some common stopping rule choices
are either to just run optimization for a fixed number of iterations, or to
stop when relative changes in the loss have slowed, indicating convergence of
the optimization to a local minimum. So we can at least look at changes in
the ELBO/CUBO/other loss to know if our approximation has hit a local minimum yet.

Unfortunately, that's about all monitoring the loss can tell us. Recall that An unknown,
multiplicative constant exists in $p(z,x) \propto p(z|x)$ that changes as
reparameterize our model; thus, we can't compare two different models on the
same objective and expect their ELBO or similar loss values to be comparable. So
the typical ML strategy of "which model achieves lower loss" is pretty much
out here.

Also, the loss values themselves aren't particularly meaningful: there's no
way to interpret a given ELBO as indicating a good approximation, for example. This
generally stems from our bounds being bounds, not directly optimizing the quantity
we want to optimize. While they're definitely degenerate cases, there are even some
fun counter examples I'll show in a second where you can make the ELBO/CUBO arbitrarily low, while
still allowing a mean or standard deviation to be arbitrarily wrong!

# The majesty of $\hat{k}$

So

# Wasserstein Bounds

# MCMC based diagnostics; what's old is new again

## MCMC can be practically useful even when slow

## TADDAA

# Diagnostics that don't spark joy