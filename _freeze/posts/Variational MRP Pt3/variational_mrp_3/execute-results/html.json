{
  "hash": "ac56c0717cdb5e67532ac5cea5d75c05",
  "result": {
    "markdown": "---\nlayout: post\ntitle: Variational Inference for MRP with Reliable Posterior Distributions\nsubtitle: Part 3- Some theory on why VI is hard\ndate: 2022-12-03\ndraft: True\ncategories:\n- MRP\n- Variational Inference\n---\n\n\nThis is section 3 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.\n\nIn the [last post](https://andytimm.github.io/posts/Variational%20MRP%20Pt2/Variational_MRP_pt2.html) we threw caution to the wind, and tried out some simple variational inference implementations, to build up some intuition about what bad VI might look like. Just pulling a simple variational inference implementation off the shelf and whacking run perhaps unsurprisingly produced dubious models, so in this post we'll bring in long overdue theory to understand why VI is so difficult, and what we can do about it.\n\nThe general structure for this post will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. I also have a grab bag of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below at the end.\n\nThe rough plan for the series is as follows:\n\n1.  Introducing the Problem- Why is VI useful, why VI can produce spherical cows\n2.  How far does iteration on classic VI algorithms like mean-field and full-rank get us?\n3.  **(This post)** Some theory on why posterior approximation with VI can be so poor\n4.  Better grounded diagnostics and workflow\n5.  Seeing if some more sophisticated techniques like normalizing flows help\n\nDraft problems/solutions:\n\n# Inclusive versus Exclusive KL-divergence\n\nLike I mentioned in the first post in the series, the Evidence Lower Bound (ELBO), our optimization objective we're working with is a tractable approximation of the Kullback-Leibler Divergence between our choice of approximating distribution $q(z)$ to our true posterior $p(z)$.\n\nThe KL divergence is asymmetric: in general, $KL(p||q) \\neq KL(q||p)$. Previously,\nwe saw that this asymmetry mattered quite a bit for our ELBO idea:\n\n$$argmin_{q(z) \\in \\mathscr{Q}}(q(z)||\\frac{p(z,x)}{\\bf p(x)}) = \\mathbb{E}[logq(z)] - \\mathbb{E}[logp(z,x)] + {\\bf logp(x)}$$\nWe can't calculate the bolded term $logp(x)$; if we could we wouldn't be finding\nthis inference thing so hard in the first place. The way we sidestepped that with\nthe ELBO is to note that the term is constant with respect to $q$; so we can\ngo on our merry way minimizing the above without it.\n\nIf we flip the divergence around though, we've got an issue. That term would\nthen be a $logq(x)$ ... which we can't write off in the same way- it varies as we\noptimize. So if we're\ndoing this ELBO minimizing version of variational inference, we're obligated to\nuse this \"reverse\" KL divergence, the second option below. \n\n\n$$\n\\begin{align}\nKL(p||q) = \\sum_{x \\in X}{p(x)}log[\\frac{p(x)}{q(x)}]  \\\\\nKL(q||p) = \\sum_{x \\in X}{q(x)}log[\\frac{q(x)}{p(x)}] \n\\end{align}\n$$\n\nUnfortunately, this choice to optimize the \"reverse\" KL divergence bakes in preference\nfor a certain type of solution[^1]. \n\nI found I built better intuition for this encoded preference after seeing it presented many\ndifferent ways, so here are a few of my favorites.\n\nOne way to see the difference is through a variety of labels for each direction. One could call Forward KL (1) vs. Reverse KL (2):\n\n1. Inclusive vs. Exclusive (my favorite, and so what I'm using for the section header)\n2. Mean Seeking vs. Mode Seeking\n3. Zero Avoiding vs. Zero Forcing\n\nlet's quickly illustrate this in the case of a simple mixture of normals:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(tidyverse)\n\nmixture <- data.frame(normals = c(rnorm(1000,3,1),rnorm(1000,15,2)),\n                      mode_seeking_kl = rnorm(2000,3.5,2),\n                      mean_seeking_kl = rnorm(2000,9,4))\n\nrkl_plot <- mixture %>% ggplot(aes(x = normals)) +\n  geom_density(aes(x = normals), color = \"red\") +\n  geom_density(aes(x = mode_seeking_kl), color = \"green\") + ggtitle(\"Exclusive KL\") +\n  xlab(\"\")\n\nfkl_plot <- mixture %>% ggplot(aes(x = normals)) +\n  geom_density(aes(x = normals), color = \"red\") +\n  geom_density(aes(x = mean_seeking_kl), color = \"green\") + ggtitle(\"Inclusive KL\") +\n  xlab(\"\")\n\ngrid.arrange(rkl_plot,fkl_plot)\n```\n\n::: {.cell-output-display}\n![](variational_mrp_3_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nTo approximate the same exact red distribution $p(x)$, Inclusive KL (1) and Exclusive KL (2)\noptmize the green $q(p)$ in quite different manner. \n\nTo spell out the ways to describe this above: Inclusive KL will try to cover all the probability mass in $p(x)$, even if it means a peak\nat a unfortunate middle ground. Exclusive KL, on the other hand, will try to concentrate\nit's mass on the largest mode, even if it means missing much of the mixture of normals. Alternatively,\nwe could describe the top graph as mode seeking, and the bottom as mean seeking. Finally,\nwe could say the top graph shows \"Zero Forcing\" behavior- it will heavily favor\nputting zero mass on some parts of the graph to avoid any weight where $p(x)$ has no\nmass, even if it means missing an entire mode. Conversely, Inclusive KL will\naim to cover all the mass of $p(x)$ in full even if the result is an odd solution, in order\nto avoid having zero mass where $p(x)$ has some.\n\nHow does this follow from the form of the divergence?\n\nTo start with, notice that for inclusive KL we could think of the $log(\\frac{p(x)}{q(x)})$ part of the term being weighted by $p(x)$- if in some range of $x$ $p(x)$ is 0, we don't pay a penalty if $q(x)$ puts mass. The reverse is not true however- if our $q(x)$ is zero where there should be mass\nin our true distribution, our Inclusive KL divergence is infinite[^2].\n\n$$\n\\begin{align}\nKL(p||q) = \\sum_{x \\in X}{p(x)}log[\\frac{p(x)}{q(x)}]  \\\\\nKL(q||p) = \\sum_{x \\in X}{q(x)}log[\\frac{q(x)}{p(x)}] \n\\end{align}\n$$\n\nAnd if we change the direction of the divergence, the opposite zeros and infinities\nshow up, enforcing strong preferences for a specific type of solution.\n\nWhen the example is a simple mix of two gaussians approximated with a single\ngaussian, it's fairly easy to intuit how the choice of KL divergence will influence\nthe optimization solution. This all gets a bit more opaque on harder problems-\nlike we saw with the example last post, ELBO based VI will tend to underestimate\nthe support of $p(x)$ but whether the solution is narrow but overall reasonable, or pretty much degenerate\nis hard to predict. However, this exploration of how the form of the divergence\ninfluences the results still gives a rough intuition for why our ELBO optimized\nposteriors might collapse.\n\nIf we want to try the opposite direction of KL divergence, it isn't immediately obvious there's a\nglobal objective we can choose to perhaps seek out a optimization problem\nthat favors overdispersed solutions. Like I mentioned above, if we try to make\nan ELBO-esque target but reverse the KL divergence, the $logp(x)$ which is constant with respect\nto the $q(x)$ we're optimizing becomes a $logq(x)$ which we can't so easily work around.\n\nLet's look first at a solution in the spirit of VI[^3] to the above problem which requires us to pick up a new divergence, the $\\chi^{2}$-divergence, and optimizes a new bound. Let's take a look at it.\n\n## $\\chi^{2}$ Variational Inference (CHIVI) and the CUBO bound\n\nThe $\\chi^{2}$-divergence has form:\n\n$$\nD_{\\chi^2}(p||q) = \\mathbb{E}_{q(z;\\lambda)}[(\\frac{p(z|x)}{q(z;\\lambda)})^2 -1]\n$$\nFor simplicity and comparability, I'm switching here to using Dieng et Al. (2017)'s notation here- they use $q(z;\\lambda)$ to refer to the variational family we're using, indexed by parameters $\\lambda$.\n\nThis divergence has the properties we wanted when we tried to use Inclusive KL Divergence- it tends to be mean seeking instead of mode seeking.\n\nLike with the ELBO, we need to show that we have a bound here independent of $logp(x)$, and that we have a way to estimate that bound efficiently.\n\nLet's first move around a few pieces of the first term above:\n\n$$\n\\begin{align}\n\\mathbb{E}_{q(z;\\lambda)}[(\\frac{p(z|x)}{q(z;\\lambda)})^2& = 1 + D_{\\chi^2}(p(z|x)|q(z;\\lambda)) \\\\\n&= p(x)^2[1 + D_{\\chi^2}(p(z|x)|q(z;\\lambda))]\n\\end{align}\n$$\nThen we can take the log of both the right hand side equations, which gives us:\n\n$$\n\\frac{1}{2}log(1 + D_{\\chi^2}(p(z|x)|q(z;\\lambda))) = -logp(x) + \\frac{1}{2}log\\mathbb{E}_{q(z;\\lambda)}[(\\frac{p(z|x)}{q(z;\\lambda)})^2]  \n$$\n...and this is starting to feel a lot like the ELBO derivation. Log is monotonic, and logp(x) is constant as we optimize $q$, so we've found something that we're close to able to minimize:\n\n$$\nCUBO_{2}(\\lambda) = \\frac{1}{2}log\\mathbb{E}_{q(z;\\lambda)}[(\\frac{p(z|x)}{q(z;\\lambda)})^2]\n$$\nSince this new divergence is non-negative as well, this is a upper bound of the model evidence. This is thus named $\\chi$ upper bound (CUBO)[^4].\n\n## ... But can we estimate it?\n\nOne other issue here: how do we estimate this? The CUBO objective got rid of the $logp(x)$ we were worried about, but it seems like that expectation is going to be difficult to estimate in general. \n\nIf you're a good Bayesian your first idea might be to Monte Carlo (not MCMC) estimate it roughly like this:\n\n$$\nCUBO_2(\\lambda) = \\frac{1}{2}log\\frac{1}{S}\\sum_{s=1}^{S}[(\\frac{p(x,z^{s})}{q(z^{s};\\lambda)})^2]\n$$\nUnfortunately, the $log$ transform here means our Monte Carlo estimator will be biased: we can see this by applying Jensen's inequality to the above. To make this stably act as an upper bound, we can apply a clever transformation:\n\n$$\n\\bf{L} = exp(n* CUBO_2(\\lambda))\n$$\n\nSince exp is monotonic, this has the same objective as the CUBO, but we can Monte Carlo estimate it unbiasedly. Is that the last problem to solve?\n\n## ... But can we calculate gradients efficiently?\n\nWait, wait no. Sorry to keep saying there's one more step here, but there's a lot that goes into making a full, convenient, general use algorithm here. The last step (for real this time) is that we need to figure out how to get gradients for the estimate of $\\bf{L}$ above, $\\bf{\\hat{L}}$. The issue is that we don't have any guarantee that a unbiased Monte Carlo estimator of $\\bf{\\hat{L}}$ gets us a Monte Carlo way to estimate $\\nabla_\\lambda\\bf{\\hat{L}}$- we can't guarantee that the gradient of the expectation is equal to the expectation of the gradient[^5].\n\nFor this, we need to pull out a trick from the variational autoencoder literature. This is usually referred to as the \"Reparameterization Trick\", but the original CHIVI paper refers to them as \"reparmeterization gradients\". We will assume we can rewrite the generative process of our model as $z = g(\\lambda,\\epsilon)$, where $\\epsilon \\sim p(\\epsilon)$ and g being a deterministic function. Then we have a new estimator for both $\\bf{\\hat{L}}$ and it's gradient:\n\n$$\n\\begin{align}\n\\bf{\\hat{L}} &= \\frac{1}{B}\\sum_{b=1}^B(\\frac{p(x,g(\\lambda,\\epsilon^{(b)}))}{q(g(\\lambda,\\epsilon^{(b)};\\lambda))})^{2} \\\\ \n\\nabla_\\lambda\\bf{\\hat{L}}  &= \\frac{2}{B}\\sum_{b=1}^B(\\frac{p(x,g(\\lambda,\\epsilon^{(b)}))}{q(g(\\lambda,\\epsilon^{(b)};\\lambda))})^2 \\nabla_\\lambda log(\\frac{p(x,g(\\lambda,\\epsilon^{(b)}))}{q(g(\\lambda,\\epsilon^{(b)};\\lambda))})\n\\end{align}\n$$\n\n# Not all points are equal \n\n## PSIS-Variational Inference\n\n# Can we bound error in terms of ELBO or CHIVI?\n\n## Wasserstein Bounds\n\n# Conclusions\n\n[^1]: In truth, both KL divergences encode structural preferences for the type\nof optimization solution they admit- neither will be the right choice for every\nproblem and  variational family combination. But as we'll see, being able to\nchoose will give us more options to fit models we believe.\n\n[^2]: This is the footnote for those of you that are annoyed because you tried\nto write out how this would happen, and got something like $p(x) log \\frac{p(x)}{0}$,\nwhich should be undefined if we're following normal math rules. But this is information\ntheory, and in this strange land we say $p(x) log \\frac{p(x)}{0} = \\infty$. I don't\nhave a strong intuition for why this is the best solution, but a information encoding perspective\nmakes it make more sense at least: if we know the distribution of $p$,\nwe can construct a code for it with average description length $H(x)$. One way to understand\nthe KL divergence is as what happens when we try to use the code for a distribution $q$ to describe $p$,\nwe'd need $H(p) + KL(p||q)$ bits on average to describe $p$. In the code for $q$ has no way to represent some element of $p$, then requiring... infinite bits feels like the right way to describe the breakdown of meaning? All this to say this condition is something our optimizer will try hard to avoid.\n[^3]: I'll mention an alternative approach, Expectation Propagation, that takes a different (not global objective based) approach further down.\n[^4]: This approach actually defines a family of $n$ new divergences, where you replace the $\\frac{1}{2}$ with $\\frac{1}{n}$ and similarly replace the square an exponent with n. Fully stepping through why this is neat wasn't worth how far afield it'd take us, but the original $CHIVI$ paper has some cool derivations based on this, one of which I'll discuss on the next section on importance sampling. \n[^5]: I thought about a section to explain the reparameterization trick, but there are enough good explanations online of the trick. If you're interested in learning more about why this is important for optimization through stocastic models, I'd recommend starting with Gregory Gundersen's explanation [here](https://gregorygundersen.com/blog/2018/04/29/reparameterization/) and then move on to the original Kingma & Welling, 2013 paper. As general advice on understanding it better though, I'll echo Greg's point that some of the online explanations I've seen are a bit loose- the key is that we want to express a gradient of an expectation (can't MC estimate for sure) as an expectation of a gradient (which we can MC estimate provided our convenient deterministic function $g$ is differentiable). ",
    "supporting": [
      "variational_mrp_3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}