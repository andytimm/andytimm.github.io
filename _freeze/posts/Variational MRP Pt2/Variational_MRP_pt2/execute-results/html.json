{
  "hash": "6b31bc5927aef21bffa77ac5fb5cb01a",
  "result": {
    "markdown": "---\nlayout: post\ntitle: Variational Inference for MRP with Reliable Posterior Distributions\nsubtitle: Part 2\ndate: 2022-10-28\ndraft: True\ncategories:\n- MRP\n- BART\n- Variational Inference\n---\n\n\nThis is the second post in my series on using Variational Inference to speed up relatively complex Bayesian like Multilevel Regression and Poststratification models without the approximation being of disastrously poor quality. In the last post, I laid out why such reformulating the Bayesian inference problem as optimization might be desirable, but previewed why this might be quite hard to find high quality approximations amenable to optimization. I then introduced our running example (predicting national/sub-national opinion on an abortion question form the CCES using MRP), and gave an initial introduction to a version of Variational Inference where we maximize the Evidence Lower Bound (ELBO) as an objective, and do so using a mean-field Gaussian approximation. We saw that with 60k examples, this took about 8 hours to fit with MCMC, but 144 seconds (!) with VI.\n\nIn this post, we'll explore the shortcomings of this initial approximation, and take a first pass at trying to better with a more complex (full rank) variational approximation. The goal is to get a better feel for what failing models could look like, at least in this relatively simple case.\n\nThe rough plan for the series is as follows:\n\n1.  [Introducing the Problem- Why is VI useful, why VI can produce spherical cows](https://andytimm.github.io/posts/Variational%20MRP%20Pt1/variational_mrp_pt1.html)\n2.  **(This post)** How far does iteration on classic VI algorithms like mean-field and full-rank get us?\n3.  Some theory on why posterior approximation with VI can be so poor\n4.  Seeing if some more sophisticated techniques like normalizing flows help\n\n\n::: {.cell}\n\n:::\n\n\n# The disclaimer\n\nOne sort of obvious objections to how I've set up this series is \"Why not talk about theory on why VI approximations can be poor before trying stuff?\". While in practice I did read a lot of the papers for the next post before writing this one, I think there's a lot of value is looking at failed solutions to a problem to build up intuition about what our failure mode looks like, and what it might require to get it right. <Example or two to tease this once I have them>.\n\n# Toplines\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeanfield_60k <- readRDS(\"fit_60k_meanfield.rds\")\nmcmc_60k <- readRDS(\"fit_60k_mcmc.rds\")\n\n# Meanfield \nepred_mat_mf <- posterior_epred(meanfield_60k, newdata = poststrat_df_60k, draws = 1000)\nmrp_estimates_vector_mf <- epred_mat_mf %*% poststrat_df_60k$n / \n                                              sum(poststrat_df_60k$n)\nmrp_estimate_mf <- c(mean = mean(mrp_estimates_vector_mf),\n                     sd = sd(mrp_estimates_vector_mf))\n\n\n# MCMC \nepred_mat_mcmc <- posterior_epred(mcmc_60k, newdata = poststrat_df_60k, draws = 1000)\nmrp_estimates_vector_mcmc <- epred_mat_mcmc %*% poststrat_df_60k$n /\n                                                  sum(poststrat_df_60k$n)\nmrp_estimate_mcmc <- c(mean = mean(mrp_estimates_vector_mcmc),\n                       sd = sd(mrp_estimates_vector_mcmc))\n\ncat(\"Meanfield MRP estimate mean, sd: \", round(mrp_estimate_mf, 3))\ncat(\"MCMC MRP estimate mean, sd: \", round(mrp_estimate_mcmc, 3))\n```\n:::\n\n\n|   | Mean  | SD  |\n|---|---|---|\n| MCMC  | 43.9%  |  .2% |\n| mean-field VI  |  43.7% | .2%  |\n\nStarting with basics, the toplines are pretty much identical, which is a good start. The minor difference here could easily be simulation error- from a few quick re-runs these\noften end up having matching means to 3 decimals.\n\n# State Level Estimates\n\nWhat happens if we produce state level estimates, similar to the plot last post\ncomparing MRP to a simple weighted estimate? Note that I'll steer away from\nthe MRP Case Study example here in a few ways. I'll use `tidybayes` for\nworking with the draws (more elegant than their loop based approach),\nand I'll use more draws (helps with simulation error in smaller states).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_state_level <- poststrat_df_60k %>% add_epred_draws(mcmc_60k)\nmfvi_state_level <- poststrat_df_60k %>% add_epred_draws(meanfield_60k)\n\nmcmc_state_level %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 48,000,000\nColumns: 14\nGroups: state, eth, male, age, educ, n, repvote, region, state_proportion, .row [12,000]\n$ state            <chr> \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\",…\n$ eth              <chr> \"White\", \"White\", \"White\", \"White\", \"White\", \"White\",…\n$ male             <dbl> -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,…\n$ age              <chr> \"18-29\", \"18-29\", \"18-29\", \"18-29\", \"18-29\", \"18-29\",…\n$ educ             <chr> \"No HS\", \"No HS\", \"No HS\", \"No HS\", \"No HS\", \"No HS\",…\n$ n                <dbl> 23948, 23948, 23948, 23948, 23948, 23948, 23948, 2394…\n$ repvote          <dbl> 0.6437414, 0.6437414, 0.6437414, 0.6437414, 0.6437414…\n$ region           <chr> \"South\", \"South\", \"South\", \"South\", \"South\", \"South\",…\n$ state_proportion <dbl> 0.00652231, 0.00652231, 0.00652231, 0.00652231, 0.006…\n$ .row             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ .chain           <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ .iteration       <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ .draw            <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ .epred           <dbl> 0.5336533, 0.5153217, 0.5350318, 0.4859690, 0.5187578…\n```\n:::\n:::\n\n\nIf you haven't worked with `tidybayes` before, the glimpse above should help\ngive some intuition about the new shape of the data- we've take the 12,000 row\n`poststrat_df_60k`, and added a row per observation per draw, with the prediction\n(.epred) and related metadata. This gives\n12,000 x 4,000 = 48,000,000 million rows. This really isn't the most space\nefficient storage, but it allows for very elegant `dplyr` style manipulation of\nresults and quick exploration.\n\n# More Subtle Interactions\n\n# Do 'Minor' corrections solve anything?\n\n## Lowering the tolerance\n\n## What if we evaluate Holdout ELBO\n\n# Where to from here? (Why is it like this!?)",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}