{
  "hash": "1044d7d07d6b9d655b5335d649bc14c3",
  "result": {
    "markdown": "---\nlayout: post\ntitle: Variational Inference for MRP with Reliable Posterior Distributions\nsubtitle: Part 5- Normalizing Flows\ndate: 2023-05-27\ndraft: True\ncategories:\n  - MRP\n  - Variational Inference\n  - Normalizing Flows\n---\n\nThis is section 5 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality. \n\nThe general structure for this post and the around it will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in these four posts will go a long way towards more robust variational inference. I'll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.\n\nIn the [last post](https://andytimm.github.io/posts/Variational%20MRP%20Pt4/variational_mrp_4.html) we saw a variety of different ways importance sampling can be used to improve VI and make it more robust, from defining a tighter bound to optimize in the importance weighted ELBO, to weighting $q(x)$ samples together efficiently to look more like $p(x)$, to combining entirely different variational approximations together to cover different parts of the posterior with multiple importance sampling.\n\nIn this post, we'll tackle the problem of how to define a deeply flexible variational\nfamily $\\mathscr{Q}$ that can adapt to each problem while still being easy to sample from.\nTo do this, we'll draw on normalizing flows, a technique for defining a composition\nof invertible transformations on top of a simple base distribution like a normal\ndistribution. We'll build our way up to using increasingly complex neural networks\nto define those transformations, allowing for for truly complex variational\nfamilies that are problem adaptive, training as we train our variational model.\n\nThe rough plan for the series is as follows:\n\n1.  Introducing the Problem- Why is VI useful, why VI can produce spherical cows\n2.  How far does iteration on classic VI algorithms like mean-field and full-rank get us?\n3.  Problem 1: KL-D prefers exclusive solutions; are there alternatives?\n4.  Problem 2: Not all VI samples are of equal utility; can we weight them cleverly?\n5.  **(This post)** Problem 3: How can we get deeply flexible variational approximations; are Normalizing Flows the answer?\n6. Problem 4: How can we know when VI is wrong? Are there useful error bounds?\n7. Better grounded diagnostics and workflow\n\n# A problem adaptive variational family with less tinkering?\n\n![](images/flows_stairs_meme.png){fig-alt=\"Something about NNs makes me meme more\"}\n\nJumping from mean-field or full-rank Gaussians and similar distributions\nto neural networks feels a little... dramatic[^1],  so I want to spend\nsome time justifying why this is a good idea.\n\nFor VI to work well, we need something that's still simple to sample from, but capable\nof, in aggregate, representing a posterior that is probably pretty complex. Certainly,\nsome problems are amenable to the simple variational families $\\mathscr{Q}$ we've tried so far,\nbut it's worth re-emphasizing that we're probably trying to represent something complex,\nand even moderate success at that using a composition of normals should be\na little surprising, not the expected outcome.\n\nIf we need $\\mathscr{Q}$ to be more complex, aren't there choices between what\nwe've seen and a neural network? There's a whole literature of them- from using\nmixture distributions as variational distributions to inducing some additional\nstructure into a mean-field type solution if you have some specific knowledge\nabout your target posterior you can use. By and large though, this type of\nclass of solutions has been surpassed by normalizing flows in much of modern\nuse for more complex posteriors.\n\nWhy? A first reason is described in the paper that started the normalizing flows\nfor VI literature, Rezende and Mohamed's [**Variational Inference with Normalizing Flows\n**](https://arxiv.org/pdf/1505.05770.pdf): making our base variational distribution\nmore complex adds a variety of different computational costs, which add up quickly.\nThis isn't the most face-valid argument when I'm claiming a neural network\nis a good alternative, but it gets more plausible when you think through\nhow poorly it'd scale to keep making your mixture distribution more and more\ncomplex as your posteriors get harder to handle. So this is a *scalability*\nargument- it might sound extreme to bring in a neural net, but as problems\nget bigger, scaling matters.\n\nThe other point I'd raise is that all these other tools aren't very black box at\nall- if we can make things work with a problem-adapted version of mean-field with\nsome structure based on the knowledge of a specific problem we have, that sounds\nlike it gets time consuming fast. If I'm going to have\nto find a particular, problem-specific solution each time I want to use variational\ninference, that feels fragile and fiddly as well- that's a poor user experience.\n\nThe novel idea with normalizing flows is that we'll start with a simple base\ndensity like a normal distribution that is easy to sample from, but instead of only optimizing the parameters\nof that normal distribution, we'll also use the training on our ELBO or\nother objective to learn a transformation that reshapes that normal distribution to\nlook like our posterior. By having that transforming component be partially\ncomposed of a neural network,\nwe give ourselves access to an incredibly expressive, automatically problem adaptive,\nand heavily scalable variant of variational inference that is quite\nwidely used.\n\nAnd if the approximation isn't expressive enough? Deep Learning researchers have\nan unfussy, general purpose innovation for that: MORE LAYERS![^2]\n\n![](images/more_layers.png){fig-alt=\"Wow such estimator, very deep\"}\n\n# What is a normalizing flow?\n\nA normalizing flow transforms a simple base density into a complex one through\na sequence of invertible transformations. By stacking more and more of these\ninvertible transformations (having the density \"flow\" through them), we can create\narbitrarily complex distributions that remain valid probability distributions. Since\nit isn't universal in the flows literature, let me be explicit that I'll consider\n\"forward\" to be the direction flowing from the base density to the posterior, and\nthe \"backward\" or \"normalizing\" direction as towards the base density.\n\n![Image Credit to [Simon Boehm](https://siboehm.com/articles/19/normalizing-flow-network) here](images/normalizing-flow.png)\n\nIf we have a random variable $x$, with distribution $q(x)$, some function $f$ with an inverse\n$f^{-1} = g, g \\circ f(x) = x$, then the distribution of the result of\none iteration of x through, $q^\\prime(x)$ is:\n\n$$\nq\\prime(z) = q(x) \\lvert det \\frac{\\partial f^{-1}}{\\partial x^\\prime} \\rvert = q(x) \\lvert \\frac{\\partial f}{\\partial x} \\rvert^{-1}\n$$\nI won't derive this identity[^3], but it follows from the chain rule and the\n properties of Jacobians of invertible functions.\n\nThe real power comes in here when we see that these transformations stack. If\nwe've got a chain of transformations (eg $f_K(...(f_2f_1(x))$:\n\n$$\nx_K = f(x) \\circ ... \\circ f_2 \\circ f_1(x_0)\n$$\n\nthen the resulting density $q_K(x)$ looks like:\n\n$$\nln q_K (x_K) = lnq_0(x_0) - \\sum \\limits_{K = 1}\\limits^{K} ln  \\lvert \\frac{\\partial f_k}{\\partial x_{k-1}} \\rvert^{-1}\n$$\n\nNeat, and surprisingly simple! If the terms above are all easy to calculate,\nwe can very efficiently stack a bunch of these transformations and make\nan expressive model.\n\n## Normalizing Flows for variational inference versus other applications\n\nOne source of confusion when I was learning about normalizing flows for\nvariational inference was that variational inference makes up a fairly\nsmall proportion of the use cases for normalizing flows, and thus the academic\nliterature and online discussion. More common applications include density estimation, image generation,\nrepresentation learning, and reinforcement learning. In addition to making specifically applicable\ndiscussions harder to find, often resources will make strong claims about properties of a given\nflow structure, that really only holding in some subset of the above applications[^4].\n\nBy taking a second to explain this crisply and compare different application's needs,\nhopefully I can save you some confusion and make engaging with the broader literature easier.\n\nTo start, consider the relevant operations we've introduced so far:\n\n1. computing $f$, that is pushing a sample through the transformations\n2. computing $g$, $f$'s inverse which undoes the manipulations\n3. computing the (log) determinant of the Jacobian\n \n1 and 3 definitely need to be efficient for our use case, since we need to be\nable to sample and push through using the formula above efficiently to calculate\nan ELBO and train our model. 2 is where things get\nmore subtle: we definitely need $f$ to be invertible, since our formulas above\nare dependent on a property of Jacobians of invertible functions. But we don't\nactually really need to explicitly compute $g$ for variational inference. Even knowing the inverse\nexists but not having a formula might be fine for us!\n\nContrast\nthis with density estimation, where the goal would not to sample from the distribution,\nbut instead to estimate the density. In this case, most of the time would be\nspent going in the opposite direction, so that they can evaluate the log-likliehood\nof the data, and maximize it to improve the model[^5]. The need for an expressive\ntransformation of densities unite these two cases, but the goal is quite different!\n\nThis level of goal disagreement also shows it face in what direction papers\nchoose to call forward: Most papers outside of variational inference applications consider forward to be the opposite of what I do here, the direction towards\nthe base density, the \"normalizing\" direction. \n\nFor our use, hopefully this short digression has clarified which operations we need to be\nfast versus just exist. If you dive deeper into\nfurther work on normalizing flows, hopefully recognizing there are two\ndifferent ways to point this thing helps you more quickly orient yourself\nto how other work describe flows.\n\n# How to train your neural net\n\nNow, let's turn to how we actually fit a normalizing flow. Since this would be a bit\nhard to grok a code presentation if I took advantage of the full flexibility and abstraction that\nsomething like [`vistan`](https://github.com/abhiagwl/vistan/tree/master) provides, before\nheading into general purpose tools I'll talk through a bit more explicit implementation\nof a simpler flow called a planar flow `PyTorch` for illustration. Rather than\nreinventing the wheel, I'll leverage Edvard Hulten's implementation [here](https://github.com/e-hulten/planar-flows).\n\nIn this section,\nI'll define conceptually how we're fitting the model, and build out a fun\ntarget distribution and loss`- since I expect many people reading\nthis may moderately new to PyTorch, I'll explain in detail\nthan normal what each operation is doing and why we need it.\n\n\n\nLet's first make a fun target posterior distribution from an image to model. I\nthink it'd be a fun preview gif for the post to watch the model say Hi:\n\n![](images/hard_to_draw_posterior.png){fig-alt=\"Wow such estimator, very deep\"}\n\nIt's quick to turn the 300x300 pixel image above into a 300x300 PyTorch tensor.\nTo represent this as a 2-D density we can fit models against, we'll read in\nthe image, collapse along the color dimension, and transform it into a torch\ntensor:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nraw_img = Image.open(\"images/hard_to_draw_posterior.png\")\n\n# Sum the 3 color channels\ngreyscale_img = np.array(raw_img).sum(axis = 2)\n\n# Replace white values (1020), with 0, so density is all at letters\nindices = greyscale_img == 1020\n\n# Replace the selected values with 0\ngreyscale_img[indices] = 50\n\n\n# Normalize values to help with fitting\nnormalized_image  = (greyscale_img - greyscale_img.min()) / (10000000)\n \n# Make a torch tensor of the target to use\ntorch_posterior = torch.from_numpy(normalized_image).to(device)\n\n#300 x 300 px, normalized grayscale representation of the above image\ntorch_posterior.sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\ntensor(0.9065, device='cuda:0', dtype=torch.float64)\n```\n:::\n:::\n\n\nNow let's define our loss for training, which will just be a slight\nreformulation of our ELBO:\n\n$$\n \\mathbb{E}[logp(z,x)] - \\mathbb{E}[logq(z)]\n$$\n\nTo do this, we'll define a class for the loss.\n\nFirst, we pick a simple base distribution to push through our flow, here a \n2-D Normal distribution called `base_distr`. We'll also include the interesting\ntarget we just made above, `distr`.\n\nNext, the forward pass structure. The `forward` method is the is the core of the computational graph structure in PyTorch. It defines operations that are applied to the input tensors to compute the output, and \ngives PyTorch the needed information for automatic differentiation, which allows smooth calculation\nand backpropogation of loss through the model to train it. This `VariationalLoss`\nmodule will run at the end of the forward pass to calculate the loss and allow us\nto pass it back through the graph for training.\n\nKeeping with the structure above of numbering successive stages of the flow,\n`z0` here is our base distribution, and `z` will be the learned approximation\nto the target. In addition to the terms you'd expect in the ELBO, we're also\ntracking and making use of the sum of the log determinant of the Jacobians to\na handle on the distortion of the base density the flows apply.\n\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# https://github.com/e-hulten/planar-flows/blob/master/loss.py\nclass VariationalLoss(nn.Module):\n  def __init__(self,distribution):\n      super().__init__()\n      self.distr = distribution\n      self.base_distr = MultivariateNormal(torch.zeros(2), torch.eye(2))\n\n  def forward(self, z0: Tensor, z: Tensor, sum_log_det_J: float) -> float:\n      base_log_prob = self.base_distr.log_prob(z0)\n      target_density_log_prob = -self.distr(z)\n      return (base_log_prob - target_density_log_prob - sum_log_det_J).mean()\n```\n:::\n\n\n# A basic flow\n\nNext, let's define the structure of the actual flow. To do this, we'll first\ndescribe a single layer of the flow, then we'll show structure to stack\nthe flow in layers.\n\nOur first flow we look at will be the **planar flow** from the original\nNormalizing Flows for variational Inference paper mentioned above. The name\ncomes from how the function defines a (hyper)plane, and compress or expand\nthe density around it:\n\n$$\nf(x) = x + u*tanh(w^Tx + b), w, u \\in \t\\mathbb{R}^d, b \\in\t\\mathbb{R} \n$$\n\n$w$ and $b$ define the hyperplane and u specifies the direction and strength\nof the expansion. I'll show a visualization of just one layer of that below.\n\nIf you're more used to working with neural nets, you might wonder why we\nchoose the non-linearity $tanh$ here, which generally isn't as popular as something\nlike $relu$ or it's variants in more recent years due to it's more unstable\ngradient flows. As the authors show in appendix $A.1$, functions like the\nabove aren't actually always invertible, and choosing $tanh$ allows them\nto impose some constraints that make things reliably invertible. See the Appendix\nfor more details about how that works, or take a careful look at Edvard's\nimplementation of the single function below:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# From https://github.com/e-hulten/planar-flows/blob/master/planar_transform.py\n\nclass PlanarTransform(nn.Module):\n  \"\"\"Implementation of the invertible transformation used in planar flow:\n      f(z) = z + u * h(dot(w.T, z) + b)\n  See Section 4.1 in https://arxiv.org/pdf/1505.05770.pdf. \n  \"\"\"\n\n  def __init__(self, dim: int = 2):\n      \"\"\"Initialise weights and bias.\n      \n      Args:\n          dim: Dimensionality of the distribution to be estimated.\n      \"\"\"\n      super().__init__()\n      self.w = nn.Parameter(torch.randn(1, dim).normal_(0, 0.1))\n      self.b = nn.Parameter(torch.randn(1).normal_(0, 0.1))\n      self.u = nn.Parameter(torch.randn(1, dim).normal_(0, 0.1))\n\n  def forward(self, z: Tensor) -> Tensor:\n      if torch.mm(self.u, self.w.T) < -1:\n          self.get_u_hat()\n\n      return z + self.u * nn.Tanh()(torch.mm(z, self.w.T) + self.b)\n\n  def log_det_J(self, z: Tensor) -> Tensor:\n      if torch.mm(self.u, self.w.T) < -1:\n          self.get_u_hat()\n      a = torch.mm(z, self.w.T) + self.b\n      psi = (1 - nn.Tanh()(a) ** 2) * self.w\n      abs_det = (1 + torch.mm(self.u, psi.T)).abs()\n      log_det = torch.log(1e-4 + abs_det)\n\n      return log_det\n\n  def get_u_hat(self) -> None:\n      \"\"\"Enforce w^T u >= -1. When using h(.) = tanh(.), this is a sufficient condition \n      for invertibility of the transformation f(z). See Appendix A.1.\n      \"\"\"\n      wtu = torch.mm(self.u, self.w.T)\n      m_wtu = -1 + torch.log(1 + torch.exp(wtu))\n      self.u.data = (\n          self.u + (m_wtu - wtu) * self.w / torch.norm(self.w, p=2, dim=1) ** 2\n      )\n```\n:::\n\n\nWhere things will start to get exciting is multiple layers of the flow; here's\nhow we can make an abstraction that allows us to stack up $K$ layers\nof the flow to control the flexibility of our approximation.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nclass PlanarFlow(nn.Module):\n    def __init__(self, dim: int = 2, K: int = 6):\n        \"\"\"Make a planar flow by stacking planar transformations in sequence.\n\n        Args:\n            dim: Dimensionality of the distribution to be estimated.\n            K: Number of transformations in the flow. \n        \"\"\"\n        super().__init__()\n        self.layers = [PlanarTransform(dim) for _ in range(K)]\n        self.model = nn.Sequential(*self.layers)\n\n    def forward(self, z: Tensor) -> Tuple[Tensor, float]:\n        log_det_J = 0\n\n        for layer in self.layers:\n            log_det_J += layer.log_det_J(z)\n            z = layer(z)\n\n        return z, log_det_J\n```\n:::\n\n\nLet's run this for a single layer to introduce the training loop, and build some\nintuition on the planar flow. Note that I'm hiding setting up the plot code.\n\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n#From https://github.com/e-hulten/planar-flows/blob/master/train.py\ntarget_distr = \"ring\"  # U_1, U_2, U_3, U_4, ring\nflow_length = 2\ndim = 2\nnum_batches = 20000\nbatch_size = 128\nlr = 6e-4\naxlim = xlim = ylim = 5  # 5 for U_1 to U_4, 7 for ring\n# ------------------------------------\n\ndensity = TargetDistribution(target_distr)\nmodel = PlanarFlow(dim, K=flow_length)\nbound = VariationalLoss(density)\noptimiser = torch.optim.Adam(model.parameters(), lr=lr)\n\n# Train model.\nfor batch_num in range(1, num_batches + 1):\n    # Get batch from N(0,I).\n    batch = torch.zeros(size=(batch_size, 2)).normal_(mean=0, std=1)\n    # Pass batch through flow.\n    zk, log_jacobians = model(batch)\n    \n    # Compute loss under target distribution.\n    loss = bound(batch, zk, log_jacobians)\n\n    optimiser.zero_grad()\n    loss.backward()\n    optimiser.step()\n\n    if batch_num % 100 == 0:\n        print(f\"(batch_num {batch_num:05d}/{num_batches}) loss: {loss}\")\n        #print(log_jacobians)\n\n    if batch_num == 1 or batch_num % 100 == 0:\n        # Save plots during training. Plots are saved to the 'train_plots' folder.\n        plot_training(model, flow_length, batch_num, lr, axlim) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Python311\\Lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 00100/20000) loss: 23.53183937072754\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\timma\\AppData\\Local\\Temp\\ipykernel_1480\\4251992244.py:69: UserWarning: The input coordinates to pcolormesh are interpreted as cell centers, but are not monotonically increasing or decreasing. This may lead to incorrectly calculated cell edges, in which case, please supply explicit cell edges to pcolormesh.\n  ax.pcolormesh(\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 00200/20000) loss: 20.994998931884766\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 00300/20000) loss: 20.90053939819336\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 00400/20000) loss: 18.84044647216797\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 00500/20000) loss: 16.77310562133789\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 00600/20000) loss: 16.30998992919922\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 00700/20000) loss: 11.962639808654785\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 00800/20000) loss: 11.01216983795166\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 00900/20000) loss: 10.37910270690918\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 01000/20000) loss: 8.418041229248047\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 01100/20000) loss: 6.021894931793213\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 01200/20000) loss: 5.717703819274902\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 01300/20000) loss: 3.8855597972869873\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 01400/20000) loss: 3.86718487739563\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 01500/20000) loss: 3.907670021057129\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 01600/20000) loss: 1.847822904586792\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 01700/20000) loss: 4.638023853302002\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 01800/20000) loss: 3.2167813777923584\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 01900/20000) loss: 1.8979766368865967\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 02000/20000) loss: 2.2743802070617676\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 02100/20000) loss: 1.3806898593902588\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 02200/20000) loss: 1.5938010215759277\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 02300/20000) loss: 2.74418044090271\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 02400/20000) loss: 3.166958808898926\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 02500/20000) loss: 2.990528106689453\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 02600/20000) loss: 1.1335718631744385\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 02700/20000) loss: 1.306429386138916\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 02800/20000) loss: 1.2546987533569336\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 02900/20000) loss: 0.30904117226600647\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 03000/20000) loss: 1.9958691596984863\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 03100/20000) loss: 1.4669091701507568\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 03200/20000) loss: 1.7111603021621704\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 03300/20000) loss: 1.8202102184295654\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 03400/20000) loss: 1.8184399604797363\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 03500/20000) loss: 1.2064381837844849\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 03600/20000) loss: 0.8316801190376282\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 03700/20000) loss: 0.12350399792194366\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 03800/20000) loss: 0.7493021488189697\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 03900/20000) loss: 1.421569585800171\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 04000/20000) loss: 0.44843626022338867\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 04100/20000) loss: 0.6824027895927429\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 04200/20000) loss: 0.7544111013412476\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 04300/20000) loss: 0.8090195655822754\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 04400/20000) loss: 0.5537429451942444\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 04500/20000) loss: 0.04105612635612488\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 04600/20000) loss: 0.6974716186523438\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 04700/20000) loss: 0.19744260609149933\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 04800/20000) loss: 0.20983920991420746\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 04900/20000) loss: -0.1809486746788025\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 05000/20000) loss: 0.37607288360595703\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 05100/20000) loss: 0.9134331941604614\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 05200/20000) loss: 0.3730510175228119\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 05300/20000) loss: -0.47192516922950745\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 05400/20000) loss: 0.8290470838546753\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 05500/20000) loss: 1.3974835872650146\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 05600/20000) loss: 0.09541454911231995\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 05700/20000) loss: -0.13839580118656158\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 05800/20000) loss: 0.11648982018232346\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 05900/20000) loss: -0.6386630535125732\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 06000/20000) loss: 0.9324575662612915\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 06100/20000) loss: -0.24909564852714539\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 06200/20000) loss: 1.4798192977905273\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 06300/20000) loss: 0.43216240406036377\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 06400/20000) loss: -0.7252155542373657\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 06500/20000) loss: 0.5799909234046936\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 06600/20000) loss: -0.21017222106456757\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 06700/20000) loss: -0.3734526038169861\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 06800/20000) loss: 0.5551584959030151\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 06900/20000) loss: 0.1255159080028534\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 07000/20000) loss: -0.5630631446838379\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 07100/20000) loss: -0.030557140707969666\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 07200/20000) loss: -0.7508930563926697\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 07300/20000) loss: -0.5233619213104248\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 07400/20000) loss: 0.28095123171806335\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 07500/20000) loss: 0.07843410968780518\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 07600/20000) loss: 0.5511898994445801\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 07700/20000) loss: -0.1848641335964203\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 07800/20000) loss: 0.11113755404949188\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 07900/20000) loss: 0.008256569504737854\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 08000/20000) loss: -0.19947832822799683\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 08100/20000) loss: -0.45735034346580505\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 08200/20000) loss: 0.5538589954376221\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 08300/20000) loss: -0.10944157838821411\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 08400/20000) loss: -0.4294120669364929\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 08500/20000) loss: -0.25491130352020264\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 08600/20000) loss: -0.5831103324890137\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 08700/20000) loss: 0.04871252179145813\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 08800/20000) loss: -0.4299813508987427\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 08900/20000) loss: 0.2146015167236328\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 09000/20000) loss: 0.1435202658176422\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 09100/20000) loss: -0.716264009475708\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 09200/20000) loss: -0.6629137396812439\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 09300/20000) loss: -0.30255424976348877\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 09400/20000) loss: -0.09136298298835754\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 09500/20000) loss: -0.39034590125083923\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 09600/20000) loss: -0.4760989844799042\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 09700/20000) loss: 0.29063454270362854\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 09800/20000) loss: -0.5586217641830444\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 09900/20000) loss: -0.43544644117355347\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 10000/20000) loss: -0.9282410144805908\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 10100/20000) loss: 0.19365458190441132\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 10200/20000) loss: 0.0816102921962738\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 10300/20000) loss: -1.0993659496307373\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 10400/20000) loss: -0.21989655494689941\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 10500/20000) loss: -0.5877478718757629\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 10600/20000) loss: -0.40770483016967773\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 10700/20000) loss: -1.0903253555297852\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 10800/20000) loss: -0.5532143712043762\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 10900/20000) loss: -0.16332033276557922\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 11000/20000) loss: -0.4744631052017212\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 11100/20000) loss: -0.09918088465929031\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 11200/20000) loss: -0.567780077457428\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 11300/20000) loss: -0.7111999988555908\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 11400/20000) loss: -0.7287379503250122\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 11500/20000) loss: -0.0649578869342804\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 11600/20000) loss: 0.25484105944633484\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 11700/20000) loss: -0.5245593786239624\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 11800/20000) loss: -0.6941218376159668\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 11900/20000) loss: 0.5740460157394409\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 12000/20000) loss: -0.20360657572746277\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 12100/20000) loss: -0.7829161882400513\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 12200/20000) loss: -0.08825202286243439\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 12300/20000) loss: -1.2766939401626587\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 12400/20000) loss: -0.7791504263877869\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 12500/20000) loss: -0.39922642707824707\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 12600/20000) loss: -0.15501278638839722\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 12700/20000) loss: -0.3032105565071106\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 12800/20000) loss: -0.5128021240234375\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 12900/20000) loss: -0.8823572993278503\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 13000/20000) loss: -0.6840925216674805\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 13100/20000) loss: -0.12149892747402191\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 13200/20000) loss: -0.666481614112854\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 13300/20000) loss: -0.8941734433174133\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 13400/20000) loss: -1.0827946662902832\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 13500/20000) loss: -1.045516848564148\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 13600/20000) loss: -0.36946970224380493\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 13700/20000) loss: -1.2924668788909912\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 13800/20000) loss: -0.9714236259460449\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 13900/20000) loss: -0.13862578570842743\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 14000/20000) loss: -0.6488382816314697\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 14100/20000) loss: -0.8653355836868286\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 14200/20000) loss: 0.01034960150718689\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 14300/20000) loss: -0.2832479178905487\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 14400/20000) loss: -0.6680377721786499\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 14500/20000) loss: -0.8564409017562866\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 14600/20000) loss: -0.06087149679660797\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 14700/20000) loss: -0.5138503909111023\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 14800/20000) loss: -0.7603799104690552\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 14900/20000) loss: -0.7985144853591919\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 15000/20000) loss: -0.7117128372192383\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 15100/20000) loss: -0.8555217981338501\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 15200/20000) loss: -0.22392134368419647\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 15300/20000) loss: -0.27232199907302856\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 15400/20000) loss: -0.9488778114318848\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 15500/20000) loss: -0.5534208416938782\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 15600/20000) loss: -0.5648906826972961\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 15700/20000) loss: -0.7468466758728027\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 15800/20000) loss: -0.3864515721797943\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 15900/20000) loss: 0.24845197796821594\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 16000/20000) loss: -1.1112899780273438\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 16100/20000) loss: -0.3495236039161682\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 16200/20000) loss: -0.842694878578186\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 16300/20000) loss: -0.7514711618423462\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 16400/20000) loss: -0.956265389919281\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 16500/20000) loss: -0.6691103577613831\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 16600/20000) loss: -0.6804472208023071\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 16700/20000) loss: -1.0454661846160889\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 16800/20000) loss: -0.8576145172119141\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 16900/20000) loss: -0.7169828414916992\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 17000/20000) loss: -0.4931994378566742\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 17100/20000) loss: -0.8349542617797852\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 17200/20000) loss: -0.6177186369895935\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 17300/20000) loss: -0.8593571186065674\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 17400/20000) loss: -1.1710288524627686\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 17500/20000) loss: -0.45717254281044006\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 17600/20000) loss: -0.7136792540550232\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 17700/20000) loss: -0.30101335048675537\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 17800/20000) loss: -0.7737146019935608\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 17900/20000) loss: -0.7789114713668823\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 18000/20000) loss: -0.40764886140823364\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 18100/20000) loss: -0.46933722496032715\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 18200/20000) loss: -0.5195136070251465\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 18300/20000) loss: -0.29774248600006104\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 18400/20000) loss: -0.45873814821243286\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 18500/20000) loss: -0.9430521726608276\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 18600/20000) loss: -0.8074394464492798\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 18700/20000) loss: -0.6546366214752197\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 18800/20000) loss: -0.6442867517471313\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 18900/20000) loss: -1.2402918338775635\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 19000/20000) loss: -0.9669927358627319\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 19100/20000) loss: -0.6719714403152466\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 19200/20000) loss: -0.6797963976860046\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 19300/20000) loss: -0.45326051115989685\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 19400/20000) loss: -0.6933085322380066\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 19500/20000) loss: -0.8500641584396362\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 19600/20000) loss: -1.0281943082809448\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 19700/20000) loss: -0.7677603363990784\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 19800/20000) loss: -0.24082374572753906\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 19900/20000) loss: -0.24140426516532898\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(batch_num 20000/20000) loss: -0.9767911434173584\n```\n:::\n:::\n\n\n# What more complicated NNs look like\n\n# Conclusion \n\n[^1]: It also almost has a bit of \"no brain no pain\" ML guy energy, in the sense\nthat we're really pulling out the biggest algorithm possible. It really is a funny\ntrajectory to me to go from \"I'd like to still be Bayesian, but avoid MCMC because it's slow\"\nto \"screw subtle design, let's throw a NN at it\".\n[^2]: This is mostly a joke, but it really is a tremendous convenience that\nthere's such a straight forward knob to turn for \"expressivity\" in this context.\nWe'll get into the ways that isn't completely true soon, but NNs provide fantastic\nconvenience in terms of workflow for improving model flexibility.\n[^3]: You can see it in the original Normalizing Flows paper linked above, or\ncombined with a nice matrix calc review by [Lilian Weng](https://lilianweng.github.io/posts/2018-10-13-flow-models/). As a more general note, since this is a common topic on a few different talented\npeople's blogs, I'll try to focus on covering material I think I can provide\nmore intuition for, or that are most relevant for variational inference.\n[^4]: A great example of this is Lilian Weng's [NF walkthrough](https://lilianweng.github.io/posts/2018-10-13-flow-models/) which\nI reccomended above- It\nhas a fantastic review of the needed linear algebra and covers a lot of different\nflow types, but is a bit overly general about what properties are most desirable\nin a flow, and therefore initially a bit fuzzy on the value different flows\nhave.\n[^5]: Deriving precisely how this works would take us too far afield, but see [Kobyzev et\nal. (2020)](https://arxiv.org/abs/1908.09257) if you're interested. It's a great review paper that does a lot of work to recognize there are multiple different possible applications of normalizing flows, and thus\ndifferent notations and framings that they very successfully bridge.\nare many different implicit and explicit objectives and \n\n",
    "supporting": [
      "variational_mrp_pt5_files"
    ],
    "filters": [],
    "includes": {}
  }
}