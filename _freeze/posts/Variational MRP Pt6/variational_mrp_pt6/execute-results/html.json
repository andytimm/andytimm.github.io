{
  "hash": "d852eccf13d270bcfb2e771b19691328",
  "result": {
    "markdown": "---\nlayout: post\ntitle: Variational Inference for MRP with Reliable Posterior Distributions\nsubtitle: Part 6- Diagnostics\ndate: 2023-06-17\ndraft: True\ncategories:\n  - MRP\n  - Variational Inference\n  - Diagnostics\n---\n\n   \n   \nThis is section 6 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality. \n\nThe general structure for this post and the posts around it will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in these four posts will go a long way towards more robust variational inference. I'll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.\n\nIn the [last post](https://andytimm.github.io/posts/Variational%20MRP%20Pt5/variational_mrp_5.html) we looked at normalizing flows, a way to leverage neural networks to learn significantly\nmore expressive variational families in a way that adapt to specific problems.\n\nIn this post, we'll explore different diagnostics for variational inference,\nranging from simple statistics that are easy to calculate as we fit our approximation to solving the problem\nin parallel with MCMC to compare and contrast. Some recurring themes will be \naiming to be precise about what constitutes failure under each diagnostic tool,\nand providing intuition building examples where each diagnostic will fail to\ndo anything useful. While no single diagnostic provides strong guarantees of\nvariational inference's correctness on their own, taken together the tools\nin this post broaden our ability to know when our models fall short.\n\nThe rough plan for the series is as follows:\n\n1.  Introducing the Problem- Why is VI useful, why VI can produce spherical cows\n2.  How far does iteration on classic VI algorithms like mean-field and full-rank get us?\n3.  Problem 1: KL-D prefers exclusive solutions; are there alternatives?\n4.  Problem 2: Not all VI samples are of equal utility; can we weight them cleverly?\n5.  Problem 3: How can we get deeply flexible variational approximations; are Normalizing Flows the answer?\n6.  **(This post)** Problem 4: How can we know when VI is wrong? Are there useful error bounds?\n7.  Putting the workflow all together\n\n# Looking at our loss function\n\nOne logical place to start with diagnostics is to discuss what we can and can't\ninfer from our optimization objectives like an ELBO or CUBO.\n\nIn training a model with variational inference some common stopping rule choices\nare either to just run optimization for a fixed number of iterations, or to\nstop when relative changes in the loss have slowed, indicating convergence of\nthe optimization to a local minimum. So we can at least look at changes in\nthe ELBO/CUBO/other loss to know if our approximation has hit a local minimum yet.\n\nUnfortunately, that's about all monitoring the loss can tell us. Recall that An unknown,\nmultiplicative constant exists in $p(z,x) \\propto p(z|x)$ that changes as\nreparameterize our model; thus, we can't compare two different models on the\nsame objective and expect their ELBO or similar loss values to be comparable. So\nthe typical ML strategy of \"which model achieves lower loss\" is pretty much\nout here.\n\nAlso, the loss values themselves aren't particularly meaningful: there's no\nway to interpret a given ELBO as indicating a good approximation, for example. This\ngenerally stems from our bounds being bounds, not directly optimizing the quantity\nwe want to optimize. While they're definitely degenerate cases, there are even some\nfun counter examples I'll show in a second where you can make the ELBO/CUBO arbitrarily low, while\nstill allowing the posterior mean or standard deviation to be arbitrarily wrong!\n\n# The majesty of $\\hat{k}$\n\nSo if we can't just look at our loss, what can we look at? One broadly applicable\ndiagnostic tool is $\\hat{k}$, which we already introduced in the post on using\nimportance sampling to improve variational inference.\n\nAs a several sentence refresher, Pareto smoothed importance sampling (PSIS) proposes to stabilize\nimportance ratios $r(\\theta)$ used in importance sampling by modeling the tail of the distribution\nas a generalized Pareto distribution:\n\n$$\n\\frac{1}{\\sigma} \\left(1 + k\\frac{r - \\tau}{\\sigma} \\right)^{-1/k-1}\n$$\nwhere $\\tau$ is a lower bound parameter, which in our case defines how many ratios from the tail\nwe'll actually model. $\\sigma$ is a scale parameter, and $k$ is a unconstrained shape parameter.\n\nTo see\nhow this provides a natural diagnostic for importance sampling, it's useful to know that importance sampling depends on how many moments $r(\\theta)$ has- for example, if at least two moments exist, the vanilla IS estimator has finite variance (which is obviously required, but no guarantee of performance since it might be finite but massive). The GPD\nhas $k^{-1}$ finite fractional moments when $k > 0$. [Vehtari et Al. (2015)](https://arxiv.org/abs/1507.02646) show through extensive theoretical digging and simulations that PSIS\nworks fantastically when $\\hat{k} < .5$. and acceptably if $.5 < \\hat{k} < .7$. Beyond $\\hat{k} = .7$ there the number of samples needed rapidly become impractically large.\n\nWhy should we think $\\hat{k}$ is a relevant diagnostic for variational inference? [Chaterjee and Draconis (2018)](https://arxiv.org/abs/1511.01437)\nshowed that for a given accuracy, how big $S$ needs to be for importance sampling\nmore broadly depends on how close $q(x)$ is to $p(x)$ in KL distance- we need to\nsatisfy $log(S) \\geq \\mathbb{E}_{\\theta \\sim q(x)}[r(\\theta)log(r(\\theta))]$ to\nget reasonable accuracy. So a good $\\hat{k}$ indicates importance sampling is\nfeasible, which in tern indicates that $q(x)$ is likely close to $p(x)$ in KL Divergence- exactly what we're hoping to get at!\n\nFleshing out the use of $\\hat{k}$ as a VI diagnostic was done by [Yao et al. (2018)](https://arxiv.org/abs/1802.02538),\nwho generally show that high values of $\\hat{k}$ do generally map onto posterior approximations\nwith variational inference being quite poor. This is really useful, and generally\nmaps well on to my experience- if $\\hat{k}$ is bigger than .7, you probably\nneed to go back to the drawing board on how you're fitting your VI.\n\nWhat I want to stress though, is that the inverse isn't broadly true- a low $\\hat{k}$\nisn't necessarily a guarantee the VI approximation is good. Let's look at a couple\ndifferent ways this can happen.\n\n## Problem Case 1: Importance sampling $\\neq$ direct variational inference\n\nWe should keep in mind that $\\hat{k}$ is ultimately a diagnostic tool for\nimportance sampling, and in cases where the needs of importance sampling\nand simple variational inference diverge, $\\hat{k}$ can give a misleading answer.\n\nLet's re-use an example from the importance sampling post to illustrate this.\nWhat happens if we approximate the red distribution below with the green one?\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmixture %>% ggplot(aes(x = normals)) +\n  geom_density(aes(x = normals), color = \"red\") +\n  geom_density(aes(x = mean_seeking_kl), color = \"green\") + ggtitle(\"The green approxmiation is great for IS, terrible on it's own\") +\n  xlab(\"\")\n```\n\n::: {.cell-output-display}\n![](variational_mrp_pt6_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThe green distribution here is a prime candidate to importance sample to approximate\nthe red one- it coves all the needed mass, and we can massively down weight\nthe irrelevant points in the center. On the other hand, this'd be a really,\nreally bad variational approximation to use raw, since it has a ton of mass\nbetween the two modes which will blow up our loss. Because the needs of PSIS-based\nestimators and unadjusted VI diverge, $\\hat{k}$ is low, but the approximation\nwould be pretty bad:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimportance_ratios <- tibble(\nq_x = rnorm(200000,9,4),\np_x = c(rnorm(100000,3,1),rnorm(100000,15,2)),\nratios = (.5*(dnorm(q_x,3,1)) + .5*(dnorm(q_x,15,2)))/dnorm(q_x,9,4))\n\npsis_result <- psis(log(importance_ratios$ratios),\n                       r_eff = NA)\n\npsis_result$diagnostics$pareto_k\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -1.737515\n```\n:::\n:::\n\n\nSo our $\\hat{k}$ says everything is beautiful, but in reality it's really\nonly a happy time for PSIS, not the raw VI estimator. This ultimately isn't\nthe most concerning failure mode: if you do the work to calculate $\\hat{k}$,\nyou're pretty much ready to use PSIS to improve your variational inference anyway. That \nsaid, this should provide intuition that $\\hat{k}$ isn't in general super well equipped to\ntell you much about non-IS augmented VI.\n\n## Problem Case 2: $\\hat{k}$ is a local diagnostic\n\n$\\hat{k}$ inherits a common issue with most KL Divergence adjacent metrics: it's\nultimately something we evaluate locally, so if there's a part of the posterior totally\nunknown to our $q(x)$, it won't be able to tell you what you're missing.\n\nWe already used 1 example from the importance sampling post, so let's keep that\nmoving. What do you think will happen with $\\hat{k}$ with the green approximation\nbelow that misses a whole mode?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmixture %>% ggplot(aes(x = normals)) +\n  geom_density(aes(x = normals), color = \"red\") +\n  geom_density(aes(x = mode_seeking_kl), color = \"green\") + ggtitle(\"We're missing a whole mode here\") +\n  xlab(\"\")\n```\n\n::: {.cell-output-display}\n![](variational_mrp_pt6_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nIf you guessed $\\hat{k}$ will say everything is perfect when it's not, you're\ncorrect:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsecond_importance_ratios <- tibble(\nq_x = rnorm(200000,3.5,1),\np_x = c(rnorm(100000,3,1),rnorm(100000,15,2)),\n# Notice: these density calls are at the points defined by q(x)!\nratios = (.5*(dnorm(q_x,3,1)) + .5*(dnorm(q_x,15,2)))/dnorm(q_x,3.5,1))\n\npsis_result_2 <- psis(log(second_importance_ratios$ratios),\n                       r_eff = NA)\n\npsis_result_2$diagnostics$pareto_k\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.07343881\n```\n:::\n:::\n\n\nThat's... not great. Since we evaluate the importance ratio and thus eventually  $\\hat{k}$\nat the collection of values in $q(x)$, the diagnostic has no real way to know\nwe're missing an entire mode, and unlike in the above case there's no easy fix here.\n\nAnother interesting question this example raises is what happens in high dimensions,\nwhere it's much less intuitive what \"missing one or several modes\" looks like. Just\nby increasing the sd of the normal $q(x)$ a little in the example, we see a sudden,\nlarge increase in $\\hat{k}$;\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthird_importance_ratios <- tibble(\nq_x = rnorm(200000,3.5,2),\np_x = c(rnorm(100000,3,1),rnorm(100000,15,2)),\nratios = (.5*(dnorm(q_x,3,1)) + .5*(dnorm(q_x,15,2)))/dnorm(q_x,3.5,2))\n\npsis_result_3 <- psis(log(third_importance_ratios$ratios),\n                       r_eff = NA)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.\n```\n:::\n\n```{.r .cell-code}\npsis_result_3$diagnostics$pareto_k\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.70381\n```\n:::\n:::\n\n\nsimilar sudden shifts in $\\hat{k}$ can frequently occur as you\nincrease the dimension of a posterior you're approximating- intuitively, the mass\nyou do and don't know about becomes much harder to keep track of in high dimensions and\nfor complex posteriors. This can lead to $\\hat{k}$ being a bit less stable than\nyou'd like over different initializations or other slight modifications of a VI\nmodel, with this pattern being common both in my own applications and documented in several\npapers like [Wang et al. (2023)](https://arxiv.org/abs/2302.12419)'s testing.\n\n\n## Problem Case 3: $\\hat{k}$ is a joint posterior level tool\n\nA final, more conceptual problem with $\\hat{k}$ that [Yao et al. (2018)](https://arxiv.org/abs/1802.02538) point out is that it's ultimately a diagnostic\nof the joint posterior, not the specific marginal or summary statistic you may ultimately care about.\n\nVariational inference is hard: we often know that the overall posterior approximation is deeply\nflawed, but it may be up to the task of representing some metrics we care about correctly enough. For \nexample, in the MRP example I introduced earlier in the series, the mean-field\nvariational inference fit was reasonable at representing the state-level means, but garbage at\npretty much anything related to uncertainty. The $\\hat{k}$ from that model was greater than\n2, so we clearly know the broader posterior approximation was poor, but $\\hat{k}$\nmight be a false positive sign if what you really care about was just the means. For\nthe most complicated posteriors, we should expect to spend a lot of time in this\nfeeling of \"some parts of the posterior may be good enough\", so this is a useful\ntrap to know about.\n\n...Let's step back for a second. Since I introduced $\\hat{k}$ as a diagnostic with a bunch of cases\nwhere it falls short in surprising ways, I do want to emphasize it is a very useful *heuristic* diagnostic tool\nin general. Large $\\hat{k}$ tells you something is very likely wrong with your joint posterior,\nand that's generally practically helpful information. Where we need to be cautious\nis in inferring whether the wrongness $\\hat{k}$ picks up on is something we care about,\nand also in remembering that low $\\hat{k}$ doesn't provide guarantees of correctness.\n\n\n# Wasserstein Bounds\n\nSo we've seen some limitations of using our objectives and $\\hat{k}$ as\ndiagnostics. Let's break out some fun propositions and examples from [Huggins et al. (2020)](https://arxiv.org/abs/1910.04102) to really drive home the\nneed for a bound that actually makes guarantees on errors of posterior summaries we care about\nlike means and variances.\n\n----\n\n### **Proposition 3.1 (Arbitrarily Poor Mean Approximation):**\n\nFor any t > 0, there exist (A) one dimensional,\nunimodal distributions $q$ and $p$ such that $\\mathrm{KL}(q \\mid p)$ < 0.9 and $\\left(m_{q}-m_p\\right)^2>t \\sigma_p^2$, and $(B)$ one-dimensional, unimodal distributions $q$ and $p$ such that $\\mathrm{KL}(q \\mid p) < 0.3$ and $\\left(m_{q}-m_p\\right)^2>t \\sigma_{q}^2$.\n\n### **Proposition 3.2 (Arbitrarily Poor variance Approximation):**\nFor any $t \\in(1, \\infty]$, there exist one-dimensional, mean-zero, unimodal distributions $q$ and $p$ such that $\\mathrm{KL}(q \\mid p)<0.12$ but $\\sigma_p^2 \\geq t \\sigma_{q}^2$.\n\n----\n\nSo, these have some fairly scary names, huh? We can make the KL divergence pretty\nsmall, but have arbitrarily bad mean and variance approximation. How to do this? Given\nthe examples here have to be unimodal and 1-D, they can't be that, that weird,\nright?\n\n----\n\n### **Proposition 3.1 Example**\n\nLet Weibull $(k, 1)$ denote the Weibull distribution with shape $k>0$ and scale 1. For (A), for any $t>0$, we can choose $k=k(t), p=\\operatorname{Weibull}(k, 1)$, and $q= \\operatorname{Weibull}(k / 2,1)$, where $k(t) \\searrow 0$ as $t \\rightarrow \\infty$. We can just exchange the two distributions for (B).\n\n### **Proposition 3.2 Example**\n\nFor any $t>0$ we let $h=h(t), p=\\mathcal{T}_h$ (standard $t$ distribution with $h$ degrees of freedom), and $q=\\mathcal{N}(0,1)$ (standard Gaussian), where $h(t) \\searrow 2$ as $t \\rightarrow \\infty$.\n\n----\n\nI won't show it here, but the Huggins et al. paper provides a similar proposition\nand example for $\\chi^2$ divergences and the CUBO bound we discussed earlier in the\nseries, and the example has pretty much the same form.\n\nWhenever presented with a \"counterexample\" to things working properly like this, it's worth asking how broad\nthe case's applicability is: often counterexamples reside in the land of extremes,\nand we should be cautious in interpreting the result's negative implications too broadly. There's certainly\nsome of that going on here, in the sense that usually a lower ELBO/CUBO will give some (if not perfect)\ntraction in improving our posterior estimates of mean and variance. The intuitive point here though is that **the\nbounds we optimize give no rigorous guarantees of posterior summaries we care about**, even in the loosest\nsense. \n\n\nTo get some better guarantees like this, [Huggins et al. (2020)](https://arxiv.org/abs/1910.04102) propose bounds on\nthe the mean and uncertainty estimates that arise from variational inference, which\nleverage the Wasserstein distance. In addition to providing actual[^1] bounds\non quantities we care about, these bounds come at very reasonable computational\ncost, as they are readily computable from the bounds we already have (ELBO and CUBO) plus\nsome additional Monte Carlo estimation and quick analytic calculation.\n\nLet's first discuss what the Wasserstein distance is, and then discuss the\nfairly involved path from our existing estimates to actually calculating the\nbounds.\n\n\n## What's a Wassterstein?\n\nThe $p$-Wasserstein distance between $\\xi$ and $\\pi$ is\n$$\n\\mathcal{W}_p(\\xi, \\pi):=\\inf _{\\gamma \\in \\Gamma(\\xi, \\pi)}\\left\\{\\int\\left\\|\\theta-\\theta^{\\prime}\\right\\|_2^p \\gamma\\left(\\mathrm{d} \\theta, \\mathrm{d} \\theta^{\\prime}\\right)\\right\\}^{1 / p}\n$$\nwhere $\\Gamma(\\xi, \\pi)$ is the set of couplings between $\\xi$ and $\\pi$.\n\nA reasonable first question if you haven't seen this distance before: What? This\nlooks a lot more involved than something like the KL Divergence. For example,\nthe fact that we have an infinum over something complicated looking suggests this'll\nbe a real pain to calculate. As we'll see in a second, Huggins et al. don't\nactually seek to calculate it or approximate it, they seek to bound it [^2]. \n\nBefore we get there though, let's seek to understand the distance and it's properties a little better.\n\nA good way to start unpacking this is to consider the optimal transport\nproblem. Given some probability mass $\\xi(\\theta)$ on a space $X$, we wish to transport\nit such that it is transformed into the distribution $\\pi(\\theta)$. To provide\nphysical intuition, this is often formulated as a problem of moving an equal\namount of dirt/earth in pile $\\xi(\\theta)$ to make pile $\\pi(\\theta)$- hence the name\ncommonly used in several disciplines, the [Earthmovers Distance](https://en.wikipedia.org/wiki/Earth_mover%27s_distance).\n\nLet's say we have some non-negative cost function for moving mass from $\\theta$ to $theta^{\\prime}$,\n$c(\\theta,\\theta^{\\prime})$. A single transport plan for moving from $\\xi(\\theta)$ to $\\pi(\\theta^{\\prime})$ is a function $\\gamma(\\xi, \\pi)$ which describes the amount of mass to move at each point. If we assume $\\gamma$ is a valid joint probability mass with marginals\n$\\xi\\theta)$ and $\\pi(\\theta^{prime})$ [^3], then the infinitesimal mass\nwe transport from $\\theta$ to $\\theta{\\prime}$ is $\\gamma(\\theta, \\theta^{\\prime}) d\\theta d\\theta^{\\prime}$,\nwith cost\n\n$$\n\\int \\int c(\\theta,\\theta^{\\prime}) \\gamma(\\theta, \\theta^{\\prime}) d\\theta d\\theta^{\\prime} = \\int c(\\theta,\\theta^{\\prime}) d \\gamma(\\theta,\\theta^{\\prime})\n$$\n\nFinally getting close to something that looks like our Wasserstein distance. There\nare many such plans, but the one we want, the solution to the optimal transport\nproblem is the one with minimal cost out of all such plans. \n\nOne last point to cover to define this: what's our cost? If the cost here is\nthe $p$-distance between our $\\theta$s, then this is the p-Wassterstein distance.\n\nWhy are some properties of this distance? I already mentioned a major downside (this looks nasty to estimate in general, and indeed it is). What are the upsides of\nthis?\n\nUnlike the KL or $\\chi^2$ divergences we've looked at before, the Wasserstein\ndistance takes into account the metric on the underlying space! Let's unpack\nthat by again drawing on the optimal transport problem for intuition.  The Wasserstein distance takes into account not only the differences in the values or probabilities assigned to different points in the distributions but also the actual \"spatial\"[^4] arrangement of those points. \n\nThis is a incredibly property because the summary of the posterior we care about\nin general also rely on the underlying metric. This is basically how the arbitrarily\npoor mean and variance examples above work; they exploit the lack of use of\nan underlying metric. That allows Huggins et al. to derive one of the key\nresults of the paper\n\n----\n**Theorem 3.4.** If $\\mathcal{W}_1(q, p) \\leq \\varepsilon$ or $\\mathcal{W}_2(q, p) \\leq \\varepsilon$, then $\\left\\|m_{q}-m_p\\right\\|_2 \\leq \\varepsilon$ and $\\max _i\\left|\\mathrm{MAD}_{q, i}-\\mathrm{MAD}_{p, i}\\right| \\leq 2 \\varepsilon$.\n\nIf $\\mathcal{W}_2(q, p) \\leq \\varepsilon$, then for $S := \\sqrt{min (\\left\\|\\Sigma_{q}\\right\\|_2, \\left\\|\\Sigma_p\\right\\|_2)}$ $\\max _i\\left|\\sigma_{q, i}-\\sigma_{p, i}\\right| \\leq \\varepsilon$ and $\\left\\|\\Sigma_{q}-\\Sigma_p\\right\\|_2<2 \\varepsilon(S+\\varepsilon)$.\n\n----\n\nA similar type of result holds for the difference between expectations of any smooth\nfunction, so this result is somewhat extensible with additional work.\n\nThis is a nice improvement over the KL or $\\chi^2$ divergences as far as an\na diagnostic, since we have some guarantees of correctness where we had literally none. I'll return to how tight these are bounds in practice in a bit, since that's\nentangled with how we can actually estimate them in the variational inference use\ncase.\n\n\n## Bounds of Bounds via... Bounds!\n\nOne contribution of the Huggins at al. paper is the above result, but where\nthings get even more impressive is that they find a reasonable and practical\nway to bound these quantities. It's certainly not simple, but it works.\n\nHere's the plan to get real bounds on our posterior summaries in full:\n1. Use the ELBO and CUBO to to bound the KL and $\\chi^2$ divergences.\n2. Use tail properties of the distribution $q$ to get bounds on the Wasserstein\ndistance through the KL and $\\chi^2$ divergences.\n3. Finally, bound posterior summaries using the Wasserstein bounds.\n\nThat's a lot of layers of bounding, and it's reasonable to wonder why this is\nneeded and whether the bounds are usefully tight after such transformations. One\nkey reason this type of bounding is so involved is that we're using a set of\nscale-invariant distances to bound a scale-dependent one- we need to incorporate\nsome notion of scale into the bounding process to make it work.\n\nTo do this, define the moment constants $C_p^{\\mathrm{PI}}(\\xi)$ and $C_p^{\\mathrm{EI}}(\\xi)$. For $p \\geq 1$, $\\xi$ is p-polynomially integrable if\n$$\nC_p^{\\mathrm{PI}}(\\xi):=2 \\inf _{\\theta_0}\\left\\{\\int\\left\\|\\theta-\\theta_0\\right\\|_2^p \\xi(\\mathrm{d} \\theta)\\right\\}^{\\frac{1}{p}}<\\infty\n$$\nand that $\\xi$ is p-exponentially integrable if\n$$\nC_p^{\\mathrm{EI}}(\\xi):=2 \\inf _{\\theta_0, \\epsilon>0}\\left[\\frac{1}{\\epsilon}\\left\\{\\frac{3}{2}+\\log \\int e^{\\epsilon\\left\\|\\theta-\\theta_0\\right\\|_2^p} \\xi(\\mathrm{d} \\theta)\\right\\}\\right]^{\\frac{1}{p}}<\\infty\n$$\n\nNext, with the assumption that the variational approximation $q$ has polynomial (respectively, exponential) tails, our next result provides a bound on the $p$-Wasserstein distance using the $\\chi^2$-divergence (respectively, the KL divergence). \n\nThis is saying we require at least polynomial, and ideally exponential moments\nfor $q$ and $p$, which isn't that strenuous of a requirement. Then:\n\n----\n**Proposition 4.2.** If $p$ is absolutely continuous w.r.t. to $q$ then\n$$\n\\mathcal{W}_p(\\hat{\\pi}, \\pi) \\leq C_{2 p}^{\\mathrm{PI}}(\\hat{\\pi})\\left[\\exp \\left\\{\\mathrm{D}_2(\\pi \\mid \\hat{\\pi})\\right\\}-1\\right]^{\\frac{1}{2 p}}\n$$\nand\n$$\n\\mathcal{W}_p(\\hat{\\pi}, \\pi) \\leq C_p^{\\mathrm{EI}}(\\hat{\\pi})\\left[\\mathrm{KL}(\\pi \\mid \\hat{\\pi})^{\\frac{1}{p}}+\\{\\mathrm{KL}(\\pi \\mid \\hat{\\pi}) / 2\\}^{\\frac{1}{2 p}}\\right]\n$$\n\n-----\n\nA reasonable question here: does using the KL and $\\chi^2$ inherit KL/$\\chi^2$\narbitrary wrongness? Nope! I won't reproduce here, but the counter examples\nshown above for these divergences on their own no longer work to make our\nestimates arbitrarily wrong.\n\nNext step: how do we Use the ELBO and CUBO to bound the KL and $\\chi^2$ terms in the proposition above above?\n\nWe first define for any distribution $\\eta$:\n\n$$\n\\mathrm{H}_\\alpha(\\xi, \\eta):=\\frac{\\alpha}{\\alpha-1}\\left\\{\\operatorname{CUBO}_\\alpha(\\xi)-\\operatorname{ELBO}(\\eta)\\right\\}\n$$\n\nThen we get:\n\n----\n**Lemma 4.5.** For any distribution $\\eta$ such that $p$ is absolutely continuous w.r.t.  $\\eta$-\n$$\n\\mathrm{KL}(\\pi \\mid \\hat{\\pi}) \\leq \\mathrm{D}_\\alpha(\\pi \\mid \\hat{\\pi}) \\leq \\mathrm{H}_\\alpha(\\hat{\\pi}, \\eta)\n$$\n\n----\n\nBy combining the lemma above and proposition from 4.2 earlier, we can bound the\nWasserstein distance finally! To do this, we need all of $C_p^{\\mathrm{PI}}(\\xi)$, $C_p^{\\mathrm{EI}}(\\xi)$, CUBO, and ELBO. All of these are efficiently calculable\nmuch of the time (we'll get to when it's not soon), and largely result from\nthings we were already calculating as promised. Great! Our combined result:\n\n----\n\n**Theorem 4.6.** For any $p \\geq 1$ and any distribution $\\eta$, if $\\pi$ is absolutely continuous w.r.t. $\\hat{\\pi}$, then\n$$\n\\mathcal{W}_p(\\hat{\\pi}, \\pi) \\leq C_{2 p}^{\\mathrm{PI}}(\\hat{\\pi})\\left[\\exp \\left\\{\\mathrm{H}_2(\\hat{\\pi}, \\eta)\\right\\}-1\\right]^{\\frac{1}{2 p}}\n$$\nand\n$$\n\\mathcal{W}_p(\\hat{\\pi}, \\pi) \\leq C_p^{\\mathrm{EI}}(\\hat{\\pi})\\left[\\mathrm{H}_2(\\hat{\\pi}, \\eta)^{\\frac{1}{p}}+\\left\\{\\mathrm{H}_2(\\hat{\\pi}, \\eta) / 2\\right\\}^{\\frac{1}{2 p}}\\right] .\n$$\n\n-----\n\nThis was a very, very long derivation, but hopefully it gave you a better feel\nfor the limitations of our normal divergences in variation inference.\n\nStepping back, these results give us some pretty easy to calculate ways to\neasily check when \n\n\n## So what's the bad news about this diagnostic?\n\n\n\n# MCMC based diagnostics; what's old is new again\n\n## MCMC can be practically useful even when slow\n\n## TADDAA\n\n# Diagnostics that don't spark joy\n\n[^1]: Alternatively, *not arbitrarily wrong*.\n[^2]: No, we're not winding up to define another bound like the CUBO or ELBO and optimize it like you might think. There's an interesting little sub-literature on making calculating\nthe Wasserstein distance (really, approximations of it) efficiently enough\nthat we could use it for scalable bayesian inference tasks like variational\ninference. I haven't looked back at this literature in a few years, but\n[Srivastava et al. (2018)](https://arxiv.org/abs/1508.05880) is at least\na starting point in this literature if you're interested.\n[^3]: For further intuition, think about how this requirement maps onto our earth\nmoving scenario. Integrating $\\gamma$ with respect to $\\theta^\\prime$ (marginalizing) gives $\\xi\\theta)$- physically, this means that the earth moved\nfrom point x needs to be equal to the amount starting there. The opposite\nmarginalization gives $\\pi(\\theta^\\prime)$ as you'd expect. Hopefully the\nphysical intuition of the constraints that come with this being a valid probability\ndistribution make sense here; visually: \n![](1024px-Transport-plan.png){By Lambdabadger - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=64872543}\n[^4]: This is imprecise, but hopefully this conveys the intuition well here:\nwe want a distance measure that takes into account the distance between the points\nand how they need to be arranged to form one another, not just the values themselves.\n",
    "supporting": [
      "variational_mrp_pt6_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}