{
  "hash": "4554b2647ef713a8879bc27d8d696e9a",
  "result": {
    "markdown": "---\nlayout: post\ntitle: Variational Inference for MRP with Reliable Posterior Distributions\nsubtitle: Part 7- Putting it all together\ndate: 2023-07-12\ndraft: True\ncategories:\n  - MRP\n  - Variational Inference\n---\n\nThis is the final post in my series about using Variational Inference to speed up complex Bayesian models, such as Multilevel Regression and Poststratification. Ideally, we want to do this without the approximation being of hilariously poor quality.\n\nThe last few posts in the series have explored several different major advances\nin black box variational inference. This post puts a bunch of these tools together\nto build a pretty decent approximation that runs ~5x faster than MCMC, and points\nto some other advances in BBVI I haven't had time to cover in the series.\n\nThe other posts in the series are:\n\n1.  Introducing the Problem- Why is VI useful, why VI can produce spherical cows\n2.  How far does iteration on classic VI algorithms like mean-field and full-rank get us?\n3.  Problem 1: KL-D prefers exclusive solutions; are there alternatives?\n4.  Problem 2: Not all VI samples are of equal utility; can we weight them cleverly?\n5.  Problem 3: How can we get deeply flexible variational approximations; are Normalizing Flows the answer?\n6.  **(This post)** Problem 4: How can we know when VI is wrong? Are there useful error bounds?\n7.  Putting it all together\n\n# Cutting to the chase\n\n![](plots/CI_plot.png){width=100%}\n\nTo cut to the chase, the new and improved variational approximation is looking\npretty, pretty good!\n\nLike with the simpler meanfield and fullrank models from earlier in the series, this has the medians basically correct, but we also have reasonable uncertainty estimation too. First, the\nstate distributions are much more smooth and unimodal- no more \"lumpy\" distributions\nwith odd spikes of probability that make no sense as a model of public opinion. Further,\nthe approximation is more consistent: while there's still some variation state\nto state in how closely VI matches MCMC, pretty much all states are reasonable.\n\nCertainly, we're still to some degree understating the full size of MCMC's\ncredible interval. Considering this model runs in an hour and change versus\nMCMC's 8 hours on 60,000 datapoints (!), this feels pretty acceptable. As I'll write\na bit more about later, there are a few ways to trade compute and/or time to\nfill out the CI's as well.\n\nLast time we look at a variational approximation in post 2, we found a dot plot\nwas a significantly more exacting standard which made it clear how bad the\nfirst try at VI in the series was. How does that look here?\n\n![](plots/dot_plot.png){width=100%}\n\nAgain, pretty solid- no more weird spikes, and the concentration of mass\nlooks pretty comparable (if a bit compressed) versus MCMC. VI is now much more\nuncertain about the same states as MCMC, and no longer shows any signs of degenerate\noptimization to fit data points. Nice!\n\nFinally, how are the diagnostics we learned in the last point? The $\\hat{k}$ is .9, which is at least much improved[^1]. The approximation is good enough that the Wasserstein\nbounds aren't tight enough to inform us much about any issues[^2], although\nwe should be a bit careful in trusting them giving that $\\hat{k}$. In one sense,\nnone of the diagnostics here are \"great\", but this is pretty typical of my\nexperience with BBVI for non-trivial models. We're almost always losing\nsomething from the true posterior, and these diagnostics are not sufficiently\nfine-grained to differentiate important from unimportant losses.\n\n# How it works\n\nSo the caption above gives some hint, but what all is in this model?\n\nTo fit this variational approximation, I'm using Agrawal, Domke, and Sheldon's [vistan](https://github.com/abhiagwl/vistan/tree/master), which is a companion\npython package to their great paper [Advances In Black-Box VI](https://proceedings.neurips.cc/paper/2020/file/c91e3483cf4f90057d02aa492d2b25b1-Paper.pdf).\n\nHere's a separate repository with full setup details, but for purposes of this post, I'll\njust reference the parameters of the main setup function here:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nvistan.algorithm(\n    vi_family = \"rnvp\",\n    full_step_search = True,\n    full_step_search_scaling = True,\n    step_size_exp = 0,\n    step_size_exp_range = [0, 1, 2, 3, 4],\n    step_size_base = 0.1,\n    step_size_scale = 4.0,\n    max_iters = 1000,\n    optimizer = 'adam',\n    M_iw_train = 1,\n    M_iw_sample = 10,\n    grad_estimator = \"DReG\",\n    per_iter_sample_budget = 100,\n    fix_sample_budget = True,\n    evaluation_fn = \"IWELBO\",\n    rnvp_num_transformations =  10,\n    rnvp_num_hidden_units =  32,\n    rnvp_num_hidden_layers =  2,\n    rnvp_params_init_scale =  0.01\n)\n```\n:::\n\n\nLet's talk about:\n\n1. Normalizing Flows\n2. Importance Sampling\n3. Optimization\n4. Sampling Budgets\n\n### Normalizing Flows\n\nFirst, this is using a [Real NVP](https://arxiv.org/abs/1605.08803) normalizing\nflow, with a fairly small (`10`) number of transformation layers, each of which is using\na pretty shallow neural net (`2 layers, 32 hidden units`). This helps make the approximating distribution\ncomplex enough to handle our model. I didn't find much benefit from either\nadding more transform layers or making the neural nets deeper- that sort of makes\nsense, since the jump from mean-field or full-rank VI to using a normalizing\nflow at all is a fairly big one in terms of model representation capacity.\n\n### Importance Sampling\n\nSecond, this is using importance sampling only at the sampling stage, and with\njust `10` IW samples per returned sample[^3]. I didn't find much benefit from\nimportance weighted training[^4] here, although for relatively more complex models\nthan this I've found it to sometimes matter. As a point of comparison, in their\npaper linked above, Agrawal et al. find that about 20% of models actually get\n*worse* with IW-Training, and only ~10% get improvements, although those improvements\nseem to be clustered in more complex models and can be fairly significant.\n\nAt least for this particular model, using 10 importance samples per returned sample during provided most of the benefit of importance weighting, although pushing this\nhigher to 100 or 200 helped fill out the outcome distribution's tails a bit (more on this in a bit).\n\n### Optimization\n\nFinally, many of the parameters here are about optimization, which we haven't\ntalked about in this series too much yet, despite it being critical to good performance.\n\nFirst, Agrawal et al. found that variational\ninference is reasonably sensitive to optimization hyperparameters like the step\nsize the Adam optimizer uses- to handle this, they suggest initial runs\nat few different step sizes (in `step_size_exp_range`), selecting the best one via an ELBO average over the whole optimization trace. This model benefits quite a lot from this, and\nAgrawal et al.'s results see strong improvements for ~25-30% of models using this\nadjustment.\n\nSecond, they use the `DReG` gradient estimator, which I'll link out to later, but\nis essentially a variant of the reparameterization trick gradient discussed in\npost 3 with improved variance, leading to faster convergence.\n\nA final optimization hyperparameter here is the number of iterations (both for that step search\nprocedure and the final run)- I found `1000` was more than sufficient for optimization of\nthis model, with going up to 2000 iterations not getting us much benefit, but going\ndown to 500 leading to a heavily over-dispersed posterior.\n\n\n### A sample budget?\n\nSince their paper is ultimately a bakeoff, Agrawal et al.'s paper touches on\nthe theme of a sample budget again and again, and it's a great concept to consider\nhere. Essentially, a computation budget is somewhat **fungible**: given an amount\nof run time, compute available, etc, we can, for example, trade a larger \n`max_iters` for using (more) importance weighted training, or use fewer iterations\nfor (more) importance weighting our samples. \n\nHow to best spend this budget\nis an open question and fairly model dependent- here, I decided not to present\nthe best model I could possibly fit with variational inference, but the\nbest one I could get to fit in ~1h. The point of this series is to fit something\nnearly as good fast, not a different, more complex way but also in 8h. \n\nNext, I'll talk through some ideas for what could improve the model further\nat greater compute and wall time cost.\n\n# What I might change next\n\nThe major shortfall of the current approximation (assuming we continue to mostly care about state level estimates) is the overly narrow credible intervals. If I did want to improve the accuracy of this model, what might I consider next?\n\nThe low hanging fruit here would be to use some combination of more samples and/or\nmore importance weighted samples per retained sample. In my testing this produces\ncredible intervals about halfway between what I showed above and the MCMC ones, at\nthe cost of another hour of runtime. It might be possible to go further than\nthis and get closer to the MCMC interval, but once I was at doubling the run time\nthat started to feel like cheating. For some applications though, that might be the\nright trade off to make!\n\nAs I already mentioned above, I didn't find much benefit from more training iterations,\nimportance weighted training, or making the RNVP component deeper. Thus, if\nI wanted to really invest a lot of time to improve this further, my next step\nmight be to consider fitting another model in parallel to ensemble with this one.\nFor example, perhaps an objective like the CUBO that tends to emphasize coverage\nwould be worth combining with this one either via multiple importance sampling\nor more simplistic model averaging.\n\n# What I might change for other models\n\nA logical next question: for models in general, which situations suggest\ntweaking which hyperparameters? While this is a really hard question, I'll offer\nsome tentative thoughts:\n\n**The model is compl**\n\n# Other things I didn’t cover\n\n(optimization, better gradients, pathfinder, DIS IKLD)\n\n# Cleaned up files to reproduce\n\n[^1]: This is still not a \"good\" (< .7) $\\hat{k}$, but as we saw in the last post,\n$\\hat{k}$ is not a infallible metric, especially if you are interested in just\na summary or two of the posterior where any defects it suggests may not be relevant.\nMy guess is that to get $\\hat{k}$ down, we'd need to move much further along\nin fully fleshing out the tails of the approximation, but even then we might\nstill have issues given the dimensionality of this posterior.\n[^2]: This happens to me a lot, where these bounds don't really tell me anything\nunless a model won't even pass a basic smell test. If you have some counterexamples,\nI'd love to see them!\n[^3]: The way `vistan` implements importance weighted sampling is to importance\nweight N candidate samples and select the highest weight. I'm unsure if this\nis an optimal procedure versus keeping all samples and applying the weights,\nbut it does make the sense in context of their bake off paper, where they want\nto maintain some sense of a fixed number of samples to keep comparisons fair.\n[^4]: `evaluation_fn = \"IWELBO\"` is just a quirk of their syntax, where with\n`M_iw_train = 1`, it's equivalent to not doing IW-weighted training at all.\n\n",
    "supporting": [
      "variational_mrp_pt7_files"
    ],
    "filters": [],
    "includes": {}
  }
}