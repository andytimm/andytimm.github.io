<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Andy Timm</title>
<link>https://andytimm.github.io/blog.html</link>
<atom:link href="https://andytimm.github.io/blog.xml" rel="self" type="application/rss+xml"/>
<description>Personal website of Andy Timm</description>
<generator>quarto-1.3.450</generator>
<lastBuildDate>Tue, 05 Dec 2023 05:00:00 GMT</lastBuildDate>
<item>
  <title>My talk on Regularized Raking at NYOSPM</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/NYSOPM_talk_regularized_raking/NYOSPM_talk.html</link>
  <description><![CDATA[ 




<p>I recently gave a talk on <strong>Regularized Raking</strong> at the <a href="https://nyhackr.org/index.html">New York Open Statistical Programming Meetup</a>.</p>
<p>Here is the abstract:</p>
<blockquote class="blockquote">
<p>Raking is among the most common algorithms for producing survey weights, but it is often opaque what qualities of the resulting weights set are prioritized by the method. This is especially true when practitioners turn to heuristic methods like trimming to improve weights. After reviewing the basic raking algorithm and showing some examples in R, I’ll show that survey weighting can also be understood as an optimization problem, one which allows for explicit regularization. In addition to providing a conceptually crisp view of what (vanilla) raking optimizes for, I’ll show that this regularized raking (implemented via the rsw python package) can allow for more fine-grained control over weights distributions, and ultimately more accurate weighted estimates. Examples will be drawn from US elections surveys.</p>
</blockquote>
<p>The slides and reproduction materials can be found here: <a href="https://github.com/andytimm/Regularized-Raking">https://github.com/andytimm/Regularized-Raking</a>. It looks like the presentation on stream froze for a bit in the middle part of the talk, so you may want to pop the slides open to follow along.</p>
<p>For anyone else in the New York area, the meetup is a great group of smart folks working in a bunch of interesting industries- come join us sometime.</p>
<p>The recording is below:</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/qeGltVhozNI" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div id="quarto-reuse" class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2023,
  author = {Timm, Andy},
  title = {My Talk on {Regularized} {Raking} at {NYOSPM}},
  date = {2023-12-05},
  url = {https://andytimm.github.io/posts/NYSOPM_talk_regularized_raking/NYOSPM_talk.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2023" class="csl-entry quarto-appendix-citeas">
Timm, Andy. 2023. <span>“My Talk on Regularized Raking at
NYOSPM.”</span> December 5, 2023. <a href="https://andytimm.github.io/posts/NYSOPM_talk_regularized_raking/NYOSPM_talk.html">https://andytimm.github.io/posts/NYSOPM_talk_regularized_raking/NYOSPM_talk.html</a>.
</div></div></section></div> ]]></description>
  <category>surveys</category>
  <category>weighting</category>
  <guid>https://andytimm.github.io/posts/NYSOPM_talk_regularized_raking/NYOSPM_talk.html</guid>
  <pubDate>Tue, 05 Dec 2023 05:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/NYSOPM_talk_regularized_raking/image/December_2023_Meetup_Card.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Variational Inference for MRP with Reliable Posterior Distributions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Variational MRP Pt7/variational_mrp_pt7.html</link>
  <description><![CDATA[ 




<p><strong>Note:</strong> Since</p>
<hr>
<p>This is the final post in my series about using Variational Inference to speed up complex Bayesian models, such as Multilevel Regression and Poststratification. Ideally, we want to do this without the approximation being of hilariously poor quality.</p>
<p>The last few posts in the series have explored several different major advances in black box variational inference. This post puts a bunch of these tools together to build a pretty decent approximation that runs ~8x faster than MCMC, and points to some other advances in BBVI I haven’t had time to cover in the series.</p>
<p>The other posts in the series are:</p>
<ol type="1">
<li>Introducing the Problem- Why is VI useful, why VI can produce spherical cows</li>
<li>How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li>Problem 1: KL-D prefers exclusive solutions; are there alternatives?</li>
<li>Problem 2: Not all VI samples are of equal utility; can we weight them cleverly?</li>
<li>Problem 3: How can we get deeply flexible variational approximations; are Normalizing Flows the answer?</li>
<li>Problem 4: How can we know when VI is wrong? Are there useful error bounds?</li>
<li><strong>(This post)</strong>: Putting it all together</li>
</ol>
<section id="cutting-to-the-chase" class="level1">
<h1>Cutting to the chase</h1>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt7/plots/CI_plot.png" class="img-fluid" style="width:100.0%"></p>
<p>To cut to the chase, the new and improved variational approximation is looking pretty, pretty good!</p>
<p>Like with the simpler mean-field and full-rank models from earlier in the series, this has the medians basically correct, but we also have reasonable uncertainty estimation too. First, the state distributions are much more smooth and unimodal- no more “lumpy” distributions with odd spikes of probability that make no sense as a model of public opinion. Further, the approximation is more consistent: while there’s still some variation state to state in how closely VI matches MCMC, pretty much all states are reasonable.</p>
<p>Certainly, we’re still to some degree understating the full size of MCMC’s credible interval. Considering this model runs in an hour and change versus MCMC’s 8 hours on 60,000 datapoints (!), this feels pretty acceptable. As I’ll write a bit more about later, there are a few ways to trade compute and/or runtime to fill out the CI’s as well.</p>
<p>Last time we look at a variational approximation in post 2, we found a dot plot was a significantly more revealing visual, which made it clear how bad the first try at VI in the series was. How does that look here?</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt7/plots/dot_plot.png" class="img-fluid" style="width:100.0%"></p>
<p>Again, pretty solid- no more weird spikes, and the concentration of mass looks pretty comparable (if a bit compressed) versus MCMC. VI is now much more uncertain about the same states as MCMC, and no longer shows any signs of degenerate optimization to fit data points. Nice!</p>
<p>Finally, how are the diagnostics we learned in the last post? The <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is .9, which is at least much improved<sup>1</sup>. The approximation is good enough that the Wasserstein bounds aren’t tight enough to inform us much about any issues<sup>2</sup>, although we should be a bit careful in trusting them giving that <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> (recall: the Wasserstein bounds aren’t super reliable when <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is high). In one sense, none of the diagnostics here are “great”, but this is pretty typical of my experience with BBVI for non-trivial models. We’re almost always losing something from the true posterior, and these diagnostics are not sufficiently fine-grained to differentiate important from unimportant losses.</p>
</section>
<section id="what-worked-here" class="level1">
<h1>What worked here?</h1>
<p>So the caption above gives some hints, but what all is in this model?</p>
<p>To fit this variational approximation, I’m using Agrawal, Domke, and Sheldon’s <a href="https://github.com/abhiagwl/vistan/tree/master">vistan</a>, which is a companion python package to their great paper <a href="https://proceedings.neurips.cc/paper/2020/file/c91e3483cf4f90057d02aa492d2b25b1-Paper.pdf">Advances In Black-Box VI</a>.</p>
<p>Here’s a footnote with more implementation details<sup>3</sup>, but for purposes of this post, I’ll just reference the parameters of the main setup function here:</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">vistan.algorithm(</span>
<span id="cb1-2">    vi_family <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rnvp"</span>,</span>
<span id="cb1-3">    full_step_search <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb1-4">    full_step_search_scaling <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb1-5">    step_size_exp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,</span>
<span id="cb1-6">    step_size_exp_range <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>],</span>
<span id="cb1-7">    step_size_base <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>,</span>
<span id="cb1-8">    step_size_scale <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">4.0</span>,</span>
<span id="cb1-9">    max_iters <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>,</span>
<span id="cb1-10">    optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'adam'</span>,</span>
<span id="cb1-11">    M_iw_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb1-12">    M_iw_sample <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>,</span>
<span id="cb1-13">    grad_estimator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"STL"</span>,</span>
<span id="cb1-14">    per_iter_sample_budget <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>,</span>
<span id="cb1-15">    fix_sample_budget <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb1-16">    evaluation_fn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"IWELBO"</span>,</span>
<span id="cb1-17">    rnvp_num_transformations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>,</span>
<span id="cb1-18">    rnvp_num_hidden_units <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>,</span>
<span id="cb1-19">    rnvp_num_hidden_layers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,</span>
<span id="cb1-20">    rnvp_params_init_scale <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb1-21">)</span></code></pre></div>
</div>
<p>Let’s talk about:</p>
<ol type="1">
<li>Normalizing Flows</li>
<li>Importance Sampling</li>
<li>Optimization</li>
<li>Sampling Budgets</li>
</ol>
<section id="normalizing-flows" class="level3">
<h3 class="anchored" data-anchor-id="normalizing-flows">Normalizing Flows</h3>
<p>First, this is using a <a href="https://arxiv.org/abs/1605.08803">Real NVP</a> normalizing flow, with a fairly small (<code>10</code>) number of transformation layers, each of which is using a pretty shallow neural net (<code>2 layers, 32 hidden units</code>). This helps make the approximating distribution complex enough to handle our model. I didn’t find much benefit from either adding more transform layers or making the neural nets deeper- that sort of makes sense, since the jump from mean-field or full-rank VI to using a normalizing flow at all is a fairly big one in terms of model representation capacity.</p>
</section>
<section id="importance-sampling" class="level3">
<h3 class="anchored" data-anchor-id="importance-sampling">Importance Sampling</h3>
<p>Second, this is using importance sampling only at the sampling stage, and with just <code>10</code> IW samples per returned sample. I didn’t find much benefit from importance weighted training<sup>4</sup> here, although for relatively more complex models than this I’ve found it to sometimes matter. As a point of comparison, in their paper linked above, Agrawal et al.&nbsp;find that about 20% of models actually get <em>worse</em> with IW-Training, and only ~10% get improvements, although those improvements seem to be clustered in more complex models and can be fairly significant.</p>
<p>At least for this particular model, using 10 importance samples per returned sample during provided most of the benefit of importance weighting, although pushing this higher to 100 or 200 helped fill out the outcome distribution’s tails a bit (more on this in a bit).</p>
</section>
<section id="optimization" class="level3">
<h3 class="anchored" data-anchor-id="optimization">Optimization</h3>
<p>Finally, many of the parameters here are about optimization, which we haven’t talked about in this series too much yet, despite it being critical to good performance.</p>
<p>First, Agrawal et al.&nbsp;found that variational inference is reasonably sensitive to optimization hyperparameters like the step size the Adam optimizer uses- to handle this, they suggest initial runs at few different step sizes (in <code>step_size_exp_range</code>), selecting the best one via an ELBO average over the whole optimization trace. This model benefits quite a lot from this, and Agrawal et al.’s results see strong improvements for ~25-30% of models using this adjustment.</p>
<p>Second, they use the <a href="https://arxiv.org/abs/1703.09194">Sticking the Landing</a> (<code>STL</code>) gradient estimator, which I’ll link out to later, but is essentially removing the score term from the gradient to reduce it’s variance.</p>
<p>A final optimization hyperparameter here is the number of iterations (both for that step search procedure and the final run)- I found <code>1000</code> was more than sufficient for optimization of this model, with going up to 2000 iterations not getting us much benefit, but going down to 500 leading to a heavily over-dispersed posterior.</p>
</section>
<section id="a-sample-budget" class="level3">
<h3 class="anchored" data-anchor-id="a-sample-budget">A sample budget?</h3>
<p>Since their paper is ultimately a bakeoff, Agrawal et al.’s paper touches on the theme of a sample budget again and again, and it’s a great concept to consider here. Essentially, a sample or computation budget is somewhat <strong>fungible</strong>: given an amount of run time, compute available, etc, we can, for example, trade a larger <code>max_iters</code> for using (more) importance weighted training, or use fewer iterations for (more) importance weighting our samples.</p>
<p>How to best spend this budget is an open question and fairly model dependent- here, I decided not to present the best model I could possibly fit with variational inference, but the best one I could get to fit in ~1h. The point of this series is to fit something nearly as good fast, not a different, more complex way but also in 8h.</p>
<p>Next, I’ll talk through some ideas for what could improve the model further at greater compute and wall time cost.</p>
</section>
</section>
<section id="what-i-might-change-next" class="level1">
<h1>What I might change next</h1>
<p>The major shortfall of the current approximation (assuming we continue to mostly care about state level estimates) is the overly narrow credible intervals. If I did want to improve the accuracy of this model, what might I consider next?</p>
<p>The low hanging fruit here would be to use some combination of more samples and/or more importance weighted samples per retained sample. In my testing this produces credible intervals about halfway between what I showed above and the MCMC ones, at the cost of another hour of runtime. It might be possible to go further than this and get closer to the MCMC interval, but likely suffers from quite harsh diminishing returns. For some applications though, that might be the right trade off to make!</p>
<p>As I already mentioned above, I didn’t find much benefit from more training iterations, importance weighted training, or making the RNVP component deeper. Thus, if I wanted to really invest a lot of time to improve this further, my next step might be to consider fitting another model in parallel to ensemble with this one. For example, perhaps an objective like the CUBO that tends to emphasize coverage would be worth combining with this one either via multiple importance sampling or more simplistic model averaging.</p>
</section>
<section id="what-i-might-change-for-other-models" class="level1">
<h1>What I might change for other models</h1>
<p>A logical next question: for models in general, which situations suggest tweaking which hyperparameters? While this is a really hard question, I’ll offer some tentative thoughts:</p>
<p><strong>The variational approximation is unable to represent the complexity of my model:</strong> In this situation, I’ve had the best luck increasing the complexity of my normalizing flow. Just like increasing the number of transforms helped in our ring density approximation example in post 6, more transforms and/or a deeper neural net within each transformation seems to be the most straightforward way I’ve found to improve representation capacity of variational inference. For the most complex models, perhaps changing the flow type will be necessary or efficient, but I’ve had surprisingly good luck just scaling up RNVP.</p>
<p>Like I mentioned above for this series’ specific model, I’ve had pretty meh results with IW-Training. I’ve had 1-2 models actually really benefit, but it hurts as often as it helps it seems, so it’s not something I reach for first anymore.</p>
<p><strong>The approximation is close, but it misses some minor aspect the posterior:</strong> In this type of situation, like the one above, adding more samples or using more importance weighting to produce the samples has worked well. In my experience, diminishing returns on the number of importance samples kick in faster than on the number of full model samples. I rarely see benefits beyond a couple hundred importance samples, but more draws often continue to provide benefits well into the thousands sometimes. Keep in mind that having trained a VI model, sampling is often orders of magnitude faster than MCMC: “just sample more” is much less time consuming advice to take than with MCMC.</p>
<p><strong>I can’t get the model to meaningfully converge:</strong> This often looks like the result we got in the second post in the series with mean-field and full-rank Variational Inference. Like with that post, there’s a variety of reasons this can happen. If you’re using mean-field or full-rank for a complex model, there’s a good chance you just need a normalizing flow or otherwise more complex approximating distribution to get any sort of useful convergence.</p>
<p>If you’re using something complex to make the approximation and you still see massively under/over-dispersed posteriors, then consider broadening the step size search, or grid searching a bit over the other Adam hyperparameters. Unlike with models based on deeper neural nets, I haven’t ever really seen a variational approximation plateau on loss for a long time and make a breakthrough; it’s pretty reasonable to trust early stopping and try to find something that actually gets optimization traction early on.</p>
<p><strong>There’s a whole part of the posterior entirely missing:</strong> I’ve only really seen this with highly multi-modal posteriors, but sometimes a single ELBO based model will only meaningfully cover a single mode in a parameter you care about. In this case, I’ve found a few smaller models averaged/MIS’d together to be the simplest solution- a single model that covers all modes is often quite hard given the mode seeking behavior of the ELBO that I discussed in post 3. Trying a different loss here is an option, but for more complex posteriors, I often struggle to get convergence with the CUBO.</p>
<p>This is by no means a authoritative list, but hopefully this set of suggestions for the most common issues I’ve had with variational inference in a variety of applied models is helpful. I’d also highly recommend the <a href="https://proceedings.neurips.cc/paper/2020/file/c91e3483cf4f90057d02aa492d2b25b1-Paper.pdf">Advances In Black-Box VI</a> paper mentioned above for more practical guidance of this type.</p>
</section>
<section id="other-things-i-didnt-cover" class="level1">
<h1>Other things I didn’t cover</h1>
<p>Variational Inference is a decent sized research area, so I couldn’t cover everything in this series. As a way to wrap up, I want to gesture at some other papers that are worthwhile, but weren’t worth a full post in this series.</p>
<p><strong>Better Optimization for Variational Inference:</strong> Besides the work on step search in <a href="https://proceedings.neurips.cc/paper/2020/file/c91e3483cf4f90057d02aa492d2b25b1-Paper.pdf">Advances In Black-Box VI</a>, there are two really good papers on improving the underlying gradients we optimize on in variational inference. First, the <a href="https://arxiv.org/abs/1703.09194">Sticking the Landing</a> gradient estimator removes the score term in the total gradient with respect to the variational parameters. The result is a still unbiased<sup>5</sup>, but lower variance gradient estimator which helps a lot with both normalizing flow and simpler approximation fitting. Second, for when using importance weighted training, there’s the <a href="https://arxiv.org/abs/1810.04152">doubly reparameterized gradient (DReG)</a> estimator, which is both lower variance and unbiased via a clever (second) application of the reparameterization trick.</p>
<p><strong>Pathfinder:</strong> Coming soon to <code>brms</code> and Stan in general, <a href="https://arxiv.org/pdf/2108.03782.pdf">Pathfinder</a> is an attempt at a variational inference algorithm by Lu Zhang, Andrew Gelman, Aki Vehtari, and Bob Carpenter<sup>6</sup>. It uses a quasi-Newtonian optimization algorithm, and then samples from (a MIS combination of) Gaussian approximations along the optimization. There’s a ton of clever work here to make this incredibly fast, and comparatively parallel versus other variational algorithms. For even moderately complex posteriors, it’s blazingly fast, and quite accurate.</p>
<p>My only problem with it is that for more challenging posteriors, I’ve found it a little limited: it doesn’t seem to have the representation capacity possible with normalizing flows, which unfortunately is necessary for most of the non-blog-post-series-examples applications I use VI for. Like so many papers with these authors though, there’s a ton here that’s deeply insightful and more broadly applicable knowledge here, even if you need normalizing flows for your work.</p>
<p><strong>Boosting Variational Inference:</strong> Given how dominant boosting based algorithms are in basic machine learning, applying boosting to variational inference definitely had my attention when I first saw papers like <a href="https://arxiv.org/abs/1906.01235?utm_source=pocket_saves">Universal Boosting Variational Inference</a>. What’s weird about this strain of papers though is that there don’t see to be any public replication materials that sufficiently implement this for me to test it out, and there are no comparative bake off versus other serious, modern variational inference algorithms I can find.</p>
<p>This may be a lack of familiarity on my part (and I’m most uncertain about boosting VI of everything I’ve discussed in this series), but the vibes of this sub-literature feel off to me. Why is no one releasing a public, serious implementation of this<sup>7</sup>? If the authors claims about performance and ease of use are true, this pattern of “lots of papers, little code” is even weirder. Again, I’m super uncertain here, but after investing some time to dig here, this corner of the variational inference literature is surprisingly hard to engage with, and so I haven’t really made the time yet.</p>
<p>Thanks for reading this end to the series! Writing it as I learned more and more about variational inference has been incredibly helpful to me, and hopefully it has been useful to you as well.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>This is still not a “good” (&lt; .7) <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D">, but as we saw in the last post, <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is not a infallible metric, especially if you are interested in just a summary or two of the posterior where any defects it suggests may not be relevant. My guess is that to get <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> down, we’d need to move much further along in fully fleshing out the tails of the approximation, but even then we might still have issues given the dimensionality of this posterior.↩︎</p></li>
<li id="fn2"><p>This happens to me a lot, where these bounds don’t really tell me anything unless a model won’t even pass a basic smell test. If you have some counterexamples, I’d love to see them!↩︎</p></li>
<li id="fn3"><p>To fit this, I first used brms’s <code>make_standata</code> on both the cleaned survey responses and the ~12,000 bin matrix we want to post-stratify on to get data to pass to Stan, which I just saved out as JSON files to read back in later. Then, I used <code>make_stancode</code> to extract the basic Stan model, and added in logic to produce predictions onto the post-stratification matrix. You can see that modified Stan model <a href="https://github.com/andytimm/vistan_mrp_predictions">here</a>. After that, it was fairly easy to pass these inputs into vistan in python. One quick final note: vistan is quite sensetive to the version of python you use, so I recommend making a virtual environment with python 3.8 or 3.9.↩︎</p></li>
<li id="fn4"><p><code>evaluation_fn = "IWELBO"</code> is just a quirk of their syntax, where with <code>M_iw_train = 1</code>, it’s equivalent to not doing IW-weighted training at all.↩︎</p></li>
<li id="fn5"><p>See the paper for a longer explanation for why it’s still unbiased, but essentially this term has expectation zero. for some samples it may not be zero, but it’s sufficient for the unbiasedness of the broader gradient that it have expectation zero.↩︎</p></li>
<li id="fn6"><p>See the paper for a longer explanation for why it’s still unbiased, but essentially this term has expectation zero. for some samples it may not be zero, but it’s sufficient for the unbiasedness of the broader gradient that it have expectation zero.↩︎</p></li>
<li id="fn7"><p>Beyond this <a href="https://pyro.ai/examples/boosting_bbvi.html">toy example</a> in Pyro, I can’t find much. Again, prove me wrong if you know of something.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div id="quarto-reuse" class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2023,
  author = {Timm, Andy},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2023-07-12},
  url = {https://andytimm.github.io/posts/Variational MRP Pt7/variational_mrp_pt7.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2023" class="csl-entry quarto-appendix-citeas">
Timm, Andy. 2023. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> July 12, 2023. <a href="https://andytimm.github.io/posts/Variational MRP Pt7/variational_mrp_pt7.html">https://andytimm.github.io/posts/Variational
MRP Pt7/variational_mrp_pt7.html</a>.
</div></div></section></div> ]]></description>
  <category>MRP</category>
  <category>Variational Inference</category>
  <guid>https://andytimm.github.io/posts/Variational MRP Pt7/variational_mrp_pt7.html</guid>
  <pubDate>Wed, 12 Jul 2023 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Variational Inference for MRP with Reliable Posterior Distributions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Variational MRP Pt6/variational_mrp_pt6.html</link>
  <description><![CDATA[ 




<p>This is section 6 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.</p>
<p>The general structure for this post and the posts around it will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in these four posts will go a long way towards more robust variational inference. I’ll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.</p>
<p>In the <a href="https://andytimm.github.io/posts/Variational%20MRP%20Pt5/variational_mrp_5.html">last post</a> we looked at normalizing flows, a way to leverage neural networks to learn significantly more expressive variational families in a way that adapt to specific problems.</p>
<p>In this post, we’ll explore different diagnostics for variational inference, ranging from simple statistics that are easy to calculate as we fit our approximation to solving the problem in parallel with MCMC to compare and contrast. Some recurring themes will be aiming to be precise about what constitutes failure under each diagnostic tool, and providing intuition building examples where each diagnostic will fail to do anything useful. While no single diagnostic provides strong guarantees of variational inference’s correctness on their own, taken together the tools in this post broaden our ability to know when our models fall short.</p>
<p>The rough plan for the series is as follows:</p>
<ol type="1">
<li>Introducing the Problem- Why is VI useful, why VI can produce spherical cows</li>
<li>How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li>Problem 1: KL-D prefers exclusive solutions; are there alternatives?</li>
<li>Problem 2: Not all VI samples are of equal utility; can we weight them cleverly?</li>
<li>Problem 3: How can we get deeply flexible variational approximations; are Normalizing Flows the answer?</li>
<li><strong>(This post)</strong> Problem 4: How can we know when VI is wrong? Are there useful error bounds?</li>
<li>Putting the workflow all together</li>
</ol>
<section id="looking-at-our-loss-function" class="level1">
<h1>Looking at our loss function</h1>
<p>One logical place to start with diagnostics is to discuss what we can and can’t infer from our optimization objectives like an ELBO or CUBO.</p>
<p>In training a model with variational inference some common stopping rule choices are either to just run optimization for a fixed number of iterations, or to stop when relative changes in the loss have slowed, indicating convergence of the optimization to a local minimum. So we can at least look at changes in the ELBO/CUBO/other loss to know if our approximation has hit a local minimum yet.</p>
<p>Unfortunately, that’s about all monitoring the loss can tell us. Recall that an unknown, multiplicative constant exists in <img src="https://latex.codecogs.com/png.latex?p(z,x)%20%5Cpropto%20p(z%7Cx)"> that changes as reparameterize our model; thus, we can’t compare two different models on the same objective and expect their ELBO or similar loss values to be comparable. So the typical ML strategy of “which model achieves lower loss” is pretty much out here.</p>
<p>Also, the loss values themselves aren’t particularly meaningful: there’s no way to interpret a given ELBO as indicating a good approximation, for example. This generally stems from our bounds being bounds, not directly optimizing the quantity we want to optimize. While they’re definitely degenerate cases, there are even some fun counter examples I’ll show in a second where you can make the ELBO/CUBO arbitrarily low, while still allowing the posterior mean or standard deviation to be arbitrarily wrong!</p>
</section>
<section id="the-majesty-of-hatk" class="level1">
<h1>The majesty of <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"></h1>
<p>So if we can’t just look at our loss, what can we look at? One broadly applicable diagnostic tool is <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D">, which we already introduced in the post on using importance sampling to improve variational inference.</p>
<p>As a several sentence refresher, Pareto smoothed importance sampling (PSIS) proposes to stabilize importance ratios <img src="https://latex.codecogs.com/png.latex?r(%5Ctheta)"> used in importance sampling by modeling the tail of the distribution as a generalized Pareto distribution:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7B%5Csigma%7D%20%5Cleft(1%20+%20k%5Cfrac%7Br%20-%20%5Ctau%7D%7B%5Csigma%7D%20%5Cright)%5E%7B-1/k-1%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Ctau"> is a lower bound parameter, which in our case defines how many ratios from the tail we’ll actually model. <img src="https://latex.codecogs.com/png.latex?%5Csigma"> is a scale parameter, and <img src="https://latex.codecogs.com/png.latex?k"> is a unconstrained shape parameter.</p>
<p>To see how this provides a natural diagnostic for importance sampling, it’s useful to know that importance sampling depends on how many moments <img src="https://latex.codecogs.com/png.latex?r(%5Ctheta)"> has- for example, if at least two moments exist, the vanilla IS estimator has finite variance (which is obviously required, but no guarantee of performance since it might be finite but massive). The GPD has <img src="https://latex.codecogs.com/png.latex?k%5E%7B-1%7D"> finite fractional moments when <img src="https://latex.codecogs.com/png.latex?k%20%3E%200">. <a href="https://arxiv.org/abs/1507.02646">Vehtari et Al. (2015)</a> show through extensive theoretical digging and simulations that PSIS works fantastically when <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D%20%3C%20.5">. and acceptably if <img src="https://latex.codecogs.com/png.latex?.5%20%3C%20%5Chat%7Bk%7D%20%3C%20.7">. Beyond <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D%20=%20.7"> there the number of samples needed rapidly become impractically large.</p>
<p>Why should we think <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is a relevant diagnostic for variational inference? <a href="https://arxiv.org/abs/1511.01437">Chaterjee and Draconis (2018)</a> showed that for a given accuracy, how big our number of samples <img src="https://latex.codecogs.com/png.latex?S"> needs to be for importance sampling more broadly depends on how close <img src="https://latex.codecogs.com/png.latex?q(x)"> is to <img src="https://latex.codecogs.com/png.latex?p(x)"> in KL distance- we need to satisfy <img src="https://latex.codecogs.com/png.latex?log(S)%20%5Cgeq%20%5Cmathbb%7BE%7D_%7B%5Ctheta%20%5Csim%20q(x)%7D%5Br(%5Ctheta)log(r(%5Ctheta))%5D"> to get reasonable accuracy. So a good <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> indicates importance sampling is feasible, which in turn indicates that <img src="https://latex.codecogs.com/png.latex?q(x)"> is likely close to <img src="https://latex.codecogs.com/png.latex?p(x)"> in KL Divergence- exactly what we’re hoping to get at!</p>
<p>Fleshing out the use of <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> as a VI diagnostic was done by <a href="https://arxiv.org/abs/1802.02538">Yao et al.&nbsp;(2018)</a>, who generally show that high values of <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> do generally map onto posterior approximations with variational inference being quite poor. This is really useful, and generally maps well on to my experience- if <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is bigger than .7, you probably need to go back to the drawing board on how you’re fitting your VI.</p>
<p>What I want to stress though, is that the inverse isn’t broadly true- a low <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> isn’t necessarily a guarantee the VI approximation is good. Let’s look at a couple different ways this can happen.</p>
<section id="problem-case-1-importance-sampling-neq-direct-variational-inference" class="level2">
<h2 class="anchored" data-anchor-id="problem-case-1-importance-sampling-neq-direct-variational-inference">Problem Case 1: Importance sampling <img src="https://latex.codecogs.com/png.latex?%5Cneq"> direct variational inference</h2>
<p>We should keep in mind that <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is ultimately a diagnostic tool for importance sampling, and in cases where the needs of importance sampling and simple variational inference diverge, <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> can give a misleading answer.</p>
<p>Let’s re-use an example from the importance sampling post to illustrate this. What happens if we approximate the red distribution below with the green one?</p>
<div class="cell">

</div>
<div class="cell">

</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">mixture <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-2">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"red"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> mean_seeking_kl), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"green"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggtitle</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The green approximation is great for IS, terrible on its own"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt6/variational_mrp_pt6_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The green distribution here is a prime candidate to importance sample to approximate the red one- it coves all the needed mass, and we can massively down weight the irrelevant points in the center. On the other hand, this’d be a really, really bad variational approximation to use raw, since it has a ton of mass between the two modes which will blow up our loss. Because the needs of PSIS-based estimators and unadjusted VI diverge, <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is low, but the approximation would be pretty bad:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">importance_ratios <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tibble</span>(</span>
<span id="cb2-2"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">q_x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>),</span>
<span id="cb2-3"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">p_x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)),</span>
<span id="cb2-4"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ratios =</span> (.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)))<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>))</span>
<span id="cb2-5"></span>
<span id="cb2-6">psis_result <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">psis</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(importance_ratios<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>ratios),</span>
<span id="cb2-7">                       <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">r_eff =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span>)</span>
<span id="cb2-8"></span>
<span id="cb2-9">psis_result<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>diagnostics<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>pareto_k</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -1.737515</code></pre>
</div>
</div>
<p>So our <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> says everything is beautiful, but in reality it’s really only a happy time for PSIS, not the raw VI estimator. This ultimately isn’t the most concerning failure mode: if you do the work to calculate <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D">, you’re pretty much ready to use PSIS to improve your variational inference anyway. That said, this should provide intuition that <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> isn’t in general super well equipped to tell you much about non-IS augmented VI.</p>
</section>
<section id="problem-case-2-hatk-is-a-local-diagnostic" class="level2">
<h2 class="anchored" data-anchor-id="problem-case-2-hatk-is-a-local-diagnostic">Problem Case 2: <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is a local diagnostic</h2>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> inherits a common issue with most KL Divergence adjacent metrics: it’s ultimately something we evaluate locally, so if there’s a part of the posterior totally unknown to our <img src="https://latex.codecogs.com/png.latex?q(x)">, it won’t be able to tell you what you’re missing.</p>
<p>We already used 1 example from the importance sampling post, so let’s keep that moving. What do you think will happen with <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> with the green approximation below that misses a whole mode?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">mixture <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-2">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"red"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> mode_seeking_kl), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"green"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggtitle</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"We're missing a whole mode here"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt6/variational_mrp_pt6_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>If you guessed <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> will say everything is perfect when it’s not, you’re correct:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">second_importance_ratios <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tibble</span>(</span>
<span id="cb5-2"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">q_x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200000</span>,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb5-3"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">p_x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)),</span>
<span id="cb5-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Notice: these density calls are at the points defined by q(x)!</span></span>
<span id="cb5-5"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ratios =</span> (.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)))<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb5-6"></span>
<span id="cb5-7">psis_result_2 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">psis</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(second_importance_ratios<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>ratios),</span>
<span id="cb5-8">                       <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">r_eff =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span>)</span>
<span id="cb5-9"></span>
<span id="cb5-10">psis_result_2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>diagnostics<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>pareto_k</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.07343881</code></pre>
</div>
</div>
<p>That’s… not great. Since we evaluate the importance ratio and thus eventually <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> at the collection of values in <img src="https://latex.codecogs.com/png.latex?q(x)">, the diagnostic has no real way to know we’re missing an entire mode, and unlike in the above case there’s no easy fix here.</p>
<p>Another interesting question this example raises is what happens in high dimensions, where it’s much less intuitive what “missing one or several modes” looks like. Just by increasing the sd of the normal <img src="https://latex.codecogs.com/png.latex?q(x)"> a little in the example, we see a sudden, large increase in <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D">;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1">third_importance_ratios <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tibble</span>(</span>
<span id="cb7-2"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">q_x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200000</span>,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>),</span>
<span id="cb7-3"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">p_x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)),</span>
<span id="cb7-4"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ratios =</span> (.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)))<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))</span>
<span id="cb7-5"></span>
<span id="cb7-6">psis_result_3 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">psis</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(third_importance_ratios<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>ratios),</span>
<span id="cb7-7">                       <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">r_eff =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1">psis_result_3<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>diagnostics<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>pareto_k</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.70381</code></pre>
</div>
</div>
<p>similar sudden shifts in <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> can frequently occur as you increase the dimension of a posterior you’re approximating- intuitively, the mass you do and don’t know about becomes much harder to keep track of in high dimensions and for complex posteriors. This can lead to <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> being a bit less stable than you’d like over different initializations or other slight modifications of a VI model, with this pattern being common both in my own applications and documented in several papers like <a href="https://arxiv.org/abs/2302.12419">Wang et al.&nbsp;(2023)</a>’s testing.</p>
</section>
<section id="problem-case-3-hatk-is-a-joint-posterior-level-tool" class="level2">
<h2 class="anchored" data-anchor-id="problem-case-3-hatk-is-a-joint-posterior-level-tool">Problem Case 3: <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is a joint posterior level tool</h2>
<p>A final, more conceptual problem with <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> that <a href="https://arxiv.org/abs/1802.02538">Yao et al.&nbsp;(2018)</a> point out is that it’s ultimately a diagnostic of the joint posterior, not the specific marginal or summary statistic you may ultimately care about.</p>
<p>Variational inference is hard: we often know that the overall posterior approximation is deeply flawed, but it may be up to the task of representing some metrics we care about correctly enough. For example, in the MRP example I introduced earlier in the series, the mean-field variational inference fit was reasonable at representing the state-level means, but garbage at pretty much anything related to uncertainty. The <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> from that model was greater than 2, so we clearly know the broader posterior approximation was poor, but <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> might be a false positive sign if what you really care about was just the means. For the most complicated posteriors, we should expect to spend a lot of time in this feeling of “some parts of the posterior may be good enough”, so this is a useful trap to know about.</p>
<p>…Let’s step back for a second. Since I introduced <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> as a diagnostic with a bunch of cases where it falls short in surprising ways, I do want to emphasize it is a very useful <em>heuristic</em> diagnostic tool in general. Large <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> tells you something is very likely wrong with your joint posterior, and that’s generally practically helpful information. Where we need to be cautious is in inferring whether the wrongness <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> picks up on is something we care about, and also in remembering that low <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> doesn’t provide guarantees of correctness.</p>
</section>
</section>
<section id="wasserstein-bounds" class="level1">
<h1>Wasserstein Bounds</h1>
<p>So we’ve seen some limitations of using our objectives and <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> as diagnostics. Let’s break out some fun propositions and examples from <a href="https://arxiv.org/abs/1910.04102">Huggins et al.&nbsp;(2020)</a> to really drive home the need for a bound that actually makes guarantees on errors of posterior summaries we care about like means and variances.</p>
<hr>
<section id="proposition-3.1-arbitrarily-poor-mean-approximation" class="level3">
<h3 class="anchored" data-anchor-id="proposition-3.1-arbitrarily-poor-mean-approximation"><strong>Proposition 3.1 (Arbitrarily Poor Mean Approximation):</strong></h3>
<p>For any t &gt; 0, there exist (A) one dimensional, unimodal distributions <img src="https://latex.codecogs.com/png.latex?q"> and <img src="https://latex.codecogs.com/png.latex?p"> such that <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BKL%7D(q%20%5Cmid%20p)"> &lt; 0.9 and <img src="https://latex.codecogs.com/png.latex?%5Cleft(m_%7Bq%7D-m_p%5Cright)%5E2%3Et%20%5Csigma_p%5E2">, and <img src="https://latex.codecogs.com/png.latex?(B)"> one-dimensional, unimodal distributions <img src="https://latex.codecogs.com/png.latex?q"> and <img src="https://latex.codecogs.com/png.latex?p"> such that <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BKL%7D(q%20%5Cmid%20p)%20%3C%200.3"> and <img src="https://latex.codecogs.com/png.latex?%5Cleft(m_%7Bq%7D-m_p%5Cright)%5E2%3Et%20%5Csigma_%7Bq%7D%5E2">.</p>
</section>
<section id="proposition-3.2-arbitrarily-poor-variance-approximation" class="level3">
<h3 class="anchored" data-anchor-id="proposition-3.2-arbitrarily-poor-variance-approximation"><strong>Proposition 3.2 (Arbitrarily Poor Variance Approximation):</strong></h3>
<p>For any <img src="https://latex.codecogs.com/png.latex?t%20%5Cin(1,%20%5Cinfty%5D">, there exist one-dimensional, mean-zero, unimodal distributions <img src="https://latex.codecogs.com/png.latex?q"> and <img src="https://latex.codecogs.com/png.latex?p"> such that <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BKL%7D(q%20%5Cmid%20p)%3C0.12"> but <img src="https://latex.codecogs.com/png.latex?%5Csigma_p%5E2%20%5Cgeq%20t%20%5Csigma_%7Bq%7D%5E2">.</p>
<hr>
<p>So, these have some fairly scary names, huh? We can make the KL divergence pretty small, but have arbitrarily bad mean and variance approximation. How to do this? Given the examples here have to be unimodal and 1-D, they can’t be that, that weird, right?</p>
<hr>
</section>
<section id="proposition-3.1-example" class="level3">
<h3 class="anchored" data-anchor-id="proposition-3.1-example"><strong>Proposition 3.1 Example:</strong></h3>
<p>Let Weibull <img src="https://latex.codecogs.com/png.latex?(k,%201)"> denote the Weibull distribution with shape <img src="https://latex.codecogs.com/png.latex?k%3E0"> and scale 1. For (A), for any <img src="https://latex.codecogs.com/png.latex?t%3E0">, we can choose <img src="https://latex.codecogs.com/png.latex?k=k(t),%20p=%5Coperatorname%7BWeibull%7D(k,%201)">, and <img src="https://latex.codecogs.com/png.latex?q=%20%5Coperatorname%7BWeibull%7D(k%20/%202,1)">, where <img src="https://latex.codecogs.com/png.latex?k(t)%20%5Csearrow%200"> as <img src="https://latex.codecogs.com/png.latex?t%20%5Crightarrow%20%5Cinfty">. We can just exchange the two distributions for (B).</p>
</section>
<section id="proposition-3.2-example" class="level3">
<h3 class="anchored" data-anchor-id="proposition-3.2-example"><strong>Proposition 3.2 Example:</strong></h3>
<p>For any <img src="https://latex.codecogs.com/png.latex?t%3E0"> we let <img src="https://latex.codecogs.com/png.latex?h=h(t),%20p=%5Cmathcal%7BT%7D_h"> (standard <img src="https://latex.codecogs.com/png.latex?t"> distribution with <img src="https://latex.codecogs.com/png.latex?h"> degrees of freedom), and <img src="https://latex.codecogs.com/png.latex?q=%5Cmathcal%7BN%7D(0,1)"> (standard Gaussian), where <img src="https://latex.codecogs.com/png.latex?h(t)%20%5Csearrow%202"> as <img src="https://latex.codecogs.com/png.latex?t%20%5Crightarrow%20%5Cinfty">.</p>
<hr>
<p>I won’t show it here, but the Huggins et al.&nbsp;paper provides a similar proposition and example for <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> divergences and the CUBO bound we discussed earlier in the series, and the example has pretty much the same form.</p>
<p>Whenever presented with a “counterexample” to things working properly like this, it’s worth asking how broad the case’s applicability is: often counterexamples reside in the land of extremes, and we should be cautious in interpreting the result’s negative implications too broadly. There’s certainly some of that going on here, in the sense that usually a lower ELBO/CUBO will give some (if not perfect) traction in improving our posterior estimates of mean and variance. The intuitive point here though is that <strong>the bounds we optimize give no rigorous guarantees of posterior summaries we care about</strong>, even in the loosest sense.</p>
<p>To get some better guarantees like this, <a href="https://arxiv.org/abs/1910.04102">Huggins et al.&nbsp;(2020)</a> propose bounds on the the mean and uncertainty estimates that arise from variational inference, which leverage the Wasserstein distance. In addition to providing actual<sup>1</sup> bounds on quantities we care about, these bounds come at very reasonable computational cost, as they are readily computable from the bounds we already have (ELBO and CUBO) plus some additional Monte Carlo estimation and quick analytic calculation.</p>
<p>Let’s first discuss what the Wasserstein distance is, and then discuss the fairly involved path from our existing estimates to actually calculating the bounds.</p>
</section>
<section id="whats-a-wassterstein" class="level2">
<h2 class="anchored" data-anchor-id="whats-a-wassterstein">What’s a Wassterstein?</h2>
<p>The <img src="https://latex.codecogs.com/png.latex?p">-Wasserstein distance between <img src="https://latex.codecogs.com/png.latex?%5Cxi"> and <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BW%7D_p(%5Cxi,%20p):=%5Cinf%20_%7B%5Cgamma%20%5Cin%20%5CGamma(%5Cxi,%20p)%7D%5Cleft%5C%7B%5Cint%5Cleft%5C%7C%5Ctheta-%5Ctheta%5E%7B%5Cprime%7D%5Cright%5C%7C_2%5Ep%20%5Cgamma%5Cleft(%5Cmathrm%7Bd%7D%20%5Ctheta,%20%5Cmathrm%7Bd%7D%20%5Ctheta%5E%7B%5Cprime%7D%5Cright)%5Cright%5C%7D%5E%7B1%20/%20p%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?%5CGamma(%5Cxi,%20p)"> is the set of couplings between <img src="https://latex.codecogs.com/png.latex?%5Cxi"> and <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
<p>As a quick note on notation, I’m overloading <img src="https://latex.codecogs.com/png.latex?p"> here a bit; given I’ve used <img src="https://latex.codecogs.com/png.latex?p"> for our target posterior all series, I’m not going to switch that now, and calling it anything other than a <img src="https://latex.codecogs.com/png.latex?p">-Wasserstein distance would just be confusing to anyone who’se seen this distance before.</p>
<p>This looks a lot more involved than something like the KL Divergence. For example, the fact that we have an infinum over something complicated looking suggests this’ll be a real pain to calculate. As we’ll see in a second, Huggins et al.&nbsp;don’t actually seek to calculate it or approximate it, they seek to bound it <sup>2</sup>.</p>
<p>Before we get there though, let’s seek to understand the distance and it’s properties a little better.</p>
<p>A good way to start unpacking this is to consider the optimal transport problem. Given some probability mass <img src="https://latex.codecogs.com/png.latex?%5Cxi(%5Ctheta)"> on a space <img src="https://latex.codecogs.com/png.latex?X">, we wish to transport it such that it is transformed into the distribution <img src="https://latex.codecogs.com/png.latex?p(%5Ctheta)">. To provide physical intuition, this is often formulated as a problem of moving an equal amount of dirt/earth in pile <img src="https://latex.codecogs.com/png.latex?%5Cxi(%5Ctheta)"> to make pile <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Ctheta)">- hence the name commonly used in several disciplines, the <a href="https://en.wikipedia.org/wiki/Earth_mover%27s_distance">Earthmovers Distance</a>.</p>
<p>Let’s say we have some non-negative cost function for moving mass from <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> to <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B%5Cprime%7D">, <img src="https://latex.codecogs.com/png.latex?c(%5Ctheta,%5Ctheta%5E%7B%5Cprime%7D)">. A single transport plan for moving from <img src="https://latex.codecogs.com/png.latex?%5Cxi(%5Ctheta)"> to <img src="https://latex.codecogs.com/png.latex?p(%5Ctheta%5E%7B%5Cprime%7D)"> is a function <img src="https://latex.codecogs.com/png.latex?%5Cgamma(%5Cxi,%20%5Cpi)"> which describes the amount of mass to move at each point. If we assume <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is a valid joint probability mass with marginals <img src="https://latex.codecogs.com/png.latex?%5Cxi%5Ctheta)"> and <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Ctheta%5E%7B%5Cprime%7D)"> <sup>3</sup>, then the infinitesimal mass we transport from <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> to <img src="https://latex.codecogs.com/png.latex?%5Ctheta%7B%5Cprime%7D"> is <img src="https://latex.codecogs.com/png.latex?%5Cgamma(%5Ctheta,%20%5Ctheta%5E%7B%5Cprime%7D)%20d%5Ctheta%20d%5Ctheta%5E%7B%5Cprime%7D">, with cost</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cint%20%5Cint%20c(%5Ctheta,%5Ctheta%5E%7B%5Cprime%7D)%20%5Cgamma(%5Ctheta,%20%5Ctheta%5E%7B%5Cprime%7D)%20d%5Ctheta%20d%5Ctheta%5E%7B%5Cprime%7D%20=%20%5Cint%20c(%5Ctheta,%5Ctheta%5E%7B%5Cprime%7D)%20d%20%5Cgamma(%5Ctheta,%5Ctheta%5E%7B%5Cprime%7D)%0A"></p>
<p>Finally getting close to something that looks like our Wasserstein distance. There are many such plans, but the one we want, the solution to the optimal transport problem, is the one with minimal cost out of all such plans.</p>
<p>One last point to cover to define this: what’s our cost? If the cost here is the <img src="https://latex.codecogs.com/png.latex?p">-distance between our <img src="https://latex.codecogs.com/png.latex?%5Ctheta">s, then this is the p-Wassterstein distance.</p>
<p>What are some properties of this distance? I already mentioned a major downside (this looks nasty to estimate in general, and indeed it is). What are the upsides of this?</p>
<p>Unlike the KL or <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> divergences we’ve looked at before, the Wasserstein distance takes into account the metric on the underlying space! Let’s unpack that by again drawing on the optimal transport problem for intuition. The Wasserstein distance takes into account not only the differences in the values or probabilities assigned to different points in the distributions but also the actual “spatial”<sup>4</sup> arrangement of those points.</p>
<p>This is a incredibly useful property because the summaries of the posterior we care about in general also rely on the underlying metric. This is basically how the arbitrarily poor mean and variance examples above work; they exploit the lack of use of an underlying metric. That allows Huggins et al.&nbsp;to derive one of the key results of the paper</p>
<table class="table">
<colgroup>
<col style="width: 6%">
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Theorem 3.4.</strong> If <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BW%7D_1(q,%20p)%20%5Cleq%20%5Cvarepsilon"> or <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BW%7D_2(q,%20p)%20%5Cleq%20%5Cvarepsilon">, then <img src="https://latex.codecogs.com/png.latex?%5Cleft%5C%7Cm_%7Bq%7D-m_p%5Cright%5C%7C_2%20%5Cleq%20%5Cvarepsilon"> and <img src="https://latex.codecogs.com/png.latex?%5Cmax%20_i%5Cleft%7C%5Cmathrm%7BMAD%7D_%7Bq,%20i%7D-%5Cmathrm%7BMAD%7D_%7Bp,%20i%7D%5Cright%7C%20%5Cleq%202%20%5Cvarepsilon">.</td>
</tr>
<tr class="even">
<td>If <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BW%7D_2(q,%20p)%20%5Cleq%20%5Cvarepsilon">, then for <img src="https://latex.codecogs.com/png.latex?S%20:=%20%5Csqrt%7Bmin%20(%5Cleft%5C%7C%5CSigma_%7Bq%7D%5Cright%5C%7C_2,%20%5Cleft%5C%7C%5CSigma_p%5Cright%5C%7C_2)%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cmax%20_i%5Cleft%7C%5Csigma_%7Bq,%20i%7D-%5Csigma_%7Bp,%20i%7D%5Cright%7C%20%5Cleq%20%5Cvarepsilon"> and <img src="https://latex.codecogs.com/png.latex?%5Cleft%5C%7C%5CSigma_%7Bq%7D-%5CSigma_p%5Cright%5C%7C_2%3C2%20%5Cvarepsilon(S+%5Cvarepsilon)">.</td>
</tr>
</tbody>
</table>
<p>A similar type of result holds for the difference between expectations of any smooth function, so this result is somewhat extensible with additional work.</p>
<p>This is a nice improvement over the KL or <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> divergences as far as an a diagnostic, since we have some guarantees of correctness where we had literally none. I’ll return to how tight these are bounds in practice in a bit, since that’s entangled with how we can actually estimate them in the variational inference use case.</p>
</section>
<section id="bounds-of-bounds-via-bounds" class="level2">
<h2 class="anchored" data-anchor-id="bounds-of-bounds-via-bounds">Bounds of Bounds via… Bounds!</h2>
<p>One contribution of the Huggins at al.&nbsp;paper is the above result, but where things get even more impressive is that they find a reasonable and practical way to bound these quantities. It’s certainly not simple, but it works.</p>
<p>Here’s the plan to get real bounds on our posterior summaries in full:</p>
<ol type="1">
<li>Use the ELBO and CUBO to to bound the KL and <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> divergences.</li>
<li>Use tail properties of the distribution <img src="https://latex.codecogs.com/png.latex?q"> to get bounds on the Wasserstein distance through the KL and <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> divergences.</li>
<li>Finally, bound posterior summaries using the Wasserstein bounds.</li>
</ol>
<p>That’s a lot of layers of bounding, and it’s reasonable to wonder why this is needed and whether the bounds are usefully tight after such transformations. One key reason this type of bounding is so involved is that we’re using a set of scale-invariant distances to bound a scale-dependent one- we need to incorporate some notion of scale into the bounding process to make it work.</p>
<p>To do this, define the moment constants <img src="https://latex.codecogs.com/png.latex?C_p%5E%7B%5Cmathrm%7BPI%7D%7D(%5Cxi)"> and <img src="https://latex.codecogs.com/png.latex?C_p%5E%7B%5Cmathrm%7BEI%7D%7D(%5Cxi)">. For <img src="https://latex.codecogs.com/png.latex?p%20%5Cgeq%201">, <img src="https://latex.codecogs.com/png.latex?%5Cxi"> is p-polynomially integrable if <img src="https://latex.codecogs.com/png.latex?%0AC_p%5E%7B%5Cmathrm%7BPI%7D%7D(%5Cxi):=2%20%5Cinf%20_%7B%5Ctheta_0%7D%5Cleft%5C%7B%5Cint%5Cleft%5C%7C%5Ctheta-%5Ctheta_0%5Cright%5C%7C_2%5Ep%20%5Cxi(%5Cmathrm%7Bd%7D%20%5Ctheta)%5Cright%5C%7D%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D%3C%5Cinfty%0A"> and that <img src="https://latex.codecogs.com/png.latex?%5Cxi"> is p-exponentially integrable if <img src="https://latex.codecogs.com/png.latex?%0AC_p%5E%7B%5Cmathrm%7BEI%7D%7D(%5Cxi):=2%20%5Cinf%20_%7B%5Ctheta_0,%20%5Cepsilon%3E0%7D%5Cleft%5B%5Cfrac%7B1%7D%7B%5Cepsilon%7D%5Cleft%5C%7B%5Cfrac%7B3%7D%7B2%7D+%5Clog%20%5Cint%20e%5E%7B%5Cepsilon%5Cleft%5C%7C%5Ctheta-%5Ctheta_0%5Cright%5C%7C_2%5Ep%7D%20%5Cxi(%5Cmathrm%7Bd%7D%20%5Ctheta)%5Cright%5C%7D%5Cright%5D%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D%3C%5Cinfty%0A"></p>
<p>Next, with the assumption that the variational approximation <img src="https://latex.codecogs.com/png.latex?q"> has polynomial (respectively, exponential) tails, our next result provides a bound on the <img src="https://latex.codecogs.com/png.latex?p">-Wasserstein distance using the <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2">-divergence (respectively, the KL divergence).</p>
<p>This is saying we require at least polynomial, and ideally exponential moments for <img src="https://latex.codecogs.com/png.latex?q"> and <img src="https://latex.codecogs.com/png.latex?p">, which isn’t that strenuous of a requirement. Then:</p>
<table class="table">
<colgroup>
<col style="width: 6%">
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Proposition 4.2.</strong> If <img src="https://latex.codecogs.com/png.latex?p"> is absolutely continuous w.r.t. to <img src="https://latex.codecogs.com/png.latex?q"> then <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BW%7D_p(q,%20p)%20%5Cleq%20C_%7B2%20p%7D%5E%7B%5Cmathrm%7BPI%7D%7D(q)%5Cleft%5B%5Cexp%20%5Cleft%5C%7B%5Cchi_2(p%20%5Cmid%20q)%5Cright%5C%7D-1%5Cright%5D%5E%7B%5Cfrac%7B1%7D%7B2%20p%7D%7D%0A"> and <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BW%7D_p(q,%20p)%20%5Cleq%20C_p%5E%7B%5Cmathrm%7BEI%7D%7D(q)%5Cleft%5B%5Cmathrm%7BKL%7D(p%20%5Cmid%20q)%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D+%5C%7B%5Cmathrm%7BKL%7D(p%20%5Cmid%20q)%20/%202%5C%7D%5E%7B%5Cfrac%7B1%7D%7B2%20p%7D%7D%5Cright%5D%0A"></td>
</tr>
</tbody>
</table>
<p>A reasonable question here: does using the KL and <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> as part of building the bounds inherit KL/<img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2">’s arbitrarily poor posterior summaries? Nope! I won’t reproduce here, but the counter examples shown above for these divergences on their own no longer work to make our estimates arbitrarily wrong.</p>
<p>Next step: how do we use the ELBO and CUBO to bound the KL and <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> terms in the proposition above above?</p>
<p>We first define for any distribution <img src="https://latex.codecogs.com/png.latex?%5Ceta">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathrm%7BH%7D_%5Calpha(%5Cxi,%20%5Ceta):=%5Cfrac%7B%5Calpha%7D%7B%5Calpha-1%7D%5Cleft%5C%7B%5Coperatorname%7BCUBO%7D_%5Calpha(%5Cxi)-%5Coperatorname%7BELBO%7D(%5Ceta)%5Cright%5C%7D%0A"></p>
<p>Then we get:</p>
<table class="table">
<colgroup>
<col style="width: 6%">
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Lemma 4.5.</strong> For any distribution <img src="https://latex.codecogs.com/png.latex?%5Ceta"> such that <img src="https://latex.codecogs.com/png.latex?p"> is absolutely continuous w.r.t. <img src="https://latex.codecogs.com/png.latex?%5Ceta">- <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathrm%7BKL%7D(p%20%5Cmid%20q)%20%5Cleq%20%5Cchi_%5Calpha(p%20%5Cmid%20q)%20%5Cleq%20%5Cmathrm%7BH%7D_%5Calpha(q,%20%5Ceta)%0A"></td>
</tr>
</tbody>
</table>
<p>By combining the lemma above and proposition from 4.2 earlier, we can bound the Wasserstein distance finally! To do this, we need all of <img src="https://latex.codecogs.com/png.latex?C_p%5E%7B%5Cmathrm%7BPI%7D%7D(%5Cxi)">, <img src="https://latex.codecogs.com/png.latex?C_p%5E%7B%5Cmathrm%7BEI%7D%7D(%5Cxi)">, CUBO, and ELBO. All of these are efficiently calculable much of the time (we’ll get to when it’s not soon), and largely result from things we were already calculating as promised. Great! Our combined result:</p>
<hr>
<p><strong>Theorem 4.6.</strong> For any <img src="https://latex.codecogs.com/png.latex?p%20%5Cgeq%201"> and any distribution <img src="https://latex.codecogs.com/png.latex?%5Ceta">, if <img src="https://latex.codecogs.com/png.latex?p"> is absolutely continuous w.r.t. <img src="https://latex.codecogs.com/png.latex?q">, then <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BW%7D_p(q,%20p)%20%5Cleq%20C_%7B2%20p%7D%5E%7B%5Cmathrm%7BPI%7D%7D(q)%5Cleft%5B%5Cexp%20%5Cleft%5C%7B%5Cmathrm%7BH%7D_2(q,%20%5Ceta)%5Cright%5C%7D-1%5Cright%5D%5E%7B%5Cfrac%7B1%7D%7B2%20p%7D%7D%0A"> and <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BW%7D_p(q,%20p)%20%5Cleq%20C_p%5E%7B%5Cmathrm%7BEI%7D%7D(q)%5Cleft%5B%5Cmathrm%7BH%7D_2(q,%20%5Ceta)%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D+%5Cleft%5C%7B%5Cmathrm%7BH%7D_2(q,%20%5Ceta)%20/%202%5Cright%5C%7D%5E%7B%5Cfrac%7B1%7D%7B2%20p%7D%7D%5Cright%5D%20.%0A"></p>
<hr>
<p>This basically completes the process, other than some discussion of how to actually compute the quantities needed for the bounds above. The ELBO and CUBO we can compute as introduced previously in the series.</p>
<p>They have a good practical suggestion for a workflow given their bounds rely on CUBO, and thus on Monte Carlo estimation<sup>5</sup>. Before proceeding any further with a VI approximation, they suggest using <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D%20%3C%20.7"> as an initial diagnostic of the approximation, before calculating their bounds or leveraging importance sampling or PSIS to improve the estimate. Their point is that if <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is high, the Monte Carlo work involved in generating their bounds will be unreliable at reasonable sample sizes, and thus you won’t be able to gaurantee the bounds are useful.</p>
<p>A final computational point you may be wondering about is how to calculate the moment constants, <img src="https://latex.codecogs.com/png.latex?C_p%5E%7B%5Cmathrm%7BPI%7D%7D(%5Cxi)"> and <img src="https://latex.codecogs.com/png.latex?C_p%5E%7B%5Cmathrm%7BEI%7D%7D(%5Cxi)">. They provide a helpful example showing how to do this when <img src="https://latex.codecogs.com/png.latex?q"> is a T distribution, and so the moments used are analytically calculable. What about when this isn’t possible? They suggest one can reasonably do this by fixing <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> and <img src="https://latex.codecogs.com/png.latex?%5Ctheta_0"> (for example, setting <img src="https://latex.codecogs.com/png.latex?%5Ctheta_0"> at the mean of the distribution), and sampling from <img src="https://latex.codecogs.com/png.latex?q">, which seems reasonable on a bit of reflection. The main reason this is worth bringing up: you don’t need a <img src="https://latex.codecogs.com/png.latex?q"> with easy to calculate moments to make this work, which was a worry when I first saw the moment constants.</p>
<p>So the final, combined workflow<sup>6</sup> they suggest is:</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt6/algo.png" class="img-fluid" style="width:50.0%"></p>
<p>This was a very, very long derivation, but hopefully walking through why we would want a distance with a sense of scale and how to calculate it helped build your intuition around variational inference.</p>
</section>
<section id="so-whats-the-bad-news-about-this-diagnostic" class="level2">
<h2 class="anchored" data-anchor-id="so-whats-the-bad-news-about-this-diagnostic">So what’s the bad news about this diagnostic?</h2>
<p>While these bounds are genuinely useful, let’s talk through some caveats and limitations of this diagnostic tool.</p>
<p>First, the bound really only is trustworthy when we can reliably estimate the CUBO, as I discussed above. Fortunately, as Higgins et al.&nbsp;note, we have an affordable way to check this in <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D">. Of course, we then take on the responsibility of finding a solution where we can trust <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D">, one which doesn’t fall into any of the blind spots the algorithm has that I discussed above. If we want to leverage the bounds, we’re also sort of forced to find a variational family that works well with the CUBO bound, even if an ELBO-based solution might work better. None of this is insurmountable, and much of the time my experience is that you probably need to change your variational family anyway if the <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> for the CUBO optimized approximation is greater than .7.</p>
<p>The second issue here is tightness of the bounds. As you might expect, the whole “bounds via bounds of bounds” thing can result in fairly loose bounding behavior. Wang et al.&nbsp;(2023) show some informative examples where the bounds are anywhere from 10-1000x (!) too conservative. For example, here’s an example of theirs over Neal’s funnel:</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt6/funnel_example.png" class="img-fluid" style="width:60.0%"></p>
<p>The <img src="https://latex.codecogs.com/png.latex?W%5E2"> based bounds (stars) here are 10-100x times too large compared to the true values (dashed lines). In my experience, this example and others from the paper aren’t pathological examples- the bounds are frequently this loose, especially for high dimensional and complex posteriors. Exactly what drives the achieved tightness of the bounds is fairly opaque to me; there are several stages of bounds, and it’s not really been possible to pinpoint the source of problems when the bounds are particularly loose.</p>
<p>However, this isn’t to say the bounds aren’t useful, far from it. In practice, especially on variance or covariance parameters, when things go off the rails, they often <em>really go off the rails</em>. If your variational family is nowhere near up to fitting a model, knowing you’re not within a few OOMs of reasonable values can actually be pretty helpful<sup>7</sup>. Also, these bounds provide some rigorous sense of approximation error where we previously had none, so in that sense this is a big step forward, even if they are loose.</p>
</section>
</section>
<section id="mcmc-based-diagnostics-whats-old-is-new-again" class="level1">
<h1>MCMC based diagnostics; what’s old is new again</h1>
<p>So <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> and Wasserstein bounds are both useful, but don’t tell us nearly as much as we really want to know, or do it as reliably as we’d hope. When the show isn’t going well, play the greatest hits: can we go back to using MCMC?</p>
<p>…Ok yeah, this feels like a bit of moving the goalposts, and in some sense it is. I motivated this whole series by saying I have models I’d like to fit where MCMC was impractically slow. But if you really want your variational approximation to look like what MCMC gives you, and do it over some summary of the posterior that isn’t a top-level mean, variance, or other simple summary, it may be the only responsible thing to do to break out your markov chains again. And things aren’t as bad as they sound here; there are a couple of ways to sanity check your VI with MCMC without committing to days of runtime.</p>
<section id="mcmc-can-be-practically-useful-even-when-its-slow" class="level2">
<h2 class="anchored" data-anchor-id="mcmc-can-be-practically-useful-even-when-its-slow">MCMC can be practically useful even when it’s slow</h2>
<p>Probably the most obvious way to use MCMC to sanity check variational inference is to not sanity check every model, just the occasional one. For example, say I was going to fit something like our MRP model to a running poll, and re-run the inferences weekly or daily. We can pretty reasonably make the leap from assuming if the variational approximation compares favorably to MCMC in the first such fit, the subsequent ones will also fit alright given the underlying data doesn’t fundamentally change in some way.</p>
<p>This is a pretty practical way to get the benefits of VI with the comfort MCMC gives you, as long as MCMC fits in a manageable time window. It’s just important to make sure to set up a realistic process where you spot check the occasional fit along the way, or perhaps retest with MCMC for other questions or shifts in the respondent pool that might plausibly break your model. This is the best strategy I’ve found for validating variational inference so far; even if an MCMC run can take a week or more, as long as it only has to happen once in a while that’s a totally reasonable price of admission.</p>
</section>
<section id="giving-mcmc-an-environment-to-succeed" class="level2">
<h2 class="anchored" data-anchor-id="giving-mcmc-an-environment-to-succeed">Giving MCMC an environment to succeed</h2>
<p>…But what if you’re using variational inference because MCMC will not finish at all? This is totally possible if you have millions of data points, and/or a complex model to estimate. In this case, you may need to give MCMC a good environment to succeed in.</p>
<p>A simple way to do this is to subsample the full data if that’s the choking point. For example, if I have a model that I want to use variational inference to fit on millions of rows, I can reasonably infer most of the time that 100k observations will still tell me at least something useful. By fitting the subsampled data with both MCMC and variational inference, we can make sure that at least at that scale the fits align.</p>
<p>Another, more challenging, but perhaps more efficient way to test when MCMC is unworkably slow on the full data is through data simulation. By simulating data that contains some of the core features I’m hoping my model will understand, and only simulating a moderate amount of it, I can see if VI can capture those features the same way MCMC can. For example, if I think understanding immigration survey question responses is a complex interaction of race, education, and location, I can simulate data which has the patterns I believe exist, and see if VI does meaningfully worse than VI for that covariance structure in the model. This approach or something close to it is something mentioned by David Shor in several of his <a href="https://www.youtube.com/watch?v=maddf8Emzds">talks</a> about Blue Rose Research’s Bayesian models which they scale to hundreds of millions of observations.</p>
</section>
<section id="taddaa" class="level2">
<h2 class="anchored" data-anchor-id="taddaa">TADDAA</h2>
<p>A final newer and more efficient way to gut check VI with MCMC is through <a href="https://arxiv.org/abs/2302.12419">Wang et al.&nbsp;(2023)</a>’s TArgeted Diagnostic for Distribution Approximation Accuracy algorithm, or TADDAA<sup>8</sup>. TADDAA provides a relatively compute efficient way to bound the error of VI via MCMC. They have two main motivations for this paper: first, that many existing VI diagnostics penalize approximations that are bad in any way, not necessarily just the ways you care about most (hence, TArgeted). Realistically, for complex models, it’s not a question of <em>if</em> a variational approximations are worse than MCMC, it’s a question of <em>how</em>. Second, they note that the Wasserstein bounds we discussed earlier are often so loose as to be impractical. Again, true.</p>
<p>These are both sensible points, but both are really properties of MCMC, not their algorithm, so the real juice in the paper is how they bound the error efficiently. Their strategy is to fit a variational inference model, draw values, and then start many short chains of MCMC at those points. If the variational approximation to the target distribution is good, we shouldn’t expect the MCMC running for a while to change much- the points should already be in highly plausible parts of parameter space. If the approximation is less good, then if the MCMC is setup well, the chains should move towards more correct values. Even if the chains don’t reach stationarity, this can be used to provide a lower bound on the amount of approximation error a variational approximation has.</p>
<p>Before I discuss a few technical details, visually the idea is:</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt6/taddaa cartoon.png" class="img-fluid" style="width:80.0%"></p>
<p>If things aren’t too poorly setup for MCMC, we can reasonably assume that the blue distribution (MCMC modified) will be between the red one (original VI posterior) and the black (true posterior). Neat!</p>
<p>In practice, this can work pretty well as you’d expect, and provides much tighter bounds than Wasserstein bounds given it’s a flavor of MCMC. Repeating the plot example I used earlier, notice how much closer the solid lines from the TADDAA bounds are to the dashed ones (ground truth) than the stars (denoting Wasserstein bounds):</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt6/funnel_example.png" class="img-fluid" style="width:60.0%"></p>
<p>I won’t wade too, too far into implementation details here as there a lot of them, but I do want to give enough information to discuss compute cost legibly. A first point worth raising is that “how many chains/iterations do I need” is shown by the paper to be a function of the accuracy you want on the bound- this adaptivity and ability to calculate what number of iterations you need ahead of time is convenient. Second, they’re using a lot of fairly sophisticated techniques in MCMC all together to make this efficient- strong sampling algorithms like MALA/HMC/Barker, preconditioning, and inter-chain adaptation (INCA) to adapt proposal parameters across the chains together. That’s both a great thing (this paper taught me about several new techniques around MCMC I didn’t previously know), and a bad one (to implement this for broader use there is a LOT of work to implement TADDAA efficiently<sup>9</sup>).</p>
<p>In their tests, all of this heavy machinery buys them some fairly impressive speed: implementing TADDAA takes from 2-18% of the gradient evaluations that the actual variational approximation takes. It’s a little opaque to me how that translates into wall time- on the one hand the many little chains are parallizeable, on the other if the number of the iterations is large that’d be the major driver of actual time this takes to run. Still, given it’s never several times VI’s compute needs like more generic MCMC would be in their tests, this seems pretty promising.</p>
<p>This paper is only a few months old, so I should be clear I’ve only had a bit of time to digest and play with the algorithm. If a more robust implementation became available, and the computational efficiencies they suggest are real for complex posteriors too, then this will be a fantastic new tool. They are absolutely right on the point that targeted diagnostics are valuable, and it seems like this is a way forward to getting bounds on the most relevant posterior summaries efficiently. As is, the time to set this up isn’t worth the effort versus letting simpler MCMC comparisons run for longer.</p>
</section>
</section>
<section id="conclusions" class="level1">
<h1>Conclusions</h1>
<p>So let’s take stock of the state of variational inference diagnostics. In terms of fast to calculate solutions, there are several tools that can help us detect quite poor approximations in <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> and Wasserstein bounds, but both have significant limitations. Each help detect some classes of posterior issues, but not others, and it’s a little opaque when we can strongly feel that their answers are reliable, even using both together. These feel worth running, but I’m not sure any of them are load bearing yet.</p>
<p>Using MCMC to validate variational approximations currently feels pretty necessary to me- the other metrics currently available can go wrong in numerous subtle and not-so-subtle ways. Of course, as this post hopefully shows, there are a lot of ways to make that validation price cheaper, from only testing one in several runs in a family of model, to using synthetic data to check results align between MCMC and VI at lower N sizes, to exploring new tools like TADDAA.</p>
<p>This post concludes my theoryposting streak. In the next post we’ll see if all the improvements to variational inference we’ve worked through in the past few posts buy us a better end product. We’ll finally return to trying to fit our MRP model better.</p>
<p>Thanks for reading. The code for this post can be found <a href="https://github.com/andytimm/andytimm.github.io/blob/main/posts/Variational%20MRP%20Pt6/variational_mrp_pt6.qmd">here</a>.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Alternatively, <em>not arbitrarily wrong</em>.↩︎</p></li>
<li id="fn2"><p>No, we’re not winding up to define another bound like the CUBO or ELBO and optimize it like you might think. There’s an interesting little sub-literature on making calculating the Wasserstein distance (really, approximations of it) efficiently enough that we could use it for scalable Bayesian inference tasks like variational inference. I haven’t looked back at this literature in a few years, but <a href="https://arxiv.org/abs/1508.05880">Srivastava et al.&nbsp;(2018)</a> is at least a starting point in this literature if you’re interested.↩︎</p></li>
<li id="fn3"><p>For further intuition, think about how this requirement maps onto our earth moving scenario. Integrating <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%5Cprime"> (marginalizing) gives <img src="https://latex.codecogs.com/png.latex?%5Cxi(%5Ctheta)">- physically, this means that the earth moved from point x needs to be equal to the amount starting there. The opposite marginalization gives <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Ctheta%5E%5Cprime)"> as you’d expect. Hopefully the physical intuition of the constraints that come with this being a valid joint probability distribution make sense here; visually: <img src="https://andytimm.github.io/posts/Variational MRP Pt6/1024px-Transport-plan.png" class="img-fluid">{By Lambdabadger - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=64872543}↩︎</p></li>
<li id="fn4"><p>This is imprecise, but hopefully this conveys the intuition well here: we want a distance measure that takes into account the distance between the points and how they need to be arranged to form one another, not just the values themselves.↩︎</p></li>
<li id="fn5"><p>See post 3 in the series if you want more of an introduction to this point, but the CUBO bound requires Monte Carlo (not MCMC) to calculate, which isn’t a huge computational cost when the approximation is good, but can quickly become unworkable or at least computationally tedious with bad or middling choice of variational families.↩︎</p></li>
<li id="fn6"><p>Bayesians love a good workflow.↩︎</p></li>
<li id="fn7"><p>More specifically, a common pattern is the variance estimates collapse, and collapse quite hard. Let’s say I fit a simpler model to our mrp survey example first; a lot of reasonable (even non-Bayesian) models will probably have a topline margin of errors of a couple percentage points. If the bounds on that was something like .0005% or so, that provides realistically useful news that our approximation is collapsing to weird point estimates like we saw in some poorly fit examples earlier in the series.↩︎</p></li>
<li id="fn8"><p>Fun fact: a key algorithmic detail here is any software implementation of TADDAA is that it must print “taddaa!” upon completing. Without this, users will lack a sense of magic or wonder that the algorithm otherwise instills.↩︎</p></li>
<li id="fn9"><p>They provide an implementation and replication materials here: https://github.com/TARPS-group/TADDAA. But from my early looks at this, this is more what’s needed to make the paper reproducible than a robust suite of tools for using TADDAA fully. It’s worth pointing out this is a major downside, since the other metrics I discuss all have pretty easy to use tools which implement them for general models at this point.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2023,
  author = {Andy Timm},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2023-06-17},
  url = {https://andytimm.github.io/variational_mrp_pt6.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2023" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2023. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> June 17, 2023. <a href="https://andytimm.github.io/variational_mrp_pt6.html">https://andytimm.github.io/variational_mrp_pt6.html</a>.
</div></div></section></div> ]]></description>
  <category>MRP</category>
  <category>Variational Inference</category>
  <category>Diagnostics</category>
  <guid>https://andytimm.github.io/posts/Variational MRP Pt6/variational_mrp_pt6.html</guid>
  <pubDate>Sat, 17 Jun 2023 04:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Variational MRP Pt6/funnel_example.png" medium="image" type="image/png" height="104" width="144"/>
</item>
<item>
  <title>Variational Inference for MRP with Reliable Posterior Distributions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Variational MRP Pt5/variational_mrp_pt5.html</link>
  <description><![CDATA[ 




<p>This is section 5 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.</p>
<p>The general structure for this post and the posts around it will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in these four posts will go a long way towards more robust variational inference. I’ll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.</p>
<p>In the <a href="https://andytimm.github.io/posts/Variational%20MRP%20Pt4/variational_mrp_4.html">last post</a> we saw a variety of different ways importance sampling can be used to improve VI and make it more robust, from defining a tighter bound to optimize in the importance weighted ELBO, to weighting <img src="https://latex.codecogs.com/png.latex?q(x)"> samples together efficiently to look more like <img src="https://latex.codecogs.com/png.latex?p(x)">, to combining entirely different variational approximations together to cover different parts of the posterior with multiple importance sampling.</p>
<p>In this post, we’ll tackle the problem of how to define a deeply flexible variational family <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BQ%7D"> that can adapt to each problem while still being easy to sample from. To do this, we’ll draw on normalizing flows, a technique for defining a composition of invertible transformations on top of a simple base distribution like a normal distribution. We’ll build our way up to using increasingly complex neural networks to define those transformations, allowing for for truly complex variational families that are problem adaptive, training as we train our variational model.</p>
<p>The rough plan for the series is as follows:</p>
<ol type="1">
<li>Introducing the Problem- Why is VI useful, why VI can produce spherical cows</li>
<li>How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li>Problem 1: KL-D prefers exclusive solutions; are there alternatives?</li>
<li>Problem 2: Not all VI samples are of equal utility; can we weight them cleverly?</li>
<li><strong>(This post)</strong> Problem 3: How can we get deeply flexible variational approximations; are Normalizing Flows the answer?</li>
<li>Problem 4: How can we know when VI is wrong? Are there useful error bounds?</li>
<li>Better grounded diagnostics and workflow</li>
</ol>
<section id="a-problem-adaptive-variational-family-with-less-tinkering" class="level1">
<h1>A problem adaptive variational family with less tinkering?</h1>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt5/images/flows_stairs_meme.png" class="img-fluid" alt="Something about NNs makes me meme more"></p>
<p>Jumping from mean-field or full-rank Gaussians and similar distributions to neural networks feels a little… dramatic<sup>1</sup>, so I want to spend some time justifying why this is a good idea.</p>
<p>For VI to work well, we need something that’s still simple to sample from, but capable of, in aggregate, representing a posterior that is probably pretty complex. Certainly, some problems are amenable to the simple variational families <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BQ%7D"> we’ve tried so far, but it’s worth re-emphasizing that we’re probably trying to represent something complex, and even moderate success at that using a composition of normals should be a little surprising, not the expected outcome.</p>
<p>If we need <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BQ%7D"> to be more complex, aren’t there choices between what we’ve seen and a neural network? There’s a whole literature of them- from using mixture distributions as variational distributions to inducing some additional structure into a mean-field type solution if you have some specific knowledge about your target posterior you can use. By and large though, this type of class of solutions has been surpassed by normalizing flows in much of modern use for more complex posteriors.</p>
<p>Why? A first reason is described in the paper that started the normalizing flows for VI literature, Rezende and Mohamed’s <a href="https://arxiv.org/pdf/1505.05770.pdf"><strong>Variational Inference with Normalizing Flows </strong></a>: making our base variational distribution more complex adds a variety of different computational costs, which add up quickly. This isn’t the most face-valid argument when I’m claiming a neural network is a good alternative, but it gets more plausible when you think through how poorly it’d scale to keep making your mixture distribution more and more complex as your posteriors get harder to handle. So this is a <em>scalability</em> argument- it might sound extreme to bring in a neural net, but as problems get bigger, scaling matters.</p>
<p>The other point I’d raise is that all these other tools aren’t very black box at all- if we can make things work with a problem-adapted version of mean-field with some structure based on the knowledge of a specific problem we have, that sounds like it gets time consuming fast. If I’m going to have to find a particular, problem-specific solution each time I want to use variational inference, that feels fragile and fiddly.</p>
<p>The novel idea with normalizing flows is that we’ll start with a simple base density like a normal distribution that is easy to sample from, but instead of only optimizing the parameters of that normal distribution, we’ll also use the training on our ELBO or other objective to learn a transformation that reshapes that normal distribution to look like our posterior. By having that transforming component be partially composed of a neural network, we give ourselves access to an incredibly expressive, problem adaptive, and heavily scalable variant of variational inference that is quite widely used.</p>
<p>And if the approximation isn’t expressive enough? Deep learning researchers have an unfussy, general purpose innovation for that: MORE LAYERS!<sup>2</sup></p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt5/images/more_layers.png" class="img-fluid" alt="Wow such estimator, very deep"></p>
</section>
<section id="what-is-a-normalizing-flow" class="level1">
<h1>What is a normalizing flow?</h1>
<p>A normalizing flow transforms a simple base density into a complex one through a sequence of invertible transformations. By stacking more and more of these invertible transformations (having the density “flow” through them), we can create arbitrarily complex distributions that remain valid probability distributions. Since it isn’t universal in the flows literature, let me be explicit that I’ll consider “forward” to be the direction flowing from the base density to the posterior, and the “backward” or “normalizing” direction as towards the base density.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt5/images/normalizing-flow.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Image Credit to <a href="https://siboehm.com/articles/19/normalizing-flow-network">Simon Boehm</a> here</figcaption><p></p>
</figure>
</div>
<p>If we have a random variable <img src="https://latex.codecogs.com/png.latex?x">, with distribution <img src="https://latex.codecogs.com/png.latex?q(x)">, some function <img src="https://latex.codecogs.com/png.latex?f"> with an inverse <img src="https://latex.codecogs.com/png.latex?f%5E%7B-1%7D%20=%20g,%20g%20%5Ccirc%20f(x)%20=%20x">, then the distribution of the result of one iteration of x through, <img src="https://latex.codecogs.com/png.latex?q%5E%5Cprime(x)"> is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aq%5Cprime(x)%20=%20q(x)%20%5Clvert%20det%20%5Cfrac%7B%5Cpartial%20f%5E%7B-1%7D%7D%7B%5Cpartial%20x%5E%5Cprime%7D%20%5Crvert%20=%20q(x)%20%5Clvert%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20%5Crvert%5E%7B-1%7D%0A"> I won’t derive this identity<sup>3</sup>, but it follows from the chain rule and the properties of Jacobians of invertible functions.</p>
<p>The real power comes in here when we see that these transformations stack. If we’ve got a chain of transformations (e.g.&nbsp;<img src="https://latex.codecogs.com/png.latex?f_K(...(f_2(f_1(x))))">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax_K%20=%20f(x)%20%5Ccirc%20...%20%5Ccirc%20f_2%20%5Ccirc%20f_1(x_0)%0A"></p>
<p>then the resulting density <img src="https://latex.codecogs.com/png.latex?q_K(x)"> looks like:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aln%20q_K%20(x_K)%20=%20lnq_0(x_0)%20-%20%5Csum%20%5Climits_%7BK%20=%201%7D%5Climits%5E%7BK%7D%20ln%20%20%5Clvert%20%5Cfrac%7B%5Cpartial%20f_k%7D%7B%5Cpartial%20x_%7Bk-1%7D%7D%20%5Crvert%5E%7B-1%7D%0A"></p>
<p>Neat, and surprisingly simple! If the terms above are all easy to calculate, we can very efficiently stack a bunch of these transformations and make an expressive model.</p>
<section id="normalizing-flows-for-variational-inference-versus-other-applications" class="level2">
<h2 class="anchored" data-anchor-id="normalizing-flows-for-variational-inference-versus-other-applications">Normalizing flows for variational inference versus other applications</h2>
<p>One source of confusion when I was learning about normalizing flows for variational inference was that variational inference makes up a fairly small proportion of use cases, and thus the academic literature and online discussion. More common applications include density estimation, image generation, representation learning, and reinforcement learning. In addition to making specifically applicable discussions harder to find, often resources will make strong claims about properties of a given flow structure, that really only holding in some subset of the above applications<sup>4</sup>.</p>
<p>By taking a second to explain this crisply and compare different application’s needs, hopefully I can save you some confusion and make engaging with the broader literature easier.</p>
<p>To start, consider the relevant operations we’ve introduced so far:</p>
<ol type="1">
<li>computing <img src="https://latex.codecogs.com/png.latex?f">, that is pushing a sample through the transformations</li>
<li>computing <img src="https://latex.codecogs.com/png.latex?g">, <img src="https://latex.codecogs.com/png.latex?f">’s inverse which undoes the manipulations</li>
<li>computing the (log) determinant of the Jacobian</li>
</ol>
<p>1 and 3 definitely need to be efficient for our use case, since we need to be able to sample and push through using the formula above efficiently to calculate an ELBO and train our model. 2 is where things get more subtle: we definitely need <img src="https://latex.codecogs.com/png.latex?f"> to be invertible, since our formulas above are dependent on a property of Jacobians of invertible functions. But we don’t actually really need to explicitly compute <img src="https://latex.codecogs.com/png.latex?g"> for variational inference. Even knowing the inverse exists but not having a formula might be fine for us!</p>
<p>Contrast this with density estimation, where the goal would not to sample from the distribution, but instead to estimate the density. In this case, most of the time would be spent going in the opposite direction, so that they can evaluate the log-likliehood of the data, and maximize it to improve the model<sup>5</sup>. The need for an expressive transformation of densities unite these two cases, but the goal is quite different!</p>
<p>This level of goal disagreement also shows it face in what direction papers choose to call forward: Most papers outside of variational inference applications consider forward to be the opposite of what I do here. For them, “forward” is the direction towards the base density, the normalizing direction.</p>
<p>For our use, hopefully this short digression has clarified which operations we need to be fast versus just exist. If you dive deeper into further work on normalizing flows, hopefully recognizing there are two different ways to consider forward helps you more quickly orient yourself to how other work describes flows.</p>
</section>
</section>
<section id="how-to-train-your-neural-net" class="level1">
<h1>How to train your neural net</h1>
<p>Now, let’s turn to how we actually fit a normalizing flow. Since this would be a bit hard to grok a code presentation if I took advantage of the full flexibility and abstraction that something like <a href="https://github.com/abhiagwl/vistan/tree/master"><code>vistan</code></a> provides, before heading into general purpose tools I’ll talk through a bit more explicit implementation of a simpler flow called a planar flow in <code>PyTorch</code> for illustration. Rather than reinventing the wheel, I’ll leverage Edvard Hulten’s great implementation <a href="https://github.com/e-hulten/planar-flows">here</a>.</p>
<p>In this section, I’ll define conceptually how we’re fitting the model, and build out a fun target distribution and loss- since I expect many people reading this may moderately new to PyTorch, I’ll explain in detail than normal what each operation is doing and why we need it.</p>
<p>Let’s first make a fun target posterior distribution from an image to model. I think it’d be a fun preview gif for the post to watch this be fit from a normal, so let’s use this ring shaped density.</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt5/images/ring_true_density.png" class="img-fluid" alt="Wow such estimator, very deep"></p>
<p>This is a solid starting example in that this’d be quite hard to fit with a mean-field normal variational family, but it’s pretty easy to define in PyTorch as well:</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># https://github.com/e-hulten/planar-flows/blob/master/target_distribution.py</span></span>
<span id="cb1-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> ring_density(z):</span>
<span id="cb1-3">                exp1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.exp(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> ((z[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-4">                exp2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.exp(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> ((z[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-5">                u <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> ((torch.norm(z, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb1-6">                u <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> u <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> torch.log(exp1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> exp2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-6</span>)</span>
<span id="cb1-7">                <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> u</span></code></pre></div>
</div>
<p>Now let’s define our loss for training, which will just be a slight reformulation of our ELBO:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5Blogp(z,x)%5D%20-%20%5Cmathbb%7BE%7D%5Blogq(z)%5D%0A"></p>
<p>To do this, we’ll define a class for the loss.</p>
<p>First, we pick a simple base distribution to push through our flow, here a 2-D Normal distribution called <code>base_distr</code>. We’ll also include the interesting target we just made above, <code>distr</code>.</p>
<p>Next, the forward pass structure. The <code>forward</code> method is the is the core of the computational graph structure in PyTorch. It defines operations that are applied to the input tensors to compute the output, and gives PyTorch the needed information for automatic differentiation, which allows smooth calculation and backpropagation of loss through the model to train it. This <code>VariationalLoss</code> module will run at the end of the forward pass to calculate the loss and allow us to pass it back through the graph for training.</p>
<p>Keeping with the structure above of numbering successive stages of the flow, <code>z0</code> here is our base distribution, and <code>z</code> will be the learned approximation to the target. In addition to the terms you’d expect in the ELBO, we’re also tracking and making use of the sum of the log determinant of the Jacobians to a handle on the distortion of the base density the flows apply.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># https://github.com/e-hulten/planar-flows/blob/master/loss.py</span></span>
<span id="cb2-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> VariationalLoss(nn.Module):</span>
<span id="cb2-3">  <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,distribution):</span>
<span id="cb2-4">      <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb2-5">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.distr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> distribution</span>
<span id="cb2-6">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.base_distr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> MultivariateNormal(torch.zeros(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), torch.eye(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))</span>
<span id="cb2-7"></span>
<span id="cb2-8">  <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, z0: Tensor, z: Tensor, sum_log_det_J: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>:</span>
<span id="cb2-9">      base_log_prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.base_distr.log_prob(z0)</span>
<span id="cb2-10">      target_density_log_prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.distr(z)</span>
<span id="cb2-11">      <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> (base_log_prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> target_density_log_prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> sum_log_det_J).mean()</span></code></pre></div>
</div>
</section>
<section id="a-basic-flow" class="level1">
<h1>A basic flow</h1>
<p>Next, let’s define the structure of the actual flow. To do this, we’ll first describe a single layer of the flow, then we’ll show structure to stack the flow in layers.</p>
<p>Our first flow we look at will be the <strong>planar flow</strong> from the original Normalizing Flows for Variational Inference paper mentioned above. The name comes from how the function defines a (hyper)plane, and compresses or expand the density around it:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af(x)%20=%20x%20+%20u*tanh(w%5ETx%20+%20b),%20w,%20u%20%5Cin%20%20%20%5Cmathbb%7BR%7D%5Ed,%20b%20%5Cin%20%5Cmathbb%7BR%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?w"> and <img src="https://latex.codecogs.com/png.latex?b"> define the hyperplane and u specifies the direction and strength of the expansion. I’ll show a visualization of just one layer of that below.</p>
<p>If you’re more used to working with neural nets, you might wonder why we choose the non-linearity <img src="https://latex.codecogs.com/png.latex?tanh"> here, which generally isn’t as popular as something like <img src="https://latex.codecogs.com/png.latex?ReLU"> or its variants in more recent years due to it’s more unstable gradient flows. As the authors show in appendix <img src="https://latex.codecogs.com/png.latex?A.1">, functions like the above aren’t actually always invertible, and choosing <img src="https://latex.codecogs.com/png.latex?tanh"> allows them to impose some constraints that make things reliably invertible. See the appendix for more details about how that works, or take a careful look at Edvard’s implementation of the single function below. Realize this sort of constitutes a workaround; ideally we wouldn’t have to force constraints like this for our flow to work.</p>
<p>There isn’t that much that’s new conceptually in this PyTorch code; we’re defining the layer as a stackable module, which provides torch what it needs to calculate both the forward and backward pass of an arbitrary number of layers.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># From https://github.com/e-hulten/planar-flows/blob/master/planar_transform.py</span></span>
<span id="cb3-2"></span>
<span id="cb3-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> PlanarTransform(nn.Module):</span>
<span id="cb3-4">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Implementation of the invertible transformation used in planar flow:</span></span>
<span id="cb3-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">      f(z) = z + u * h(dot(w.T, z) + b)</span></span>
<span id="cb3-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">  See Section 4.1 in https://arxiv.org/pdf/1505.05770.pdf. </span></span>
<span id="cb3-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">  """</span></span>
<span id="cb3-8"></span>
<span id="cb3-9">  <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, dim: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>):</span>
<span id="cb3-10">      <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Initialise weights and bias.</span></span>
<span id="cb3-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">      </span></span>
<span id="cb3-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">      Args:</span></span>
<span id="cb3-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">          dim: Dimensionality of the distribution to be estimated.</span></span>
<span id="cb3-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">      """</span></span>
<span id="cb3-15">      <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb3-16">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.w <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Parameter(torch.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, dim).normal_(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>))</span>
<span id="cb3-17">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Parameter(torch.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).normal_(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>))</span>
<span id="cb3-18">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.u <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Parameter(torch.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, dim).normal_(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>))</span>
<span id="cb3-19"></span>
<span id="cb3-20">  <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, z: Tensor) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tensor:</span>
<span id="cb3-21">      <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> torch.mm(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.u, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.w.T) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:</span>
<span id="cb3-22">          <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.get_u_hat()</span>
<span id="cb3-23"></span>
<span id="cb3-24">      <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.u <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> nn.Tanh()(torch.mm(z, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.w.T) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.b)</span>
<span id="cb3-25"></span>
<span id="cb3-26">  <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> log_det_J(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, z: Tensor) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tensor:</span>
<span id="cb3-27">      <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> torch.mm(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.u, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.w.T) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:</span>
<span id="cb3-28">          <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.get_u_hat()</span>
<span id="cb3-29">      a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.mm(z, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.w.T) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.b</span>
<span id="cb3-30">      psi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> nn.Tanh()(a) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.w</span>
<span id="cb3-31">      abs_det <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> torch.mm(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.u, psi.T)).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>()</span>
<span id="cb3-32">      log_det <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.log(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-4</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> abs_det)</span>
<span id="cb3-33"></span>
<span id="cb3-34">      <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> log_det</span>
<span id="cb3-35"></span>
<span id="cb3-36">  <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> get_u_hat(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb3-37">      <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Enforce w^T u &gt;= -1. When using h(.) = tanh(.), this is a sufficient condition </span></span>
<span id="cb3-38"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">      for invertibility of the transformation f(z). See Appendix A.1.</span></span>
<span id="cb3-39"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">      """</span></span>
<span id="cb3-40">      wtu <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.mm(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.u, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.w.T)</span>
<span id="cb3-41">      m_wtu <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> torch.log(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> torch.exp(wtu))</span>
<span id="cb3-42">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.u.data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb3-43">          <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.u <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (m_wtu <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> wtu) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.w <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> torch.norm(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.w, p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb3-44">      )</span></code></pre></div>
</div>
<p>Where things will start to get exciting is multiple layers of the flow; here’s how we can make an abstraction that allows us to stack up <img src="https://latex.codecogs.com/png.latex?K"> layers of the flow to control the flexibility of our approximation.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> PlanarFlow(nn.Module):</span>
<span id="cb4-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, dim: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, K: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>):</span>
<span id="cb4-3">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Make a planar flow by stacking planar transformations in sequence.</span></span>
<span id="cb4-4"></span>
<span id="cb4-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Args:</span></span>
<span id="cb4-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            dim: Dimensionality of the distribution to be estimated.</span></span>
<span id="cb4-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            K: Number of transformations in the flow. </span></span>
<span id="cb4-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        """</span></span>
<span id="cb4-9">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb4-10">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.layers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [PlanarTransform(dim) <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(K)]</span>
<span id="cb4-11">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Sequential(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.layers)</span>
<span id="cb4-12"></span>
<span id="cb4-13">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, z: Tensor) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tuple[Tensor, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>]:</span>
<span id="cb4-14">        log_det_J <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb4-15"></span>
<span id="cb4-16">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> layer <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.layers:</span>
<span id="cb4-17">            log_det_J <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> layer.log_det_J(z)</span>
<span id="cb4-18">            z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> layer(z)</span>
<span id="cb4-19"></span>
<span id="cb4-20">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> z, log_det_J</span></code></pre></div>
</div>
<p>Let’s run this for a single layer to introduce the training loop, and build some intuition on the planar flow.</p>
<p>The hyperparameter names here should be fairly intuitive, but it’s worth pointing out that the batch size, learning rate (<code>lr</code>), and choice of Adam as an optimizer are all pretty basic reasonable first tries, but something you’d want to consider tuning in a more complicated context- we inherit that level of fiddly-ness when we choose to approach VI using normalizing flows. Also, note that I’m hiding setting up the plot code since it doesn’t add anything to the intuition here.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#From https://github.com/e-hulten/planar-flows/blob/master/train.py</span></span>
<span id="cb5-2">target_distr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ring"</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># U_1, U_2, U_3, U_4, ring</span></span>
<span id="cb5-3">flow_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb5-4">dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb5-5">num_batches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20000</span></span>
<span id="cb5-6">batch_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span></span>
<span id="cb5-7">lr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">6e-4</span></span>
<span id="cb5-8">axlim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xlim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ylim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 5 for U_1 to U_4, 7 for ring</span></span>
<span id="cb5-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ------------------------------------</span></span>
<span id="cb5-10"></span>
<span id="cb5-11">density <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TargetDistribution(target_distr)</span>
<span id="cb5-12">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PlanarFlow(dim, K<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>flow_length)</span>
<span id="cb5-13">bound <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> VariationalLoss(density)</span>
<span id="cb5-14">optimiser <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.optim.Adam(model.parameters(), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>lr)</span>
<span id="cb5-15"></span>
<span id="cb5-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Train model.</span></span>
<span id="cb5-17"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> batch_num <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, num_batches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb5-18">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get batch from N(0,I).</span></span>
<span id="cb5-19">    batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.zeros(size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(batch_size, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)).normal_(mean<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, std<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb5-20">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Pass batch through flow.</span></span>
<span id="cb5-21">    zk, log_jacobians <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(batch)</span>
<span id="cb5-22">    </span>
<span id="cb5-23">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute loss under target distribution.</span></span>
<span id="cb5-24">    loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bound(batch, zk, log_jacobians)</span>
<span id="cb5-25"></span>
<span id="cb5-26">    optimiser.zero_grad()</span>
<span id="cb5-27">    loss.backward()</span>
<span id="cb5-28">    optimiser.step()</span>
<span id="cb5-29"></span>
<span id="cb5-30">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> batch_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb5-31">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"(batch_num </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>batch_num<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:05d}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>num_batches<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">) loss: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>loss<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb5-32">        </span>
<span id="cb5-33"></span>
<span id="cb5-34">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> batch_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">or</span> batch_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb5-35">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Save plots during training. Plots are saved to the 'train_plots' folder.</span></span>
<span id="cb5-36">        plot_training(model, flow_length, batch_num, lr, axlim)</span></code></pre></div>
</div>
<p>At each iteration, we pass the normal draws thorugh the flow, calculate the loss, and propagate that loss backward through the flow to train it using gradient descent.</p>
<p>Here’s a gif<sup>6</sup> of what that looks like over the course of training. With just a single layer of planar flow of course, this isn’t expressive enough to capture the full density, but we can see why this approach has some promise- it’s learning to cover the target density, rather than us having to get creative in specifying a base density that does this.</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt5/images/1_layer.gif" class="img-fluid" alt="Maximally engineered two biomodal"></p>
<p>Let’s try a more serious attempt, with a depth of 32:</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#From https://github.com/e-hulten/planar-flows/blob/master/train.py</span></span>
<span id="cb6-2">target_distr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ring"</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># U_1, U_2, U_3, U_4, ring</span></span>
<span id="cb6-3">flow_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span></span>
<span id="cb6-4">dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb6-5">num_batches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20000</span></span>
<span id="cb6-6">batch_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span></span>
<span id="cb6-7">lr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">6e-4</span></span>
<span id="cb6-8">axlim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xlim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ylim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 5 for U_1 to U_4, 7 for ring</span></span>
<span id="cb6-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ------------------------------------</span></span>
<span id="cb6-10"></span>
<span id="cb6-11">density <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TargetDistribution(target_distr)</span>
<span id="cb6-12">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PlanarFlow(dim, K<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>flow_length)</span>
<span id="cb6-13">bound <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> VariationalLoss(density)</span>
<span id="cb6-14">optimiser <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.optim.Adam(model.parameters(), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>lr)</span>
<span id="cb6-15"></span>
<span id="cb6-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Train model.</span></span>
<span id="cb6-17"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> batch_num <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, num_batches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb6-18">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get batch from N(0,I).</span></span>
<span id="cb6-19">    batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.zeros(size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(batch_size, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)).normal_(mean<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, std<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb6-20">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Pass batch through flow.</span></span>
<span id="cb6-21">    zk, log_jacobians <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(batch)</span>
<span id="cb6-22">    </span>
<span id="cb6-23">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute loss under target distribution.</span></span>
<span id="cb6-24">    loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bound(batch, zk, log_jacobians)</span>
<span id="cb6-25"></span>
<span id="cb6-26">    optimiser.zero_grad()</span>
<span id="cb6-27">    loss.backward()</span>
<span id="cb6-28">    optimiser.step()</span>
<span id="cb6-29"></span>
<span id="cb6-30">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> batch_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb6-31">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"(batch_num </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>batch_num<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:05d}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>num_batches<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">) loss: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>loss<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb6-32">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#print(log_jacobians)</span></span>
<span id="cb6-33"></span>
<span id="cb6-34">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> batch_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">or</span> batch_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb6-35">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Save plots during training. Plots are saved to the 'train_plots' folder.</span></span>
<span id="cb6-36">        plot_training(model, flow_length, batch_num, lr, axlim)</span></code></pre></div>
</div>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt5/images/32_layer.gif" class="img-fluid" alt="An actually useful normalizing flow"></p>
<p>Now we’ve got it! With this planar flow, we’ve transformed our base normal into a pretty complex (for 2-D) distribution, cool!</p>
<p>This took about 20 minutes to train, so this is adding some considerable time to our VI workflow, but on the other hand, we’re not spending the human time needed to figure out what weird base density could be fitted to look like this, which is a win. Also worth pointing out here is that we started with a simple example for illustration purposes, so if we did have something much more complicated or high dimensional to fit, we’d start to see the scalability of normalizing flows start to shine more.</p>
<p>Planar flows are a great learning tool, but in reality they aren’t a great choice once we get outside relatively low-dimensional examples. See <a href="https://arxiv.org/abs/2006.00392">Kong and Chadhuri, (2020)</a> if you want more mathematical rigor, but intuitively, expansion or compression around a hyperplane doesn’t scale to high dimensions well given the operation is pretty simple. We can get around this partially by making the flow deeper, but that introduces its own problems. Very deep planar flows can struggle to fit given the somewhat artificial constraints imposed in computing the log determinant of the Jacobian (implemented in <code>get_u_hat</code> above) to ensure invertability. Finally, there are flows developed since the original normalizing flows paper that both are more expressive and have cheaper to compute transformations and log determinants of the Jacobian- let’s turn to those now.</p>
</section>
<section id="what-more-complicated-flows-look-like" class="level1">
<h1>What more complicated Flows look like</h1>
<p>More complex flows are an active area of research, and I won’t attempt to talk through the whole zoo- I’d recommend either Lilian Weng’s <a href="https://lilianweng.github.io/posts/2018-10-13-flow-models/">blog post</a>, or <a href="https://arxiv.org/abs/1908.09257">Kobyzev et al.&nbsp;(2020)</a> as good starting points for seeing the full range of available flows.</p>
<p>Instead, I’ll introduce just a single more complex flow, RealNVP, introduced in <a href="https://arxiv.org/abs/1605.08803">Dinh et al.&nbsp;(2017)</a>. This is a good example both because the flow is shown to be robustly good for high dimensional variational inference tasks in review papers like <a href="https://arxiv.org/abs/2103.01085">Dhaka et al.&nbsp;(2021)</a> and <a href="https://arxiv.org/abs/2006.10343">Agrawal et al.&nbsp;(2020)</a>, and because it illustrates some generalizable ideas about flow design.</p>
<p>Dinh et al.&nbsp;start the RealNVP paper by noting some goals: they want a Jacobian that is <em>triangular</em>, because this makes computing the determinant incredibly cheap (it’s just the product of the diagonal terms). Second, they want a transformation that’s simple to invert, but complex via inducing interdependencies between different parts of the data.</p>
<p>To do both of these at once, the key insight the authors come to is the idea of a <em>coupling layer</em>, where if the layer is <img src="https://latex.codecogs.com/png.latex?D">-dimensional, the first half of the dimensions <img src="https://latex.codecogs.com/png.latex?1:d"> remain unchanged, and <img src="https://latex.codecogs.com/png.latex?d+1:D"> are transformed as complex function of the first half:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0Ay_%7B1:d%7D%20&amp;=%20x_%7B1:d%7D%5C%5C%0Ay_%7Bd+1:D%7D%20&amp;=%20x_%7B1:d%7D%20%5Codot%20exp(s(x_%7B1:d%7D))%20+%20t(x_%7B1:d%7D)%0A%5Cend%7Balign%7D%0A"></p>
<p>Where s and t are scaling and transformation functions from <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Ed%20%5Crightarrow%20%5Cmathbb%7BR%7D%5E%7BD-d%7D">, and <img src="https://latex.codecogs.com/png.latex?%5Codot"> is the Hadamard (element-wise) product. Visually, at each layer:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt5/images/real_nvp_illustration.png" class="img-fluid figure-img" alt="transforming half of the dimensions as a function of the other half"></p>
<p></p><figcaption class="figure-caption">Figure credit due to <a href="https://blog.evjang.com/2018/01/nf2.html">Eric Jang</a>; he uses the notation <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Cmu"> instead of <img src="https://latex.codecogs.com/png.latex?s"> and <img src="https://latex.codecogs.com/png.latex?t"></figcaption><p></p>
</figure>
</div>
<p>This has a lot of really appealing properties. First, this has a triangular Jacobian:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x%5ET%7D=%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%5Cmathbb%7BI%7D_d%20&amp;%200%20%5C%5C%20%5Cfrac%7B%5Cpartial%20y_%7Bd+1:%20D%7D%7D%7B%5Cpartial%20x_%7B1:%20d%7D%5ET%7D%20&amp;%20%5Coperatorname%7Bdiag%7D%5Cleft(%5Cexp%20%5Cleft%5Bs%5Cleft(x_%7B1:%20d%7D%5Cright)%5Cright%5D%5Cright)%5Cend%7Barray%7D%5Cright%5D%0A"></p>
<p>which means that we can really efficiently compute the determinant as <img src="https://latex.codecogs.com/png.latex?%5Cexp%20%5Cleft%5B%5Csum_j%20s%5Cleft(x_%7B1:%20d%7D%5Cright)_j%5Cright%5D">. For a sense of scale, with no specific structure to exploit, calculating the determinant is roughly <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(n%5E3)"> or a little better<sup>7</sup>, but for triangular matrices the same operation takes just <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(n)">; that’s a massive speedup!</p>
<p>Another nice characteristic here is that we don’t need to compute the Jacobian of <img src="https://latex.codecogs.com/png.latex?s"> or <img src="https://latex.codecogs.com/png.latex?t"> in computing the determinant of the above Jacobian, so <img src="https://latex.codecogs.com/png.latex?s"> and <img src="https://latex.codecogs.com/png.latex?t"> are much easier to make quite complex. Contrast this with the planar flow, where we needed to use a specific (tanh) non-linearity, and impose somewhat arbitrary constraints to ensure invertability at all, let alone easy, fast invertability. With a realNVP flow constructed out of many such coupling layers, it’s easy to throw in a lot of improvements that make training large neural networks much more reliable, like batch normalization, weight normalization, and architectures like residual connections.</p>
<p>As a last appealing property here, realize this can be really expressive: by varying at each layer which dimensions <img src="https://latex.codecogs.com/png.latex?d"> are held constant and which are transformed, we can build up quite complex interrelationships between different dimensions over the flow. This can be done simply at random, or perhaps even using structure of the problem to decide how to partition the dimensions. For example, Dinh et al.&nbsp;provide an example on image data where a checkerboard pattern is used to structure the partitions. Kingma and Dhariwal take this further with <a href="https://arxiv.org/abs/1807.03039">Glow (2018)</a>, a flow using 1x1 convolutions. Again, it’s really nice we don’t need the Jacobian of <img src="https://latex.codecogs.com/png.latex?s"> and <img src="https://latex.codecogs.com/png.latex?t">; they can have arbitrarily complex structure and we don’t need pay the computational cost of computing their Jacobians.</p>
<p>It doesn’t add that much intuition to see another flow in code, so I’ll hold off on showing off the implementation of RealNVP for another post or two when I return to fitting our MRP model better using all the tools we’ve built up.</p>
<p>Like I said at the start of this section, there are tons and tons of possible flow structures that get more computationally complex in exchange for expressiveness. RealNVP is a great start though, and for many variational inference problems provides the amount of expressiveness we need. It also illustrates a lot of the core strategy for building flow structures well:</p>
<ol type="1">
<li>Make the log determinant of the Jacobian fast to calculate.</li>
<li>Impose structure such that calculating the log determinant of the Jacobian isn’t entangled with your source of learnable complexity; this allows expressiveness not fitting restrictions to guide what’s implemented.</li>
<li>Leverage tools for scalable, stable neural networks, from batch norm to architecture choices like residual connections to GPU computing speed ups.</li>
</ol>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Let’s take stock of how normalizing flows continue our project of extending vanilla variational inference. Normalizing flows allow us to learn the variational family rather than iterating through a bunch of base densities by hand until one works, and can do so for much more complex posteriors than any of the simple choices like a mean-field or full-rank gaussian we’ve seen so far. This is both a gain of functionality (we can now fit posteriors with VI that we absolutely couldn’t before), and a gain of convenience (the workflow for “make my neural network expressive” is much, much more convenient than the one where the analyst tries to find or make increasingly weird distributions themselves).</p>
<p>Of course, this adds compute time, and a requirement to start understanding neural network implementation choices reasonably well. This isn’t a free lunch- even the simple planar flow on a toy example above took about 20 minutes to fit on my laptop, and having to understand neural nets well to fit a Bayesian model feels kind of silly. Still though, in the telling of review papers like <a href="https://arxiv.org/abs/2103.01085">Dhaka et al.&nbsp;(2021)</a> and <a href="https://arxiv.org/abs/2006.10343">Agrawal et al.&nbsp;(2020)</a>, a basic RealNVP flow is a serious improvement for many complex posterior distributions at fairly palatable run times. This is a pretty good tradeoff for many realistic models, and it’s for that reason that normalizing flows are an increasingly popular part of the variational inference toolbox.</p>
<p>Like with alternative optimization objectives or the various uses of (Pareto smoothed) importance sampling from the last post, normalizing flows give us tools to fit a wider range of models with variational inference, and do so more robustly and conveniently. This can come with it’s own problems, but these trades are often worth it. In the next post, we’ll add a final set of tools to our VI toolbox: robust diagnostics to know if our approximation is good or not.</p>
<p>Thanks for reading. The code for this post can be found <a href="https://github.com/andytimm/andytimm.github.io/blob/main/posts/Variational%20MRP%20Pt5/variational_mrp_pt5.qmd">here</a>.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>It also almost has a bit of “no brain no pain” ML guy energy, in the sense that we’re really pulling out the biggest algorithm possible. It really is a funny trajectory to me to go from “I’d like to still be Bayesian, but avoid MCMC because it’s slow” to “screw subtle design, let’s throw a NN at it”.↩︎</p></li>
<li id="fn2"><p>This is mostly a joke, but it really is a tremendous convenience that there’s such a straight forward knob to turn for “expressivity” in this context. We’ll get into the ways that isn’t completely true soon, but NNs provide fantastic convenience in terms of workflow for improving model flexibility.↩︎</p></li>
<li id="fn3"><p>You can see it in the original Normalizing Flows paper linked above, or combined with a nice matrix calc review by <a href="https://lilianweng.github.io/posts/2018-10-13-flow-models/">Lilian Weng</a>. As a more general note, since this is a common topic on a few different talented people’s blogs, I’ll try to focus on covering material I think I can provide more intuition for, or that are most relevant for variational inference.↩︎</p></li>
<li id="fn4"><p>A great example of this is Lilian Weng’s <a href="https://lilianweng.github.io/posts/2018-10-13-flow-models/">NF walkthrough</a> which I recommended above- It has a fantastic review of the needed linear algebra and covers a lot of different flow types, but is a bit overly general about what properties are most desirable in a flow, and is therefore initially a bit fuzzy on the value different flows have.↩︎</p></li>
<li id="fn5"><p>Deriving precisely how this works would take us too far afield, but see <a href="https://arxiv.org/abs/1908.09257">Kobyzev et al.&nbsp;(2020)</a> if you’re interested. It’s a great review paper that does a lot of work to recognize there are multiple different possible applications of normalizing flows, and thus different notations and framings that they very successfully bridge.↩︎</p></li>
<li id="fn6"><p>Depending on your browser settings you may need to refresh the page here to watch it run.↩︎</p></li>
<li id="fn7"><p>Ok fine, you probably get that down to <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(n%5E%7B2.8...%7D)"> using <a href="https://en.wikipedia.org/wiki/Strassen_algorithm">Strassen</a> which is implemented essentially everywhere that matters.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2023,
  author = {Andy Timm},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2023-06-11},
  url = {https://andytimm.github.io/variational_mrp_pt5.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2023" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2023. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> June 11, 2023. <a href="https://andytimm.github.io/variational_mrp_pt5.html">https://andytimm.github.io/variational_mrp_pt5.html</a>.
</div></div></section></div> ]]></description>
  <category>MRP</category>
  <category>Variational Inference</category>
  <category>Normalizing Flows</category>
  <guid>https://andytimm.github.io/posts/Variational MRP Pt5/variational_mrp_pt5.html</guid>
  <pubDate>Sun, 11 Jun 2023 04:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Variational MRP Pt5/images/32_layer.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Variational Inference for MRP with Reliable Posterior Distributions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Variational MRP Pt4/variational_mrp_pt4.html</link>
  <description><![CDATA[ 




<p>This is section 4 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.</p>
<p>The general structure for this post and the ones after it will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in these four posts will go a long way towards more robust variational inference. I’ll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.</p>
<p>In the <a href="https://andytimm.github.io/posts/Variational%20MRP%20Pt3/variational_mrp_3.html">last post</a> we took a look at how our ELBO objective requires specific version of KL Divergence (the “Exclusive” formulation of KLD), and saw that it encoded a preference for a certain type of solution to the VI problem. Then we looked at CUBO and CHIVI, an alternative bound and algorithm that avoid this problem, often leading to a more useful posterior distribution by pursuing a more “inclusive” solution.</p>
<p>In this post, we’ll leverage importance sampling to make the most of the samples we do have, emphasizing the parts of our <img src="https://latex.codecogs.com/png.latex?q(x)"> that look like <img src="https://latex.codecogs.com/png.latex?p(x)"> and de-emphasizing the parts that do not.</p>
<p>The rough plan for the series is as follows:</p>
<ol type="1">
<li>Introducing the Problem- Why is VI useful, why VI can produce spherical cows</li>
<li>How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li>Problem 1: KL-D prefers exclusive solutions; are there alternatives?</li>
<li><strong>(This post)</strong> Problem 2: Not all VI samples are of equal utility; can we weight them cleverly?</li>
<li>Problem 3: How can we get deeply flexible variational approximations; are Normalizing Flows the answer?</li>
<li>Problem 4: How can we know when VI is wrong? Are there useful error bounds?</li>
<li>Better grounded diagnostics and workflow</li>
</ol>
<section id="not-all-samples-are-equally-good" class="level1">
<h1>Not all samples are equally good</h1>
<p>So we’ve made an approximation <img src="https://latex.codecogs.com/png.latex?q(x)"> that’s cheap to sample from, and is somewhat close to <img src="https://latex.codecogs.com/png.latex?p(x)">, our true posterior. The way to improve the approximation we’ve focused on so far is to just go back to the start and make <img src="https://latex.codecogs.com/png.latex?q(x)"> better; for example, through changing up the variational family, or to switching to a different optimization objective like the CUBO. That’s one solution that’s often necessary, but can we work with a particular <img src="https://latex.codecogs.com/png.latex?q(x)"> we have and make better use of the parts of it that are the closest to being right?</p>
<p>… Phased this way, this sounds a lot like importance sampling. If you haven’t seen them before, an importance sampling estimator allows us to take draws from a (preferably) easy to sample from distribution<sup>1</sup> and reweight the samples to look more like our true target distribution. The weight <img src="https://latex.codecogs.com/png.latex?w_i"> (or ratio, <img src="https://latex.codecogs.com/png.latex?r_i">) for each sample <img src="https://latex.codecogs.com/png.latex?i"> take form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw_i%20=%20%5Cfrac%7Bp(x_i)%7D%7Bq(x_i)%7D%0A"> Before you get worried that we don’t have <img src="https://latex.codecogs.com/png.latex?p(x_i)"> because of the normalizing constant like every time we talk about having <img src="https://latex.codecogs.com/png.latex?p(x)"> in this series, there’s a clever estimator that “self-normalizes” such that this can be a reasonable strategy. Intuitively, we’re just placing more weight on samples more likely under <img src="https://latex.codecogs.com/png.latex?p(x)">.</p>
<p>This footenote<sup>2</sup> has a selection of some of my favorite resources for learning more or refreshing your memory about importance sampling, but for the main discussion let me pull out some particularly important sub-problems to solve in making a good importance sampling estimator, and good important sampling estimator for VI.</p>
<p>First, our choice of the “proposal” distribution we’re reweighting to be more like <img src="https://latex.codecogs.com/png.latex?p(x)"> matters for making this process practically feasible. We need the proposal distribution to be close enough to <img src="https://latex.codecogs.com/png.latex?p(x)"> that a realistic number of the draws get non-negligible weights. It might be true that we could draw proposals from a big <img src="https://latex.codecogs.com/png.latex?N"> dimensional uniform distribution for every problem, but if we want to be done sampling enough this century we need to at least get fairly close with our initial <img src="https://latex.codecogs.com/png.latex?q(x)">.</p>
<p>A second, but related problem is that it’s quite common for the unmodified importance sampling estimator to have some weights which are orders and orders of magnitude higher than the average weight, blowing up the variance of the estimator. Dan Simpson’s slides I linked above has an instructive example with not too weird <img src="https://latex.codecogs.com/png.latex?p(x)"> and <img src="https://latex.codecogs.com/png.latex?q(x)">’s, but high dimension, that has a max weight ~1.4 million (!) times the average. If that happens, our estimator will essentially ignore most samples without gigantic weights, and it’ll take ages for that estimator to tell us anything remotely reliable.</p>
<p>So with those points we need to address, here are the next topics in this post:</p>
<ol type="1">
<li>Importance Weighted Variational Inference</li>
<li>Robust importance sampling with built in diagnostics via Pareto-Smoothed Importance Sampling</li>
<li>Combining multiple proposal distributions via Multiple Importance Sampling</li>
</ol>
<section id="importance-weighted-variational-inference" class="level2">
<h2 class="anchored" data-anchor-id="importance-weighted-variational-inference">Importance Weighted Variational Inference</h2>
<p>Importance Weighting for VI in it’s simplest form is pretty intuitive (draw samples from an already trained <img src="https://latex.codecogs.com/png.latex?q(x)">, weight them…), so let’s derive the new Importance Weighted Variational Inference (IWVI) estimator first since some nice intuition will come with it.</p>
<p>I want to emphasize something that wasn’t clear to me for a good while- these two ideas are not equivalent. While both are useful tools, the “train time”, objective-modifying IWVI estimator is a distinct approach from the “test time” importance sampling approach that takes draws from a fixed <img src="https://latex.codecogs.com/png.latex?q(x)"> and reweights them as best it can.</p>
<p>We’ll aim to show that we can get a tighter ELBO by using importance sampling. This type of tighter ELBO was first shown by <a href="https://arxiv.org/abs/1509.00519">Burda et Al. (2015)</a> in the context of Variational Autoencoders after which is was fairly clear this could apply to variational inference, but <a href="https://arxiv.org/abs/1808.09034">Domke and Sheldon (2018)</a> fleshed out some details of that extension- I’ll be explaining some of the latter group’s main results first.</p>
<p>To start, imagine a random variable <img src="https://latex.codecogs.com/png.latex?R">, such that <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%7BR%7D%20=%20p(x)">, which we’ll think of as a estimator of <img src="https://latex.codecogs.com/png.latex?p(x)">. Then by Jensen’s Inequality:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Alogp(x)%20=%20%5Cmathbb%7BE%7DlogR%20+%20%5Cmathbb%7BE%7Dlog%5Cfrac%7Bp(x)%7D%7BR%7D%0A"></p>
<p>The first term is the bound, which will be tighter if <img src="https://latex.codecogs.com/png.latex?R"> is highly concentrated.</p>
<p>This is a more general form of the ELBO; we can make it quite familiar looking by having our <img src="https://latex.codecogs.com/png.latex?R"> above be:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AR%20=%20%5Cfrac%7Bp(z,x)%7D%7Bq(z)%7D,%20z%20%5Csim%20q%0A"></p>
<p>The reason for pointing out this fairly simple generalization is helpful is that it frames how to tighten our ELBO on <img src="https://latex.codecogs.com/png.latex?logp(x)"> via alternative estimators <img src="https://latex.codecogs.com/png.latex?R">.</p>
<p>By drawing <img src="https://latex.codecogs.com/png.latex?M"> samples and averaging them as in importance sampling, we get:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AR_M%20=%20%5Cfrac%7B1%7D%7BM%7D%5Csum_%7Bm=1%7D%5E%7BM%7D%5Cfrac%7Bp(z_m,x)%7D%7Bq(z_m)%7D,%20z_m%20%5Csim%20q%0A"> From there, we can derive a tighter bound on <img src="https://latex.codecogs.com/png.latex?logp(x)">, referred to as the IW-ELBO:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AIW-ELBO_M%5Bq(z)%7C%7Cp(z,x)%5D%20:=%20%5Cmathbb%7BE%7D_%7Bq(z_%7B1:M%7D)%7Dlog%5Cfrac%7B1%7D%7BM%7D%20%5Csum_%7Bm=1%7D%5E%7BM%7D%5Cfrac%7Bp(z_m,x)%7D%7Bq(z_m)%7D%0A"> Where we’re using the <img src="https://latex.codecogs.com/png.latex?1:M"> as a shorthand for <img src="https://latex.codecogs.com/png.latex?q(z_%7B1:M%7D)%20=%20q(z_1)...q(z_M)">.</p>
<p>It’s worth noting that the last few lines don’t specify a particular form of importance sampling- we’re getting the tighter theoretical bounding behavior from the averaging of samples from <img src="https://latex.codecogs.com/png.latex?q">. We’ll see a particularly good form of importance sampling with desirable practical properties in a moment.</p>
<section id="how-does-iw-elbo-change-the-vi-problem-conceptually" class="level3">
<h3 class="anchored" data-anchor-id="how-does-iw-elbo-change-the-vi-problem-conceptually">How does IW-ELBO change the VI problem conceptually?</h3>
<p>The tighter bound is nice, but importance sampling also has the side effect (done right, side benefit) of modifying our incentives in choosing a variational family. To see what I mean, we can re-use the example distributions from last post we used to build intuition for KL Divergence, where red was the true distribution, and green were our potential approximations. If we’re not going to draw multiple samples and weight them, it makes sense to choose something like the first plot below. Every draw in the middle of the two target modes is expensive per our ELBO objective, so better to choose a mode.</p>
<div class="cell">

</div>
<div class="cell">

</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">rkl_plot <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> mixture <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-2">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"red"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> mode_seeking_kl), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"green"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggtitle</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Without weighting, we prefer to capture a mode"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>)</span>
<span id="cb1-5"></span>
<span id="cb1-6">fkl_plot <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> mixture <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-7">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"red"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-8">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> mean_seeking_kl), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"green"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggtitle</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"With importance sampling, weights allow us to prefer coverage"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-9">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>)</span>
<span id="cb1-10"></span>
<span id="cb1-11"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">grid.arrange</span>(rkl_plot,fkl_plot)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt4/variational_mrp_pt4_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>If we can use importance sampling though, quite the opposite is be true! Note that we’re still using the ELBO, a reverse-KL based metric- that hasn’t changed. What has changed is our ability to mitigate the objective costs of those samples between the two extremes. Via this “train time” implementation of IS, points outside the two target modes will get lower importance weights, and points within the modes will get higher ones, so as long as we’re covering the modes with some reasonable amount of probability mass, and drawing enough samples we can actually do better with the distribution centered between the modes.</p>
<p>To further drive home the point about how a “train time” and “test time” implementations of IS differ, could “test time” IS do this? Not really- because the ability to better minimize the ELBO via sampling requires the IW-ELBO variant and associated training process. If we hard-coded <img src="https://latex.codecogs.com/png.latex?q(x)"> as the green <img src="https://latex.codecogs.com/png.latex?N(9,4)"> shown above, “test time” IS could weight the right samples up to better approximate <img src="https://latex.codecogs.com/png.latex?p(x)">, but it doesn’t fundamentally alter our optimization problem the way the IWVI objective does.</p>
<p>We can also imagine how varying the number of samples might effect optimization. Between <img src="https://latex.codecogs.com/png.latex?S=1"> and “enough draws to get all the benefits of IS”, we can imagine there’s a slow transition from “just stick with 1 mode” and “go with IS”. So it seems like we should be worried about getting the number of samples right, but fortunately as we’ll see in the next section there are great rules of thumb in some variants of IS. We’ll still need to bear the cost of sampling (which gets higher as <img src="https://latex.codecogs.com/png.latex?q(x)"> becomes “further” from <img src="https://latex.codecogs.com/png.latex?p(x)">, as we’ll need more samples to weight into a good approximation), but the cost of sampling for most VI implementations will often be pretty manageable if our proposal distribution is somewhat close to <img src="https://latex.codecogs.com/png.latex?p(x)">.</p>
<p>Another way to think about how importance sampling changes our task with variational inference is to think about what sorts of distributions make sense to have as our variational family, and even which objective might be better given IS. On choice of a variational family, if we’re aiming for coverage, moving towards thicker-tailed distributions like t distributions makes a lot of sense. While we explored the IW-ELBO above to build intuition, there’s no reason not to apply IW to the CUBO and thus CHIVI- this also naturally produces nicely overdispersed distributions which can be importance sampled closer to the true <img src="https://latex.codecogs.com/png.latex?p(x)">. This idea of aiming for a wide proposal to sample from is referred to in the importance sampling literature (eg <a href="https://artowen.su.domains/mc/">Owen, 2013</a>) as “defensive sampling”, with <a href="https://arxiv.org/abs/1808.09034">Domke and Sheldon (2018)</a> exploring the VI connection more fully. For intuition, by ensuring most of <img src="https://latex.codecogs.com/png.latex?p(x)"> is covered by some reasonable mass makes it easier to efficiently get draws that can be weighted into a final posterior, even if the unweighted posterior might be too wide.</p>
</section>
</section>
<section id="solving-our-is-problems-with-pareto-smoothed-importance-sampling" class="level2">
<h2 class="anchored" data-anchor-id="solving-our-is-problems-with-pareto-smoothed-importance-sampling">Solving our IS problems with Pareto-Smoothed Importance Sampling</h2>
<p>As we’ve been talking about importance sampling, we’ve been leaving some of the messier details aside (how many samples to draw, how to deal with the cases when some of the weights get huge, how to know when our proposal distribution is “close” enough).</p>
<p>While the Importance Sampling Literature is huge and there are a lot of possible solutions here, I’ll next introduce <a href="https://arxiv.org/abs/1507.02646">Vehtari et Al. (2015)</a>’s Pareto-Smoothed Importance Sampling. I’m a huge fan of this paper- it’s a really elegant and powerful tool, derived from taking Bayesian principles seriously.</p>
<p>Above, I described a common failure mode for IS estimators, where some weights are orders of magnitude larger than others, with this long right tail of ratios dominating the weighted average and blowing up the variance of the estimator. Pareto-Smoothed Importance Sampling proposes to model those tail values as coming from a Generalized Pareto Distribution, a distribution for describing extreme values, and replace the most extreme weights with modeled (and more stable) values.</p>
<p>For concreteness, let’s introduce a simple 1-D example. We’ll aim to use importance sampling to approximate distributions <span style="color:red;"><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BT%7D(%5Cmu%20=%200,%5Csigma%20=%201,t%20=5">)</span> and <span style="color:blue;"><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D(x_0=%200,%5Cgamma%20=%2010)"></span> with a <span style="color:green;"><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BN%7D(%5Cmu%20=%200,%5Csigma%20=%201">)</span> distribution. If that sounds like the opposite of preferring wide tails on <img src="https://latex.codecogs.com/png.latex?q(x)">’s I described above, you’re right, but using a poor choice here will illustrate some useful properties.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">simulated_data <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tibble</span>(</span>
<span id="cb2-2"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">q_x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>),</span>
<span id="cb2-3"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">manageable_p_x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rt</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>),</span>
<span id="cb2-4"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">unmanageable_p_x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rcauchy</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>),</span>
<span id="cb2-5"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">manageable_ratios =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dt</span>(q_x,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x),</span>
<span id="cb2-6"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">unmanageable_ratios =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dcauchy</span>(q_x,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x)</span>
<span id="cb2-7">)</span>
<span id="cb2-8"></span>
<span id="cb2-9">simulated_data <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb2-10">            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pivot_longer</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(q_x,manageable_p_x,unmanageable_p_x),</span>
<span id="cb2-11">                         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">values_to =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"draws"</span>,</span>
<span id="cb2-12">                         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">names_to =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"distributions"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb2-13">            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> draws, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> distributions)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-14">            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-15">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># If you wanted to show the full reach of the Cauchy, it'd be</span></span>
<span id="cb2-16">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># hard to see the shape of the T vs N; it's that wide.</span></span>
<span id="cb2-17">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Hence the 6k values removed</span></span>
<span id="cb2-18">            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlim</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-19">            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggtitle</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Visualizing the distributions in question"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-20">            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">legend.position=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"none"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Removed 6245 rows containing non-finite values (stat_density).</code></pre>
</div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt4/variational_mrp_pt4_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The tails on that Cauchy distribution are super, super wide compared to our normal, so the samples far, far out in the tails of the normal will need massive weights to approximate the cauchy. The t-distribution is wider too, so we’ll need some higher weights, but not nearly as many. As a way to visualize this, you can see that just a handful of draws have weights away from ~1, but these weights are as much as 5000x higher than the mean ratio, and will dominate any average we make of them.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">simulated_data <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-2">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">arrange</span>(unmanageable_ratios) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">n =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">seq</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> n,<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> unmanageable_ratios)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-5">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_point</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-6">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggtitle</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"A pretty typical 'unsaveable' set of importance ratios"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt4/variational_mrp_pt4_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The t-distribution ratio plot would look similar, but with a much smaller y-scale. The max weight would still be much larger than the average, but more than an order of magnitude or so less large:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">mean_t <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mean</span>(simulated_data<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>manageable_ratios)</span>
<span id="cb5-2">max_t <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">max</span>(simulated_data<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>manageable_ratios)</span>
<span id="cb5-3"></span>
<span id="cb5-4">mean_c <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mean</span>(simulated_data<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>unmanageable_ratios)</span>
<span id="cb5-5">max_c <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">max</span>(simulated_data<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>unmanageable_ratios)</span>
<span id="cb5-6"></span>
<span id="cb5-7"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">print</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">paste0</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"the mean of the t is: "</span>,mean_t,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" compared to a max of "</span>,max_t,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">";"</span>,</span>
<span id="cb5-8">             <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The cauchy cause is more extreme- the mean of the cauchy is: "</span>,mean_c,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" compared to a max of "</span>,max_c))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "the mean of the t is: 0.995904306317625 compared to a max of 164.448932152079;The cauchy cause is more extreme- the mean of the cauchy is: 0.283655933763642 compared to a max of 1428.22324178729"</code></pre>
</div>
</div>
<p>So let’s bring this back to Pareto smoothing here. We want to model and smooth that long tail of the ratio distribution. It turns out there’s plenty of study of the distribution of extreme events, and there’s some classical limit results showing:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ar(%5Ctheta)%20%7C%20r(%5Ctheta)%20%3E%20%5Ctau%20%5Crightarrow%20GPD(%5Ctau,%5Csigma,k),%20%5Ctau%20%5Crightarrow%20%5Cinfty%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Ctau"> is a lower bound parameter, which in our case defines how many ratios from the tail we’ll actually model. <img src="https://latex.codecogs.com/png.latex?%5Csigma"> is a scale parameter, and <img src="https://latex.codecogs.com/png.latex?k"> is a unconstrained shape parameter. Without getting too far into the weeds, we can implicitly define <img src="https://latex.codecogs.com/png.latex?%5Ctau"> via using a well-supported role of thumb suggesting to use the M largest ratios, <img src="https://latex.codecogs.com/png.latex?M%20=%20min(0.2S,3%5Csqrt%7BS%7D)"><sup>3</sup>. From there, the <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D"> have easy and efficient estimators. The Generalized Pareto Distribution has form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7B%5Csigma%7D%20%5Cleft(1%20+%20k%5Cfrac%7Br%20-%20%5Ctau%7D%7B%5Csigma%7D%20%5Cright)%5E%7B-1/k-1%7D%0A"></p>
<p>and we can replace our M biggest ratios with estimated values calculated via the CDF of the Generalized Pareto Distribution.</p>
<p>One of the best things about PSIS is it comes with a built in diagnostic via <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D">. To see how this works, it’s useful to know that importance sampling depends on how many moments <img src="https://latex.codecogs.com/png.latex?r(%5Ctheta)"> has- for example, if at least two moments exist, the vanilla IS estimator has finite variance (which is obviously required, but no guarantee of performance since it might be finite but massive). The GPD has <img src="https://latex.codecogs.com/png.latex?k%5E%7B-1%7D"> finite fractional moments when <img src="https://latex.codecogs.com/png.latex?k%20%3E%200">.</p>
<p>Vehtari et al.&nbsp;show that the replacement of the largest M ratios above changes PSIS to have finite variance and an error distribution converging to normal when <img src="https://latex.codecogs.com/png.latex?k%20%5Cin%20(.5,1)">. Intuitively, <img src="https://latex.codecogs.com/png.latex?k%20%3E%20.5"> implies the raw ratios have infinite variance, but PSIS trades a little bias to make the variance finite again.</p>
<p>What about actually practical to work with variance? This is the really cool bit- <img src="https://latex.codecogs.com/png.latex?k%20%3C%20.7"> turns out to be a remarkably robust indicator of when we can expect PSIS to work in a ton of different simulation studies and practical examples.</p>
<p>Why is this true? 1.4 fractional moments seems awful arbitrary, right? Let’s ask an alternative question, and kill two birds with one stone: what sample size do we need for PSIS to work? <a href="https://arxiv.org/abs/1511.01437">Chaterjee and Draconis (2018)</a> showed that for a given accuracy, how big <img src="https://latex.codecogs.com/png.latex?S"> needs to be for importance sampling more broadly depends on how close <img src="https://latex.codecogs.com/png.latex?q(x)"> is to <img src="https://latex.codecogs.com/png.latex?p(x)"> in KL distance- we need to satisfy <img src="https://latex.codecogs.com/png.latex?log(S)%20%5Cgeq%20%5Cmathbb%7BE%7D_%7B%5Ctheta%20%5Csim%20q(x)%7D%5Br(%5Ctheta)log(r(%5Ctheta))%5D"> to get accuracy.</p>
<p>Well, we don’t know the distribution of <img src="https://latex.codecogs.com/png.latex?r">, we should have some pretty good intuition that the important part (read: that explosive, variance ruining tail) is Pareto. If we take <img src="https://latex.codecogs.com/png.latex?r"> as exactly Pareto, you can trace out <img src="https://latex.codecogs.com/png.latex?S"> for different <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"><sup>4</sup>, and to give a few example points-</p>
<table class="table">
<caption>for given <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D">, roughly what <img src="https://latex.codecogs.com/png.latex?S"> is needed if <img src="https://latex.codecogs.com/png.latex?r"> is exactly Pareto</caption>
<colgroup>
<col style="width: 75%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"></th>
<th><img src="https://latex.codecogs.com/png.latex?S"> needed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>.5</td>
<td>~1,000</td>
</tr>
<tr class="even">
<td>.7</td>
<td>~140,000</td>
</tr>
<tr class="odd">
<td>.8</td>
<td>1,000,000,000,000</td>
</tr>
<tr class="even">
<td>.9</td>
<td>please stop you’re making your compute sad.</td>
</tr>
</tbody>
</table>
<p>While we of course know <img src="https://latex.codecogs.com/png.latex?r"> isn’t Pareto exactly exactly, hopefully this helps with intuition around <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> telling us when we’re getting into “sampling forever to have any chance at all to control the variance” land.</p>
<p>Neat! So what does that look like for our Cauchy and T distribution example?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1">manageable_psis <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">psis</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(simulated_data<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>manageable_ratios),</span>
<span id="cb7-2">                       <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">r_eff =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span>)</span>
<span id="cb7-3"></span>
<span id="cb7-4">unmanageable_psis <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">psis</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(simulated_data<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>unmanageable_ratios),</span>
<span id="cb7-5">                          <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">r_eff =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span>)</span>
<span id="cb7-6"></span>
<span id="cb7-7">manageable_psis<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>diagnostics</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$pareto_k
[1] 0.5963495

$n_eff
[1] 62963.93</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1">unmanageable_psis<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>diagnostics</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$pareto_k
[1] 0.8533127

$n_eff
[1] 249.2052</code></pre>
</div>
</div>
<p>As we expected, the the Normal proposal distribution isn’t ideal for the T distribution, but it’s manageable. On the other hand, we’d need somewhere between <strong>a trillion and “oh god no :(”</strong> samples to make the normal proposal work out for the Cauchy.</p>
<p>Bringing the discussion back to variational inference, PSIS is super helpful- importance sampling more generally broadens the class of <img src="https://latex.codecogs.com/png.latex?q(x)">es that are close enough to <img src="https://latex.codecogs.com/png.latex?p(x)"> for variational inference to work, and PSIS considerably widens that basin of feasibility. The extensive theoretical and simulation framework around the method also give us a solid way to realize when importance sampling isn’t feasible via the <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> diagnostic, and tells us how roughly samples we need to draw. Super, super cool.</p>
<p>One more great thing PSIS does for variational inference- <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> serves as a powerful diagnostic for variational inference itself! I’ll save most of this discussion for the post on diagnostics, but to sketch out the logic- <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> tells us when <img src="https://latex.codecogs.com/png.latex?q(x)"> is too far from <img src="https://latex.codecogs.com/png.latex?p(x)"> for importance sampling to work, which is a function of KL Divergence from <img src="https://latex.codecogs.com/png.latex?q(x)"> to <img src="https://latex.codecogs.com/png.latex?p(x)">- if that distance is too great for importance sampling to allow us to bridge, that implies we aren’t close enough to trust our base variational approximation either!</p>
</section>
<section id="multiple-proposal-distributions-with-multiple-importance-sampling" class="level2">
<h2 class="anchored" data-anchor-id="multiple-proposal-distributions-with-multiple-importance-sampling">Multiple Proposal Distributions with Multiple Importance Sampling</h2>
<p>Why stop at just one proposal distribution? This is basically the jumping off point for Multiple Importance sampling, or MIS. If we have several different <img src="https://latex.codecogs.com/png.latex?q(x)">, and each does a somewhat better job of handling a certain region of the target posterior, then we can efficiently combine them using MIS into an overall better final estimate, and this will work out to be pretty obviously more optimal than just fitting a bunch of VI approximations and averaging them.</p>
<p>If we can suddenly have multiple different <img src="https://latex.codecogs.com/png.latex?q(x)"> working together, this naturally explodes the search space for a good VI strategy. I’d refer the more interested reader to <a href="https://projecteuclid.org/journals/statistical-science/volume-34/issue-1/Generalized-Multiple-Importance-Sampling/10.1214/18-STS668.full">Elvira et al.&nbsp;(2019)</a> which lays out a framework for thinking about all the decision space of MIS more comprehensively, but for the purposes of improving VI specifically, I’ll cover:</p>
<ol type="1">
<li>How do we weight the proposals together?</li>
<li>Which proposals make sense to include in a MIS framework?</li>
<li>How practical is fitting multiple proposals?</li>
</ol>
<section id="how-do-mis-weights-work" class="level3">
<h3 class="anchored" data-anchor-id="how-do-mis-weights-work">How do MIS weights work?</h3>
<p>How do we generalize a notion of importance weights like the one introduced above:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw_i%20=%20%5Cfrac%7Bp(x_i)%7D%7Bq(x_i)%7D%0A"></p>
<p>to multiple proposals? While there are some obviously not good properties we want to avoid (it’d be pretty silly to give up our unbiasedness), there are a ton of apparent degrees of freedom in MIS weighting. we’ll relax this assumption in a bit, but let’s start by assuming we don’t have any prior information about which proposals might be better, and that we’ll draw the same number of samples from each proposal.</p>
<p>While I won’t work through as extensive of an example as in the last section, let’s fix an example where we’ll have <img src="https://latex.codecogs.com/png.latex?J%20=%203"> different proposals, <span style="color:cyan;"><img src="https://latex.codecogs.com/png.latex?q_1(x)">)</span>, <span style="color:purple;"><img src="https://latex.codecogs.com/png.latex?q_2(x)"></span>, and <span style="color:pink;"><img src="https://latex.codecogs.com/png.latex?q_3(x)">)</span>.</p>
<p>A first question is how to choose the denominator in the weight. One simple and efficient option is to simply use the density of a sample from <img src="https://latex.codecogs.com/png.latex?j"> to make a weight, for example weighting a draw from <span style="color:pink;"><img src="https://latex.codecogs.com/png.latex?q_3(x)">)</span> as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw_%7Bi%7D%20=%20%5Cfrac%7Bp(x_i)%7D%7B%5Ctextcolor%7Bpink%7D%7Bq_3(x_i)%7D%7D%0A"> This works, and is pretty common in MIS applications, but we’re not really using all the information we have from having several proposals. We can get a provably lower variance estimator by defining the mixture of the densities <img src="https://latex.codecogs.com/png.latex?%5Cpsi(x)"> as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpsi(x)%20=%20%5Cfrac%7B1%7D%7BJ%7D%20%5Csum%5Climits_%7BJ%20=%201%7D%5Climits%5E%7BJ%7D%20q_j(x)%0A"></p>
<p>and using that as the denominator. So for the example above, this’d be:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw_%7Bi%7D%20=%20%5Cfrac%7Bp(x_i)%7D%7B%5Cfrac%7B1%7D%7B3%7D(%5Ctextcolor%7Bcyan%7D%7Bq_1(x)%7D%20+%20%5Ctextcolor%7Bpurple%7D%7Bq_2(x_i)%7D%20+%20%5Ctextcolor%7Bpink%7D%7Bq_3(x_i))%7D%7D%0A"> By defining this mixture and and incorporating it into our weighting, we intuitively should have more efficient exchange of information between the different <img src="https://latex.codecogs.com/png.latex?q(x)">. By this, I mean that we no longer just are weighting each sample from a proposal using information from that one proposal; we’re now using everything at hand.</p>
<p>This feels like it should be pretty solidly better than just using a single proposal density, and indeed Elvira et al.&nbsp;have a result showing that the variance of the mixture based weighting scheme is under pretty general conditions lesser than or equal to that of the single proposal density one<sup>5</sup>.</p>
<p><a href="https://dl.acm.org/doi/10.1145/218380.218498">Veach and Guibas (1995)</a>, the paper to introduce MIS, called this weighting scheme the <em>balance heuristic</em>, since the weighting scheme is unique in that each sample value at particular <img src="https://latex.codecogs.com/png.latex?x"> is the same regardless of which distribution produced it. They also prove a bound on the variance of this estimator, showing that there isn’t a lot of room to improve on it, even in the most ideal circumstances. Without getting into the weeds, their result suggests that there isn’t a massively better general-case weighting scheme, which is a helpful guide to practical use.</p>
<p>When can we do (a bit) better than the weighting scheme above? The answer is essentially in cases where we know some of our <img src="https://latex.codecogs.com/png.latex?J"> proposals are much better than others. In these situations, the variance can often be lowered by pushing weights towards the extremes, making low weights closer to zero, and high weights closer to 1. Their <em>cutoff heuristic</em> suggests an estimator where you pick some bound <img src="https://latex.codecogs.com/png.latex?%5Calpha">, below which low weights are reassigned to zero (and the rest of the distribution is adjusted back to sum correctly). Their also propose the <em>power heuristic</em>-</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw_i%20=%20%5Cfrac%7Bp_i%5E%5Cbeta%7D%7B%5Csum%5Climits_%7Bj%7Dp_j%5E%5Cbeta%7D%0A"> which raises the weights to a power <img src="https://latex.codecogs.com/png.latex?%5Cbeta">, and normalizes. For intuition, notice that if <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20=%201">, then this is the <em>balance heuristic</em> again, and as <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20%5Crightarrow%20%5Cinfty">, this moves towards only selecting the best proposal at each point.</p>
<p>As a final note, we can also Pareto Smooth any of these types of weights once we have them, and this sparks joy, as we can add begin to envision model setups with glorious abbreviations like IW-ELBO/IW-CUBO-PSIS-MIS-VI.</p>
<p>So stepping back, we have some provably efficient, provably hard to beat ways to use MIS to combine variational approximations together. Again, there’s a whole literature on MIS which the Elvira paper above reviews, but fairly intuitive weighting schemes exist that work well in most cases, and there are reasonable things to try in more atypical cases to reduce the variance of the MIS estimator as well.</p>
</section>
<section id="what-proposals-combine-best" class="level3">
<h3 class="anchored" data-anchor-id="what-proposals-combine-best">What proposals combine best?</h3>
<p>A next natural question is what different proposals should we use? There’s a little less work in this area than I expected, but there are a couple of papers; my favorite is <a href="https://arxiv.org/abs/2002.07217">Lopez et al.&nbsp;(2020)</a><sup>6</sup>.</p>
<p>They find that using VI approximations based on different objectives is quite performant- for example, having all of a vanilla ELBO, IW-ELBO and <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> divergence based VI approximation works particularly well, and as you’d expect, better than any individual model, just like we’d expect with regular ensembling techniques. They also look at taking some samples directly from our priors, which is moderately surprising to me given how broad weakly-informative priors usually are. Overall though, a core nugget of logic from ensembling more generally applies here too: we want to find proposals that are both good and sufficiently different from one another that combining them adds value.</p>
<p>It seems to me there’s a lot of room to explore this search space still; there are a lot of generic ML ensembling tricks that feel like they could work. For example, could we save state several times throughout optimizing a variational approximation, and MIS combine samples from each of those, similar to how people cheaply ensemble for neural networks? Or are there ways to optimize the proposals for use together in this way?</p>
</section>
<section id="how-practical-is-mis-for-vi" class="level3">
<h3 class="anchored" data-anchor-id="how-practical-is-mis-for-vi">How practical is MIS for VI?</h3>
<p>A last obvious question is whether fitting many variational approximations and combining them is computationally practical. While MIS for VIS certainly trades back some computational cost and time for potential accuracy, the good news is everything feels cheap compared to MCMC.</p>
<p>Fitting <img src="https://latex.codecogs.com/png.latex?J"> VI approximations instead of 1 roughly scales your compute need for fitting the models by a factor of ~<img src="https://latex.codecogs.com/png.latex?J">, and then there’s a small additional cost in the MIS combination stage to evaluate all the models to make each importance sampling weight denominator. Unlike with MCMC, these computational needs are parallelizable.</p>
<p>Lopez et al.&nbsp;(2020) find that using 3 proposals slightly more than triples their compute cost given all the objective based models take around the same time to fit, and in practice slightly more than triples their compute time as well since they didn’t do the work to parallelize their models. On the problems they were working on, this is a pretty small (~30s more) time cost in exchange for a meaningful accuracy improvement in the real world biology application they apply this to.</p>
<p>Depending on what you’re working on, the answer may well be yes, this can be computationally feasible and well worth it.</p>
</section>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>Importance Sampling is a workhorse of modern computational statistics, and it should be no surprise it brings a lot to variational inference.</p>
<p>Like with the last post, my overall impression is of decreasing fragility for variational inference and a broader set of tools for increasing performance. With IW-ELBO and similar objectives, we can get a tighter bound than the vanilla ELBO, and introduce some new incentives in training our approximation as well. With importance sampling in general and PSIS especially, we can weight an approximation that is close to the target but not perfect into a much, much better approximation of our posterior, and do some in a principled and theoretically grounded way with built-in diagnostics. With MIS, we can make the most of several approximations at once, if we’re willing to pay that computational cost. Collectively, we’re building up a set of tools that broaden the class of problems for which VI works, provided you’re willing to spend time searching for a combination of tools that works well for your specific application.</p>
<p>Thanks for reading. In the next post, we’ll look at Normalizing Flows, an incredibly powerful and general tool for making maximally flexible variational distributions. All code for this post can be found <a href="https://github.com/andytimm/andytimm.github.io/blob/main/posts/Variational%20MRP%20Pt4/variational_mrp_pt4.qmd">here</a>.</p>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>we’ll call it q(x) here to make the application super clear, but often I see the “proposal” distribution called g(x) and the the distribution we want to approximate called p(x). Another common notation would be <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)"> for the target and <img src="https://latex.codecogs.com/png.latex?q(x)"> for the proposal. It’s also helpful to know that the computer graphics (as in, image rendering) community is the source of a lot of work especially around Multiple Importance Sampling since they need to solve lots of light transport integrals, and they have yet another set of conventions from most statisticians, but you can usually figure out their choices by squinting a bit.↩︎</p></li>
<li id="fn2"><p>If you’re looking to learn about importance sampling for the first time, a great place to start is Ben Lambert’s video introductions to the basic idea: <a href="https://www.youtube.com/watch?v=V8f8ueBc9sY">video 1</a>, and <a href="https://www.youtube.com/watch?v=F5PdIQxMA28">video 2</a>. For building more intuition about why we need all these variance reducing modifications to general IS, Dan Simpson has some great <a href="https://dpsimpson.github.io/pages/talks/Importance_sampling_unsw_2019.pdf">slides</a> which have a side benefit of being hilarious. Those slides will mention a lot of the books/papers I find most instructive, but it’s worth calling out especially Vehtari et Al’s Pareto Smoothed Importance Sampling <a href="https://arxiv.org/abs/1507.02646">paper</a> as particularly well written and paradigm shaping. Finally, Elvira et Al’s (2019) Multiple Importance Sampling <a href="https://projecteuclid.org/journals/statistical-science/volume-34/issue-1/Generalized-Multiple-Importance-Sampling/10.1214/18-STS668.full">paper</a> is the most thorough I know, but isn’t particularly approachable. Instead, for MIS I’d recommend starting with the first few minutes of <a href="https://www.youtube.com/watch?v=dxFSwplfdpk">this talk</a> (although the main topic of their talk is less relevant, the visualizations are super helpful), and the first ~8 pages of <a href="https://arxiv.org/pdf/2102.05407.pdf">this paper</a>, also by Elvira et Al. (2021) (I especially like that it spends a bit more time on notation; since multiple importance sampling comes from/comes up in computer graphics, the notational choices sometimes feel a bit annoying to me). Finally, the <a href="https://dl.acm.org/doi/10.1145/218380.218498">original MIS paper itself</a>, Veach &amp; Guibas (1995) is quite readable, but requires a bit of reading around or reading into computer graphics to grok their examples and notational choices.↩︎</p></li>
<li id="fn3"><p>Slight more weeds here- it turns out that this idea to use Vehtari et al.’s rule of thumb for selecting M and getting <img src="https://latex.codecogs.com/png.latex?%5Ctau"> from there is fairly important. The GPD Approximation is pretty sensitive to getting <img src="https://latex.codecogs.com/png.latex?%5Ctau"> right- it’ll be poor if <img src="https://latex.codecogs.com/png.latex?%5Ctau"> is too low. Having a deterministic rule of thumb that preforms better than alternative more complicated schemes for estimating <img src="https://latex.codecogs.com/png.latex?%5Ctau"> is great, and they work through showing it works well in most reasonable cases.↩︎</p></li>
<li id="fn4"><p>I’ll be lazy here and not derive or plot this- you can see the plot in Dan Simpson’s slides mentioned above.↩︎</p></li>
<li id="fn5"><p>One fascinating caveat here is that they proved this only for the case where we know the normalizing constant, not the self-normalized case we pretty much always have to live with, but they have some numerical results and some pretty common sense arguments that the result should extend in most reasonable cases to SNIS as well.↩︎</p></li>
<li id="fn6"><p>Worth noting that one of the authors here is Michael l. Jordan, which is a pretty good heuristic for “this will be a banger of a stats paper”.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2023,
  author = {Andy Timm},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2023-05-27},
  url = {https://andytimm.github.io/variational_mrp_pt4.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2023" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2023. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> May 27, 2023. <a href="https://andytimm.github.io/variational_mrp_pt4.html">https://andytimm.github.io/variational_mrp_pt4.html</a>.
</div></div></section></div> ]]></description>
  <category>MRP</category>
  <category>Variational Inference</category>
  <guid>https://andytimm.github.io/posts/Variational MRP Pt4/variational_mrp_pt4.html</guid>
  <pubDate>Sat, 27 May 2023 04:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Variational MRP Pt4/images/importance-weights-preview.png" medium="image" type="image/png" height="72" width="144"/>
</item>
<item>
  <title>Variational Inference for MRP with Reliable Posterior Distributions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Variational MRP Pt3/variational_mrp_3.html</link>
  <description><![CDATA[ 




<p><strong>Note:</strong> I’ve gotten a lot more pessimistic about how generally useful the alternatives to simple KL-Divergence are on their own since writing this post. I still think these are really useful ideas to think to build intuition about VI, and techniques like CHIVI are useful for some lower dimensional problems or as part of an ensemble of techniques for high dimensional ones. However, <a href="https://arxiv.org/abs/2103.01085">this paper</a> from Dhaka et al.&nbsp;is very convincing that CHIVI and currently available similar algorithms are in practice very hard to optimize for high dimensional posteriors, and that some of the intuitive benefits shown about CHIVI below in low dimensions don’t really generalize the way we’d expect to higher dimensions.</p>
<hr>
<p>This is section 3 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.</p>
<p>In the <a href="https://andytimm.github.io/posts/Variational%20MRP%20Pt2/Variational_MRP_pt2.html">last post</a> we threw caution to the wind, and tried out some simple variational inference implementations, to build up some intuition about what bad VI might look like. Just pulling a simple variational inference implementation off the shelf and whacking run perhaps unsurprisingly produced dubious models, so in this post we’ll bring in long overdue theory to understand why VI is so difficult, and what we can do about it.</p>
<p>The general structure for the next couple of posts will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in the next three posts will go a long way towards more robust variational inference. I’ll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.</p>
<p>The rough plan for the series is as follows:</p>
<ol type="1">
<li>Introducing the Problem- Why is VI useful, why VI can produce spherical cows</li>
<li>How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li><strong>(This post)</strong> Problem 1: KL-D prefers exclusive solutions; are there alternatives?</li>
<li>Problem 2: Not all samples are of equal utility; can we weight them cleverly?</li>
<li>Problem 3: How can we know when VI is wrong? Are there useful error bounds?</li>
<li>Better grounded diagnostics and workflow</li>
<li>Seeing if some more sophisticated techniques like normalizing flows add much</li>
</ol>
<section id="inclusive-versus-exclusive-kl-divergence" class="level1">
<h1>Inclusive versus Exclusive KL-divergence</h1>
<p>Like I mentioned in the first post in the series, the Evidence Lower Bound (ELBO), our optimization objective we’re working with is a tractable approximation of the Kullback-Leibler Divergence between our choice of approximating distribution <img src="https://latex.codecogs.com/png.latex?q(z)"> to our true posterior <img src="https://latex.codecogs.com/png.latex?p(z)">.</p>
<p>The KL divergence is asymmetric: in general, <img src="https://latex.codecogs.com/png.latex?KL(p%7C%7Cq)%20%5Cneq%20KL(q%7C%7Cp)">. Previously, we saw that this asymmetry mattered quite a bit for our ELBO idea:</p>
<p><img src="https://latex.codecogs.com/png.latex?argmin_%7Bq(z)%20%5Cin%20%5Cmathscr%7BQ%7D%7D(q(z)%7C%7C%5Cfrac%7Bp(z,x)%7D%7B%5Cbf%20p(x)%7D)%20=%20%5Cmathbb%7BE%7D%5Blogq(z)%5D%20-%20%5Cmathbb%7BE%7D%5Blogp(z,x)%5D%20+%20%7B%5Cbf%20logp(x)%7D"> We can’t calculate the bolded term <img src="https://latex.codecogs.com/png.latex?logp(x)">; if we could we wouldn’t be finding this inference thing so hard in the first place. The way we sidestepped that with the ELBO is to note that the term is constant with respect to <img src="https://latex.codecogs.com/png.latex?q">; so we can go on our merry way minimizing the above without it.</p>
<p>If we flip the divergence around though, we’ve got an issue. That term would then be a <img src="https://latex.codecogs.com/png.latex?logq(x)"> … which we can’t write off in the same way- it varies as we optimize. So if we’re doing this ELBO minimizing version of variational inference, we’re obligated to use this “reverse” KL divergence, the second option below.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AKL(p%7C%7Cq)%20=%20%5Csum_%7Bx%20%5Cin%20X%7D%7Bp(x)%7Dlog%5B%5Cfrac%7Bp(x)%7D%7Bq(x)%7D%5D%20%20%5C%5C%0AKL(q%7C%7Cp)%20=%20%5Csum_%7Bx%20%5Cin%20X%7D%7Bq(x)%7Dlog%5B%5Cfrac%7Bq(x)%7D%7Bp(x)%7D%5D%0A%5Cend%7Balign%7D%0A"></p>
<p>Unfortunately, this choice to optimize the “reverse” KL divergence bakes in preference for a certain type of solution<sup>1</sup>.</p>
<p>I found I built better intuition for this encoded preference after seeing it presented many different overlapping ways, so here are a few of my favorites.</p>
<p>One way to see the difference is through a variety of labels for each direction. One could call Forward KL (1) vs.&nbsp;Reverse KL (2):</p>
<ol type="1">
<li>Inclusive vs.&nbsp;Exclusive (my favorite, and so what I’m using for the section header)</li>
<li>Mean Seeking vs.&nbsp;Mode Seeking</li>
<li>Zero Avoiding vs.&nbsp;Zero Forcing</li>
</ol>
<p>let’s quickly sketch what this might look like in the case of a simple mixture of normals with a single normal as a variational family:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(ggplot2)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'ggplot2' was built under R version 4.2.3</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(gridExtra)</span>
<span id="cb3-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(tidyverse)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'tidyverse' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'tibble' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'tidyr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'readr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'purrr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'dplyr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'stringr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'forcats' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'lubridate' was built under R version 4.2.3</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1">mixture <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">normals =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)),</span>
<span id="cb13-2">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">mode_seeking_kl =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2000</span>,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>),</span>
<span id="cb13-3">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">mean_seeking_kl =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>))</span>
<span id="cb13-4"></span>
<span id="cb13-5">rkl_plot <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> mixture <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb13-6">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"red"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb13-7">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> mode_seeking_kl), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"green"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggtitle</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Exclusive KL"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb13-8">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>)</span>
<span id="cb13-9"></span>
<span id="cb13-10">fkl_plot <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> mixture <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb13-11">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"red"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb13-12">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> mean_seeking_kl), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"green"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggtitle</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Inclusive KL"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb13-13">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>)</span>
<span id="cb13-14"></span>
<span id="cb13-15"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">grid.arrange</span>(rkl_plot,fkl_plot)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt3/variational_mrp_3_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>To approximate the same exact red distribution <img src="https://latex.codecogs.com/png.latex?p(x)">, Inclusive KL (1) and Exclusive KL (2) would optimize the green <img src="https://latex.codecogs.com/png.latex?q(p)"> in quite different manner.</p>
<p>To spell out the ways to describe this above: Inclusive KL will try to cover all the probability mass in <img src="https://latex.codecogs.com/png.latex?p(x)">, even if it means a peak at a unfortunate middle ground. Exclusive KL, on the other hand, will try to concentrate it’s mass on the largest mode, even if it means missing much of the mixture of normals. Alternatively, we could describe the top graph as mode seeking, and the bottom as mean seeking. Finally, we could say the top graph shows “Zero Forcing” behavior- it will heavily favor putting zero mass on some parts of the graph to avoid any weight where <img src="https://latex.codecogs.com/png.latex?p(x)"> has no mass, even if it means missing an entire mode. Conversely, Inclusive KL will aim to cover all the mass of <img src="https://latex.codecogs.com/png.latex?p(x)"> in full even if the result is an odd solution, in order to avoid having zero mass where <img src="https://latex.codecogs.com/png.latex?p(x)"> has some.</p>
<p>How does this follow from the form of the divergence?</p>
<p>To start with, notice that for inclusive KL we could think of the <img src="https://latex.codecogs.com/png.latex?log(%5Cfrac%7Bp(x)%7D%7Bq(x)%7D)"> part of the term being weighted by <img src="https://latex.codecogs.com/png.latex?p(x)">- if in some range of <img src="https://latex.codecogs.com/png.latex?x"> <img src="https://latex.codecogs.com/png.latex?p(x)"> is 0, we don’t pay a penalty if <img src="https://latex.codecogs.com/png.latex?q(x)"> puts mass. The reverse is not true however- if our <img src="https://latex.codecogs.com/png.latex?q(x)"> is zero where there should be mass in our true distribution, our Inclusive KL divergence is infinite<sup>2</sup>.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AKL(p%7C%7Cq)%20=%20%5Csum_%7Bx%20%5Cin%20X%7D%7Bp(x)%7Dlog%5B%5Cfrac%7Bp(x)%7D%7Bq(x)%7D%5D%20%20%5C%5C%0AKL(q%7C%7Cp)%20=%20%5Csum_%7Bx%20%5Cin%20X%7D%7Bq(x)%7Dlog%5B%5Cfrac%7Bq(x)%7D%7Bp(x)%7D%5D%0A%5Cend%7Balign%7D%0A"></p>
<p>And if we change the direction of the divergence, the opposite zeros and infinities show up, enforcing strong preferences for a specific type of solution.</p>
<p>When the example is a simple mix of two gaussians approximated with a single gaussian, it’s fairly easy to intuit how the choice of KL divergence will influence the optimization solution. This all gets a bit more opaque on harder problems- like we saw with the example last post, ELBO based VI will tend to underestimate the support of <img src="https://latex.codecogs.com/png.latex?p(x)"> but whether the solution is narrow but overall reasonable, or pretty much degenerate, is hard to predict. However, this exploration of how the form of the divergence influences the results still gives a rough intuition for why our ELBO optimized posteriors might collapse.</p>
<p>If we want to try the opposite direction of KL divergence, it isn’t immediately obvious there’s a global objective we can choose that favors overdispersed solutions. Like I mentioned above, if we try to make an ELBO-esque target but reverse the KL divergence, the <img src="https://latex.codecogs.com/png.latex?logp(x)"> which is constant with respect to the <img src="https://latex.codecogs.com/png.latex?q(x)"> we’re optimizing becomes a <img src="https://latex.codecogs.com/png.latex?logq(x)"> which we can’t so easily work around.</p>
<p>Let’s look first at a solution in the spirit of VI<sup>3</sup> to the above problem which requires us to pick up a new divergence, the <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E%7B2%7D">-divergence, and optimizes a new bound. Let’s take a look at it.</p>
<section id="chi2-variational-inference-chivi-and-the-cubo-bound" class="level2">
<h2 class="anchored" data-anchor-id="chi2-variational-inference-chivi-and-the-cubo-bound"><img src="https://latex.codecogs.com/png.latex?%5Cchi%5E%7B2%7D"> Variational Inference (CHIVI) and the CUBO bound</h2>
<p>The <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E%7B2%7D">-divergence has form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AD_%7B%5Cchi%5E2%7D(p%7C%7Cq)%20=%20%5Cmathbb%7BE%7D_%7Bq(z;%5Clambda)%7D%5B(%5Cfrac%7Bp(z%7Cx)%7D%7Bq(z;%5Clambda)%7D)%5E2%20-1%5D%0A"> For simplicity and comparability, I’m switching here to using <a href="https://arxiv.org/abs/1611.00328">Dieng et Al. (2017)</a>’s notation here- they use <img src="https://latex.codecogs.com/png.latex?q(z;%5Clambda)"> to refer to the variational family we’re using, indexed by parameters <img src="https://latex.codecogs.com/png.latex?%5Clambda">.</p>
<p>This divergence has the properties we wanted when we tried to use Inclusive KL Divergence- it tends to be mean seeking instead of mode seeking.</p>
<p>Like with the ELBO, we need to show that we have a bound here independent of <img src="https://latex.codecogs.com/png.latex?logp(x)">, and that we have a way to estimate that bound efficiently.</p>
<p>Let’s first move around a few pieces of the first term above:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cmathbb%7BE%7D_%7Bq(z;%5Clambda)%7D%5B(%5Cfrac%7Bp(z%7Cx)%7D%7Bq(z;%5Clambda)%7D)%5E2&amp;%20=%201%20+%20D_%7B%5Cchi%5E2%7D(p(z%7Cx)%7Cq(z;%5Clambda))%20%5C%5C%0A&amp;=%20p(x)%5E2%5B1%20+%20D_%7B%5Cchi%5E2%7D(p(z%7Cx)%7Cq(z;%5Clambda))%5D%0A%5Cend%7Balign%7D%0A"> Then we can take the log of both sides of the equation, which gives us:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7B2%7Dlog(1%20+%20D_%7B%5Cchi%5E2%7D(p(z%7Cx)%7Cq(z;%5Clambda)))%20=%20-logp(x)%20+%20%5Cfrac%7B1%7D%7B2%7Dlog%5Cmathbb%7BE%7D_%7Bq(z;%5Clambda)%7D%5B(%5Cfrac%7Bp(z%7Cx)%7D%7Bq(z;%5Clambda)%7D)%5E2%5D%20%20%0A"> …and this is starting to feel a lot like the ELBO derivation. Log is monotonic, and the <img src="https://latex.codecogs.com/png.latex?-logp(x)"> term is constant as we optimize <img src="https://latex.codecogs.com/png.latex?q">, so we’ve found something that we’re close to able to minimize:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ACUBO_%7B2%7D(%5Clambda)%20=%20%5Cfrac%7B1%7D%7B2%7Dlog%5Cmathbb%7BE%7D_%7Bq(z;%5Clambda)%7D%5B(%5Cfrac%7Bp(z%7Cx)%7D%7Bq(z;%5Clambda)%7D)%5E2%5D%0A"> Since this new divergence is non-negative as well, this is a upper bound of the model evidence. This is thus named <img src="https://latex.codecogs.com/png.latex?%5Cchi"> upper bound (CUBO)<sup>4</sup>.</p>
</section>
<section id="but-can-we-estimate-it" class="level2">
<h2 class="anchored" data-anchor-id="but-can-we-estimate-it">… But can we estimate it?</h2>
<p>One other issue here: how do we estimate this? The CUBO objective got rid of the <img src="https://latex.codecogs.com/png.latex?logp(x)"> we were worried about, but it seems like that expectation is going to be difficult to estimate in general.</p>
<p>Your first idea might be to Monte Carlo (not MCMC) estimate it roughly like this:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ACUBO_2(%5Clambda)%20=%20%5Cfrac%7B1%7D%7B2%7Dlog%5Cfrac%7B1%7D%7BS%7D%5Csum_%7Bs=1%7D%5E%7BS%7D%5B(%5Cfrac%7Bp(x,z%5E%7Bs%7D)%7D%7Bq(z%5E%7Bs%7D;%5Clambda)%7D)%5E2%5D%0A"> Unfortunately, the <img src="https://latex.codecogs.com/png.latex?log"> transform here means our Monte Carlo estimator will be biased: we can see this by applying Jensen’s inequality to the above. To make this stably act as an upper bound, we can apply a clever transformation:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbf%7BL%7D%20=%20exp(n*%20CUBO_2(%5Clambda))%0A"></p>
<p>Since exp is monotonic, this has the same objective as the CUBO, but we can Monte Carlo estimate it unbiasedly. Is that the last problem to solve?</p>
</section>
<section id="but-can-we-calculate-gradients-efficiently" class="level2">
<h2 class="anchored" data-anchor-id="but-can-we-calculate-gradients-efficiently">… But can we calculate gradients efficiently?</h2>
<p>Wait, wait no. Sorry to keep saying there’s one more step here, but there’s a lot that goes into making a full, convenient, general use algorithm here. The last step (for real this time) is that we need to figure out how to get gradients for the estimate of <img src="https://latex.codecogs.com/png.latex?%5Cbf%7BL%7D"> above, <img src="https://latex.codecogs.com/png.latex?%5Cbf%7B%5Chat%7BL%7D%7D">. The issue is that we don’t have any guarantee that a unbiased Monte Carlo estimator of <img src="https://latex.codecogs.com/png.latex?%5Cbf%7B%5Chat%7BL%7D%7D"> gets us a Monte Carlo way to estimate <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%5Clambda%5Cbf%7B%5Chat%7BL%7D%7D">- we can’t guarantee that the gradient of the expectation is equal to the expectation of the gradient.</p>
<p>For this, we need to pull out a trick from the variational autoencoder literature. This is usually referred to as the “Reparameterization Trick”<sup>5</sup>, but the original CHIVI paper refers to them as “reparmeterization gradients”. We will assume we can rewrite the generative process of our model as <img src="https://latex.codecogs.com/png.latex?z%20=%20g(%5Clambda,%5Cepsilon)">, where <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%20%5Csim%20p(%5Cepsilon)"> and g being a deterministic function. Then we have a new estimator for both <img src="https://latex.codecogs.com/png.latex?%5Cbf%7B%5Chat%7BL%7D%7D"> and it’s gradient:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cbf%7B%5Chat%7BL%7D%7D%20&amp;=%20%5Cfrac%7B1%7D%7BB%7D%5Csum_%7Bb=1%7D%5EB(%5Cfrac%7Bp(x,g(%5Clambda,%5Cepsilon%5E%7B(b)%7D))%7D%7Bq(g(%5Clambda,%5Cepsilon%5E%7B(b)%7D;%5Clambda))%7D)%5E%7B2%7D%20%5C%5C%0A%5Cnabla_%5Clambda%5Cbf%7B%5Chat%7BL%7D%7D%20%20&amp;=%20%5Cfrac%7B2%7D%7BB%7D%5Csum_%7Bb=1%7D%5EB(%5Cfrac%7Bp(x,g(%5Clambda,%5Cepsilon%5E%7B(b)%7D))%7D%7Bq(g(%5Clambda,%5Cepsilon%5E%7B(b)%7D;%5Clambda))%7D)%5E2%20%5Cnabla_%5Clambda%20log(%5Cfrac%7Bp(x,g(%5Clambda,%5Cepsilon%5E%7B(b)%7D))%7D%7Bq(g(%5Clambda,%5Cepsilon%5E%7B(b)%7D;%5Clambda))%7D)%0A%5Cend%7Balign%7D%0A"> There are one or two more neat computational tricks in the paper I won’t explain here (essentially: how do we extend this to work in minibatch fashion, and how do we avoid numerical underflow issues), but this is now essentially functional. The whole algorithm, which they dubbed CHIVI is below:</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt3/images/CHIVI_algorithm.png" class="img-fluid"></p>
</section>
</section>
<section id="the-bigger-picture-again" class="level1">
<h1>The Bigger Picture Again</h1>
<p>Stepping back, let’s talk about some practical properties of the algorithm we’ve been stepping through.</p>
<p>First, and probably most exciting given what we saw in the last post, CHIVI’s objective has the property that it is inclusive, unlike the ELBO we were using earlier. This won’t be the right choice for all variational inference problems, but given our prior issues with very narrow posteriors this will be exciting to test<sup>6</sup>. And as we’ll see in the next section, this overdispersion tendency in the posterior will often have a beneficial interaction with importance sampling which can improve our estimates further.</p>
<p>Another nice thing here is that if we were to estimate both the ELBO and CUBO for a given problem, we’d get both a upper and lower bound on the model evidence. This is theoretically convenient in that we now have a sandwich estimator, which actually obtains reasonably tight bounds. We’ll even be able use the fact we have both later to get some bounds on practical bounds on quantities we tend to report in practice like means and covariances!</p>
<p>A final neat benefit here is that to the extent we are willing to consider ensembling models (again, more on that soon), this CHIVI framework will produce estimates that succeed (and fail) in less similar ways that the the ELBO based estimators we looked at last post. Expanding our available set of tools is always good, but it’s even better when we’re ensembling because we can lean more heavily on each model for the tasks it succeeds on.</p>
<p>One potential downside here is that we introduced a solution that partially relies on a Monte Carlo estimator. That said, this is pretty cheap in practice; if we’re using VI as a drop in for MCMC, this is still going to be much much faster than MCMC for any big problem. We’ll need to think about a reasonable number of samples in a given case, but realistically this isn’t going to be a driving factor in determining compute time.</p>
<p>Another final problem is that the estimator we built out for the CUBO that we could actually estimate tends to end up having pretty high variance. Exponentiating the objective isn’t free in that sense; but this problem of variance reduction in estimators is something that feels like a tractable problem to iterate on.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>In truth, both KL divergences encode structural preferences for the type of optimization solution they admit- neither will be the right choice for every problem and variational family combination. But as we’ll see, being able to choose will give us more options to fit models we believe.↩︎</p></li>
<li id="fn2"><p>This is the footnote for those of you that are annoyed because you tried to write out how this would happen, and got something like <img src="https://latex.codecogs.com/png.latex?p(x)%20log%20%5Cfrac%7Bp(x)%7D%7B0%7D">, which should be undefined if we’re following normal math rules. But this is information theory, and in this strange land we say <img src="https://latex.codecogs.com/png.latex?p(x)%20log%20%5Cfrac%7Bp(x)%7D%7B0%7D%20=%20%5Cinfty">. I don’t have a strong intuition for why this is the best solution, but a information encoding perspective makes it make more sense at least: if we know the distribution of <img src="https://latex.codecogs.com/png.latex?p">, we can construct a code for it with average description length <img src="https://latex.codecogs.com/png.latex?H(x)">. One way to understand the KL divergence is as what happens when we try to use the code for a distribution <img src="https://latex.codecogs.com/png.latex?q"> to describe <img src="https://latex.codecogs.com/png.latex?p">, we’d need <img src="https://latex.codecogs.com/png.latex?H(p)%20+%20KL(p%7C%7Cq)"> bits on average to describe <img src="https://latex.codecogs.com/png.latex?p">. In the code for <img src="https://latex.codecogs.com/png.latex?q"> has no way to represent some element of <img src="https://latex.codecogs.com/png.latex?p">, then requiring… infinite bits feels like the right way to describe the breakdown of meaning? All this to say this condition is something our optimizer will try hard to avoid.↩︎</p></li>
<li id="fn3"><p>I’ll mention an alternative approach, Expectation Propagation, that takes a different (not global objective based) approach further down.↩︎</p></li>
<li id="fn4"><p>This approach actually defines a family of <img src="https://latex.codecogs.com/png.latex?n"> new divergences, where you replace the <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D"> with <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7Bn%7D"> and similarly replace the square an exponent with n.&nbsp;Fully stepping through why this is neat wasn’t worth how far afield it’d take us, but the original <img src="https://latex.codecogs.com/png.latex?CHIVI"> paper has some cool derivations based on this, one of which I’ll discuss on the next section on importance sampling.↩︎</p></li>
<li id="fn5"><p>I thought about a section to explain the reparameterization trick, but there are enough good explanations online of the trick. If you’re interested in learning more about why this is important for optimization through stocastic models, I’d recommend starting with Gregory Gundersen’s explanation <a href="https://gregorygundersen.com/blog/2018/04/29/reparameterization/">here</a> and then move on to the original Kingma &amp; Welling, 2013 paper. As general advice on understanding it better though, I’ll echo Greg’s point that some of the online explanations I’ve seen are a bit loose- the key is that we want to express a gradient of an expectation (can’t MC estimate for sure) as an expectation of a gradient (which we can MC estimate provided our convenient deterministic function <img src="https://latex.codecogs.com/png.latex?g"> is differentiable).↩︎</p></li>
<li id="fn6"><p>In a few posts, we’re theoryposting for a bit.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div id="quarto-reuse" class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2023,
  author = {Timm, Andy},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2023-05-02},
  url = {https://andytimm.github.io/posts/Variational MRP Pt3/variational_mrp_3.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2023" class="csl-entry quarto-appendix-citeas">
Timm, Andy. 2023. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> May 2, 2023. <a href="https://andytimm.github.io/posts/Variational MRP Pt3/variational_mrp_3.html">https://andytimm.github.io/posts/Variational
MRP Pt3/variational_mrp_3.html</a>.
</div></div></section></div> ]]></description>
  <category>MRP</category>
  <category>Variational Inference</category>
  <guid>https://andytimm.github.io/posts/Variational MRP Pt3/variational_mrp_3.html</guid>
  <pubDate>Tue, 02 May 2023 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Variational Inference for MRP with Reliable Posterior Distributions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Variational MRP Pt2/Variational_MRP_pt2.html</link>
  <description><![CDATA[ 




<p>This is the second post in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.</p>
<p>In the last post, I laid out why such reformulating the Bayesian inference problem as optimization might be desirable, but previewed why this might be quite hard to find high quality approximations amenable to optimization. I then introduced our running example (predicting national/sub-national opinion on an abortion question from the CCES using MRP), and gave an initial introduction to a version of Variational Inference where we maximize the Evidence Lower Bound (ELBO) as an objective, and do so using a mean-field Gaussian approximation. We saw that with 60k examples, this took about 8 hours to fit with MCMC, but 144 seconds (!) with VI.</p>
<p>In this post, we’ll explore the shortcomings of this initial approximation, and take a first pass at trying to better with a more complex (full rank) variational approximation. The goal is to get a better feel for what failing models could look like, at least in this relatively simple case.</p>
<p>The rough plan for the series is as follows:</p>
<ol type="1">
<li><a href="https://andytimm.github.io/posts/Variational%20MRP%20Pt1/variational_mrp_pt1.html">Introducing the Problem- Why is VI useful, why VI can produce spherical cows</a></li>
<li><strong>(This post)</strong> How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li>Some theory on why posterior approximation with VI can be so poor</li>
<li>Seeing if some more sophisticated techniques like normalizing flows help</li>
</ol>
<div class="cell">

</div>
<section id="the-disclaimer" class="level1">
<h1>The disclaimer</h1>
<p>One sort of obvious objections to how I’ve set up this series is “Why not talk about theory on why VI approximations can be poor before trying stuff?”. While in practice I did read a lot of the papers for the next post before writing this one, I think there’s a lot of value is looking at failed solutions to a problem to build up intuition about what our failure mode looks like, and what it might require to get it right.</p>
</section>
<section id="toplines" class="level1">
<h1>Toplines</h1>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">meanfield_60k <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">readRDS</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"fit_60k_meanfield.rds"</span>)</span>
<span id="cb1-2">mcmc_60k <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">readRDS</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"fit_60k_mcmc.rds"</span>)</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Meanfield </span></span>
<span id="cb1-5">epred_mat_mf <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">posterior_epred</span>(meanfield_60k, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">newdata =</span> poststrat_df_60k, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">draws =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb1-6">mrp_estimates_vector_mf <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> epred_mat_mf <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> poststrat_df_60k<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>n <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> </span>
<span id="cb1-7">                                              <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(poststrat_df_60k<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>n)</span>
<span id="cb1-8">mrp_estimate_mf <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">mean =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mean</span>(mrp_estimates_vector_mf),</span>
<span id="cb1-9">                     <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">sd =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sd</span>(mrp_estimates_vector_mf))</span>
<span id="cb1-10"></span>
<span id="cb1-11"></span>
<span id="cb1-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># MCMC </span></span>
<span id="cb1-13">epred_mat_mcmc <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">posterior_epred</span>(mcmc_60k, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">newdata =</span> poststrat_df_60k, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">draws =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb1-14">mrp_estimates_vector_mcmc <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> epred_mat_mcmc <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> poststrat_df_60k<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>n <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span></span>
<span id="cb1-15">                                                  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(poststrat_df_60k<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>n)</span>
<span id="cb1-16">mrp_estimate_mcmc <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">mean =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mean</span>(mrp_estimates_vector_mcmc),</span>
<span id="cb1-17">                       <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">sd =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sd</span>(mrp_estimates_vector_mcmc))</span>
<span id="cb1-18"></span>
<span id="cb1-19"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">cat</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Meanfield MRP estimate mean, sd: "</span>, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">round</span>(mrp_estimate_mf, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span>
<span id="cb1-20"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">cat</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MCMC MRP estimate mean, sd: "</span>, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">round</span>(mrp_estimate_mcmc, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span></code></pre></div>
</div>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>Mean</th>
<th>SD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MCMC</td>
<td>43.9%</td>
<td>.2%</td>
</tr>
<tr class="even">
<td>mean-field VI</td>
<td>43.7%</td>
<td>.2%</td>
</tr>
</tbody>
</table>
<p>Starting with basics, the toplines are pretty much identical, which is a good start. The minor difference here could easily reverse on a different seed- from a few quick re-runs these often end up having matching means to 3 decimals.</p>
</section>
<section id="state-level-estimates" class="level1">
<h1>State Level Estimates</h1>
<p>What happens if we produce state level estimates, similar to the plot last post comparing MRP to a simple weighted estimate? Note that I’ll steer away from the MRP Case Study example here in a few ways. I’ll use <code>tidybayes</code> for working with the draws (more elegant than their loop based approach), and I’ll use more draws (helps with simulation error in smaller states).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">mcmc_state_level <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> poststrat_df_60k <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">add_epred_draws</span>(mcmc_60k, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ndraws =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb2-2">mfvi_state_level <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> poststrat_df_60k <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">add_epred_draws</span>(meanfield_60k, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ndraws =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb2-3"></span>
<span id="cb2-4">mcmc_state_level <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">glimpse</span>()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 12,000,000
Columns: 13
Groups: state, eth, male, age, educ, n, repvote, region, .row [12,000]
$ state      &lt;chr&gt; "AL", "AL", "AL", "AL", "AL", "AL", "AL", "AL", "AL", "AL",…
$ eth        &lt;chr&gt; "White", "White", "White", "White", "White", "White", "Whit…
$ male       &lt;dbl&gt; -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,…
$ age        &lt;chr&gt; "18-29", "18-29", "18-29", "18-29", "18-29", "18-29", "18-2…
$ educ       &lt;chr&gt; "No HS", "No HS", "No HS", "No HS", "No HS", "No HS", "No H…
$ n          &lt;dbl&gt; 23948, 23948, 23948, 23948, 23948, 23948, 23948, 23948, 239…
$ repvote    &lt;dbl&gt; 0.6437414, 0.6437414, 0.6437414, 0.6437414, 0.6437414, 0.64…
$ region     &lt;chr&gt; "South", "South", "South", "South", "South", "South", "Sout…
$ .row       &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
$ .chain     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
$ .iteration &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
$ .draw      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …
$ .epred     &lt;dbl&gt; 0.5771322, 0.5189677, 0.5483006, 0.5421404, 0.5417602, 0.55…</code></pre>
</div>
</div>
<p>If you haven’t worked with <code>tidybayes</code> before, the glimpse above should help give some intuition about the new shape of the data- we’ve take the 12,000 row <code>poststrat_df_60k</code>, and added a row per observation per draw, with the prediction (.epred) and related metadata. This gives 12,000 x 1,000 = 12 million rows. This really isn’t the most space efficient storage, but it allows for very elegant <code>dplyr</code> style manipulation of results and quick exploration.</p>
<p>Let’s now plot and compare the 50 and 95% credible intervals by state between the two models.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">mcmc_state_summary <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> mcmc_state_level <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb4-2">                        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># multiply each draw by it's cell's proportion of state N</span></span>
<span id="cb4-3">                        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># this is the P in MRP</span></span>
<span id="cb4-4">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(state,.draw) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-5">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">postrat_draw =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(.epred<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(n<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(n)))) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-6">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(state) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-7">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">median_qi</span>(postrat_draw, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">.width =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">95</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-8">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MCMC"</span>)</span>
<span id="cb4-9"></span>
<span id="cb4-10">mfvi_state_summary <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> mfvi_state_level <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb4-11">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(state,.draw) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-12">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">postrat_draw =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(.epred<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(n<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(n)))) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-13">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(state) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-14">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">median_qi</span>(postrat_draw, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">.width =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">95</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-15">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MF-VI"</span>)</span>
<span id="cb4-16"></span>
<span id="cb4-17">combined_summary <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bind_rows</span>(mcmc_state_summary,mfvi_state_summary)</span>
<span id="cb4-18"></span>
<span id="cb4-19">combined_summary <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-20">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ordered_state =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">fct_reorder</span>(combined_summary<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>state,</span>
<span id="cb4-21">                                     combined_summary<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>postrat_draw)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-22">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> ordered_state,</span>
<span id="cb4-23">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> postrat_draw,</span>
<span id="cb4-24">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">xmin =</span> .lower,</span>
<span id="cb4-25">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">xmax =</span> .upper,</span>
<span id="cb4-26">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> model)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-27">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_pointinterval</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">position =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">position_dodge</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-28">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlim</span>(.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">75</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-29">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">legend.position=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"top"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-30">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Should employers be allowed to deny their employees abortion care?"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-31">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ylab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"State"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt2/Variational_MRP_pt2_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>… That looks concerning.</p>
<p>What might you get wrong if you used the VI approximation for inference here? If you only cared about the median estimate primarily, you might be ok with this effort. If you care about uncertainty though, here’s a non-exhaustive list of concerns here:</p>
<ol type="1">
<li>Probably unimodal, smooth posterior distributions from MCMC have gone off-course to the point where the Median/50/95% presentation no longer seems up to expressing the posterior shape (more on this in a second).</li>
<li>The MF-VI posteriors are often narrower in 50% or 95% CI- we’d on average underestimate various types of uncertainty here.</li>
<li>Worse<sup>1</sup>, the MF-VI posterior’s CIs aren’t <strong>consistently</strong> narrower, either in the sense they are always narrower, or that they tend to consistently distort the same way. Sometimes both the 50% and 95% are just a small amount narrower than MCMC- the Michigan posterior attempt looks passable. Sometimes things are worse, with 50% MFVI CIs almost as wide as the MCMC 95% interval- Wyoming shows such a distortion. Sometimes the probability mass between 50% and 95% is confined to such a minuscule range it looks like I forgot to plot it.</li>
</ol>
<p>That last point is particularly important because it suggests there’s no easy rule of thumb for mechanically correcting these intervals, or deciding which could be plausible approximations without the MCMC plot alongside to guide that process. We can’t use VI to save a ton of time, infer the intervals consistently need to x% be wider, and call it a day- we need to reckon more precisely with why they’re distorted.</p>
<p>Let’s return now to the point about how the shape has gone wrong. Below is a dot plot (<a href="https://dl.acm.org/doi/10.1145/2858036.2858558">Kay et al., 2016</a>)- each point here represents about 1% of the probability mass. I enjoy this approach to posterior visualization when things are getting weird, as this clarifies a lot about the full shape of the posterior distribution, making fewer smoothing assumptions like a density or eye plot might.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">mcmc_state_points <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> mcmc_state_level <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb5-2">                        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># multiply each draw by it's cell's proportion of state N</span></span>
<span id="cb5-3">                        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># this is the P in MRP</span></span>
<span id="cb5-4">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(state,.draw) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb5-5">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarize</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">postrat_draw =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(.epred<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(n<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(n)))) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb5-6">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MCMC"</span>)</span>
<span id="cb5-7">mfvi_state_points <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> mfvi_state_level <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb5-8">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(state,.draw) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb5-9">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarize</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">postrat_draw =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(.epred<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(n<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(n)))) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb5-10">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MF-VI"</span>)</span>
<span id="cb5-11"></span>
<span id="cb5-12">combined_points <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> mcmc_state_points <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb5-13">                      <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bind_rows</span>(mfvi_state_points) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb5-14">                      <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ungroup</span>()</span>
<span id="cb5-15"></span>
<span id="cb5-16">combined_points <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb5-17">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ordered_state =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">fct_reorder</span>(combined_points<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>state,</span>
<span id="cb5-18">                                     combined_points<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>postrat_draw)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb5-19">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> ordered_state,</span>
<span id="cb5-20">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> postrat_draw,</span>
<span id="cb5-21">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> model)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb5-22">     <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stat_dots</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">quantiles =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb5-23">     <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">facet_wrap</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>model) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb5-24">     <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">legend.position=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"none"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb5-25">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Should employers be allowed to deny their employees abortion care?"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb5-26">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ylab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"State"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt2/Variational_MRP_pt2_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>Eek. The closer to the individual draws we get, the less these two models seem to be producing comparable estimates. This isn’t me expressing an aesthetic preference for smooth, unimodal distributions- the MFVI plots in this view imply beliefs like “support for this policy in Wyoming is overwhelmingly likely to fall in 1 of 3 narrow ranges, all other values are unlikely”<sup>2</sup>. Other similar humorous claims are easy to find.</p>
<p>Stepping back for a second, if our use-case for this model takes pretty much any form of interest in quantifying uncertainty accurately, this is not an acceptable approximation. I could poke more holes, but I can more profitably do that after I’ve explored some theory of why VI models struggle, and brought in some more sophisticated diagnostic tools than looking with our eyeballs<sup>3</sup>; so let’s hold off on that.</p>
</section>
<section id="do-more-basic-fixes-solve-anything" class="level1">
<h1>Do more basic fixes solve anything?</h1>
<p>So I’ve been billing this simple mean-field model as a first pass- I fit it on more or less default <code>rstanarm</code> parameters. I think it’s worth taking a moment to show that getting this approximation problem right isn’t going to be solved with low hanging fruit ideas, since that will motivate our need for better diagnostics and more expressive approximations.</p>
<section id="lowering-the-tolerance" class="level2">
<h2 class="anchored" data-anchor-id="lowering-the-tolerance">Lowering the tolerance</h2>
<p>So we managed to structure our Bayesian inference problem as an optimization problem. Can’t we just optimize better? Maybe with more training the result will be less bad?</p>
<p>the <code>tol_rel_obj</code> parameter control’s the convergence tolerance on the relative norm of the objective. In other words, it controls what (change in the) Evidence Lower Bound value we consider accurate enough to stop at. The default is 0.01, which feels a bit opaque, but let’s try setting it way down to 1e-8 (1Mx lower). Then we can plot it alongside the MCMC estimates and original MF-VI attempt.</p>
<div class="cell" data-warnings="false">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tic</span>()</span>
<span id="cb6-2">fit_60k_1e8 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stan_glmer</span>(abortion <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> state) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb6-3">                                      male <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> male<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>age) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb6-4">                                      (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> repvote <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">factor</span>(region),</span>
<span id="cb6-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">binomial</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">link =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"logit"</span>),</span>
<span id="cb6-6">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> cces_all_df,</span>
<span id="cb6-7">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">autoscale =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>),</span>
<span id="cb6-8">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior_covariance =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">decov</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">scale =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.50</span>),</span>
<span id="cb6-9">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">adapt_delta =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.99</span>,</span>
<span id="cb6-10">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Printing the ELBO every 1k draws</span></span>
<span id="cb6-11">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">refresh =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>,</span>
<span id="cb6-12">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">tol_rel_obj =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-8</span>,</span>
<span id="cb6-13">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">algorithm =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"meanfield"</span>,</span>
<span id="cb6-14">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">seed =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">605</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Chain 1: ------------------------------------------------------------
Chain 1: EXPERIMENTAL ALGORITHM:
Chain 1:   This procedure has not been thoroughly tested and may be unstable
Chain 1:   or buggy. The interface is subject to change.
Chain 1: ------------------------------------------------------------
Chain 1: 
Chain 1: 
Chain 1: 
Chain 1: Gradient evaluation took 0.032 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 320 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Begin eta adaptation.
Chain 1: Iteration:   1 / 250 [  0%]  (Adaptation)
Chain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)
Chain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)
Chain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)
Chain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)
Chain 1: Success! Found best value [eta = 1] earlier than expected.
Chain 1: 
Chain 1: Begin stochastic gradient ascent.
Chain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes 
Chain 1:    100       -40291.889             1.000            1.000
Chain 1:    200       -39947.669             0.504            1.000
Chain 1:    300       -39802.182             0.337            0.009
Chain 1:    400       -39776.283             0.253            0.009
Chain 1:    500       -39733.863             0.203            0.004
Chain 1:    600       -39733.198             0.169            0.004
Chain 1:    700       -39728.255             0.145            0.001
Chain 1:    800       -39784.557             0.127            0.001
Chain 1:    900       -39724.366             0.113            0.001
Chain 1:   1000       -39732.042             0.102            0.001
Chain 1:   1100       -39731.525             0.002            0.001
Chain 1:   1200       -39732.049             0.001            0.001
Chain 1:   1300       -39728.119             0.001            0.000
Chain 1:   1400       -39740.928             0.000            0.000
Chain 1:   1500       -39726.114             0.000            0.000
Chain 1:   1600       -39734.740             0.000            0.000
Chain 1:   1700       -39734.129             0.000            0.000
Chain 1:   1800       -39740.719             0.000            0.000
Chain 1:   1900       -39743.591             0.000            0.000
Chain 1:   2000       -39737.155             0.000            0.000
Chain 1:   2100       -39720.432             0.000            0.000
Chain 1:   2200       -39738.138             0.000            0.000
Chain 1:   2300       -39731.045             0.000            0.000
Chain 1:   2400       -39716.393             0.000            0.000
Chain 1:   2500       -39729.189             0.000            0.000
Chain 1:   2600       -39722.239             0.000            0.000
Chain 1:   2700       -39719.508             0.000            0.000
Chain 1:   2800       -39718.709             0.000            0.000
Chain 1:   2900       -39735.110             0.000            0.000
Chain 1:   3000       -39725.900             0.000            0.000
Chain 1:   3100       -39726.123             0.000            0.000
Chain 1:   3200       -39718.736             0.000            0.000
Chain 1:   3300       -39718.141             0.000            0.000
Chain 1:   3400       -39717.147             0.000            0.000
Chain 1:   3500       -39725.738             0.000            0.000
Chain 1:   3600       -39732.190             0.000            0.000
Chain 1:   3700       -39723.666             0.000            0.000
Chain 1:   3800       -39725.470             0.000            0.000
Chain 1:   3900       -39741.504             0.000            0.000
Chain 1:   4000       -39722.951             0.000            0.000
Chain 1:   4100       -39721.852             0.000            0.000
Chain 1:   4200       -39717.894             0.000            0.000
Chain 1:   4300       -39717.474             0.000            0.000
Chain 1:   4400       -39716.244             0.000            0.000
Chain 1:   4500       -39727.542             0.000            0.000
Chain 1:   4600       -39716.670             0.000            0.000
Chain 1:   4700       -39723.714             0.000            0.000
Chain 1:   4800       -39727.123             0.000            0.000
Chain 1:   4900       -39722.517             0.000            0.000
Chain 1:   5000       -39722.485             0.000            0.000
Chain 1:   5100       -39719.107             0.000            0.000
Chain 1:   5200       -39722.873             0.000            0.000
Chain 1:   5300       -39720.153             0.000            0.000
Chain 1:   5400       -39718.807             0.000            0.000
Chain 1:   5500       -39719.687             0.000            0.000
Chain 1:   5600       -39730.850             0.000            0.000
Chain 1:   5700       -39719.315             0.000            0.000
Chain 1:   5800       -39717.985             0.000            0.000
Chain 1:   5900       -39715.943             0.000            0.000
Chain 1:   6000       -39721.574             0.000            0.000
Chain 1:   6100       -39716.072             0.000            0.000
Chain 1:   6200       -39715.947             0.000            0.000
Chain 1:   6300       -39716.325             0.000            0.000
Chain 1:   6400       -39716.206             0.000            0.000
Chain 1:   6500       -39720.508             0.000            0.000
Chain 1:   6600       -39717.566             0.000            0.000
Chain 1:   6700       -39718.903             0.000            0.000
Chain 1:   6800       -39716.766             0.000            0.000
Chain 1:   6900       -39724.482             0.000            0.000
Chain 1:   7000       -39717.376             0.000            0.000
Chain 1:   7100       -39721.566             0.000            0.000
Chain 1:   7200       -39725.641             0.000            0.000
Chain 1:   7300       -39717.909             0.000            0.000
Chain 1:   7400       -39720.096             0.000            0.000
Chain 1:   7500       -39716.243             0.000            0.000
Chain 1:   7600       -39738.451             0.000            0.000
Chain 1:   7700       -39715.841             0.000            0.000
Chain 1:   7800       -39716.561             0.000            0.000
Chain 1:   7900       -39716.865             0.000            0.000
Chain 1:   8000       -39721.972             0.000            0.000
Chain 1:   8100       -39723.864             0.000            0.000
Chain 1:   8200       -39716.157             0.000            0.000
Chain 1:   8300       -39720.235             0.000            0.000
Chain 1:   8400       -39718.693             0.000            0.000
Chain 1:   8500       -39727.325             0.000            0.000
Chain 1:   8600       -39716.809             0.000            0.000
Chain 1:   8700       -39716.760             0.000            0.000
Chain 1:   8800       -39721.577             0.000            0.000
Chain 1:   8900       -39716.910             0.000            0.000
Chain 1:   9000       -39721.631             0.000            0.000
Chain 1:   9100       -39721.102             0.000            0.000
Chain 1:   9200       -39718.303             0.000            0.000
Chain 1:   9300       -39715.759             0.000            0.000
Chain 1:   9400       -39719.769             0.000            0.000
Chain 1:   9500       -39719.046             0.000            0.000
Chain 1:   9600       -39720.854             0.000            0.000
Chain 1:   9700       -39717.968             0.000            0.000
Chain 1:   9800       -39721.396             0.000            0.000
Chain 1:   9900       -39728.139             0.000            0.000
Chain 1:   10000       -39715.367             0.000            0.000
Chain 1: Informational Message: The maximum number of iterations is reached! The algorithm may not have converged.
Chain 1: This variational approximation is not guaranteed to be meaningful.
Chain 1: 
Chain 1: Drawing a sample of size 1000 from the approximate posterior... 
Chain 1: COMPLETED.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Pareto k diagnostic value is 2.05. Resampling is disabled. Decreasing
tol_rel_obj may help if variational algorithm has terminated prematurely.
Otherwise consider using sampling instead.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Setting 'QR' to TRUE can often be helpful when using one of the variational inference algorithms. See the documentation for the 'QR' argument.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">toc</span>()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>298.73 sec elapsed</code></pre>
</div>
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1">lower_tol_draws <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> poststrat_df_60k <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">add_epred_draws</span>(fit_60k_1e8, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ndraws =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb12-2"></span>
<span id="cb12-3">mfvi_lower_tol_points <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> lower_tol_draws <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb12-4">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(state,.draw) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb12-5">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarize</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">postrat_draw =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(.epred<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(n<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(n)))) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb12-6">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MF-VI 1e-8"</span>)</span>
<span id="cb12-7"></span>
<span id="cb12-8">combined_points_w_lower_tol <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> combined_points <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb12-9">                      <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bind_rows</span>(mfvi_lower_tol_points) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb12-10">                      <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ungroup</span>()</span>
<span id="cb12-11"></span>
<span id="cb12-12">combined_points_w_lower_tol <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb12-13">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ordered_state =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">fct_reorder</span>(combined_points_w_lower_tol<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>state,</span>
<span id="cb12-14">                                     combined_points_w_lower_tol<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>postrat_draw)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb12-15">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> ordered_state,</span>
<span id="cb12-16">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> postrat_draw,</span>
<span id="cb12-17">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> model)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb12-18">     <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stat_dots</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">quantiles =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb12-19">     <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">facet_wrap</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>model) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb12-20">     <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">legend.position=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"none"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb12-21">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Should employers be allowed to deny their employees abortion care?"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb12-22">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ylab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"State"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt2/Variational_MRP_pt2_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>… That certainly looks different, but I don’t really think I’d say it looks meaningfully better<sup>4</sup>.</p>
<p>Looking at the printed out ELBO, it’s pretty clear that there was no traction after the first ~1000 samples. A variational family this simple isn’t going to get much better, no matter how much time you give it.</p>
</section>
<section id="full-rank-approximation" class="level2">
<h2 class="anchored" data-anchor-id="full-rank-approximation">Full-Rank Approximation</h2>
<p>So if extend training time, but improvements don’t result, maybe the next option is ask whether we need something more sophisticated than a mean-field approximation. Instead of</p>
<p><img src="https://latex.codecogs.com/png.latex?q(z)%20=%20%5Cprod_%7Bj=1%7D%5E%7Bm%7D%20q_j(z_j)"></p>
<p>let’s now try the full-rank approximation. Gather than each <img src="https://latex.codecogs.com/png.latex?z_j"> getting it’s own independent Gaussian, this uses a single multivariate normal distribution- so we can now (roughly) learn correlation structure, fancy.</p>
<p><img src="https://latex.codecogs.com/png.latex?q(z)%20=%20%5Cmathcal%7BN%7D(z%7C%5Cmu,%5CSigma)"></p>
<div class="cell" data-warnings="false">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tic</span>()</span>
<span id="cb13-2">fit_60k_fullrank <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stan_glmer</span>(abortion <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> state) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb13-3">                                      male <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> male<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>age) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb13-4">                                      (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> repvote <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">factor</span>(region),</span>
<span id="cb13-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">binomial</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">link =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"logit"</span>),</span>
<span id="cb13-6">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> cces_all_df,</span>
<span id="cb13-7">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">autoscale =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>),</span>
<span id="cb13-8">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior_covariance =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">decov</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">scale =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.50</span>),</span>
<span id="cb13-9">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">adapt_delta =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.99</span>,</span>
<span id="cb13-10">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">tol_rel_obj =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-8</span>,</span>
<span id="cb13-11">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Printing the ELBO every 1k draws</span></span>
<span id="cb13-12">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">refresh =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>,</span>
<span id="cb13-13">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">algorithm =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"fullrank"</span>,</span>
<span id="cb13-14">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">QR =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>,</span>
<span id="cb13-15">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">seed =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">605</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Chain 1: ------------------------------------------------------------
Chain 1: EXPERIMENTAL ALGORITHM:
Chain 1:   This procedure has not been thoroughly tested and may be unstable
Chain 1:   or buggy. The interface is subject to change.
Chain 1: ------------------------------------------------------------
Chain 1: 
Chain 1: 
Chain 1: 
Chain 1: Gradient evaluation took 0.025 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 250 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Begin eta adaptation.
Chain 1: Iteration:   1 / 250 [  0%]  (Adaptation)
Chain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)
Chain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)
Chain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)
Chain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)
Chain 1: Iteration: 250 / 250 [100%]  (Adaptation)
Chain 1: Success! Found best value [eta = 0.1].
Chain 1: 
Chain 1: Begin stochastic gradient ascent.
Chain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes 
Chain 1:    100      -248586.032             1.000            1.000
Chain 1:    200      -180460.369             0.689            1.000
Chain 1:    300      -121675.221             0.620            0.483
Chain 1:    400       -87431.017             0.563            0.483
Chain 1:    500      -120999.829             0.506            0.392
Chain 1:    600       -96768.296             0.463            0.392
Chain 1:    700       -93851.607             0.402            0.378
Chain 1:    800       -92494.273             0.353            0.378
Chain 1:    900       -74378.556             0.341            0.277
Chain 1:   1000       -77681.560             0.311            0.277
Chain 1:   1100       -77465.866             0.211            0.250
Chain 1:   1200       -68692.287             0.186            0.244
Chain 1:   1300       -75140.633             0.147            0.128
Chain 1:   1400       -49430.772             0.160            0.128
Chain 1:   1500       -59011.994             0.148            0.128
Chain 1:   1600       -57033.572             0.127            0.086
Chain 1:   1700       -56133.855             0.125            0.086
Chain 1:   1800       -46605.149             0.144            0.128
Chain 1:   1900       -47895.964             0.122            0.086
Chain 1:   2000       -44745.890             0.125            0.086
Chain 1:   2100       -43472.467             0.128            0.086
Chain 1:   2200       -43454.384             0.115            0.070
Chain 1:   2300       -41781.249             0.110            0.040
Chain 1:   2400       -42045.221             0.059            0.035
Chain 1:   2500       -41381.652             0.044            0.029
Chain 1:   2600       -40754.440             0.043            0.027
Chain 1:   2700       -41108.136             0.042            0.027
Chain 1:   2800       -40450.439             0.023            0.016
Chain 1:   2900       -40423.015             0.020            0.016
Chain 1:   3000       -40375.121             0.013            0.015
Chain 1:   3100       -40227.022             0.011            0.009
Chain 1:   3200       -40302.411             0.011            0.009
Chain 1:   3300       -40352.339             0.007            0.006
Chain 1:   3400       -40174.196             0.007            0.004
Chain 1:   3500       -40089.973             0.006            0.004
Chain 1:   3600       -40143.009             0.004            0.002
Chain 1:   3700       -40123.486             0.003            0.002
Chain 1:   3800       -40044.004             0.002            0.002
Chain 1:   3900       -39955.515             0.002            0.002
Chain 1:   4000       -40003.851             0.002            0.002
Chain 1:   4100       -39948.544             0.002            0.002
Chain 1:   4200       -40028.027             0.002            0.002
Chain 1:   4300       -39907.006             0.002            0.002
Chain 1:   4400       -39868.266             0.002            0.002
Chain 1:   4500       -39938.386             0.002            0.002
Chain 1:   4600       -39837.339             0.002            0.002
Chain 1:   4700       -39852.349             0.002            0.002
Chain 1:   4800       -39823.670             0.002            0.002
Chain 1:   4900       -39809.797             0.001            0.001
Chain 1:   5000       -39807.261             0.001            0.001
Chain 1:   5100       -39806.402             0.001            0.001
Chain 1:   5200       -39818.805             0.001            0.001
Chain 1:   5300       -39797.428             0.001            0.001
Chain 1:   5400       -39790.469             0.001            0.000
Chain 1:   5500       -39785.797             0.001            0.000
Chain 1:   5600       -39779.121             0.000            0.000
Chain 1:   5700       -39780.314             0.000            0.000
Chain 1:   5800       -39771.363             0.000            0.000
Chain 1:   5900       -39770.673             0.000            0.000
Chain 1:   6000       -39764.096             0.000            0.000
Chain 1:   6100       -39764.173             0.000            0.000
Chain 1:   6200       -39765.651             0.000            0.000
Chain 1:   6300       -39756.809             0.000            0.000
Chain 1:   6400       -39753.724             0.000            0.000
Chain 1:   6500       -39754.753             0.000            0.000
Chain 1:   6600       -39750.392             0.000            0.000
Chain 1:   6700       -39753.067             0.000            0.000
Chain 1:   6800       -39750.341             0.000            0.000
Chain 1:   6900       -39745.696             0.000            0.000
Chain 1:   7000       -39743.521             0.000            0.000
Chain 1:   7100       -39739.157             0.000            0.000
Chain 1:   7200       -39736.689             0.000            0.000
Chain 1:   7300       -39743.472             0.000            0.000
Chain 1:   7400       -39738.431             0.000            0.000
Chain 1:   7500       -39740.789             0.000            0.000
Chain 1:   7600       -39735.842             0.000            0.000
Chain 1:   7700       -39733.493             0.000            0.000
Chain 1:   7800       -39735.015             0.000            0.000
Chain 1:   7900       -39736.429             0.000            0.000
Chain 1:   8000       -39733.548             0.000            0.000
Chain 1:   8100       -39732.722             0.000            0.000
Chain 1:   8200       -39734.720             0.000            0.000
Chain 1:   8300       -39732.932             0.000            0.000
Chain 1:   8400       -39727.658             0.000            0.000
Chain 1:   8500       -39734.522             0.000            0.000
Chain 1:   8600       -39728.602             0.000            0.000
Chain 1:   8700       -39724.690             0.000            0.000
Chain 1:   8800       -39725.374             0.000            0.000
Chain 1:   8900       -39731.450             0.000            0.000
Chain 1:   9000       -39725.866             0.000            0.000
Chain 1:   9100       -39728.639             0.000            0.000
Chain 1:   9200       -39730.156             0.000            0.000
Chain 1:   9300       -39729.036             0.000            0.000
Chain 1:   9400       -39725.536             0.000            0.000
Chain 1:   9500       -39727.031             0.000            0.000
Chain 1:   9600       -39725.389             0.000            0.000
Chain 1:   9700       -39727.947             0.000            0.000
Chain 1:   9800       -39723.932             0.000            0.000
Chain 1:   9900       -39723.173             0.000            0.000
Chain 1:   10000       -39723.944             0.000            0.000
Chain 1: Informational Message: The maximum number of iterations is reached! The algorithm may not have converged.
Chain 1: This variational approximation is not guaranteed to be meaningful.
Chain 1: 
Chain 1: Drawing a sample of size 1000 from the approximate posterior... 
Chain 1: COMPLETED.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Pareto k diagnostic value is 2.95. Resampling is disabled. Decreasing
tol_rel_obj may help if variational algorithm has terminated prematurely.
Otherwise consider using sampling instead.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">toc</span>()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>350.16 sec elapsed</code></pre>
</div>
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1">full_rank_draws <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> poststrat_df_60k <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">add_epred_draws</span>(fit_60k_fullrank,</span>
<span id="cb18-2">                                                        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ndraws =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb18-3"></span>
<span id="cb18-4">frvi_points <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> full_rank_draws <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb18-5">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(state,.draw) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb18-6">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarize</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">postrat_draw =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(.epred<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(n<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(n)))) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb18-7">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"FR-VI"</span>)</span>
<span id="cb18-8"></span>
<span id="cb18-9">combined_points_w_frvi <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> combined_points_w_lower_tol <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb18-10">                      <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bind_rows</span>(frvi_points) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb18-11">                      <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ungroup</span>()</span>
<span id="cb18-12"></span>
<span id="cb18-13">combined_points_w_frvi <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb18-14">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ordered_state =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">fct_reorder</span>(combined_points_w_frvi<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>state,</span>
<span id="cb18-15">                                     combined_points_w_frvi<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>postrat_draw)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb18-16">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> ordered_state,</span>
<span id="cb18-17">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> postrat_draw,</span>
<span id="cb18-18">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> model)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb18-19">     <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stat_dots</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">quantiles =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb18-20">     <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">facet_wrap</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>model) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb18-21">     <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">legend.position=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"none"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb18-22">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Should employers be allowed to deny their employees abortion care?"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb18-23">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ylab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"State"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt2/Variational_MRP_pt2_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>The first thing to note here is that unlike the mean-field approximation, fitting this model required some tinkering to get it to fit. I ended up needing to set <code>QR = TRUE</code> (ie, use a QR decomposition) to get this to fit at all (unless I set the initialization to 0, at which point the posterior collapsed to nearly a single point).</p>
<p>Unfortunately, this version has a similar spiky posterior distribution. In terms of uncertainty, it’s clearly worse than the mean-field implementation. The ELBO starts from higher, spends some time actually improving, but also quickly reaches a plateau. It doesn’t seem like this is a way out either.</p>
</section>
</section>
<section id="where-to-from-here-why-is-it-like-this" class="level1">
<h1>Where to from here? (Why is it like this?)</h1>
<p>We’ve seen that simple variational families like the mean-field and full-rank can approximately mirror the central tendencies of MCMC, but things fall apart as we attempt to consider uncertainty, either through simple credible intervals, or especially once we start to visualize the unrealistic, lumpy VI posterior distributions in their entirety.</p>
<p>This isn’t something we can solve with more training time: each of these algorithms had reached the lowest ELBO they could well before we produced final draws. If I had to guess, I think we need a fundamentally more expressive class of variational family to make progress.</p>
<p>While trying to fit models without digging too much into the theory of why VI approximations can be poor has been fun, it’s time to bring in some theory. In the next post, I’ll explore the literature on why the uncertainty behavior of VI can be so dubious. In the following one, I’ll illustrate some better diagnostics as well.</p>
<p>The code for this post can be found <a href="https://github.com/andytimm/andytimm.github.io/blob/main/posts/Variational%20MRP%20Pt2/Variational_MRP_pt2.qmd">here</a>. Thanks for reading.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Really, the worst type of wrong, completely unpredictable wrong. If you spend time staring to try to infer a causal pattern of which states we can’t estimate well, you’re likely just going to end up confused.↩︎</p></li>
<li id="fn2"><p>Some of these MFVI distributions are bad enough that you might reasonably wonder if some of the badness is just plotting weirdness. That was my intuition at first. Of course though, this is sufficient granularity to make the MCMC results look reasonable. But even if you zoom in on 1 or two states and add way more points, the improbably sharp spikes remain.↩︎</p></li>
<li id="fn3"><p>Phrase due to Richard McElreath. The magic of good visualizations like Kay et al.’s is that makes it trivial to let pattern recognition go to work, and be able to go “oh, that looks wrong”.↩︎</p></li>
<li id="fn4"><p>Also, apologies for showing every 100 iterations; the rstanarm parameter to set this, <code>refresh</code> doesn’t appear to work properly with non-MCMC models, so I can either not show the ELBO or blow up the post with this.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2022,
  author = {Andy Timm},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2022-11-20},
  url = {https://andytimm.github.io/Variational_MRP_pt2.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2022" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2022. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> November 20, 2022. <a href="https://andytimm.github.io/Variational_MRP_pt2.html">https://andytimm.github.io/Variational_MRP_pt2.html</a>.
</div></div></section></div> ]]></description>
  <category>MRP</category>
  <category>Variational Inference</category>
  <guid>https://andytimm.github.io/posts/Variational MRP Pt2/Variational_MRP_pt2.html</guid>
  <pubDate>Sun, 20 Nov 2022 05:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Variational MRP Pt2/images/cover_photo.png" medium="image" type="image/png" height="180" width="144"/>
</item>
<item>
  <title>Variational Inference for MRP with Reliable Posterior Distributions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Variational MRP Pt1/variational_mrp_pt1.html</link>
  <description><![CDATA[ 




<p>This post introduces a series I intend to write, exploring using <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">Variational Inference</a> to massively speed up running complex survey estimation models like variants of <a href="https://en.wikipedia.org/wiki/Multilevel_regression_with_poststratification">Multilevel Regression and Poststratification</a> while aiming to keep approximation error from completely ruining the model.</p>
<p>The rough plan for the series is as follows:</p>
<ol type="1">
<li><strong>(This post)</strong> Introducing the Problem- Why is VI useful, why VI can produce spherical cows</li>
<li>How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li>Some theory on why posterior approximation with VI can be so poor</li>
<li>Seeing if some more sophisticated techniques like normalizing flows help</li>
</ol>
<section id="motivation-for-series" class="level1">
<h1>Motivation for series</h1>
<p>I learn well by explaining things to others, and I’ve been particularly excited to learn about variational inference and ways to improve it over the past few months. There are lots of Bayesian models I would like to fit, especially in my political work, that I would categorize as being incredibly useful, but on the edge of practically acceptable run times. For example, the somewhat but not particularly complex model I’ll use as a running example for the series <strong>takes ~8 hours to fit on 60k observations</strong>.</p>
<p>Having a model run overnight or for a full work day can be fine sometimes, but what if there is a more urgent need for the results? What if we need to iterate to find the “right” model? What if the predictions from this model need to feed into a later one? How constrained do we feel about adding just a little bit more complexity to the model, or increasing our N size just a bit more?</p>
<p>If we can get VI to fit well, we can make complex Bayesian models a lot more practical to use in a wider variety of scenarios, and maybe even extend the complexity of what we can build given time and resource constraints.</p>
</section>
<section id="spherical-cow-sadness" class="level1">
<h1>Spherical Cow Sadness</h1>
<section id="ive-got-that" class="level5">
<h5 class="anchored" data-anchor-id="ive-got-that">I’ve got that…</h5>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 67.6%;justify-content: center;">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt1/rstanarm_disclaimer.png" class="img-fluid"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 5.4%;justify-content: center;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 27.0%;justify-content: center;">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt1/blei_vi_spherical.png" class="img-fluid"></p>
</div>
</div>
</div>
<p>If VI can make Bayesian inference much faster, what’s the catch? The above two images encapsulate the problem pretty well. First, as the left screenshot from <a href="https://mc-stan.org/rstanarm/reference/rstanarm-package.html#estimation-algorithms">rstanarm’s documentation</a> shows, variational inference requires a (bold text warning requiring) set of approximating distribution choices in order to be tractable to optimize. On the right, in their survey paper on VI, <a href="https://arxiv.org/pdf/1601.00670.pdf">Blei et al.&nbsp;(2018)</a> are showing one of the potential posterior distorting consequences of our choice to approximate.</p>
<p>So stepping back for a second, we’ve taken a problem for which there’s usually no closed form solution (Bayesian inference), where even the best approximation algorithm we can usually use (MCMC) isn’t always enough for valid inference without very careful validation and tinkering. Then we decided our approximation could do with being more approximate.</p>
<p>That was perhaps an overly bleak description, but it should give some intuition why this is a hard problem. We want to choose some method of approximating our posterior such that it is amenable to optimization-based solving instead of requiring sampling, but not trade away our ability to correctly understand the full complexity of the posterior distribution<sup>1</sup>.</p>
</section>
</section>
<section id="introducing-mrp-and-our-running-example" class="level1">
<h1>Introducing MRP and our running example</h1>
<section id="introducing-mrp" class="level2">
<h2 class="anchored" data-anchor-id="introducing-mrp">Introducing MRP</h2>
<p>While I’m mostly focused on the way we choose to actually fit a given model with this series, here’s a super quick review of the intuition in building a MRP model. If you want a more complete introduction, Kastellec’s <a href="https://scholar.princeton.edu/jkastellec/publications">MRP Primer</a> is a great starting point, as are the case studies I link a bit later.</p>
<p>MRP casts estimation of a population quantity of interest <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> as a prediction problem. That is, instead of the more traditional approach of building <a href="https://www.pewresearch.org/methods/2018/01/26/how-different-weighting-methods-work/#raking">simple raked weights</a> and using weighted estimators, MRP leans more heavily on modeling and then poststratification to make the estimates representative.</p>
<p>To sketch out the steps-</p>
<ol type="1">
<li>Either gather or run a survey or collection of surveys that collect both information on the outcome of interest, <img src="https://latex.codecogs.com/png.latex?y">, and a set of demographic and geographic predictors, <img src="https://latex.codecogs.com/png.latex?%5Cleft(X_%7B1%7D,%20X_%7B2%7D,%20X_%7B3%7D,%20%5Cldots,%20X_%7Bm%7D%5Cright)">.</li>
<li>Build a poststratification table, with population counts or estimated population counts <img src="https://latex.codecogs.com/png.latex?N_%7Bj%7D"> for each possible combination of the features gathered above. Each possible combination <img src="https://latex.codecogs.com/png.latex?j"> is called a cell, one of <img src="https://latex.codecogs.com/png.latex?J"> possible cells. For example, if we poststratified only on state, there would be <img src="https://latex.codecogs.com/png.latex?J=51"> (with DC) total cells; in practice, <img src="https://latex.codecogs.com/png.latex?J"> is often several thousand.</li>
<li>Build a model, usually a Bayesian multilevel regression, to predict <img src="https://latex.codecogs.com/png.latex?y"> using the demographic characteristic from the survey or set of surveys, estimating model parameters along the way.</li>
<li>Estimate <img src="https://latex.codecogs.com/png.latex?y"> for each cell in the poststratification table, using the model built on the sample.</li>
<li>Aggregate the cells to the population of interest, weighting by the <img src="https://latex.codecogs.com/png.latex?N_%7Bj%7D">’s to obtain population level estimates: <img src="https://latex.codecogs.com/png.latex?%5Ctheta_%7B%5Cmathrm%7BPOP%7D%7D=%5Cfrac%7B%5Csum_%7Bj%20%5Cin%20J%7D%20N_%7Bj%7D%20%5Ctheta_%7Bj%7D%7D%7B%5Csum_%7Bj%20%5Cin%20J%7D%20N_%7BJ%7D%7D"></li>
</ol>
<p>Why would we want to do this over building more typical survey weights? To the extent your new model has desirable properties like the ability to incorporate priors, can partially pool to manage rare subpopulations where you don’t have a lot of sample, and so on, you can get the benefits of that more efficient model through MRP. Raking in its simplest form is really just a linear model; we have plenty of methods that can do better. Outside of bayesian multilevel models which are the most common, there’s an increasing literature on using a wide variety of machine learning algorithms like BART<sup>2</sup> to do the estimation stage; Andrew Gelman calls this <a href="https://statmodeling.stat.columbia.edu/2018/05/19/regularized-prediction-poststratification-generalization-mister-p/">RRP</a>.</p>
</section>
<section id="introducing-the-running-example" class="level2">
<h2 class="anchored" data-anchor-id="introducing-the-running-example">Introducing the Running Example</h2>
<p>Rather than reinvent the wheel, I’ll follow the lead of the excellent <a href="https://bookdown.org/jl5522/MRP-case-studies/">Multilevel Regression and Poststratification Case Studies</a> by Lopez-Martin, Philips, and Gelman, and model survey binary responses from the <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ZSBZ7K">2018 CCES</a> for the following question:</p>
<blockquote class="blockquote">
<p>Allow employers to decline coverage of abortions in insurance plans (Support / Oppose)</p>
</blockquote>
<p>From the CCES, we get information on each participant’s state, age, gender, ethnicity, and education level. Supplementing this individual level data, we also include region flags for each state, and Republican vote share in the 2016 election- these state level predictors have been shown to be critical for getting strong MRP estimates by <a href="http://www.columbia.edu/~jhp2121/publications/HowShouldWeEstimateOpinion.pdf">Lax and Philips (2009)</a> and others. and If you’d like deeper detail on the dataset itself, I’d refer you to <a href="https://bookdown.org/jl5522/MRP-case-studies/introduction-to-mister-p.html#ref-2018CCES">this part</a> MRP case study.</p>
<p>Using these, we setup the model for <img src="https://latex.codecogs.com/png.latex?Pr(y_i%20=%201)"> the probability of supporting allowing employers to decline coverage of abortions in insurance plans as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0APr(y_i%20=%201)%20=&amp;%20logit%5E%7B-1%7D(%0A%5Cgamma%5E0%0A+%20%5Calpha_%7B%5Crm%20s%5Bi%5D%7D%5E%7B%5Crm%20state%7D%0A+%20%5Calpha_%7B%5Crm%20a%5Bi%5D%7D%5E%7B%5Crm%20age%7D%0A+%20%5Calpha_%7B%5Crm%20r%5Bi%5D%7D%5E%7B%5Crm%20eth%7D%0A+%20%5Calpha_%7B%5Crm%20e%5Bi%5D%7D%5E%7B%5Crm%20educ%7D%0A+%20%5Cbeta%5E%7B%5Crm%20male%7D%20%5Ccdot%20%7B%5Crm%20Male%7D_%7B%5Crm%20i%7D%20%5C%5C%0A&amp;+%20%5Calpha_%7B%5Crm%20g%5Bi%5D,%20r%5Bi%5D%7D%5E%7B%5Crm%20male.eth%7D%0A+%20%5Calpha_%7B%5Crm%20e%5Bi%5D,%20a%5Bi%5D%7D%5E%7B%5Crm%20educ.age%7D%0A+%20%5Calpha_%7B%5Crm%20e%5Bi%5D,%20r%5Bi%5D%7D%5E%7B%5Crm%20educ.eth%7D%0A+%20%5Cgamma%5E%7B%5Crm%20south%7D%20%5Ccdot%20%7B%5Crm%20South%7D_%7B%5Crm%20s%7D%20%5C%5C%0A&amp;+%20%5Cgamma%5E%7B%5Crm%20northcentral%7D%20%5Ccdot%20%7B%5Crm%20NorthCentral%7D_%7B%5Crm%20s%7D%0A+%20%5Cgamma%5E%7B%5Crm%20west%7D%20%5Ccdot%20%7B%5Crm%20West%7D_%7B%5Crm%20s%7D%0A+%20%5Cgamma%5E%7B%5Crm%20repvote%7D%20%5Ccdot%20%7B%5Crm%20RepVote%7D_%7B%5Crm%20s%7D)%0A%5Cend%7Baligned%7D%0A"></p>
<p>Where we incorporate pretty much all of our predictors as varying intercepts to allow for pooling across demographic and geographic characteristics:</p>
<ul>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20a%7D%5E%7B%5Crm%20age%7D">: The effect of subject <img src="https://latex.codecogs.com/png.latex?i">’s age on the probability of supporting the statement.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20r%7D%5E%7B%5Crm%20eth%7D">: The effect of subject <img src="https://latex.codecogs.com/png.latex?i">’s ethnicity on the probability of supporting the statement.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20e%7D%5E%7B%5Crm%20educ%7D">: The effect of subject <img src="https://latex.codecogs.com/png.latex?i">’s education on the probability of supporting the statement.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20s%7D%5E%7B%5Crm%20state%7D">: The effect of subject <img src="https://latex.codecogs.com/png.latex?i">’s state on the probability of supporting the statement.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Cbeta%5E%7B%5Crm%20male%7D">: The average effect of being male on the probability of supporting abortion. Note that it doesn’t really make much sense to model a two category<sup>3</sup> factor as a varying intercept.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20e,r%7D%5E%7B%5Crm%20male.eth%7D">, <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20e,r%7D%5E%7B%5Crm%20educ.age%7D">, <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20e,r%7D%5E%7B%5Crm%20educ.eth%7D">: Are several reasonable guesses at important interactions for this question. We could add many more two way, or even some three way interactions here, but this is enough for my testing here.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Cgamma%5E%7B%5Crm%20south%7D,%20%5Cgamma%5E%7B%5Crm%20northcentral%7D,%20%5Cgamma%5E%7B%5Crm%20west%7D,%5Cgamma%5E%7B%5Crm%20repvote%7D">: are the state level predictors which are not represented as varying intercepts. Following the case study, I use <img src="https://latex.codecogs.com/png.latex?%5Cgamma">’s for the state level coefficients, keeping <img src="https://latex.codecogs.com/png.latex?%5Cbeta">’s for individual coefficients. Note that Northeast is the base region of the region factor here, so it doesn’t get it’s own coefficient.</p></li>
</ul>
<p>Stepping back for a second, let’s describe the complexity of this model in more general terms. This certainly isn’t state of the art for MRP, and you could definitely add in things like a lot more interactions, some varying slopes, non-univariate prior and/or structured priors, or other elements to make this a more interesting model. That said, this is already clearly enough of a model to improve on simple raking in many cases, and it produces a nuanced enough posterior that we can feasibly imagine a bad approximation going all spherical cow shaped on us.</p>
<p>Why this dataset and this model for this series? The question we model itself isn’t super important- as long as we can expect some significant regional and demographic variation in the outcome we’ll be able to explore if VI smoothes away some posterior complexity that MCMC can capture. Drawing an example from the CCES is quite useful, as the 60k total sample is much larger than typical publicly available surveys, and so we can check behavior under larger N sizes. Practically, fitting this with <code>rstanarm</code> allows us to switch easily from a great MCMC implementation to a decent VI optimizer quickly for some early tests. Finally, the complexity and runtime of the model is a nice balance of being something that we can fit with MCMC in a not terrible amount of time for comparison’s sake, and something challenging enough that it should teach us something about VI’s ability to handle non-toy models of the world.</p>
<p>Fitting this<sup>4</sup> with MCMC in <code>rstanarm</code> is as simple as:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Fit in stan_glmer</span></span>
<span id="cb1-2">fit <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stan_glmer</span>(abortion <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> state) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> male <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-3">                    (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> male<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>age) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-4">                    repvote <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">factor</span>(region),</span>
<span id="cb1-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">binomial</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">link =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"logit"</span>),</span>
<span id="cb1-6">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> cces_df,</span>
<span id="cb1-7">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">autoscale =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>),</span>
<span id="cb1-8">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior_covariance =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">decov</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">scale =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.50</span>),</span>
<span id="cb1-9">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">adapt_delta =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.99</span>,</span>
<span id="cb1-10">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">refresh =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,</span>
<span id="cb1-11">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">seed =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">605</span>)</span></code></pre></div>
</div>
<p>Since it isn’t relevant for the rest of my discussion here, I’ll summarize the model diagnostics here and say that this seems to be a pretty reasonable fit- no issues with divergences, and no issues with poor <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D">’s. Worth quickly pointing out that we did have to tune <code>adapt_delta</code> a bit to get no divergences though- even before getting to fitting this with VI, a model like this requires some adjustments to fit correctly.</p>
<p>With a model like this on just a 5k sample, we can produce pretty solid state level predictions that have clearly benefited from being fit with a Bayesian multilevel model:</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt1/5k_sample_full_results.png" class="img-fluid"></p>
<p>With a 5k sample, MRP lands much closer to the complete weighted survey than a 5k unweighted sample: neat. That’s certainly not a fully fair comparison, but it gives some intution around the promise of this approach.</p>
<p>Somewhat less neat is that even a 5k sample here takes about 13 minutes to fit. How does this change as we fit on more and more of the data?</p>
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 75%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Sample Size</th>
<th style="text-align: left;">Runtime</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">5,000</td>
<td style="text-align: left;">13 minutes</td>
</tr>
<tr class="even">
<td style="text-align: left;">10,000</td>
<td style="text-align: left;">44 minutes</td>
</tr>
<tr class="odd">
<td style="text-align: left;">60,000</td>
<td style="text-align: left;">526 minutes (~8 hours!)</td>
</tr>
</tbody>
</table>
<p>As the table above should illustrate, if you’re fitting a decently complex Bayesian model on even somewhat large N sizes, you’re pretty quickly going to cap out what you can reasonably fit in a acceptable amount of time. If you’re scaling N past the above example, or deepening the modeling complexity, you’ll pretty quickly feel effectively locked out of using these models in fast-paced environments.</p>
<p>Hopefully fitting my running example has helped for building intuition here. Even a reasonably complex Bayesian model can have some pretty desirable estimation properties. To make iterating on modelling choices faster, to scale our N or model complexity higher, or just to use a model like this day to day when time matters, we’d really like to scale these fitting times back. Can Variational Inference help?</p>
</section>
</section>
<section id="introducing-variational-inference" class="level1">
<h1>Introducing Variational Inference</h1>
<p>I’ve gotten relatively far in this post without clearly explaining what Variational Inference is, and why it might provide a more efficient and scalable way to fix large Bayesian models. Let’s fully flesh that out here to ground the rest of the series.</p>
<p>In the bigger picture, pretty much all of our efforts in Bayesian inference are a form of approximate inference. Almost no models we care about for real world applications have closed form solutions- conjugate prior type situations are a math problem for stats classes, not a general tool for inference.</p>
<p>Following <a href="https://arxiv.org/abs/1601.00670">Blei et al.&nbsp;(2018)</a>’s notation, let’s setup the general problem first, describe (briefly) how MCMC solves it, and then more slowly demonstrate how VI does. Let’s say we have some observations <img src="https://latex.codecogs.com/png.latex?x_%7B1:N%7D">, and and some latent variables that define the model <img src="https://latex.codecogs.com/png.latex?z_%7B1:M%7D">. Note for concreteness these latent variables represent our quantities of interest: key parameters and so on- we’re calling them latent in the sense that we can’t go out and directly measure a <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> or <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> from the model above, we have to gather data that allows us to estimate them. We call <img src="https://latex.codecogs.com/png.latex?p(z)"> priors, and they define our model prior to contact with the data. The goal of Bayesian inference then is conditioning on our data in order to get the posterior:</p>
<p><img src="https://latex.codecogs.com/png.latex?p(z%7Cx)%20=%20%5Cfrac%7Bp(z,x)%7D%7Bp(x)%7D"></p>
<p>If you’re reading this post series, it’s likely you recognize that the denominator on the right here (often called the “evidence”) is the sticking point; the integral <img src="https://latex.codecogs.com/png.latex?p(x)%20=%20%5Cint%7Bp(z,x)dz%7D"> won’t have a closed form solution.</p>
<p>When we use Markov Chain Monte Carlo as we did above to estimate the model, we’re defining a Markov Chain on <img src="https://latex.codecogs.com/png.latex?z">, whose stationary distribution if we’ve done everything right is <img src="https://latex.codecogs.com/png.latex?p(z%7Cx)">. There are better and worse ways to do this certainly- the development of the <a href="https://mc-stan.org/">Stan</a> language, with associated <a href="https://mc-stan.org/docs/2_19/reference-manual/hamiltonian-monte-carlo.html">Hamiltonian Monte Carlo</a> with <a href="https://arxiv.org/abs/1111.4246">NUTS</a> sampler has massively expanded what was possible to fit in recent years. However, while actively improving the speed and scalability of sampling is an active area of research (for example, by using <a href="https://mc-stan.org/cmdstanr/articles/opencl.html">GPU compute</a> where possible), some of the speed challenges just seem a bit baked into the approach. For example, the sequential nature of markov chains makes parallelization within chains seem out of reach absent some as-yet unknown clever tricks.</p>
<p>Instead of sampling, variational inference asks what we’d need to figure out to treat the Bayesian inference problem as an <strong>optimization problem</strong>, where we could bring to bear all the tools for efficient, scalable, and parallelizable optimization we have developed.</p>
<p>Let’s start with the idea of a family of approximate densities <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BQ%7D"> over our latent variables<sup>5</sup>.</p>
<p>Within that <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BQ%7D">, we want to try the best <img src="https://latex.codecogs.com/png.latex?q(z)">, call it <img src="https://latex.codecogs.com/png.latex?q%5E*(z)">, that minimizes the Kullback-Leibler divergence to the true posterior:</p>
<p><img src="https://latex.codecogs.com/png.latex?q%5E*(z)%20=%20argmin_%7Bq(z)%20%5Cin%20%5Cmathscr%7BQ%7D%7D(q(z)%7C%7Cp(z%7Cx))"></p>
<p>If we choose a good <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BQ%7D">, managing the complexity so that it includes a density close to <img src="https://latex.codecogs.com/png.latex?p(z%7Cx)">, without becoming too slow or impossible to optimize, this approach may provide a significant speed boost.</p>
<p>To start working with this approach though, there’s one major remaining problem. Do you see it in the equation above?</p>
<section id="the-elbo" class="level2">
<h2 class="anchored" data-anchor-id="the-elbo">The ELBO</h2>
<p>If you haven’t seen it yet, this quick substitution should clarify a potential issue with VI as I’ve described it so far:</p>
<p><img src="https://latex.codecogs.com/png.latex?q%5E*(z)%20=%20argmin_%7Bq(z)%20%5Cin%20%5Cmathscr%7BQ%7D%7D(q(z)%7C%7C%5Cfrac%7Bp(z,x)%7D%7B%5Cbf%20p(x)%7D)%20=%20%5Cmathbb%7BE%7D%5Blogq(z)%5D%20-%20%5Cmathbb%7BE%7D%5Blogp(z,x)%5D%20+%20%7B%5Cbf%20logp(x)%7D"> Without some new trick, all I’ve said so far is to approximate a thing I can’t analytically calculate (the posterior, specially the issue evidence piece of it), I’m going to calculate the distance between my approximation and… the thing I said has a component can’t calculate?</p>
<p>Fortunately, a clever solution exists here that makes this strategy possible. Instead of trying to minimize the above KL divergence, we can optimize the alternative objective:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5Blogp(z,x)%5D%20-%20%5Cmathbb%7BE%7D%5Blogq(z)%5D"></p>
<p>This is just the negative of the first two terms above, leaving aside the <img src="https://latex.codecogs.com/png.latex?logp(x)">. Why can we treat maximizing this as minimizing the KL divergence? The <img src="https://latex.codecogs.com/png.latex?logp(x)"> term is just a constant (with respect to q), so regardless of how we vary q, this will still be a valid alternative objective. We call this the Evidence Lower Bound (ELBO)<sup>6</sup>.</p>
<p>If it’s helpful for intuition, play around with this great interactive ELBO optimizer by Felix Köhler:</p>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt1/elboplot.png" class="img-fluid"></p>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt1/elboeqs.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Link to demonstration <a href="https://englishprobabilistic-machine-learningelbo-interactive--or5u7m.streamlitapp.com/">here</a>; check out Felix’s Youtube explanation of the ELBO <a href="https://www.youtube.com/watch?v=HxQ94L8n0vU">also</a>!</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>By twiddling the knobs on <img src="https://latex.codecogs.com/png.latex?%5Cmu"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma"> for our approximating normal, we can get our surrogate distribution pretty close to the True Posterior (which we know for purposes of demonstration, so we can calculate the true KL, not just it’s ELBO component). No matter how we twiddle though, the evidence remains constant.</p>
<p>For further intuition- notice that we can only do this trick in one direction. The KL divergence isn’t symmetrical, and if we wanted to calculate the “reverse” KL, we couldn’t use this strategy as <img src="https://latex.codecogs.com/png.latex?logq(x)"> would not be a constant. Even if we thought that optimizing other direction of KL might have desirable properties like emphasizing <a href="https://agustinus.kristia.de/techblog/2016/12/21/forward-reverse-kl/">mass-seeking over mode-seeking behavior</a>, that simply isn’t an option.</p>
</section>
</section>
<section id="a-first-try-at-vi-on-this-dataset" class="level1">
<h1>A first try at VI on this dataset</h1>
<p>Ok, so we have an objective to optimize that should actually work. What’s a good <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BQ%7D">? The choice has been shown to matter a lot, but for purposes of a first swing here, let’s try one of the simpler ideas people have explored, the mean-field family. These latent variables will be assumed mutually independent<sup>7</sup> and each get it’s own distinct factor in the variational density. A member of this would look something like:</p>
<p><img src="https://latex.codecogs.com/png.latex?q(z)%20=%20%5Cprod_%7Bj=1%7D%5E%7Bm%7D%20q_j(z_j)"></p>
<p>Each latent <img src="https://latex.codecogs.com/png.latex?z_j"> get it’s own variational factor with density <img src="https://latex.codecogs.com/png.latex?q_j(z_j)">, whose knobs we play with to maximize the ELBO. In the particular implementation below normal distributions are used, plenty of other options like t distributions are common too.</p>
<p>Probably not the best we can do, but let’s give it a roll. Since we’ve been told this will scale really well too supposedly, let’s use all 60k of the observations just to get a sense how it’ll compare to our 8+ hours in that case.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tic</span>()</span>
<span id="cb2-2">fit_60k <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stan_glmer</span>(abortion <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> state) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> male <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-3">                    (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> male<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>age) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-4">                    repvote <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">factor</span>(region),</span>
<span id="cb2-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">binomial</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">link =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"logit"</span>),</span>
<span id="cb2-6">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> cces_all_df,</span>
<span id="cb2-7">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">autoscale =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>),</span>
<span id="cb2-8">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior_covariance =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">decov</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">scale =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.50</span>),</span>
<span id="cb2-9">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">adapt_delta =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.99</span>,</span>
<span id="cb2-10">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">refresh =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,</span>
<span id="cb2-11">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">algorithm =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"meanfield"</span>,</span>
<span id="cb2-12">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">seed =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">605</span>)</span>
<span id="cb2-13"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">toc</span>()</span></code></pre></div>
</div>
<p>This finishes in a blazing <strong>144.03 seconds</strong>. Is this a good fit, or have we created a ridiculous spherical cow?</p>
<p>You’ll have to find out in the next post. Thanks for reading!</p>
<p><em>Typically, I’ll include links to code at the end of these posts, but since the only thing going on in this notebook is mentioning some runtimes of the models displayed inline at various sample sizes, I’m skipping that for now.</em></p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>If I were that type of Bayesian, this is where I’d complain that if we screw this up badly enough, we might as well be frequentists or worse, machine learning folk.↩︎</p></li>
<li id="fn2"><p>In grad school, using BART as the estimator (also combining it with some portions of the model being estimated as multilevel models) was the focus of my <a href="https://andytimm.github.io/posts/BART%20VI/2020-03-06-BART-vi.html">masters thesis</a>. This pairs the best parts of relatively black box machine learning sensibility with the advantages of still having a truly Bayesian model. With comparatively minimal iteration you can get a pretty decent set of MRP models that will be better than many basic versions of multilevel models fit early in the MRP literature. Of course, if you’re willing to spend a bunch of time iterating on the absolute best models for a given problem, and incorporate lots of problem specific knowledge into model forms you can and should do better than <a href="https://github.com/jbisbee1/BARP">BARP</a>. Also, a lot of pretty cool things you can do like jointly model multiple question responses at the same time aren’t going to be easily to implement unless you get way in the weeds of your own BART implementation.↩︎</p></li>
<li id="fn3"><p>Insert snark about CCES folks doing a poor job at gender inclusivity despite 80+ researchers working on it here.↩︎</p></li>
<li id="fn4"><p>Again, see the MRP case studies linked above if you want see all the data prep and draw manipulation here; I’ll be leaving out most such details that aren’t relevant for comparisons to fitting this model with VI from now on.↩︎</p></li>
<li id="fn5"><p>In grad school, I had a friend who insisted on calling this “spicy Q”. For a while we had a latex package that made <code>\spicy{}</code> equivalent to <code>\mathscr{}</code>. Apologies for the footnote for the dumb LaTeX joke, but now I’m pretty sure you won’t have a sudden moment of “what is that symbol again” discussing VI ever.↩︎</p></li>
<li id="fn6"><p>Why is this a lower bound? Notice that we could write the evidence from above equations as <img src="https://latex.codecogs.com/png.latex?logp(x)%20=%20KL(q(z)%7C%7Cp(z%7Cx))%20+%20ELBO(q)">. Since the KL divergence is non-negative (it’s zero when distributions <img src="https://latex.codecogs.com/png.latex?p"> and <img src="https://latex.codecogs.com/png.latex?q"> are identical), the ELBO is a lower bound of the evidence.↩︎</p></li>
<li id="fn7"><p>If this seems like it could go fully spherical cow, both literally in the sense that if we use a bunch of independent normals we make a sphere, and in the sense that this may not represent the full complexity of public opinion, you’re correct. Assuming independence here could very easily cause problems, and part of why this VI strategy is so challenging is the subset of things we can easily optimize doesn’t have the best overlap with fully realistic distributional assumptions over our latent variables.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2022,
  author = {Andy Timm},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2022-10-10},
  url = {https://andytimm.github.io/variational_mrp_pt1.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2022" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2022. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> October 10, 2022. <a href="https://andytimm.github.io/variational_mrp_pt1.html">https://andytimm.github.io/variational_mrp_pt1.html</a>.
</div></div></section></div> ]]></description>
  <category>MRP</category>
  <category>BART</category>
  <category>Variational Inference</category>
  <guid>https://andytimm.github.io/posts/Variational MRP Pt1/variational_mrp_pt1.html</guid>
  <pubDate>Mon, 10 Oct 2022 04:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Variational MRP Pt1/elboplot.png" medium="image" type="image/png" height="43" width="144"/>
</item>
<item>
  <title>BART with varying intercepts in the MRP framework</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/BART VI/2020-03-06-BART-vi.html</link>
  <description><![CDATA[ 




<p>This is the first of a few posts about some of the substantive and modeling findings from my master’s thesis, where I use Bayesian Additive Regression Trees (BART) and Poststratification to model support for a border wall from 2015-2019. In this post I explore some of the properties of using BART with varying intercepts (BART-vi) within the MRP framework.</p>
<!--more-->
<section id="choosing-a-prediction-model-for-mrp" class="level2">
<h2 class="anchored" data-anchor-id="choosing-a-prediction-model-for-mrp">Choosing a prediction model for MRP</h2>
<p>Multilevel Regression and Poststratification has seen huge success as a tool for obtaining accurate estimates of public opinion in small areas using national surveys. As the name would suggest, the most common tool used for the modeling step of these models are multilevel regressions, often Bayesian multilevel ones. The key intuition here is that pooling data across levels of predictors like state makes much more efficient use of the underlying data, which leads to more accurate estimates. However, there is no strict requirement that the models used here be multilevel regressions, it’s simply an efficient and stable way to get regularized predictions using quite a large set of predictors. Recently, academic work has begun to explore using a wide class of machine learning algorithms as the predictive component in this framework. Andrew Gelman calls this RRP: <a href="https://statmodeling.stat.columbia.edu/2018/05/19/regularized-prediction-poststratification-generalization-mister-p/">Regularized Regression and Poststratification</a>.</p>
<p>One particularly promising alternative prediction algorithm is <a href="https://arxiv.org/abs/0806.3286">BART</a>, which puts the high accuracy of tree-based algorithms like random forests or gradient boosting into a Bayesian framework. This has a number of appealing advantages compared to other machine learning options, especially for RRP. First, unlike other algorithms which might not have clear measures of their uncertainty or confidence intervals around their predictions, BART approximates a full posterior distribution. Second, BART has a number of prior and hyperparameter choices that have been shown to be highly effective in a wide variety of settings, somewhat reducing the need for parameter search. Finally, BART runs fast, especially when compared to the Bayesian multilevel models commonly used for MRP.</p>
<p>Of course, BART models are not without their disadvantages. First and foremost, there is currently only a small amount of recent work on BART models for categorical (as opposed to binary) response <a href="https://arxiv.org/abs/1701.01503">(Murray, 2019)</a>, and no public implementation of that model that I am aware of. In my case, this means modeling the border wall question as binary “support vs.&nbsp;oppose”, as opposed to the three categories “support, oppose, don’t know”. Given the salience of the issue, only 3.1% of people responded “Don’t Know”, so this is a relatively minor loss. However, for questions like the formerly crowded 2020 democratic primary, or a general election where third parties play a major role, this could be a much more serious loss.</p>
<p>While only a small amount of work has compared the two so far, estimates using BART appear to slightly outperform those using multilevel models. For example, <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12361">Montgomery &amp; Olivella (2018)</a> compared using BART to <a href="http://www.stat.columbia.edu/~gelman/research/published/misterp.pdf">Gelman &amp; Ghitza’s (2013)</a>’s fairly complex multilevel model, finding the predictions were incredibly similar, being as much as .97 correlated. As they note however, their BART model produced these results both without large amounts of iteration on model form (which has been a constant challenge for MRP), and producing such estimates orders of magnitude faster. Similarly, <a href="https://www.cambridge.org/core/journals/american-political-science-review/article/barp-improving-mister-p-using-bayesian-additive-regression-trees/630866EB47F9366EDB3C22CFD951BB6F">Bisbee (2019)</a> finds across 89 different datasets that BART and MRP produced very similar estimates, but BART’s were of slightly higher quality, both by Mean Absolute Error (MAE) and Interstate Correlation (a measure of how well the state level predictions from a model using national surveys line up with state level polls). Ending his article, Bisbee writes “One avenue of future research might focus on variants of Bayesian additive regression trees that embed a multilevel component, likely providing further improvements as the best of both worlds.”</p>
<p>This is exactly what I do in my thesis, using BART with varying intercepts by state to model support for a border wall by state. To present my findings around BART-vi, I’ll start by providing a brief overview of the MRP framework. Next, I’ll explain BART, and how BART-vi extends this model. Finally, I’ll build one model of each BART type, and compare them.</p>
</section>
<section id="a-quick-review-of-multilevel-regression-and-poststratification" class="level2">
<h2 class="anchored" data-anchor-id="a-quick-review-of-multilevel-regression-and-poststratification">A Quick Review of Multilevel Regression and Poststratification</h2>
<p>While I’m mostly focused on the modeling step with this post, here’s a quick review of the overall process in building a MRP/RRP model. If you want a more complete introduction, Kastellec’s <a href="https://scholar.princeton.edu/jkastellec/publications">MRP Primer</a> is a great starting point.</p>
<p>MRP or RRP cast estimation of a population quantity of interest <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> as a prediction problem. That is, instead of the more traditional approach of attempting to design the initial survey to be representative of the population, MRP leans more heavily on modeling and poststratification to make the estimates representative.</p>
<p>To sketch out the steps-</p>
<ol type="1">
<li>Either gather or run a survey or collection of surveys that collect both information on the outcome of interest, <img src="https://latex.codecogs.com/png.latex?y">, and a set of demographic and geographic predictors, <img src="https://latex.codecogs.com/png.latex?%5Cleft(X_%7B1%7D,%20X_%7B2%7D,%20X_%7B3%7D,%20%5Cldots,%20X_%7Bm%7D%5Cright)">.</li>
<li>Build a poststratification table, with population counts or estimated population counts <img src="https://latex.codecogs.com/png.latex?N_%7Bj%7D"> for each possible combination of the features gathered above. Each possible combination <img src="https://latex.codecogs.com/png.latex?j"> is called a cell, one of <img src="https://latex.codecogs.com/png.latex?J"> possible cells. For example, if we poststratified only on state, there would be <img src="https://latex.codecogs.com/png.latex?J=51"> (with DC) total cells; in practice, <img src="https://latex.codecogs.com/png.latex?J"> is often several thousand.</li>
<li>Build a model, usually a Bayesian multilevel regression, to predict <img src="https://latex.codecogs.com/png.latex?y"> using the demographic characteristic from the survey or set of surveys, estimating model parameters along the way.</li>
<li>Estimate <img src="https://latex.codecogs.com/png.latex?y"> for each cell in the poststratification table, using the model built on the sample.</li>
<li>Aggregate the cells to the population of interest, weighting by the <img src="https://latex.codecogs.com/png.latex?N_%7Bj%7D">’s to obtain population level estimates: <img src="https://latex.codecogs.com/png.latex?%5Ctheta_%7B%5Cmathrm%7BPOP%7D%7D=%5Cfrac%7B%5Csum_%7Bj%20%5Cin%20J%7D%20N_%7Bj%7D%20%5Ctheta_%7Bj%7D%7D%7B%5Csum_%7Bj%20%5Cin%20J%7D%20N_%7BJ%7D%7D"></li>
</ol>
</section>
<section id="bart" class="level2">
<h2 class="anchored" data-anchor-id="bart">BART</h2>
<p>In this section, I review the general BART model, and discuss the hyperparameter choices I use. Proposed by <a href="https://arxiv.org/abs/0806.3286">Chipman et al, (2008)</a>, BART is a Bayesian machine learning algorithm that has seen widespread success in a wide variety of both predictive and causal inference applications. Like most machine learning models, it treats the prediction task as modeling the outcome <img src="https://latex.codecogs.com/png.latex?y"> as an unknown function <img src="https://latex.codecogs.com/png.latex?f"> of the <img src="https://latex.codecogs.com/png.latex?k"> predictors <img src="https://latex.codecogs.com/png.latex?y%20=%20f(X_%7Bk%7D)">.</p>
<p>BART does with this a sum of decision trees:</p>
<p><img src="https://latex.codecogs.com/png.latex?Y_%7Bk%7D=%5Csum_%7Bj=1%7D%5E%7Bm%7D%20g%5Cleft(%5Cmathbf%7BX%7D_%7Bk%7D,%20T_%7Bj%7D,%20%5Cmathbf%7BM%7D_%7Bj%7D%5Cright)+%5Cepsilon_%7Bk%7D%20%5Cquad%20%5Cepsilon_%7Bk%7D%20%5Cstackrel%7Bi%20.%20i%20.%20d%7D%7B%5Csim%7D%20N%5Cleft(0,%20%5Csigma%5E%7B2%7D%5Cright)"></p>
<p>(To start with the continuous case, before generalizing to the binary case in a moment)</p>
<p>Each tree <img src="https://latex.codecogs.com/png.latex?T_j"> splits the data along a variety of predictors, seeking to improve the purity of outcomes in each group. For instance, in seeking to partition respondents into purer groups of support or opposition for a border wall, one natural split is that of white vs.&nbsp;nonwhite respondents, after which a further split by education might further partition the white node. At the end of fitting such a tree, there are <img src="https://latex.codecogs.com/png.latex?b_%7Bj%7D"> terminal nodes (nodes at the bottom of the tree), which contain groups where the average outcome <img src="https://latex.codecogs.com/png.latex?%5Cmu_%7Bj%7D"> should be purer due to iterative splitting. This iterative splitting is equivalent to the modeling of interaction effects, and combining many such trees allows for flexible and highly non-linear functions of the predictors to be calculated. Each data point <img src="https://latex.codecogs.com/png.latex?x"> is thought of as assigned to one such terminal node for each tree, which captures <img src="https://latex.codecogs.com/png.latex?E(y%20%5Cvert%20x)">, with the collection of <img src="https://latex.codecogs.com/png.latex?u_%7Bj%7D">’s referred to collectively as <img src="https://latex.codecogs.com/png.latex?M">. Together, <img src="https://latex.codecogs.com/png.latex?m"> such trees are fit to residual errors from an initial baseline prediction iteratively, ensuring that the trees are grown in varying structures that predict well for different parts of the covariate space, not just split on the same features producing identical predictions.</p>
<p>To fit such trees to the data and not overfit, BART utilizes a Bayesian framework, placing priors on tree structure, terminal node parameters, and variance, <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">. The prior for the <img src="https://latex.codecogs.com/png.latex?u_%7Bj%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2"> are:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%20%5Cmu_%7Bj%7D%20%7C%20T_%7Bj%7D%20&amp;%20%5Csim%20N%5Cleft(%5Cmu,%20%5Csigma_%7B%5Cmu%7D%5E%7B2%7D%5Cright)%20%5C%5C%20%5Csigma%5E%7B2%7D%20&amp;%20%5Csim%20I%20G%5Cleft(%5Cfrac%7B%5Cnu%7D%7B2%7D,%20%5Cfrac%7B%5Cnu%20%5Clambda%7D%7B2%7D%5Cright)%20%5Cend%7Baligned%7D"></p>
<p>Where <img src="https://latex.codecogs.com/png.latex?I%20G(%5Calpha,%20%5Cbeta)"> is the inverse gamma distribution with shape parameter <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and rate <img src="https://latex.codecogs.com/png.latex?%5Cbeta">. The priors on the tree structure can be thought of as having 3 components. First, there is a prior on the probability that a tree of depth <img src="https://latex.codecogs.com/png.latex?d%20=%200,1,2..."> is not terminal, which is <img src="https://latex.codecogs.com/png.latex?%5Calpha(1+d)%5E%7B-%5Cbeta%7D">, with <img src="https://latex.codecogs.com/png.latex?%5Calpha%20%5Cin(0,1)%20%5Ctext%20%7B%20and%20%7D%20%5Cbeta%20%5Cin%5B0,%20%5Cinfty)">. This <img src="https://latex.codecogs.com/png.latex?%5Calpha"> controls how likely a terminal node is to be split, with smaller values indicating a lower likelihood of split, and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> controls the number of terminal nodes, larger <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> implying more nodes. The second prior on the trees is on the distribution used to choose which covariate is split on. The final prior on the trees is on the value of the chosen splitting covariate at which to split. For both these later parameters, a common choice (and the one dbarts makes) is a simple discrete uniform distribution.</p>
<p>This set of priors also requires choosing the <img src="https://latex.codecogs.com/png.latex?m,%20%5Calpha,%20%5Cbeta,%20%5Cmu_%7B%5Cmu%7D,%20%5Csigma,%20%5Cnu"> and <img src="https://latex.codecogs.com/png.latex?%5Clambda"> hyperparameters, which can be chosen via cross-validation or simply set to defaults. In general, the past literature on BART finds that the defaults developed by Mculloch work quite well in a surprisingly large number of contexts <a href="https://deepblue.lib.umich.edu/handle/2027.42/147594">(Tan, 2018)</a>. More specifically in the MRP context, both <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12361">Montgomery &amp; Olivella (2018)</a> and <a href="https://www.cambridge.org/core/journals/american-political-science-review/article/barp-improving-mister-p-using-bayesian-additive-regression-trees/630866EB47F9366EDB3C22CFD951BB6F">Bisbee (2019)</a> found little reason to utilize non-default hyperparameter choices after reasonable search. For completeness, however, I ran a small number of hyperparameter searches on my complete records data, as recommended by the author of the <a href="https://cran.r-project.org/web/packages/dbarts/index.html">dbarts</a> package. Similar to prior work, I found little reason to diverge from the defaults suggested by Chipman et al, and implemented in dbarts, although I did ultimately go with <img src="https://latex.codecogs.com/png.latex?m%20=%20200"> trees as Chipman et al.&nbsp;suggest, not the <img src="https://latex.codecogs.com/png.latex?m%20=%2075"> default in dbarts. For a full derivation of these choices and their resultant properties, see <a href="https://arxiv.org/abs/0806.3286">Chipman et al.&nbsp;(2008)</a> or <a href="https://deepblue.lib.umich.edu/handle/2027.42/147594">(Tan, 2018)</a>.</p>
<p>A final modification of this formulation of BART is needed for binary outcomes. For binary outcomes, BART uses the probit link function to model the relationship between <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?y">:</p>
<p><img src="https://latex.codecogs.com/png.latex?P%5Cleft(Y_%7Bk%7D=1%20%7C%20%5Cmathbf%7BX%7D_%7Bk%7D%5Cright)=%5CPhi%5Cleft%5BG%5Cleft(%5Cmathbf%7BX%7D_%7Bk%7D%5Cright)%5Cright%5D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5CPhi%5B.%5D"> is the cumulative distribution function of a standard normal distribution, and <img src="https://latex.codecogs.com/png.latex?G"> is the full BART model we saw earlier. This slightly modifies steps for drawing from the posterior distribution, as discussed further in <a href="https://arxiv.org/abs/0806.3286">Chipman et al.&nbsp;(2008)</a>.</p>
</section>
<section id="bart-vi" class="level2">
<h2 class="anchored" data-anchor-id="bart-vi">BART-vi</h2>
<p>To the best of my knowledge, no prior work has utilized BART with varying intercepts (BART-vi) in the MRP framework. Given the huge amount of prior work on MRP that leverages a large set of varying intercepts, this seems like a natural extension. This modifies the BART predictor for binary outcomes to</p>
<p><img src="https://latex.codecogs.com/png.latex?P%5Cleft(Y_%7Bk%7D=1%20%7C%20%5Cmathbf%7BX%7D_%7Bk%7D%5Cright)=%5CPhi%5Cleft%5BG%5Cleft(%5Cmathbf%7BX%7D_%7Bk%7D%5Cright)%20+%20%5Calpha_%7Bk%7D%5Cright%5D"></p>
<p>with <img src="https://latex.codecogs.com/png.latex?a_%7Bk%7D%20%5Csim%20N%5Cleft(0,%20%5Ctau%5E%7B2%7D%5Cright)">. Critically, this also removes the varying intercept variable from the choice of possible features to split on, modeling it purely as a varying intercept. Given that the <a href="https://cran.r-project.org/web/packages/dbarts/index.html">dbarts</a> package which I use currently only supports 1 varying intercept, the natural choice is the state variable, as it both has the most categories and is one of the original motivations for varying intercepts in MRP work. All the old priors and hyperparameters remain the same, and dbarts places an additional cauchy prior on <img src="https://latex.codecogs.com/png.latex?%5Ctau%5E%7B2%7D">. While this cauchy prior is much less informative than the half-t, half-normal, or other priors typically used for MRP, at this time it is not possible to modify the prior choice except to a gamma distribution which is also not ideal. Future work could consider fitting this type of model with the more informative priors favored by the MRP literature for random effects, although such an improvement would require a time investment in learning to modify the c++ codebase of dbarts.</p>
</section>
<section id="comparing-the-predictions-of-the-two" class="level2">
<h2 class="anchored" data-anchor-id="comparing-the-predictions-of-the-two">Comparing the predictions of the two</h2>
<p>I provide two forms of evaluation for BART-vi vs regular BART, a quantitative assessment based on 10-fold cross validation, and a graphical/qualitative comparison of state level estimates resulting from the two.</p>
<p>To test the performance of this modification, I fit BART with and without varying intercept on state to the same <img src="https://latex.codecogs.com/png.latex?m%20=%2050"> imputed<sup>1</sup> datasets, using 10-fold cross validation within each dataset. Overall, while both models are extremely accurate, the BART-vi model slightly outperforms the regular BART model without varying intercepts in terms of RMSE, MSE, and AUC on average. Of course, given that this is a test of predictive accuracy before the final poststratification, this isn’t a full validation of BART-vi’s predictive superiority in the MRP context. However, this is consistent with <a href="https://deepblue.lib.umich.edu/handle/2027.42/147594">(Tan, 2018)</a>’s result in a more extensive set of simulation studies that there are small gains in accuracy to be had with BART-vi when random effects are used with an appropriate grouping variable. To make such a comparison completely rigorously, one would need to fit both types of models on a dataset with a ground truth such as vote share, poststratify, and then contrast their properties relative to that ground truth, not simply compare predictive accuracy on the initial set of surveys. However, as this is not possible for the border wall question, I take this as a rough suggestion that BART-vi may preform better in my context, and possibly in others.</p>
<p>Plotting a comparison of the state-level median prediction from the two models after poststratification shows a familiar pattern of pooling. The BART-vi estimates are pulled somewhat towards the grand mean, whereas the ones without varying intercepts are a bit more spread out. Note, however, that we don’t see the sort of <a href="https://twitter.com/rlmcelreath/status/878268413952634880/photo/1">idealized pooling</a> trend often shown in textbook examples of multilevel models, with non-multilevel predictions that are uniformly higher above the grand mean and uniformly lower below it compared to the multilevel predictions. This is due to the simple BART model modeling much more complex interactions based on the state variables than a single level regression.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/BART VI/bart-compare.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">BART models with and without pooling</figcaption><p></p>
</figure>
</div>
<p>One particularly interesting qualitative example to illustrate the differences between the models is that of DC, which is both a small state, and an incredibly liberal one. This presents a dilemma from the perspective of varying intercepts pooling: on the one hand, with only 94 observations in the full data, we should want some pooling on DC’s estimate. On the other, DC genuinely is exceptionally liberal, which suggests that pooling it too much could hurt predictive performance. While both already somewhat regularize the 13% raw approval for the wall in our aggregated polls, BART-vi does so much more. Thus, while the average predictions are of higher quality with BART-vi, the DC and other extreme state predictions are superior without random effects. Most prior MRP work has been happy to make this sort of trade off, and based on the rough accuracy comparisons I’ve made, this appears to work well for my data and my BART model as well.</p>
<p>Comparing the full posterior distributions of the two models below, we can also see BART-vi has noticably wider 50 and 90% intervals as well (the dot indicates the median, the thick bar is the 50% interval, and the thinnest bar is the 90% one). Like with my CV testing, a complete sense of which level of uncertainty provided here is appropriate will have to wait for future MRP work that leverages data with a ground truth. However, in many cases, the fixed effects intervals border on what I’d call concerningly small- I wouldn’t be suprised if the coverage properties of the BART-vi intervals are better.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/BART VI/full-post.jpeg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Full Posterior of the Two Models</figcaption><p></p>
</figure>
</div>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next steps</h2>
<p>Given that my work shows BART-vi having some desirable properties for RRP, what might be some extensions to explore next?</p>
<p>A first obvious step might be explore this type of model on data where we do have a ground truth like voter turnout, or vote share. For a more extensive comparison, one could leverage Bisbee (2019)’s replication data, which would hopefully provide a more complete answer to whether this strategy works well in general.</p>
<p>Probably the most theoretically interesting question would be how to handle the possibility of multiple random intercepts, if dbarts or another package eventually implements them. This represents a tradeoff between the benefits of flexible Bayesian non-parametrics in BART, and the pooling behavior of varying intercepts. Initially, I thought it was entirely feasible that the BART-vi I fit would have worse predictive accuracy, given the potential benefits of splitting on state. However, given that I utilize both 2012 vote share and region as predictors, it seems that the model still had ample state level information. However, as we pooled across more variables, this would increasingly weaken the non-parametric component of the BART-vi model. In this scenario, would pooling across demographic predictors that have many fewer categories make sense? While future work will have to tell, my guess is that the answer might be that only state or other geographic variables benefit from pooling.</p>
<hr>
<p><a name="imputationnote">1</a>: Given my data had a relatively large proportion of respondents who refused to answer at least 1 demographic question (10.54%), I also explored imputing the missing characteristics using a variety of different approaches. The full details of that are coming in another post, but I ran 10-fold CV on the imputations so that the evaluation would more fully mirror my final modeling scenario.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2019,
  author = {Andy Timm},
  title = {BART with Varying Intercepts in the {MRP} Framework},
  date = {2019-07-03},
  url = {https://andytimm.github.io/2020-03-06-BART-vi.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2019" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2019. <span>“BART with Varying Intercepts in the MRP
Framework.”</span> July 3, 2019. <a href="https://andytimm.github.io/2020-03-06-BART-vi.html">https://andytimm.github.io/2020-03-06-BART-vi.html</a>.
</div></div></section></div> ]]></description>
  <category>From Old Website</category>
  <category>MRP</category>
  <category>BART</category>
  <guid>https://andytimm.github.io/posts/BART VI/2020-03-06-BART-vi.html</guid>
  <pubDate>Wed, 03 Jul 2019 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Convention Prediction with a Bayesian Hierarchical Multinomial Model</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Convention Model/2019-07-13-convention-model.html</link>
  <description><![CDATA[ 




<p>Here, I use a Bayesian hierarchical multinomial model to predict the first ballot results at the 2018 DFL (Democratic) State Convention, with data aggregated to the Party Unit level (ex: State Senate district) to guarantee anonymity. While using aggregated data obviously isn’t ideal, this sort of strategy shows a lot of promise, especially if individual level predictors could be harnessed as another level of the hierarchical model. As it stands, this is mostly a proof of concept for Bayesian hierarchical models in this context. To use something like this in practice, one could use prior predictive simulation to game out the convention under various assumptions, or condition on the first ballot data and use it to analyze trends in support and predict subsequent ballots as your floor team collects further data.</p>
<p>At this convention, I was working for Erin Murphy (who ultimately won) on her data team, and so I have access to their database on the Delegates heading into the convention. A winner is declared if any candidate reaches 60% of the Delegate pool of 1307.5 Delegates, so 785 votes. For simplicity, I focus in this project solely on predicting the first ballot results.</p>
<!--more-->
<p>Briefly, my goals with this project are:</p>
<ol type="1">
<li>Represent my pre-convention uncertainty about outcomes given the data we have intelligently.</li>
<li>See how much we can improve predictions by exploiting the hierarchical nature of districts (Party Units within Congressional Districts).</li>
<li>See if this makes sense as a modeling strategy to keep developing by adding the individual level data.</li>
</ol>
<section id="the-dataset" class="level1">
<h1>The Dataset</h1>
<p>For each of the non-empty 121 Party Units in Minnesota, my response is the number of Delegate that each candidate (Erin Murphy, Rebecca Otto, and Tim Walz) received, along with a count for “No Endorsement” voters. Thus, summing across the 121 PUs would give the full first ballot results by candidate. I thus model these using a <strong>multinomial logit model</strong> in brms.</p>
<p>In terms of predictors, we have the Congressional district each party unit is in, which should explain some variation, as Otto/Walz are generally perceived to be more appealing to rural voters, while Murphy was from the Twin Cities. I also have the estimated proportion of Delegates the Murphy campaign believed they had the support of in each Party Unit, based on their field campaign, and on the subcaucuses the Delegates were elected out of. These proportions turned out to be quite accurate, and so my assumption is they’ll be strong predictors. Using the delegate level data (including issue and candidate IDs, subcaucuses, and voter file information like gender and age) would no doubt significantly improve things.</p>
<p>I exclude the data cleaning here, but it’s available in the .Rmd on my <a href="https://github.com/andytimm/ConventionPrediction/blob/master/Convention_Prediction_Final.Rmd">github</a>. One non-obvious transformation I make is to double all counts before modeling, but halve them before analysis, as some rural, low population areas are awarded “half delegates”, and I need integer outcomes to work with a multinomial model.</p>
</section>
<section id="prior-predictive-simulation-intercept-only" class="level1">
<h1>Prior Predictive Simulation: Intercept Only</h1>
<p>I start with an intercept only model, and plot realizations of the first ballot across draws. This helps explain my level of uncertainty before we include predictors and condition on the data. To keep this post short, I only include the final result of iterating to find suitably cautious priors at this early stage, not as I add further predictors to the model. With a multinomial model like this, even just enforcing the count constraint (vote counts in each party unit have to add to their total allocation of delegates) already produces a suprisingly reasonable model.</p>
<p>For context on these priors, there was a relatively large amount of uncertainty for our campaign and all the campaigns heading into first ballot for a variety of reasons. First, we had only ID’d about 2/3 of the delegate body by first ballot. Second, it was becoming increasingly clear that Rebecca Otto didn’t have the delegates to win the convention, so it was possible we’d see a decent portion of her delegates switch sides even before first ballot. Finally, a major statewide c4, ISAIAH, had been telling their delegates to hold off on committing, but the rumor was that they were going to endorse Erin Murphy’s or Otto’s campaign (whichever progressive was more viable), so a large number of Delegates had a preference they didn’t openly state.</p>
<p>All that said, while it was highly unlikely that anyone was going to reach a winning 60% of the delegate pool (785 Delegates) on the first ballot, I did want at least a bit of probability on those outcomes. From our ID data, it looked something like 40%-20%-40% was the most likely outcome for the first ballot, which is how I set the means.</p>
<p>As a final note, voting “No Endorsement” on an early ballot is extremely rare, which is correctly reflected.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Base class is No-Endorsement</span></span>
<span id="cb1-2">initial_prior <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Intercept"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dpar =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mu2"</span>),</span>
<span id="cb1-3">              <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Intercept"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dpar =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mu3"</span>),</span>
<span id="cb1-4">              <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Intercept"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dpar =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mu4"</span>)</span>
<span id="cb1-5">              )</span>
<span id="cb1-6"></span>
<span id="cb1-7">int_only <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">brm</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bf</span>(y <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">trials</span>(Total) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family=</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">multinomial</span>(),<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data=</span>erin_data, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior =</span> initial_prior, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">sample_prior=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"only"</span>)</span>
<span id="cb1-8"></span>
<span id="cb1-9">linpred <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">posterior_linpred</span>(int_only, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">transform =</span> T)</span>
<span id="cb1-10"></span>
<span id="cb1-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Divide by 2 after summing the totals to put back on original delegate count scale</span></span>
<span id="cb1-12"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">boxplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">apply</span>(linpred, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">FUN =</span> sumdiv2), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">pch =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"."</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">las =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb1-13">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">names =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"No Endorse"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Erin Murphy"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Rebeca Otto"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Tim Walz"</span>))</span></code></pre></div>
<p><img src="https://andytimm.github.io/posts/Convention Model/unnamed-chunk-2-1.png" class="img-fluid" alt="first test"><!-- --></p>
</section>
<section id="initial-posterior" class="level1">
<h1>Initial Posterior</h1>
<p>Now let’s add in our non-CD predictors and condition on our data. I add a <img src="https://latex.codecogs.com/png.latex?N(0,3)"> prior over all the <img src="https://latex.codecogs.com/png.latex?%5Cbeta">’s as a weakly informative prior to help the model fit. The model fits well; there are no divergences, all <img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D"> were 1, and I got a good number of effective samples for each parameter.</p>
<p>As we’d expect with so little data, the standard errors are very large compared to the coefficients, but generally point the right ways- the Strong/Lean Erin predictions have a positive influence on Erin’s support (mu2), for example, and similar with Walz (mu4), and Otto (mu2). Later, when I look at marginal plots, I’ll talk more about some of the more interesting coefficients, namely the ISAIAH and Unknown ones.</p>
<pre><code>##  Family: multinomial
##   Links: mu2 = logit; mu3 = logit; mu4 = logit
## Formula: y | trials(Total) ~ ISAIAH + Lean.Erin + Lean.Walz + Lean.Otto + Strong.Erin + Strong.Otto + Strong.Walz + Undecided + Unknown
##    Data: erin_data (Number of observations: 121)
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
##
## Population-Level Effects:
##                 Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## mu2_Intercept       4.15      1.07     2.07     6.27       4931 1.00
## mu3_Intercept       3.40      1.06     1.40     5.51       5520 1.00
## mu4_Intercept       4.19      1.03     2.14     6.23       5352 1.00
## mu2_ISAIAH          1.59      1.68    -1.71     4.96       5557 1.00
## mu2_Lean.Erin       1.00      1.95    -2.92     4.80       5198 1.00
## mu2_Lean.Walz       0.24      1.99    -3.72     4.10       6179 1.00
## mu2_Lean.Otto      -1.74      2.08    -5.67     2.44       5975 1.00
## mu2_Strong.Erin     2.18      1.69    -1.17     5.52       5448 1.00
## mu2_Strong.Otto    -1.91      1.71    -5.23     1.54       6238 1.00
## mu2_Strong.Walz    -1.70      1.65    -5.03     1.63       5958 1.00
## mu2_Undecided      -0.53      1.78    -4.15     3.03       5513 1.00
## mu2_Unknown         0.11      1.55    -2.94     3.17       5267 1.00
## mu3_ISAIAH         -0.64      1.74    -3.97     2.82       5673 1.00
## mu3_Lean.Erin       0.73      2.03    -3.34     4.69       4694 1.00
## mu3_Lean.Walz      -1.27      2.10    -5.35     2.89       6218 1.00
## mu3_Lean.Otto       3.26      2.14    -0.99     7.40       5679 1.00
## mu3_Strong.Erin    -1.22      1.68    -4.51     2.08       5648 1.00
## mu3_Strong.Otto     4.28      1.70     0.94     7.74       5737 1.00
## mu3_Strong.Walz    -2.46      1.66    -5.60     0.85       6205 1.00
## mu3_Undecided       0.63      1.74    -2.81     4.06       5264 1.00
## mu3_Unknown         0.42      1.56    -2.56     3.45       6235 1.00
## mu4_ISAIAH         -1.86      1.68    -5.10     1.57       5066 1.00
## mu4_Lean.Erin      -0.39      1.97    -4.21     3.44       5386 1.00
## mu4_Lean.Walz       1.51      1.98    -2.32     5.48       6163 1.00
## mu4_Lean.Otto      -1.28      2.09    -5.41     2.80       6302 1.00
## mu4_Strong.Erin    -1.48      1.68    -4.67     1.90       5654 1.00
## mu4_Strong.Otto    -2.00      1.70    -5.33     1.47       6029 1.00
## mu4_Strong.Walz     1.46      1.60    -1.65     4.61       5859 1.00
## mu4_Undecided       0.19      1.72    -3.22     3.59       5522 1.00
## mu4_Unknown         1.08      1.51    -1.94     4.07       6072 1.00
##
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample
## is a crude measure of effective sample size, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</section>
<section id="nesting-the-party-units-in-cd" class="level1">
<h1>Nesting the Party Units in CD</h1>
<p>A next logical step for the model would be to incorporate the hierarchical structure present in the data- Party Units nested within Congressional Districts. Given that the CD’s reflect both the progressive/moderate and rural/urban divides that defined the election, expecting some significant between group variation is reasonable.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">final_prior <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Intercept"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dpar =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mu2"</span>),</span>
<span id="cb3-2">              <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Intercept"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dpar =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mu3"</span>),</span>
<span id="cb3-3">              <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Intercept"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dpar =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mu4"</span>),</span>
<span id="cb3-4">              <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>),<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"b"</span>),</span>
<span id="cb3-5">              <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sd"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">group =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"CD"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dpar =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mu2"</span>),</span>
<span id="cb3-6">              <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sd"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">group =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"CD"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dpar =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mu3"</span>),</span>
<span id="cb3-7">              <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sd"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">group =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"CD"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dpar =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mu4"</span>)</span>
<span id="cb3-8">              )</span>
<span id="cb3-9"></span>
<span id="cb3-10">full_model <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">brm</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bf</span>(y <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">trials</span>(Total) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> ISAIAH <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Lean.Erin <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Lean.Walz <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Lean.Otto <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Strong.Erin <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Strong.Otto <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb3-11">        Strong.Walz <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Undecided <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Unknown <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span>CD)), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family=</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">multinomial</span>(),<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior =</span> final_prior, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data=</span>erin_data)</span></code></pre></div>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summary</span>(full_model)</span></code></pre></div>
<pre><code>##  Family: multinomial
##   Links: mu2 = logit; mu3 = logit; mu4 = logit
## Formula: y | trials(Total) ~ ISAIAH + Lean.Erin + Lean.Walz + Lean.Otto + Strong.Erin + Strong.Otto + Strong.Walz + Undecided + Unknown + (1 | CD)
##    Data: erin_data (Number of observations: 121)
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
##
## Group-Level Effects:
## ~CD (Number of levels: 9)
##                   Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sd(mu2_Intercept)     0.08      0.06     0.00     0.23       2406 1.00
## sd(mu3_Intercept)     0.14      0.09     0.01     0.35       1760 1.00
## sd(mu4_Intercept)     0.09      0.07     0.00     0.24       2311 1.00
##
## Population-Level Effects:
##                 Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## mu2_Intercept       4.14      1.08     2.03     6.29       2895 1.00
## mu3_Intercept       3.33      1.09     1.24     5.46       2566 1.00
## mu4_Intercept       4.27      1.07     2.21     6.36       2919 1.00
## mu2_ISAIAH          1.69      1.66    -1.42     5.00       3289 1.00
## mu2_Lean.Erin       1.07      1.95    -2.71     4.96       3678 1.00
## mu2_Lean.Walz       0.17      2.06    -3.81     4.18       3957 1.00
## mu2_Lean.Otto      -1.38      2.10    -5.57     2.70       3972 1.00
## mu2_Strong.Erin     2.17      1.65    -1.03     5.52       3294 1.00
## mu2_Strong.Otto    -1.84      1.73    -5.14     1.55       3077 1.00
## mu2_Strong.Walz    -1.72      1.62    -4.81     1.40       3741 1.00
## mu2_Undecided      -0.52      1.80    -4.01     3.13       2834 1.00
## mu2_Unknown         0.11      1.53    -2.93     3.17       3079 1.00
## mu3_ISAIAH         -0.70      1.73    -4.01     2.66       2931 1.00
## mu3_Lean.Erin       0.68      2.05    -3.29     4.62       4101 1.00
## mu3_Lean.Walz      -1.01      2.11    -5.12     3.11       4077 1.00
## mu3_Lean.Otto       3.16      2.17    -1.11     7.43       4086 1.00
## mu3_Strong.Erin    -1.15      1.69    -4.40     2.23       3109 1.00
## mu3_Strong.Otto     4.15      1.76     0.80     7.64       2990 1.00
## mu3_Strong.Walz    -2.19      1.62    -5.38     0.99       3156 1.00
## mu3_Undecided       0.62      1.83    -2.95     4.16       3098 1.00
## mu3_Unknown         0.47      1.55    -2.56     3.48       2478 1.00
## mu4_ISAIAH         -1.91      1.73    -5.23     1.47       3164 1.00
## mu4_Lean.Erin      -0.42      1.98    -4.21     3.45       3632 1.00
## mu4_Lean.Walz       1.34      2.02    -2.61     5.35       3723 1.00
## mu4_Lean.Otto      -1.47      2.09    -5.46     2.66       3452 1.00
## mu4_Strong.Erin    -1.57      1.63    -4.74     1.57       3367 1.00
## mu4_Strong.Otto    -1.91      1.76    -5.34     1.64       2914 1.00
## mu4_Strong.Walz     1.38      1.58    -1.84     4.50       3574 1.00
## mu4_Undecided      -0.01      1.80    -3.44     3.57       3232 1.00
## mu4_Unknown         0.99      1.54    -2.00     3.99       3137 1.00
##
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample
## is a crude measure of effective sample size, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</section>
<section id="model-comparison" class="level1">
<h1>Model Comparison</h1>
<p>Intuitively, a model with pooling across CD’s should outperform a single level model using them, but let’s make sure. Below, the multilevel level model far outperforms the single level one, with it’s ELPD more than 2 standard errors better.</p>
<p>One limitation of the analysis below is that I wasn’t able refit without the final observation (the super delegates) to calculate ELPDs for the super delegates directly. This is because the “Super” level only exists in 1 example, and the loo package doesn’t allow new factor levels- holding it out would thus throw an error.</p>
<pre><code>##                  elpd_diff se_diff
## full_model         0.0       0.0
## not_pooled_model -10.7       4.9

## Method: stacking
## ------
##        weight
## model1 1.000
## model2 0.000</code></pre>
</section>
<section id="interesting-marginal-plots" class="level1">
<h1>Interesting Marginal Plots</h1>
<p>For the most part, the marginal plots were what I’d expect- for instance, PUs with a greater estimated strong support for Erin (“Strong Erin”), had increasingly greater support for Erin, and vice versa for Walz, and so I don’t show most of these.</p>
<p>Even though the predictions are obviously very noisy however, they did pick up on two important, more subtle trends. First, in the ISAIAH plot below, it’s beginning to appear that Murphy (2) does well in places with many ISAIAH delegates, whereas Walz (4) does progressively worse.</p>
<p><img src="https://andytimm.github.io/posts/Convention Model/unnamed-chunk-6-1.png" class="img-fluid"><!-- --></p>
<p>The other interesting trend the model picked up on was that it tends to be harder to ID your opponent’s supporters than your own- as your supporters want to contact and work with the campaign, but the opponents’ have no reason to do so, and help their candidate by not giving you much information. This is correctly reflected in the negative slope in Unknown for Erin (2), but positive one for Walz (4), given the predictor data comes from the Murphy campaign.</p>
<p>While these plots suggest a understandably high level of uncertainty, the fact that they’re correctly reflecting many of the relationships I believe to be true offers some level of face validity of the model.</p>
<p><img src="https://andytimm.github.io/posts/Convention Model/unnamed-chunk-7-1.png" class="img-fluid" alt="Marginal_Plot"><!-- --></p>
</section>
<section id="posterior-predictive-check" class="level1">
<h1>Posterior Predictive Check</h1>
<p>Plotting the predictions for Erin Murphy (blue) and Tim Walz (red) against the actual results, we can see the predictions track reasonably closely considering the limited data. While many point predictions fall outside the 25-75 quantile range, the vast majority stay within the boxplot’s whiskers. Again, stressing the limitations of the small dataset we have, this is a fairly reasonable range of outcomes to predict, and there’s no systematic pattern I can see to which Party Units the model struggles with.</p>
<p><img src="https://andytimm.github.io/posts/Convention Model/unnamed-chunk-8-1.png" class="img-fluid" alt="Murphy"><!-- --><img src="https://andytimm.github.io/posts/Convention Model/unnamed-chunk-8-2.png" class="img-fluid" alt="Walz"><!-- --></p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>To actually use a model like this on a convention floor, I’d definitely want to fully incorporate the individual level data that underlie what’s shown here, as attempting to model with 1 obs/district is challenging. However, this initial model is fairly promising- nesting PUs within CDs seems like a strong overall strategy, and even with limited data, the model already can pick up on relationships like that between estimated ISAIAH and Unknown support and Delegate returns. Some further details can be found in the .rmd <a href="https://github.com/andytimm/ConventionPrediction/blob/master/Convention_Prediction_Final.Rmd">here</a>.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2019,
  author = {Andy Timm},
  title = {Convention {Prediction} with a {Bayesian} {Hierarchical}
    {Multinomial} {Model}},
  date = {2019-07-03},
  url = {https://andytimm.github.io/2019-07-13-convention-model.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2019" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2019. <span>“Convention Prediction with a Bayesian
Hierarchical Multinomial Model.”</span> July 3, 2019. <a href="https://andytimm.github.io/2019-07-13-convention-model.html">https://andytimm.github.io/2019-07-13-convention-model.html</a>.
</div></div></section></div> ]]></description>
  <category>From Old Website</category>
  <category>Stan</category>
  <guid>https://andytimm.github.io/posts/Convention Model/2019-07-13-convention-model.html</guid>
  <pubDate>Wed, 03 Jul 2019 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Is Voting Habit Forming? Replication, and additional robustness checks</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html</link>
  <description><![CDATA[ 




<p>This post walks through my replication of the fuzzy regression discontinuity portion of Coppock and Green’s 2016 paper <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12210">Is Voting Habit Forming? New Evidence from Experiments and Regression Discontinuities</a>, and details some additional robustness checks I conducted. While I was able to reproduce all of their estimates fairly easily due to great <a href="http://dx.doi.org/10.7910/DVN/ALZVAW">replication materials</a>, my additional robustness checks suggest that their results are more sensitive to bandwith choices than their testing suggests. Additionally, Coppock and Green argue the effects they find are likely due to habit alone, whereas I’m unconvinced that’s the sole mechanism involved. This is my work from Jennifer Hill and Joe Robinson-Cimpian’s Causal Inference class at NYU, and I’m grateful for both their feedback on the project.</p>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>When we first vote in an election, is that experience habit forming? If so, how strong is the effect of habit? This is a critical question of voting behavior. For example, it has implications for attempting to make the electorate more representative of the general population. If people continue to vote at a reasonable rate after their first ballot cast, then the burden of organizations doing GOTV work gets comparatively lighter; further, their work has dividends for elections to come. On the extreme other end, if almost no habit forming effect existed, then turnout work would be a project of individual races. Fortunately, practical experience from campaigns suggests there’s likely at least some habit effect. After all, the best predictor of voting in the current election is voting in the prior.</p>
<p>If we believe that there may be a habit effect on voting in an initial in future elections, how can we estimate a causal effect? Of course, we cannot use a randomized experiment: we cannot ethically randomize some citizens to vote, or more concerningly, randomize some citizens to not do so. Thus, the main observational strategies used to estimate such causal effects involve instrumental variable approaches, utilizing randomized experiments in the “upstream” election which attempt to mobilize voters in the treatment group as an instrument to estimate the Complier Average Causal Effect (CACE) in subsequent “downstream” elections. For example, <a href="https://isps.yale.edu/research/publications/isps12-024">Bedolla and Michelson (2012)</a>, utilizing a series of experiments that aimed to improve turnout in minority voters in California, find overall that voting in the upstream election results in a 23-percentage point increase in probability to vote in subsequent elections for compliers. However, these designs are frequently limited by somewhat weak instruments and low overall sample sizes, as increasing turnout through campaign intervention is extremely difficult and expensive per voter reached, especially in non-white and younger populations <a href="https://www.brookings.edu/book/get-out-the-vote-2/">(Gerber &amp; Green, 2012)</a>. The weakness of these instruments has prompted recent research using fuzzy regression discontinuity (FRD) designs, leveraging the fact that being just barely 18 or just too young to vote on the upstream election day should set voters on very different voting trajectories if such a habit effect exists <a href="https://www.sas.upenn.edu/~marcmere/workingpapers/PersistenceParticipation.pdf">(Meredith 2009</a>; <a href="https://www.tandfonline.com/doi/full/10.1080/17457289.2012.718280">Dinas (2012)</a>.</p>
<p>The latest and most comprehensive such fuzzy regression discontinuity paper is Coppock and Green’s 2016 paper <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12210">Is Voting Habit Forming? New Evidence from Experiments and Regression Discontinuities</a>, which considerably expands both the amount of data used and sophistication of modeling design used. Beyond just using the FRD design, they also incorporate results from several instrumental variable approaches in their very impressive paper.</p>
</section>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">Data</h2>
<p>States are required by the Help America Vote Act to make available to the public individual level data on every registered voter in their state, although states vary considerably in how much information about each voter they provide. For example, some states provide complete histories of which election a voter has participated in, whereas others provide only the most recent 8 elections. Similarly, some states provide birthdate, whereas others provide only age, or age buckets.</p>
<p>Given that the analysis they propose requires precise voting histories and a specific birthdate, Coppock and Green gathered the full voter file in 2013 from the 15 states where all three of</p>
<ol type="1">
<li>a complete history of which elections the individual had voted in,</li>
<li>information on that voter’s eligibility to vote (to rule out the otherwise ineligible such as felons), and</li>
<li>birthdate were available.</li>
</ol>
<p>In the replication file, these 15 voter files have been grouped by birthdate cohort and state, so that the unit of analysis is a group such as registered voters born on 11/6/1998, in Arkansas. This grouping sidesteps the potential problems arising from the fact the voter file only includes people registered to vote. While there is not a complete list of just eligible and just ineligible 17 and 18-year olds, through this cohort grouping, we can work with the 2008/2012 votes cast by cohorts above and below the eligibility threshold instead. Unfortunately, this also adds some additional complexity to interpreting results, as most potential violations of the fuzzy regression discontinuity design assumptions would occur at the individual level. Further, demographic profiles of the cohorts aren’t available, limiting our set of possible confounders to work with. The full dataset has 172,616 state and birthdate state cohorts, but for the purpose of my analysis, I work with the 11,680 birthdate state cohorts whose birthday fall within 365 days of eligibility to vote in the 2008 presidential election.</p>
<p>My outcome of interest is the number of votes cast in the downstream election, the 2012 presidential general election. The “treatment” is having voted in the upstream 2008 presidential election. The instrument is eligibility to participate in the 2008 election, which is determined by being 18 on election day, not 18 by the registration deadline as is sometimes commonly believed. The eligibility criteria being uniform across states greatly simplifies generating the remaining variables used. Due to this uniformity, the forcing variable is simply the cohort’s number of days above or below turning 18 on the upstream election day. Finally, to account for seasonal and day of the week birth trends which subsequently influence total votes cast by each cohort, Coppock and Green include a lagged downstream vote total for the birthdate cohort one year older. As an illustration of these quantities, here is an example row:</p>
<table class="table">
<colgroup>
<col style="width: 18%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 19%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Cohort</th>
<th>2008 Vote Total</th>
<th>2012 Vote Total</th>
<th>2008 Eligible?</th>
<th>Days to Eligible</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1988-11-06-AR</td>
<td>20</td>
<td>27</td>
<td>Yes</td>
<td>-364</td>
</tr>
</tbody>
</table>
<p>To show variation between states, I present below the total number of votes cast in each state for 2008 and 2012. As we’d expect with half the group ineligible in 2008, the vote totals rise considerably for 2012. The variation in state population carries through to the number of votes cast in each state in the sample, which will later influence the standard errors we are able to achieve. Overall, however, this is quite a large sample, both in the number of state-birthdate cohorts (11,680), and in total 2008/2012 votes cast (532,459 and 961,894 respectively).</p>
<table class="table">
<thead>
<tr class="header">
<th><strong>State</strong></th>
<th><strong>Sum 2008</strong></th>
<th><strong>Sum 2012</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AR</td>
<td>11,796</td>
<td>20,915</td>
</tr>
<tr class="even">
<td>CT</td>
<td>22,475</td>
<td>33,040</td>
</tr>
<tr class="odd">
<td>FL</td>
<td>88,704</td>
<td>173,999</td>
</tr>
<tr class="even">
<td>IA</td>
<td>5,336</td>
<td>10,600</td>
</tr>
<tr class="odd">
<td>IL</td>
<td>59,149</td>
<td>108,721</td>
</tr>
<tr class="even">
<td>KY</td>
<td>22,017</td>
<td>40,946</td>
</tr>
<tr class="odd">
<td>MO</td>
<td>35,603</td>
<td>63,381</td>
</tr>
<tr class="even">
<td>MT</td>
<td>6,782</td>
<td>11,030</td>
</tr>
<tr class="odd">
<td>NJ</td>
<td>52,125</td>
<td>87,395</td>
</tr>
<tr class="even">
<td>NV</td>
<td>12,494</td>
<td>24,502</td>
</tr>
<tr class="odd">
<td>NY</td>
<td>80,180</td>
<td>150,489</td>
</tr>
<tr class="even">
<td>OK</td>
<td>15,608</td>
<td>23,565</td>
</tr>
<tr class="odd">
<td>OR</td>
<td>17,900</td>
<td>36,843</td>
</tr>
<tr class="even">
<td>PA</td>
<td>95,412</td>
<td>165,622</td>
</tr>
<tr class="odd">
<td>RI</td>
<td>6,878</td>
<td>10,846</td>
</tr>
</tbody>
</table>
</section>
<section id="estimand" class="level2">
<h2 class="anchored" data-anchor-id="estimand">Estimand</h2>
<p>Given this data, we can estimate a Complier Average Causal Effect (CACE). Define <img src="https://latex.codecogs.com/png.latex?D"> to be the number of votes cast in the 2008 election, and <img src="https://latex.codecogs.com/png.latex?Y"> to be votes in the 20212 election. As a reminder, one cannot assign <img src="https://latex.codecogs.com/png.latex?D">, you can only leverage the encouragement to do so created by being just eligible, which I label <img src="https://latex.codecogs.com/png.latex?Z%20%5Cin%20%5B0,1%5D">, with 0 being too young, and 1 being old enough to vote in 2008 respectively. The forcing variable, days above or below being old enough to vote in 2008, I label <img src="https://latex.codecogs.com/png.latex?T">. <em>Lagged</em> refers to lagged downstream vote total for the birthdate cohort one year older, used to eliminate other temporal trends. For individual birthdate cohorts, I use subscript <img src="https://latex.codecogs.com/png.latex?i">’s.</p>
<p>The estimand is thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Clim%20_%7BT%20%5Cdownarrow%200%7D%20%5Csum_%7B1%7D%5E%7BN%7D%5Cleft%5BY_%7Bi%7D%20%7C%20D_%7B1%20i%7D=1%5Cright%5D-%20%5Clim%20_%7BT%20%5Cuparrow%200%7D%20%5Csum_%7B1%7D%5E%7BN%7D%5Cleft%5BY_%7Bi%7D%20%7C%20D_%7B1%20i%7D=0%5Cright%5D"></p>
<p>This is the additional number of votes in 2012 we would expect across cohorts if all voters did vote in 2008, beyond the number if each cohort’s subjects had not voted in 2008, among only the compliers, subjects who vote if and only if they receive the “encouragement” of being eligible in 2008. Following Coppock and Green, I present these as expected proportions of increase, ie .06 or 6% increase in 2012 turnout, as this makes it easier to compare amongst states with different vote totals. This must be restricted to compliers only because they are the only group in our analysis whose behavior changes just above or below being 18 on election day. For example, the encouragement provided by becoming eligible does not influence the behavior of “never takers”, those who would never vote. Our encouragement cannot create a difference in votes cast for this group or another other that is not the compliers: being just above or below the eligibility threshold only potentially alters the observed outcome of the compliers.</p>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">Methods</h2>
<p>To build up to the full fuzzy regression discontinuity method for estimating the CACE which comprises both an instrumental variable approach and a regression discontinuity, it is helpful to start by first introducing the instrumental variable approach to estimating the CACE in a simpler scenario, after which I will describe how utilizing the regression discontinuity modifies the problem.</p>
<p>For the moment, imagine if instead of a regression discontinuity, the encouragement to vote in 2008, <img src="https://latex.codecogs.com/png.latex?Z">, was a nonpartisan mailer encouraging voting. Further, voters were randomized into a control and treatment group, with some voters receiving the piece, while the other received either nothing or a placebo mailer. This is a classical randomized experiment, and if assumptions of SUTVA and ignorability hold, this allows us to estimate an average treatment effect. To allow us to estimate the effect of voting in 2008 (<img src="https://latex.codecogs.com/png.latex?D">) on voting in 2012 (<img src="https://latex.codecogs.com/png.latex?Y">) using this simple random experiment in the first election as an instrument, however, we need 3 additional assumptions. First, we must assume that any effect of the experimental encouragement on <img src="https://latex.codecogs.com/png.latex?Y"> operates only through <img src="https://latex.codecogs.com/png.latex?D">, referred to as excludability. This would be violated if the nonpartisan mailer in 2008 had such a profound effect on a voter that it still influenced the voter 4 years later, in 2012. However, as most campaign interventions exhibit relatively quick decay in effectiveness (<a href="https://www.brookings.edu/book/get-out-the-vote-2/">Gerber &amp; Green, 2012</a>), it is reasonable to make this assumption.</p>
<p>Second, we need to assume that the treatment is effective, and convinces some subjects who would have not have voted otherwise to vote. In other words, the experiment needs to generate at least some compliers, and more would improve our ability to estimate the CACE. Multiple studies have shown that such mail pieces are effective (<a href="https://www.brookings.edu/book/get-out-the-vote-2/">Gerber &amp; Green, 2012</a>), although the effect sizes are relatively weak.</p>
<p>Third, we need to make an assumption that our experimental encouragement creates no defiers, called the monotonicity assumption. To define defiers, partition the subjects into 4 groups, based on their potential outcomes arising from receiving the mail piece or not. “Always Takers” vote whether they receive the mailer or not: both potential outcomes have them voting. “Never Takers” are the reverse: they never vote regardless of the mailer. Compliers, as already discussed, are those who vote if they receive the encouragement, but don’t otherwise. Defiers invert the mailer’s intended influence, and vote only if not encouraged, refusing to vote if they are sent the mail piece. While one can easily imagine a registered voter frustrated with inundation of political mail, it seems unlikely that a single mail piece would entirely invert a subject’s attitudes towards voting.</p>
<p>Given this strong set of assumptions, it is finally possible to estimate the effect of <img src="https://latex.codecogs.com/png.latex?D"> on <img src="https://latex.codecogs.com/png.latex?Y">, through two stage least squares. First, regressing <img src="https://latex.codecogs.com/png.latex?D"> on <img src="https://latex.codecogs.com/png.latex?Z"> gives <img src="https://latex.codecogs.com/png.latex?E%5BD%20%5Cvert%20Z%5D=%5Calpha_%7B0%7D+%5Calpha_%7B1%7D%20Z+%5Cvarepsilon">. Then, using the predictions from the first stage, regressing <img src="https://latex.codecogs.com/png.latex?Y"> on <img src="https://latex.codecogs.com/png.latex?D"> gives <img src="https://latex.codecogs.com/png.latex?E%5BY%20%5Cvert%20D%5D=%5Cbeta_%7B0%7D+%5Ctau%20D+%5Cvarepsilon">, with tau being the instrumental variable estimate of voting in 2008’s effect on voting in 2012 for the compliers. Intuitively, we are utilizing the variation in 2008 turnout induced by the random experiment to isolate downstream variation in turnout for the compliers. This can be extended to include further covariates, however we will wait until the full fuzzy RD design to illustrate this.</p>
<p>In the full fuzzy regression discontinuity design, instead of the treatment being a randomly assigned treatment, Coppock and Green leverage the variability created by the fact that around election day, some subjects were just slightly too young or just old enough to vote. Informally, we replace the random assignment of an experiment in the upstream year with the as-if-random assignment of young voters near the age threshold to either ineligibility or eligibility to vote in 2008.</p>
<p>More formally, instead of the type of ignorability assumption of the random experiment, we require ignorability conditional on covariates within some bandwith of the eligibility cutoff, <img src="https://latex.codecogs.com/png.latex?Y(1),%20Y(0)%20%5Cperp%20Z%20%5Cvert%20x,%20x%20%5Cin(C-a,%20C+a)">. Also, we require that the eligibility cutoff and birthdates be determined independently of one another.</p>
<p>Finally, we must be able to model the two outcomes <img src="https://latex.codecogs.com/png.latex?Y(1)%20%5Cvert%20x,%20Y(0)%5Cvert%20x"> accurately in this region. One major challenge in modeling these are questions of how much of the data to use for estimation. While we can only estimate a causal effect right at the threshold (the “jump” created by being too young or just old enough), estimating the effects at that point can use different widths of data around the disconintuity.</p>
<p>Leaving further discussion of whether these assumptions are reasonable with Coppock and Green’s data to the next section, we can use a similar set of 2SLS regressions to estimate:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A&amp;D=%5Calpha_%7B0%7D+%5Calpha_%7B1%7D%20Z+%5Calpha_%7B2%7D%20T+%5Calpha_%7B3%7D%20T%20*%20Z+%5Calpha_%7B4%7D%20%5Ctext%20%7BLagged%7D+%5Cvarepsilon%5C%5C%0A&amp;Y=%5Cbeta_%7B0%7D+%5Cbeta_%7B1%7D%20D+%5Cbeta_%7B2%7D%20T+%5Cbeta_%7B3%7D%20T%20*%20D+%5Cbeta_%7B4%7D%20%5Ctext%20%7BLagged%7D+%5Cvarepsilon%0A%5Cend%7Baligned%7D"></p>
<p>With our fuzzy regression discontinuity estimate of the CACE as <img src="https://latex.codecogs.com/png.latex?%5Cbeta_%7B1%7D">, which is calculated separately for each state. <img src="https://latex.codecogs.com/png.latex?T"> is centered at 0, removing the need to complicate this definition by including the interaction term.</p>
<p>What properties does this CACE have? Given that is calculated using only the compliers not the full sample, its standard errors are much larger than the ITT ones we might expect from a simple random experiment, if we could run one in this scenario. The CACE also encompasses the full causal process that is set in motion by the upstream election – even effects resulting from voting in intermediate elections, or possible additional campaign outreach due to voting in 2008, although we cannot tease such effects apart. Despite these problems, the CACE from this fuzzy regression discontinuity has the potential to have much smaller standard errors than leveraging an upstream randomized experiment, given that the quasi-treatment is applied across the entire voter file, not just who an experiment would target.</p>
<p>As an extension to the methods used by Coppock and Green, I also consider bandwith selection algorithms for the regression discontinuity, the results of which I will discuss with the diagnostics. These tools attempt to find a bandwith that is optimal to some criterion, seeking to reduce the researcher degrees of freedom available in setting many possible bandwiths. For example, one such metric would be finding the bandwith that minimizes an approximation to the mean squared error (MSE) in estimating tau <a href="https://www.nber.org/papers/w14726">(Imbens and Kalyanaraman, 2009)</a>.</p>
</section>
<section id="assumptions" class="level2">
<h2 class="anchored" data-anchor-id="assumptions">Assumptions</h2>
<p>Having described the extensive assumptions needed above in order to explain the fuzzy regression discontinuity model, I now evaluate the plausibility of these assumptions for Coppock and Green's case.</p>
<p><strong>Exclusion:</strong> Is there any path other than through <img src="https://latex.codecogs.com/png.latex?D"> through which the "treatment" of being just eligible in 2008 could affect propensity to vote in 2012? Absent a history of actually participating, it is unlikely that someone who becomes eligible slightly earlier would be more likely to be targeted by a campaign to turn out. We can, however, imagine a subject who knew they would be eligible paying more attention in 2008 than someone who knew they would not be, which could spark more interest by the 2012 election. Similarly, a subject who missed voting in their first presidential election but was eligible might be more motivated in 2012 for a fear of missing out again ("I missed voting for Obama, but won't do so again"). With all such paths that imagine a more engaged citizen for their first presidential election, however, one has to imagine that most such citizens would want to put that energy towards voting in 2008. Thus, while we have no way to rule out such backdoor paths from being just eligible in 2008 to voting in 2012, we have to imagine such cases would be relatively rare.</p>
<p><strong>Effectiveness of the Instrument:</strong> While just being eligible alone likely has a relatively weak effect on 2008 turnout, given that we are working at the scale of full state voter files, we can be confident we have generated a reasonable number of compliers to work with out of over five hundred thousand ballots cast by the birthdate cohorts studied.</p>
<p><strong>Monotonicity:</strong> A defier in Coppock's and Green's design would be someone who votes despite not quite being old enough, or vice versa. Given that this constitutes a felony, and a hard to commit one with little benefit, we can be extremely confident in this assumption. Alternatively, someone could choose to not vote only if just eligible, which again implies an extremely unlikely stance towards election law.</p>
<p><strong>Ignorability:</strong> Here, we need to be clear that we mean ignorability within some cutoff of being just eligible, given our covariates: <img src="https://latex.codecogs.com/png.latex?Y(1),%20Y(0)%20%5Cperp%20Z%20%5Cvert%20x,%20x%20%5Cin(C-a,%20C+a)">. It seems plausible that being a month above or below 18 on election day shouldn't create any other major changes in a 17 or 18 year old's potential outcomes. However, as we get farther from the eligibility threshold it becomes more plausible that the groups could diverge through, for example, differences in maturity or differences in education due to birthdate.</p>
<p><strong>Cutoff and forcing variable determined independently:</strong> It seems unlikely that either Election Day or someone's recorded birthday could be moved to help a subject vote earlier. Federal Election Day has been fixed in the United States since 1945. Further, it seems exceedingly unlikely that a parent or hospital administrator would attempt to modify a birth certificate simply to allow their child to vote 1 year earlier.</p>
<p><strong>Estimation:</strong> As we will see in plots below, there seems to be almost no non-linear trend in Y above or below the cutoff, simplifying modeling <img src="https://latex.codecogs.com/png.latex?E%5BY%5Cvert%20X%5D">. Also, while there might be some concern with day or the week, seasonal, or other temporal trends in the number of dates on a given birthdate and thus the number of expected votes there, including the year lagged variable should arguably be sufficient to model them, provided any trends aren't particularly unique to 1990 births. While we'd ideally like more covariates than are available in the flattened data presented here, what we have seems sufficient to make at least a reasonable estimate.</p>
<p><strong>SUTVA:</strong> Lastly, there is little reason to believe that the encouragement to vote provided to one subject by turning 18 close to election day could influence another subject. While we can imagine students influencing each other's politics, it seems implausible that a friend turning 18 close to election day alone could change a student's propensity to vote meaningfully.</p>
</section>
<section id="diagnostics" class="level2">
<h2 class="anchored" data-anchor-id="diagnostics">Diagnostics</h2>
<p>Given that my greatest concern with Coppock and Green’s design is their large choice of bandwith at 365 days around 2008’s election day, I first briefly describe results of other diagnostics I replicated before focusing attention on a variety of ways to evaluate the bandwidth’s effect on the CACE. This emphasizes my work beyond the replication, most of which isn’t shown here- after the following two paragraphs and first table, diagnostics in this section are my extensions of Coppock and Green’s work.</p>
<p>First, being just eligible appears to have an effect on voting 2008, as checked through running just the first stage of a 2 stage least squares model, so our instrument generates compliers. There were no discontinuities in the year lagged vote counts around the 2008 eligibility cutoff, our covariate.</p>
<p>I was able to successfully reproduce Coppock and Green’s robustness check across different order polynomials, shown below. Note, however, that these estimates are from meta-analyses across states; I develop a similar table by state and bandwith later. Both 1st and 2nd order polynomials returned similar results, while a 3rd order polynomial estimates became extremely sensitive to choice of bandwith. As <a href="https://www.nber.org/papers/w20405">Gelman and Imbens (2014)</a> discuss however, cubic polynomials are problematic more generally in regression discontinuity designs, so this result was expected. Robust SE's are in parentheses below their corresponding estimate. <strong>The primary estimates presented by CG in the main paper are bolded.</strong></p>
<table class="table">
<colgroup>
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>90</th>
<th>180</th>
<th>270</th>
<th>365</th>
<th>455</th>
<th>545</th>
<th>635</th>
<th>730</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Difference-in-Means</td>
<td>0.108</td>
<td>0.118</td>
<td>0.124</td>
<td>0.119</td>
<td>0.118</td>
<td>0.117</td>
<td>0.11</td>
<td>0.103</td>
</tr>
<tr class="even">
<td></td>
<td>(0.01)</td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
</tr>
<tr class="odd">
<td>First-order Polynomial</td>
<td>0.058</td>
<td>0.089</td>
<td>0.101</td>
<td><strong>0.117</strong></td>
<td>0.118</td>
<td>0.121</td>
<td>0.13</td>
<td>0.136</td>
</tr>
<tr class="even">
<td></td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td><strong>(0.01)</strong></td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
</tr>
<tr class="odd">
<td>Second-order Polynomial</td>
<td>0.016</td>
<td>0.055</td>
<td>0.074</td>
<td>0.083</td>
<td>0.102</td>
<td>0.104</td>
<td>0.099</td>
<td>0.103</td>
</tr>
<tr class="even">
<td></td>
<td>(0.02)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
</tr>
<tr class="odd">
<td>Third-order Polynomial</td>
<td>0.012</td>
<td>0.035</td>
<td>0.05</td>
<td>0.056</td>
<td>0.06</td>
<td>0.086</td>
<td>0.095</td>
<td>0.097</td>
</tr>
<tr class="even">
<td></td>
<td>(0.02)</td>
<td>(0.02)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
</tr>
</tbody>
</table>
<p>Next, somewhat surprisingly, Coppock and Green never actually plot the discontinuity they work with either in the main paper or appendix. The plot of the discontinuity in Figure 1 below establishes several informative points. First, there is a noticeable dip in total votes cast in 2012 by cohort upon being slightly too young to vote in 2008, a good first validation of the design. Second, the trend in the data both to the left and right of the discontinuity appears to be extremely linear, which may influence my later bandwith selection algorithm findings. Finally, even in the binned plot the wide range of votes cast by birthdate state cohorts is obvious. Upon further inspection much of this variation is driven by state, which motivates my next analysis, seeing how each state's CACE changes as the cutoff varies.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Is Voting Habit Forming/discontinuity.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Discontinuity Plot</figcaption><p></p>
</figure>
</div>
<p>Note that the full 10,000+ birthdate cohorts are binned with their average plotted to make the graph legible. Lines plotted are first order polynomials.</p>
<p>While Coppock and Green present results at their final chosen bandwith by state, they conduct robustness checks only at the level of meta-analysis pooling across states. Given that some of the variation in vote total also seems to be driven by state, below are CACE estimates by state, by a variety of bandwiths. These are considerably more sensitive to the bandwith choice than the above table that pools across states, as we’d expect given the disaggregation. For example, the average across the estimates using 30-day bandwith (.01) is closer to 0 than the .1 resulting from 365 bandwith concreated on in the main paper. This somewhat lowers my confidence in the by-state estimates as Coppock and Green present them. Their claim that their results are insensitive to bandwith variation only really appear to hold when estimates are aggregated through meta-analysis, as in the earlier table.</p>
<table class="table">
<colgroup>
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>365</th>
<th></th>
<th>180</th>
<th></th>
<th>90</th>
<th></th>
<th>60</th>
<th></th>
<th>30</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AR</td>
<td>0.20</td>
<td>(0.03)</td>
<td>0.15</td>
<td>(0.04)</td>
<td>0.14</td>
<td>(0.07)</td>
<td>0.01</td>
<td>(0.10)</td>
<td>0.10</td>
<td>(0.16)</td>
</tr>
<tr class="even">
<td>CT</td>
<td>0.16</td>
<td>(0.02)</td>
<td>0.14</td>
<td>(0.02)</td>
<td>0.14</td>
<td>(0.03)</td>
<td>0.11</td>
<td>(0.03)</td>
<td>0.11</td>
<td>(0.05)</td>
</tr>
<tr class="odd">
<td>IA</td>
<td>0.08</td>
<td>(0.04)</td>
<td>0.03</td>
<td>(0.05)</td>
<td>0.03</td>
<td>(0.08)</td>
<td>-0.05</td>
<td>(0.11)</td>
<td>-0.03</td>
<td>(0.17)</td>
</tr>
<tr class="even">
<td>IL</td>
<td>0.08</td>
<td>(0.01)</td>
<td>0.05</td>
<td>(0.02)</td>
<td>0.02</td>
<td>(0.02)</td>
<td>0.01</td>
<td>(0.03)</td>
<td>0.04</td>
<td>(0.04)</td>
</tr>
<tr class="odd">
<td>FL</td>
<td>0.10</td>
<td>(0.01)</td>
<td>0.09</td>
<td>(0.02)</td>
<td>0.06</td>
<td>(0.03)</td>
<td>0.03</td>
<td>(0.04)</td>
<td>-0.01</td>
<td>(0.05)</td>
</tr>
<tr class="even">
<td>KY</td>
<td>0.08</td>
<td>(0.02)</td>
<td>0.08</td>
<td>(0.03)</td>
<td>0.05</td>
<td>(0.04)</td>
<td>0.05</td>
<td>(0.05)</td>
<td>0.08</td>
<td>(0.07)</td>
</tr>
<tr class="odd">
<td>MO</td>
<td>0.16</td>
<td>(0.02)</td>
<td>0.15</td>
<td>(0.03)</td>
<td>0.14</td>
<td>(0.04)</td>
<td>0.13</td>
<td>(0.06)</td>
<td>0.11</td>
<td>(0.08)</td>
</tr>
<tr class="even">
<td>MT</td>
<td>0.11</td>
<td>(0.03)</td>
<td>0.08</td>
<td>(0.04)</td>
<td>0.03</td>
<td>(0.07)</td>
<td>-0.01</td>
<td>(0.09)</td>
<td>-0.08</td>
<td>(0.12)</td>
</tr>
<tr class="odd">
<td>NJ</td>
<td>0.15</td>
<td>(0.01)</td>
<td>0.12</td>
<td>(0.02)</td>
<td>0.09</td>
<td>(0.03)</td>
<td>0.03</td>
<td>(0.03)</td>
<td>0.01</td>
<td>(0.05)</td>
</tr>
<tr class="even">
<td>NV</td>
<td>0.17</td>
<td>(0.03)</td>
<td>0.14</td>
<td>(0.04)</td>
<td>0.09</td>
<td>(0.06)</td>
<td>0.00</td>
<td>(0.08)</td>
<td>-0.01</td>
<td>(0.11)</td>
</tr>
<tr class="odd">
<td>NY</td>
<td>0.07</td>
<td>(0.01)</td>
<td>0.02</td>
<td>(0.02)</td>
<td>-0.03</td>
<td>(0.03)</td>
<td>-0.06</td>
<td>(0.03)</td>
<td>-0.03</td>
<td>(0.05)</td>
</tr>
<tr class="even">
<td>OK</td>
<td>0.14</td>
<td>(0.02)</td>
<td>0.11</td>
<td>(0.03)</td>
<td>0.09</td>
<td>(0.06)</td>
<td>0.01</td>
<td>(0.08)</td>
<td>0.04</td>
<td>(0.15)</td>
</tr>
<tr class="odd">
<td>OR</td>
<td>0.11</td>
<td>(0.02)</td>
<td>0.07</td>
<td>(0.04)</td>
<td>0.06</td>
<td>(0.06)</td>
<td>0.01</td>
<td>(0.08)</td>
<td>-0.07</td>
<td>(0.12)</td>
</tr>
<tr class="even">
<td>PA</td>
<td>0.12</td>
<td>(0.02)</td>
<td>0.08</td>
<td>(0.03)</td>
<td>0.01</td>
<td>(0.04)</td>
<td>-0.05</td>
<td>(0.05)</td>
<td>-0.04</td>
<td>(0.07)</td>
</tr>
<tr class="odd">
<td>RI</td>
<td>0.11</td>
<td>(0.03)</td>
<td>0.14</td>
<td>(0.05)</td>
<td>0.05</td>
<td>(0.08)</td>
<td>0.03</td>
<td>(0.10)</td>
<td>-0.15</td>
<td>(0.16)</td>
</tr>
</tbody>
</table>
<p>Bandwith Sensitivity for State estimates. Robust Standard errors are to the right of their respective estimate in parentheses.</p>
<p>As a final extension of the robustness to different bandwith checks Coppock and Green consider in their appendix, I also explored a variety of algorithms for bandwith selection. Across different order polynomials, kernels, and metrics to optimize for, all results suggested using the full 365-day bandwith. In retrospect, looking at the quite linear trend in the data, this makes some sense: given little difficult variation to model, choosing to include all the data could easily be the most effective strategy to reduce metrics such as the MSE in estimating <img src="https://latex.codecogs.com/png.latex?%5Ctau_%7BD%7D">.</p>
</section>
<section id="results-and-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="results-and-interpretation">Results and Interpretation</h2>
<p>While I was able to reproduce Coppock and Green’s results, I differ slightly in how I choose to interpret them. Their identification strategy overall makes sense. At worst, it seems possible the exclusion assumption could be subject to minor violations, but the frequency of such events would be low. Unlike Coppock and Green, however, I don’t think its fair to say that the results are completely robust to bandwith selection or result from habit alone.</p>
<p>While their meta-analysis level results are insensitive to variations in the order of polynomial used or bandwith both in their appendix and my replication, allowing the bandwith to vary by state revealed significant additional variability. Given that by state estimates are a core part of their results, I would argue it is best to present something closer to my bandwith by state table than the singular estimates resulting from a bandwith of 365. If one accepts their bandwith, either on its own or as a result of bandwith selection algorithms, it appears that the overall CACE across states is around .1, establishing that there does appear to be some causal effect on 2012 participation from 2008 participation for compliers. If a reader prefers a much narrower bandwith, this overall shrinks to roughly .06 for a 90 day bandwith, or close to 0 for a 30 day one.</p>
<p>A second concern is that Coppock and Green argue that these effects result mostly from habit, not campaigns in 2012 choosing to target those who voted in 2008. This would not represent a violation of exclusion, as campaign effects represent a valid path from <img src="https://latex.codecogs.com/png.latex?D"> to <img src="https://latex.codecogs.com/png.latex?Y">, not <img src="https://latex.codecogs.com/png.latex?Z"> to <img src="https://latex.codecogs.com/png.latex?Y">. Thus, this is a disagreement around causes of effects. To argue that habit, not campaigns, cause the CACEs they find, Coppock and Green note that battleground states do not have significantly higher estimates than non-battleground states. That is, if the cause was campaign activity, we would expect higher CACEs in battleground states in presidential years like 2012. While this would be somewhat convincing as an argument, the ability to tease out the pattern they describe is dependent on a large bandwith to generate small enough standard errors to tell states apart. Beyond this, during campaign years, even in non-battleground states, most people with some voting history can expect campaign outreach. Further, in non-battleground states, this contact sometimes prioritizes young voters and others whose turnout is more ambiguous. Overall, there does appear to be some evidence that the effects Coppock and Green find aren’t mainly due to campaign effects, however this isn’t sufficient to fully support their claim that most of what they find is habit alone.</p>
<p>To provide an interpretation of the individual CACEs, consider the Florida estimate at a bandwith of 90 days on either side of the 2008 election. If our assumptions hold, the CACE of .06 suggests that for compliers, participation in the 2008 election caused a 6% increase in total 2012 votes cast in Florida, above the total votes cast for those compliers who did not participate in 2008.</p>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<p>Overall, reproducing this paper convinced me that Coppock and Green have a strong method for estimating the causal effect of voting in the 2008 election on voting in the 2012 election for compliers. However, these estimates are sensitive to bandwith variation at the state level, with effects shrinking towards zero as the bandwith becomes tighter. Also, it is unclear if their results are due to habit alone, although the estimates do support this somewhat. Thus, I think a narrower interpretation of the 2008-2012 election pair analysis is that there is some suggestion of an effect of voting in 2008 on 2012 votes cast for compliers, provided you accept a large bandwith, and the effect is most likely not due to campaign effects.</p>
<p>Of course, replicating such a colossal paper and set of analyses has many limitations. First, I restricted my analysis and robustness checks to the 2008-2012 election pair, in order to replicate the pair the original authors used in their robustness checks. I also didn’t study or replicate their instrumental variables CACE estimates resulting from using older randomized experiments as instruments. Finally, given the flattened form of the data provided, I was unable to provide fully satisfying descriptive profiles of the birthdate-state cohorts, for example about their demographic composition.</p>
<p>Future work could consider replicating the rest of Coppock and Green’s study with additional robustness checks, or conducting the same analysis using recent advances in voter file linking. For example, given that midterm elections have much lower turnout, and thus fewer compliers in a fuzzy regression discontinuity design, it would be interesting to see if an election pair like 2006-2010 exhibits even higher sensitivity to variation in bandwith. In the past few years, companies such as Catalist and Movement Cooperative have also improved the quality of data available in voter files. These provide a number of advantages that could be helpful for a future study like Coppock and Green’s, including better cleaning of the underlying data, and richer information on the individual voters, either through integration of additional data sources, or through modeling. Coppock and Green’s analysis takes an incredible step towards identifying whether voting is habit forming, but further analysis is needed to show that their results are deeply robust to electoral context and modeling choices. Some of these steps will likely form my project for my Advanced Causal Inference course this semester, although I’m still deciding which points are the most interesting to look at.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2019,
  author = {Andy Timm},
  title = {Is {Voting} {Habit} {Forming?} {Replication,} and Additional
    Robustness Checks},
  date = {2019-07-03},
  url = {https://andytimm.github.io/2020-03-09-Voting-Habit-FRD.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2019" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2019. <span>“Is Voting Habit Forming? Replication, and
Additional Robustness Checks.”</span> July 3, 2019. <a href="https://andytimm.github.io/2020-03-09-Voting-Habit-FRD.html">https://andytimm.github.io/2020-03-09-Voting-Habit-FRD.html</a>.
</div></div></section></div> ]]></description>
  <category>From Old Website</category>
  <category>causal inference</category>
  <guid>https://andytimm.github.io/posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html</guid>
  <pubDate>Wed, 03 Jul 2019 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Type S/M errors in R with retrodesign()</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html</link>
  <description><![CDATA[ 




<p>This is a online version of the vignette for my r package <strong>retrodesign</strong>. It can be installed with:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">install.packages</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"retrodesign"</span>)</span></code></pre></div>
<p>In light of the replication and reproducibility crisis, researchers across many fields have been reexamining their relationship with the Null Hypothesis Significance Testing (NHST) framework, and developing tools to more fully understand the implications of their research designs for replicable and reproducible science. One group of papers in this vein are works by Andrew Gelman, John Carlin, Francis Tuerlinckx, and others which develop two new metrics for better understanding hypothesis testing in noisy samples. For example, Gelman and Carlin’s <a href="http://www.stat.columbia.edu/~gelman/research/published/retropower20.pdf">Assessing Type S and Type M Errors</a> (2014) argues that looking at power, type I errors, and type II errors are insufficient to fully capture the risks of NHST analyses, in that such analysis focuses excessively on statistical significance. Instead, they argue for consideration of the probability you’ll get the sign on your effect wrong, or <strong>type S error</strong>, and the factor by which your effect size might be exaggerated, or <strong>type M</strong> error. Together, these additional statistics more fully explain the dangers of working in the NHST framework, especially in noisy, small sample environments.</p>
<p><strong>retrodesign</strong> is a package designed to help researchers better understand type S and M errors and their implications for their research. In this vignette, I introduce both the need for the type S/M error metrics, and the tools retrodesign provides for examining them. I assume only a basic familiarity with hypothesis testing, and provide definitional reminders along the way.</p>
<!--more-->
<section id="an-initial-example" class="level2">
<h2 class="anchored" data-anchor-id="an-initial-example">An Initial Example</h2>
<p>To nail down the assumptions we’re working with, we’ll start with an abstract example from <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/bmsp.12132">Lu et al.</a> (2018); the second will draw on a real world scenario and follow Gelman and Carlin’s suggested workflow for design analysis more closely.</p>
<p>Let’s say we’re testing whether a true effect size is zero or not, in a two tailed test.</p>
<p><img src="https://latex.codecogs.com/png.latex?H%20_%20%7B%200%20%7D%20:%20%5Cmu%20=%200%20%5Cquad%20%5Ctext%20%7B%20vs.%20%7D%20%5Cquad%20H%20_%20%7B%201%20%7D%20:%20%5Cmu%20%5Cneq%200"></p>
<p>We’re assuming that the test statistic here is normally distributed. As <a href="http://www.stat.columbia.edu/~gelman/research/published/francis8.pdf">Gelman and Tuerlinckx</a> (2009) note, this is a pretty widely applicable assumption; through the Central Limit Theorem, it applies to common settings like differences between test and control groups in a randomized experiment, averages, or linear regression coefficients. If it helps, imagine the following in the context of one of those from your field.</p>
<p>Traditionally, we’d focus on Type I/II errors. <strong>Type I error</strong> is rejecting the null hypothesis, when it is true. <strong>Type II error</strong> is failing to reject the null hypothesis, when it is not true. The <strong>power</strong>, then, is just is the probability that the test correctly rejects the null hypothesis when a specific alternative hypothesis is true.</p>
<p>Beyond those, we’ll also consider:</p>
<ol type="1">
<li><strong>Type S (sign)</strong>: the test statistic is in the opposite direction of the true effect size, given that the statistic is statistically significant;</li>
<li><strong>Type M (magnitude or exaggeration ratio)</strong>: the test statistic in magnitude exaggerates the true effect size, given that the statistic is statistically significant.</li>
</ol>
<p>Notice that both of these are conditional on the test statistic being statistically significant; we’ll come back to this fact several times.</p>
<p>To visualize these, we’ll draw 5000 samples from a normal distribution with mean .5, and standard deviation 1. We’ll then analyze these in a NHST setting where we have a standard error of 1. We can use <code>sim_plot()</code> to do so, with the first parameter being our postulated effect size .5, and the second being our hypothetical standard error of 1. If you prefer to not use ggplot graphics like I do here, set the <code>gg</code> argument to FALSE.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sim_plot</span>(.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
<p><img src="https://andytimm.github.io/posts/Intro to Retrodesign/unnamed-chunk-2-1.png" class="img-fluid" alt="Visual of what type S and M errors are"><!-- --></p>
<p>Here, the dotted line is the true effect size, and the full lines are where the statistic becomes statistically significantly different from 0, given our standard error of 1. The greyed out points aren’t statistically significant, the squares are type M errors, and the triangles are type S errors.</p>
<p>Even though the full distribution is faithfully centered around the true effect, once we filter using statistical significance, we will both exaggerate the effect and get its sign wrong often.</p>
<p>Of course, trying to find a true effect size of .5 with a standard error of 1 is extremely underpowered. More precisely, the power, Type S and type M error here are:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">retro_design</span>(.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; $power</span></span>
<span id="cb3-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; [1] 0.07909753</span></span>
<span id="cb3-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt;</span></span>
<span id="cb3-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; $typeS</span></span>
<span id="cb3-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; [1] 0.0878352</span></span>
<span id="cb3-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt;</span></span>
<span id="cb3-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; $typeM</span></span>
<span id="cb3-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; [1] 4.788594</span></span></code></pre></div>
<p>The function arguments here the same as those for <code>sim_plot()</code> above. If you want to work with a t-distribution instead, you’ll need to use <code>retrodesign()</code> instead, and provide the degrees of freedom for the <code>df</code> argument. <code>retrodesign()</code> is the original function provided by Gelman &amp; Carlin (2014), which uses simulation rather than an exact solution to calculate the errors. It’s thus slower, but can work with t-distributions as well (the closed form solution only applies in the normal case).</p>
<p>A power of .07 isn’t considered good research in most fields, so most cases won’t be this severe. However, it does illustrate two important points. In a underpowered example like this, we will hugely overestimate our effect size (by a factor of 4.7x! on average), or even get its sign wrong if we’re unlucky (around 8% of the time). Further, these are practical measures to focus on; getting the sign wrong could mean recommending the wrong drug treatment, exaggerating the treatment effect could mean undertreating someone once the drug goes to market.</p>
</section>
<section id="a-severe-example" class="level2">
<h2 class="anchored" data-anchor-id="a-severe-example">A Severe Example</h2>
<p>Now that you hopefully have a sense of what type S/M error add to our understanding of NHST in the context of noisy, small sample studies, we’ll move onto a real world example, where we’ll focus on following Gelman and Carlin’s suggested design analysis through one of their examples.</p>
<p>We’ll be working with <a href="https://www.ncbi.nlm.nih.gov/pubmed/16949101">Kanazawa</a> (2007), which claims to have found that the most attractive people are more likely to have girls. To be more specific, each of their ~3,000 people surveyed had been assigned a “attractiveness” score from 1-5, and they then compared the first born children of the most attractive to other groups; 56% were girls compared to 48% in the other groups. They thus obtained a difference estimate of 8%, with a p-value of .015, so significant at the traditional <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%20.05">.</p>
<p>Stepping back for a second, this is fishy in numerous ways. First, comparing the first borne children is an oddly specific comparison to have run- at worst, this might be a case of p-hacking. Or maybe they saw a strong comparison, and decided to test it, and in doing so fell into a <a href="http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf">Garden of Forking Paths</a> problem. Second, if attractive people had many more girls, it seems unlikely that gender balance would be as even as it is. So again, this example has a likely spurious result, is likely to have low power, and a high chance of S/M errors.</p>
<p>To do a design analysis of this, Gelman and Carlin’s first step is to review the literature for a posited true effect size. There is plenty of prior research on variation in sex ratio of human births. A huge variety of factors have been studied such as race, parental age, season of birth, and so on, only finding effects from .3% to 2%. In the most extreme cases (conditions like extreme poverty or famine), these effects only rise to 3%. (If you’re interested, the causal reasoning seems to be that male fetuses are more likely than female ones to die under adverse conditions.)</p>
<p>Like traditional design analyses, we’ll posit a wide range of effects here, and see how our power, type S error, and type M error rates change correspondingly. Gelman and Carlin end up looking at .1-1%, reasoning that sex ratios vary very little in general, and that the subject attractiveness rating is quite noisy as well. Even if we compare their effect size to the effect sizes found in the most extreme scenarios in prior literature, it doesn’t look good.</p>
<p>We can infer their standard error of the difference to be 3.3% from their reported 8% estimate and p-value of .015; we now have everything we need.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The posited effects Gelman and Carlin consider</span></span>
<span id="cb4-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">retro_design</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.3</span>)</span>
<span id="cb4-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt;   effect_size      power    type_s   type_m</span></span>
<span id="cb4-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; 1         0.1  0.0501052 0.4646377 77.15667</span></span>
<span id="cb4-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; 2         0.3 0.05094724 0.3953041 25.74305</span></span>
<span id="cb4-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; 3           1 0.06058446 0.1950669 7.795564</span></span>
<span id="cb4-7"></span>
<span id="cb4-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># A particularly charitable set of posited effects</span></span>
<span id="cb4-9"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">retro_design</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>),<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.3</span>)</span>
<span id="cb4-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt;   effect_size      power     type_s   type_m</span></span>
<span id="cb4-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; 1         0.1  0.0501052  0.4646377 77.15667</span></span>
<span id="cb4-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; 2         0.3 0.05094724  0.3953041 25.74305</span></span>
<span id="cb4-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; 3           1 0.06058446  0.1950669 7.795564</span></span>
<span id="cb4-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; 4           2 0.09302718 0.05529112 3.982141</span></span>
<span id="cb4-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; 5           3  0.1487169 0.01384174 2.719244</span></span></code></pre></div>
<p>By providing a list for our first argument <code>A</code>, we get a dataframe with a posited effect size, and corresponding power, type S, and type M errors in each row. If you just want the lists of power/type S/type M, you can just feed in a vector, ie <code>c(.1,.3,1)</code>.</p>
<p>So if we go with a high but fairly reasonable posited effect of 1%, there’s a nearly 1 in 5 chance that such an experiment would get the direction of the effect wrong. Even if we assume that being the daughter of a highly attractive person has equivalent effect to being born in a famine (effect size 3%), this experiment would exaggerate the true effect size by a factor of 2.7x on average.</p>
<p>This analysis has given us added information beyond what we’d get from the point estimate, the confidence interval, and the p-value. Under reasonable assumptions about the true effect size, this study simply seems too small to be informative: Under most assumptions, we’re quite likely to get the sign wrong, and even with a quite generous assumption of true effect size we’ll greatly exaggerate the effect size.</p>
</section>
<section id="assessing-type-sm-errors-when-you-dont-have-prior-information" class="level2">
<h2 class="anchored" data-anchor-id="assessing-type-sm-errors-when-you-dont-have-prior-information">Assessing Type S/M errors when you don’t have prior information</h2>
<p>One obvious objection you might have to this framework is that you may not have a clear sense of what your effect size will be. As extreme as it sounds, you may not even have a sense of the right order of magnitude. In these cases, it makes even more sense to calculate type s/m errors across a variety of posited effect sizes and see how they influence your research.</p>
</section>
<section id="so-how-worried-should-we-be-in-more-reasonable-studies" class="level2">
<h2 class="anchored" data-anchor-id="so-how-worried-should-we-be-in-more-reasonable-studies">So how worried should we be in more reasonable studies?</h2>
<p>Another concern might be that my first two examples were severely underpowered. However, even with powers that are publishable in many fields, we should still be worried about type M errors, but not type S errors.</p>
<p>To sketch out the relationship between possible effect sizes and these errors, we adopt the standard error from the prior example, but greatly expand the posited effect sizes, to max out at 10, where the power would be a perfectly reasonable .85. We’ll use <code>type_s</code> and <code>type_m</code>, both of which take a single or list of possible <code>A</code>’s, and a standard error, similar to <code>retrodesign</code>, but only return the respective error.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">possible_effects <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">seq</span>(.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">by =</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb5-2">effect_s_pairs <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(possible_effects,<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">type_s</span>(possible_effects,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.3</span>))</span>
<span id="cb5-3">effect_m_pairs <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(possible_effects,<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">type_m</span>(possible_effects,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.3</span>))</span>
<span id="cb5-4"></span>
<span id="cb5-5">s_plot<span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(effect_s_pairs, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(possible_effects, type_s)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_point</span>()</span>
<span id="cb5-6">m_plot <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(effect_m_pairs, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(possible_effects, type_m)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_point</span>()</span>
<span id="cb5-7"></span>
<span id="cb5-8"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">grid.arrange</span>(s_plot, m_plot, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ncol=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span></code></pre></div>
<p><img src="https://andytimm.github.io/posts/Intro to Retrodesign/unnamed-chunk-5-1.png" class="img-fluid" alt="Comparison of Type S/M error across effect sizes"><!-- --></p>
<p>As <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/bmsp.12132">Lu et al.</a> (2018) note, the type S and M error shrink at very different rates as power rises.</p>
<p>They find the probability of type S error decreases rapidly. To ensure that $s $ and <img src="https://latex.codecogs.com/png.latex?s%5Cleq%200.01">, we only need <img src="https://latex.codecogs.com/png.latex?power%20=%200.08"> and <img src="https://latex.codecogs.com/png.latex?power%20=%200.17">, respectively. Thus, unless your study is severely underpowered, you shouldn’t need to worry about type s errors very often.</p>
<p>On the other hand, The type m error decreases relatively slowly. To ensure that <img src="https://latex.codecogs.com/png.latex?m%20%5Cleq%201.5"> and <img src="https://latex.codecogs.com/png.latex?m%20%5Cleq%201.1">, we need <img src="https://latex.codecogs.com/png.latex?power%20=%200.52"> and <img src="https://latex.codecogs.com/png.latex?power%20=%200.85">, respectively. Whereas even moderately powered studies make type s errors relatively improbable, only very high powered studies keep exaggeration of effect sizes down. If your field requires a power of .80, you should thus be cognizant that effect sizes are likely somewhat inflated.</p>
</section>
<section id="solutions-outside-nhst" class="level2">
<h2 class="anchored" data-anchor-id="solutions-outside-nhst">Solutions outside NHST</h2>
<p>For the majority of this vignette, I’ve avoided questioning whether we should be working in the NHST framework. However, <code>retrodesign</code> is a package to help address some limitations of NHST work as it’s traditionally practiced, so it makes sense to address these solutions.</p>
<p>Going back to the plot we started with, remember that the underlying gaussian does actually faithfully center around it’s mean of .5.</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sim_plot</span>(.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
<p><img src="https://andytimm.github.io/posts/Intro to Retrodesign/unnamed-chunk-6-1.png" class="img-fluid" alt="Return to showing first plot"><!-- --></p>
<p>Type S and M errors are an artifact of the hard thresholding implicit in a NHST environment, where an arbitrary p-value (usually .05) decides what is and isn’t noteworthy.</p>
<p>If you have to work in the NHST framework because it’s what your discipline publishes, you can better understand some problems it might cause by exploring type S and M errors. If you’re able to choose how you analyze your experiments though, you can avoid these errors (and many others) by abandoning statistical significance as a hard filter entirely. If learning more about problems with NHST beyond type S and M errors, and suggestions for alternative strategies is interesting to you, check out the <strong>Abandon Statistical Significance</strong> paper linked in the further reading below.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<ol type="1">
<li><strong>Gelman and Tuerlinckx’s <a href="http://www.stat.columbia.edu/~gelman/research/published/francis8.pdf">Type S error rates for classical and Bayesian single and multiple comparisons procedures</a> (2000)</strong>: A comparison of the properties of Type S errors of frequentist and Bayesian confidence statements. Useful for how this all plays out in a Bayesian context. Bayesian confidence statements have the desirable property of being more conservative than frequentist ones.</li>
<li><strong>Gelman and Carlin’s <a href="http://www.stat.columbia.edu/~gelman/research/published/retropower20.pdf">Assessing Type S and Type M Errors</a> (2014)</strong>: Gelman and Carlin compare their suggested design analysis, as we’ve written about above, to more traditional design analysis, through several examples, and discuss the desirable properties it has in more depth than I do here. It is also the source of the original retrodesign() function, which I re-use in the package with permission.</li>
<li><strong>Lu et al’s <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/bmsp.12132">A note on Type S/M errors in hypothesis testing</a> (2018)</strong>: Lu and coauthors go further into the mathematical properties of Type S/M errors, and prove the closed form solutions implimented in <code>retrodesign</code>.</li>
<li><strong>McShane et al’s <a href="https://arxiv.org/abs/1709.07588">Abandon Statistical Significance</a> (2017)</strong>: If you want a starting point on the challenges with NHST that have led many statisticians to argue for abandoning NHST all together, and starting points for alternative ways of doing science.</li>
</ol>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2018,
  author = {Andy Timm},
  title = {Type {S/M} Errors in {R} with Retrodesign()},
  date = {2018-05-11},
  url = {https://andytimm.github.io/2019-02-05-Intro_To_retrodesign.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2018" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2018. <span>“Type S/M Errors in R with Retrodesign().”</span>
May 11, 2018. <a href="https://andytimm.github.io/2019-02-05-Intro_To_retrodesign.html">https://andytimm.github.io/2019-02-05-Intro_To_retrodesign.html</a>.
</div></div></section></div> ]]></description>
  <category>From Old Website</category>
  <guid>https://andytimm.github.io/posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html</guid>
  <pubDate>Fri, 11 May 2018 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Why the normal distribution?</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Maximum Entropy Normal/2018-05-11-maxentropynormal.html</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Maximum Entropy Normal/boring_normal.jpg" class="img-fluid figure-img"></p>
</figure>
</div>
<p>When I was first studying probability theory as an undergrad, I had a bit of a conceptual hang-up with the Central Limit Theorem. <a href="http://www.rpubs.com/christopher_castle/137490">Simulating it in R</a> gave a nice visual of how each additional random variable smoothed out some of the original distribution’s individuality, and asymptotically we were left with a more generic shape. The proofs were relatively straightforward. One part, however, didn’t really make sense to me. My problem was this: <strong>Of all the many possible distributions, why is the normal distribution in particular that our i.i.d random variables converge to in distribution?</strong></p>
<!--more-->
<p>The normal distribution has lots of interesting properties that I looked at to gain some intuition, but many of the them seem to follow from the CLT, rather than explaining it. In fact, it wasn’t until a later course in machine learning that integrated some <a href="https://en.wikipedia.org/wiki/Information_theory">information theory</a> that I found a satisfactory answer to my question.</p>
<p>In this post, I’ll explain an insight from the principle of maximum entropy that conceptually justifies the normal distribution’s role in the CLT. To do so, we’ll first build up a basic introduction to entropy, how probability and entropy interact, and then explain the entropy property of the normal distribution that helped me understand the question above. For this post, all you’ll really need is a rough idea of what a random variable is, and some familiarity with common probability distributions; we’ll build up the rest from scratch.</p>
<section id="entropy" class="level2">
<h2 class="anchored" data-anchor-id="entropy">Entropy</h2>
<p>Imagine two situations in which you’ve lost your keys at home. In the first scenario, you’re well and truly confused as to where they are. As far as you’re concerned, any location within your home is equally likely. In the second scenario, you remember having them by your kitchen counter, so you’re pretty sure they’re there, or at least somewhere near there. Intuitively, the knowledge that they’re probably in your kitchen is much, much more informative that the idea that they’re equally likely to be anywhere in your house. In fact, once you accept the constraint that they’re within the bounds of your house, it’s pretty hard to imagine a more useless, uninformative statement than “they’re equally likely to be anywhere”.</p>
<p>Information theory helps makes rigorous many of our informal ideas about how much information or uncertainty is contained in situations like the above. Let’s start then, by defining <strong>entropy</strong>, a measure of the uncertainty of a random variable:</p>
<p><img src="https://latex.codecogs.com/png.latex?H(X)%20=%20-%20%5Cint_%7BI%7D%20p(x)%20%5Clog%7Bp(x)%7D%20dx"></p>
<p>Where p(x) is our continuous probability distribution over some interval I. It can be analogously defined for the discrete case by replacing the integral with a summation. To stress the intuition here, the higher the entropy, the less we know about the value of the random variable. As an example, a uniform distribution over the bounds <img src="https://latex.codecogs.com/png.latex?%5Ba,b%5D"> is analogous to our “keys could be anywhere” scenario- the entropy of our variable is high. In fact, once you accept the constraint that the distribution is supported on [a,b], one can prove that the uniform distribution is the maximum entropy distribution among all continuous distributions on that interval. We won’t include the proof here, but showing a similar maximum entropy property about the normal distribution is where we’re headed.</p>
<p>Before we get back to the normal distribution and CLT though, let’s think a little bit more about the concept of maximum entropy. Why would we want to use something specifically because it’s minimally informative? Let’s think about how we’d express or model our belief that our keys are equally likely to be anywhere. If we’re truly of the belief that they’re equally likely to be anywhere in our house, then some sort of model based on a uniform distribution over the space likely makes sense. If they’re equally likely to be anywhere, placing high probability in a specific room would be wrong, and making that additional assumption would probably slow down our search. This is Jaynes’ principle of maximum entropy- we should use the distribution with the highest entropy, subject to any prior constraints we already have.</p>
<p>This principle, then, is about epistemic modesty. We can want to choose the distribution that meets our constraints, and assumes as little additional information as possible.</p>
</section>
<section id="the-normal-distribution" class="level2">
<h2 class="anchored" data-anchor-id="the-normal-distribution">The Normal Distribution</h2>
<p>Once you specify a variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E%7B2%7D">, and that the distribution be supported on the reals from <img src="https://latex.codecogs.com/png.latex?(-%20%5Cinfty,%20%5Cinfty)">, the normal distribution is the maximum entropy distribution! <a href="http://notstatschat.tumblr.com/post/146886495511/how-do-we-prove-the-central-limit-theorem">Thomas Lumley</a> calls this a “a precise characterization of the normal’s boringness”. In our mental image of the CLT at work then, we’re approaching the normal because of it’s generic-ness, it’s lack of information. If we were mixing colors, the normal would be a nondescript grey.</p>
<p>In one sense, this result may be counter intuitive. As statisticians, when we find out that a distribution we’re working with is roughly normal, we tend to feel like we have a lot to work with. Many of our favorite tools like maximum likelihood will work well, and we have straightforward ways to estimate most quantities of interest. However, this result illustrates a subtle point: ease of use and information content aren’t the same thing.</p>
<p>If you found this use of information theory improved your intuition for probability and want more, here are some suggestions for further reading-</p>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources:</h2>
<ul>
<li>If you’re wondering how we actually prove that the normal is the maximum entropy distribution for a specified variance, there’s a self-contained proof in section <a href="http://www.deeplearningbook.org/contents/inference.html">19.4.2 of Deep Learning</a>. I excluded it in this post because it required a lot of extra math, and didn’t add much to the intuitive point I was trying to show.</li>
<li><a href="http://colah.github.io/posts/2015-09-Visual-Information/">Chris Olah’s Visual Information Theory</a> is a great introduction to the information compression and distribution comparison parts of information theory.</li>
</ul>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2018,
  author = {Andy Timm},
  title = {Why the Normal Distribution?},
  date = {2018-05-11},
  url = {https://andytimm.github.io/2018-05-11-maxentropynormal.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2018" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2018. <span>“Why the Normal Distribution?”</span> May 11,
2018. <a href="https://andytimm.github.io/2018-05-11-maxentropynormal.html">https://andytimm.github.io/2018-05-11-maxentropynormal.html</a>.
</div></div></section></div> ]]></description>
  <category>From Old Website</category>
  <guid>https://andytimm.github.io/posts/Maximum Entropy Normal/2018-05-11-maxentropynormal.html</guid>
  <pubDate>Fri, 11 May 2018 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Predicting race part 1- Bayes’ rule method and extensions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Race Models Pt 1/race_models_part1.html</link>
  <description><![CDATA[ 




<p>Race is a defining part of political identity in the United States, and so it should be no surprise that accurately modeling race can be beneficial for many political campaign activities. For instance, many organizations work to improve turnout in specific communities of color, or want to target persuasion on a given issue to certain racial group. Alternatively, race and ethnicity might be desired as an input to a larger voting or support likelihood model, given that race is generally predictive of both <a href="http://www.electproject.org/home/voter-turnout/demographics">voting likelihood</a> and <a href="https://www.nytimes.com/interactive/2016/11/08/us/politics/election-exit-polls.html">candidate support</a>.</p>
<p>Unfortunately, complete self-reported race data is only available in 7 states, where it is required by law: Alabama, Florida, Georgia, Louisiana, Mississippi, North Carolina, and South Carolina. Pennsylvania and Tennessee also have an optional field on voter registration forms. Outside of these states, race and ethnicity need to be collected individually, or modeled. These models commonly take advantage of the name (especially surname) of the individual voter, and other information in the voter file to do so.</p>
<p>In this post, I’ll explore a Bayes’ rule based method for modeling racial identity, and how it can be extended with additional information from state voter files where available. Obviously, it won’t be as predictive as something like Catalist or Civis Analytics’ ones (which have the benefit of huges swathes of additional survey and other data, not to mention more sophisticated modeling), but it can help illustrate the benfits and limitations of such tools. In the next posts, I’ll explain how natural language processing models can achieve still higher accuracy by extracting more information from names themselves. Since the Florida voter file has self-reported race data and is <a href="http://flvoters.com/downloads.html">easy to access</a>, we’ll test our models’ effectiveness against that ground truth.</p>
<section id="a-simple-first-method" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-first-method">A simple first method</h2>
<p>The <a href="https://www.census.gov/data/developers/data-sets/surnames.html">census surname files</a> are a an incredible source of information on how race correlates with names, and are the starting point of our model. In these files, the census provides race percentage breakdowns for any surname with more than 100 occurrences in the United States, except where redacted for privacy reasons. The data is available only aggregated at the national level. In practice, this means coverage of about 90% of the population.</p>
<p>For many voters, surname is the only information we need to make an accurate classification: names such as Carlson, Meyer, and Hanson are in excess of 90% white, with similar surnames existing for other races. Unfortunately, many names aren’t as clear cut, and some names like Chavis are less than 40% likely to be any race. This is a good starting point, but we can do better.</p>
<p>As our first improvement, <a href="https://link.springer.com/article/10.1007/s10742-009-0047-1">Elliot et al.&nbsp;(2009)</a> incorporates census geolocation (county, tract, or block) data, using Bayes’ theorem.</p>
<p>Bayes’ theorem is a natural way to integrate the new census evidence we have, updating our initial beliefs with more data. <img src="https://latex.codecogs.com/png.latex?P(A)"> and <img src="https://latex.codecogs.com/png.latex?P(B)"> are the probabilities of events A and B occurring independently of each other, and <img src="https://latex.codecogs.com/png.latex?P(A%20%5Cvert%20B)"> and <img src="https://latex.codecogs.com/png.latex?P(B%20%5Cvert%20A)"> are conditional probabilities, such as the probability a given voter is white given their surname, or <img src="https://latex.codecogs.com/png.latex?P(white%20%5Cvert%20surname)">. Bayes’ theorem appears in it’s simplest form below, giving us an updated probability utilizing the new information:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20P(A%7CB)%20=%20%5Cfrac%7BP(B%7CA)%20P(A)%7D%7BP(B)%7D%20"></p>
<p>Of course, we’re interested in integrating multiple pieces of evidence (surname/geolocation for now, with more coming later), and applying it to multiple voters. The equations will be much more involved, but remember that we’re essentially just extending the above equation to work with multiple inputs, over multiple voters. Note that I’ll be using the notation from <a href="https://imai.princeton.edu/research/race.html">Imai and Khanna (2016)</a>, which is somewhat clearer than the notion from Elliot et al.</p>
<p>We want <img src="https://latex.codecogs.com/png.latex?P(R_i%20=%20r%20%5Cvert%20S_i%20=%20s,%20G_i%20=%20g)">, the conditional probability that the voter <img src="https://latex.codecogs.com/png.latex?i"> belongs to the racial group <img src="https://latex.codecogs.com/png.latex?r"> given their surname <img src="https://latex.codecogs.com/png.latex?s"> and geolocation <img src="https://latex.codecogs.com/png.latex?g">. <img src="https://latex.codecogs.com/png.latex?R">, <img src="https://latex.codecogs.com/png.latex?S">, and <img src="https://latex.codecogs.com/png.latex?G"> are the sets of all racial groups, all surnames, and all geolocations respectively. Thus, as a final expression we’ll get:</p>
<p><img src="https://latex.codecogs.com/png.latex?P(R_i%20=%20r%20%5Cvert%20S_i%20=%20s,%20G_i%20=%20g)%20=%20%5Cfrac%7BP(G_i%20=%20g%20%5Cvert%20R_i%20=%20r)%20P(R_i%20=%20r%7CS_i%20=%20s)%7D%7B%5Csum_%7Br'%5Cin%20R%7D%20P(G_i%20=%20g%7CR_i%20=%20r)%20P(R_i%20=%20r%7CS_i%20=%20s)%20%7D"></p>
<p>We already have almost all these probabilities between the census surname list and census demographic data. <img src="https://latex.codecogs.com/png.latex?P(R_i%20=%20r%20%5Cvert%20S_i%20=%20s)"> is the racial composition of surnames from the surname list. But what about <img src="https://latex.codecogs.com/png.latex?P(G_i%20=%20g%20%5Cvert%20R_i%20=%20r)">? As an intermediate step, we first need to calculate <img src="https://latex.codecogs.com/png.latex?P(G_i%20=%20g%20%5Cvert%20R_i%20=%20r)"> , which we can calculate using Bayes’ rule again. <img src="https://latex.codecogs.com/png.latex?P(G_i%20=%20g%20%5Cvert%20R_i%20=%20r)"> is then <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BP(R_i%20=%20r%20%5Cvert%20G_i%20=%20g)%20P(G_i%20=%20g)%7D%20%7B%5Csum_%7Br'%5Cin%20R%7D%20P(R_i%20=%20r%20%5Cvert%20G_i%20=%20g')%20P(G_i%20=%20g)%7D">, completing everything we need to produce our second model.</p>
<p>This model, like the surname list, produces probabilistic predictions of race, for instance, a given voter is 94.3% likely to be white, given their name and geolocation. The probabilities sum to 1 across the races.</p>
<p>We’ll get to how to use and evaluate such models soon, but first: given that we’ve started to include information from the voter file to improve our predictions, why don’t we use other fields we have such as age, party registration, and gender? They all are likely to contain some conditional information about a voter’s race, and while party registration isn’t reported in every state, it is in Florida.</p>
<p>That’s exactly the proposal of <a href="https://imai.princeton.edu/research/race.html">Imai and Khanna (2016)</a>, and it works well. The equations get slightly more complicated, but extending Bayes’ rule to include more and more variables doesn’t change all that much. We have to calculate more intermediate probabilities, but the essential process and reasoning of incorporating new information to update our belief is the same. If you want to see the full model written out, with space for an arbitrary number of new variables <img src="https://latex.codecogs.com/png.latex?X_i">, you can find it in the linked paper.</p>
</section>
<section id="use-and-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="use-and-evaluation">Use and Evaluation</h2>
<p>Now that we have a probability distribution over the racial groups, how do we utilize them?</p>
<p>We might take the highest probability race and use that as our prediction. Alternatively, as an input in a later model, we might choose to simply incorporate all 5 probabilities, allowing our following model as much information as possible about the racial identity of a voter. Catalist, the democratic data vendor, turns probabilities into simple categories such as “likely white” or “possibly black”, which simplify working with the results on a campaign.</p>
<p>As a final idea, we might have an application in mind where you want to only predict a certain race when you’re very confident in your prediction. For example, you might be hoping to target a turnout mailer written in Spanish to only Hispanics, and as few non-Hispanics as possible. To do this, we’d utilize only high probability predictions- say, above 85% likely to be Hispanic. By changing that threshold, you could optimize the size of your mail universe versus the specificity and efficiency of it, finding the best balance for your campaign.</p>
<p>As our last example showed, when utilizing such probabilistic predictions, there is naturally an accuracy tradeoff involved in selecting what type of threshold to use. We could misleadingly say our model is extremely accurate if we only use a high threshold, but a fairer, more systematic evaluation would require looking at how it preforms over multiple such thresholds. That’s what we’ll develop next: Area Under the Curve (AUC), a systematic method for evaluating classifiers.</p>
<p>There are 4 possible outcomes to making a classification: a True Positive (TP), False Positive (FP), True negative (TN), and False Negative (FN). As an example then, a true positive is when we predicted positive, and the ground truth label was actually positive.</p>
<p>Rather than building a table of these 4 outcomes, called a confusion matrix, we’ll be working with summary statistics of these outcomes. The True Positive Rate (or sensitivity or recall) is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BTP%7D%7BTP+FN%7D">. In other words, out of all the points that are (ground truth) positive, how many did we correctly classify? Similarly, the False Positive Rate is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BFP%7D%7BFP+TN%7D">. High True Positive Rate is good: it means we’ll miss relatively few positive examples. On the other end of things, low False Positive Rate is what we want: it means fewer negative points will be misclassified.</p>
<p>By calculating these two statistics at a variety of thresholds, then plotting a curve with the FPR on the x-axis and TPR on the y-axis, we can get a deeper understanding of the tradeoff. This is called an Receiver Operating Characteristic. Given what I’ve said about the meaning of the TPR and FPR, can you figure out quality of the 3 classifiers that are graphed below?</p>
<p><img src="https://andytimm.github.io/posts/Race Models Pt 1/example_curves.jpg" class="img-fluid" alt="Example ROCs"> The first is a perfect classifier: at all tradeoff points, it has a TPR of 1, and FPR of 0. The second is pretty good: at most tradeoff points it does well. The third, a straight line from (0, 0) to (1,1) is a random classifier: it’s equivalent to guessing.</p>
<p>As an overall summary then, the area under this curve (AUC) is of our one number summary of these graphs. The shown classifiers have AUC 1, .8, and .5 respectively.</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>Now that we’ve built up a relatively complex model, and learned how to use and evaluate it, let’s plot some ROC curves, and look at AUCs for our models.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Race Models Pt 1/ROC_for_wru.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">ROC graphs for 4 models</figcaption><p></p>
</figure>
</div>
<p>And here’s the AUC table: bold is the highest overall for each race.</p>
<table class="table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>White</strong></th>
<th><strong>Black</strong></th>
<th><strong>Hispanic</strong></th>
<th><strong>Asian</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Surname</strong></td>
<td>0.8414369</td>
<td>0.8562679</td>
<td>0.9325489</td>
<td>0.8606716</td>
</tr>
<tr class="even">
<td><strong>Geo/Surname</strong></td>
<td>0.8792853</td>
<td>0.8918873</td>
<td><strong>0.9492531</strong></td>
<td><strong>0.8717517</strong></td>
</tr>
<tr class="odd">
<td><strong>Kitchen Sink</strong></td>
<td><strong>0.8903922</strong></td>
<td><strong>0.8979003</strong></td>
<td>0.9362794</td>
<td>0.7648270</td>
</tr>
</tbody>
</table>
<p>Looking at these, there are a lot of interesting patterns!</p>
<p><strong>White:</strong> The White models’ results are perhaps what we most expected. With more data, each subsequent model does better, and the overall result is strong.</p>
<p><strong>Black:</strong> Interestingly, the kitchen sink model still does the best overall with black voters, but by a much smaller margin than with whites. Also, once the true positive rate gets to around .90, curves cross! This is a good illustration of the importance of plotting ROC curves when doing classification- depending on your campaign, you might correctly choose either the Geo/Surname model or the Kitchen Sink one, depending on what type of cutoff you need.</p>
<p><strong>Hispanic:</strong> The Hispanic models are all very close together: with surname information being so effective in classifying Hispanics (more effective than any model for the other races), there isn’t much room for census or voter file data to improve things.</p>
<p><strong>Asian:</strong> These models are significantly weaker than all the others, but still reasonable. The unexpected trend though, is that the kitchen sink model preforms much worse than the other two. Given how few Asian voters there are overall in Florida, this downturn is probably explained by the relatively low density of any Asians across the other inputs, like age, sex, or party. Thus, while geographic information might slightly improve things, Floridians are so unlikely to be Asian overall that all of those extra variables don’t carry any useful information about who might be Asian.</p>
<p>Overall, these Bayes’ Theorem models have a lot of attractive features. While the predictiveness of names, and what variables you have from the voter file might vary state to state, the models can used anywhere in the US. They’re also quite a strong baseline for accuracy as well- far, far better than random, even for Asian voters. Unlike models we’ll discuss in the next post, these models require no training data. Finally, they’re transparent- if you want to check how surname, geolocation, and party weighed in to a particular decision, it’s only a few calculations with Bayes’ rule away.</p>
<p>Of course, we’d love higher accuracy, and we can get there with natural language processing. What about those ~10% of voters whose name aren’t in the census surname file? If a name starts with “Mc”, but isn’t in the census surname list, I personally would guess they’re of Irish descent, and therefore white. And what about using first names, middle names, and name suffixes? It’s linguistic patterns like these that we’ll hope to exploit with NLP, in the next post.</p>
<p>You can find the code used to write this post <a href="https://github.com/andytimm/CampaignBlog/tree/master/Race_Prediction/part1_wru">here</a>.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2018,
  author = {Andy Timm},
  title = {Predicting Race Part 1- {Bayes’} Rule Method and Extensions},
  date = {2018-04-10},
  url = {https://andytimm.github.io/race_models_part1.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2018" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2018. <span>“Predicting Race Part 1- Bayes’ Rule Method and
Extensions.”</span> April 10, 2018. <a href="https://andytimm.github.io/race_models_part1.html">https://andytimm.github.io/race_models_part1.html</a>.
</div></div></section></div> ]]></description>
  <category>From Old Website</category>
  <guid>https://andytimm.github.io/posts/Race Models Pt 1/race_models_part1.html</guid>
  <pubDate>Tue, 10 Apr 2018 04:00:00 GMT</pubDate>
</item>
</channel>
</rss>
