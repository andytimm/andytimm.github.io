<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Andy Timm</title>
<link>https://andytimm.github.io/blog.html</link>
<atom:link href="https://andytimm.github.io/blog.xml" rel="self" type="application/rss+xml"/>
<description>Personal website of Andy Timm</description>
<generator>quarto-1.6.39</generator>
<lastBuildDate>Wed, 27 Nov 2024 05:00:00 GMT</lastBuildDate>
<item>
  <title>A Tale of Two Design Effects-</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/a_tale_of_two_design_effects/two_design_effects.html</link>
  <description><![CDATA[ 




<p>Now that the cycle is over, and I have a bit more time to write, I want to unpack some interesting (also, <em>frustrating</em>) moments in polling that occurred this cycle.</p>
<p>Let’s start with one that occupied pollster twitter for several days: are we all dirty herders? Or are we just increasingly weighting on quite predictive things like party or 2020 vote choice? Moreover, why do these look similar to so many folks, and what could pollsters have done to avoid this misconception?</p>
<p>Here’s a rough outline:</p>
<ol type="1">
<li>Weighting can reduce variance (I’ll show how), but some people interpreted low variability of 2024 vote choice estimates<sup>1</sup> as a blindingly obvious sign of herding.</li>
<li>The above misconception isn’t terribly surprising, given how pervasively our industry tends to think of and communicate about variance and weighting in terms of Kish’s design effect. I’ll explain how Kish’s design effect generally encodes an assumption about weighting increasing variance, and show an alternative design effect (Henry/Valliant) that does correctly get smaller when weighting should reduce variance.</li>
<li>…But any discussion of design effects that get smaller due to weighting need to grapple with the fact that empirically, our MoE’s are generally far too narrow, not too wide. As Andrew Gelman loves to say, take the stated MoE, and double it to get a realistic one. So to square that circle, I’ll discuss about what each deff gets right and wrong, and sketch out about what more ambitious uncertainty quantification could look like.</li>
</ol>
<p>More ambitiously, I’d like to use this post as a jumping off point to discuss the design based inference roots of the polling industry, and the tensions that perspective faces as accuracy increasingly demands a more and more model based approach in the era of low responses rates and broad adoption of non-probability methods.</p>
<section id="weighting-can-reduce-variance" class="level1">
<h1>Weighting Can Reduce Variance</h1>
<p>Let’s briefly lay out the argument I’m responding to here:</p>
<ol type="1">
<li>Polls give us their margin of error<sup>2</sup>, so we should have a reasonable sense of the expected spread of estimates we should expect over lots of surveys. For example, a <img src="https://latex.codecogs.com/png.latex?N%20=%20800"> poll should have roughly a +/-6pp margin of error on the difference between Harris and Trump’s vote share<sup>3</sup>.</li>
<li>In October, it felt like nearly all the battleground state polls showed the candidates within like 2.5pp and often even less.</li>
<li>If we take the reported MoEs seriously, we should see about 20% of polls show differences larger than 3.8 points, and 5% should show differences larger than 6 points. If we take the reported MoEs, something has to be seriously fishy.</li>
</ol>
</section>
<section id="why-does-that-look-like-herding" class="level1">
<h1>Why does that look like Herding?</h1>
<section id="kishs-design-effect" class="level2">
<h2 class="anchored" data-anchor-id="kishs-design-effect">Kish’s Design Effect</h2>
</section>
<section id="henry-valliants-design-effect" class="level2">
<h2 class="anchored" data-anchor-id="henry-valliants-design-effect">Henry &amp; Valliant’s Design Effect</h2>
</section>
</section>
<section id="but-gelman-says-my-moe-is-too-small" class="level1">
<h1>But Gelman says my MoE is too small!</h1>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Relative to the polls stated margin of error.↩︎</p></li>
<li id="fn2"><p>Possibly adjusted to be wider by a design effect.↩︎</p></li>
<li id="fn3"><p>Note that this is the MoE on the difference between the two (higher than just the MoE on 1 proportion), and that I’m not adjusting for a design effect here. The specifics might make this a bit wider or narrower, but I’ll be more precise in a moment when it actually matters.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div id="quarto-reuse" class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2024,
  author = {Timm, Andy},
  title = {A {Tale} of {Two} {Design} {Effects-}},
  date = {2024-11-27},
  url = {https://andytimm.github.io/posts/a_tale_of_two_design_effects/two_design_effects.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2024" class="csl-entry quarto-appendix-citeas">
Timm, Andy. 2024. <span>“A Tale of Two Design Effects-.”</span> November
27, 2024. <a href="https://andytimm.github.io/posts/a_tale_of_two_design_effects/two_design_effects.html">https://andytimm.github.io/posts/a_tale_of_two_design_effects/two_design_effects.html</a>.
</div></div></section></div> ]]></description>
  <category>Survey Weighting</category>
  <category>Nate Silver griping</category>
  <guid>https://andytimm.github.io/posts/a_tale_of_two_design_effects/two_design_effects.html</guid>
  <pubDate>Wed, 27 Nov 2024 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Better discrete choice modeling through the rank ordered logit</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/doing_maxdiff_better/better_maxdiff.html</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/doing_maxdiff_better/imgs/jimmy_dunk.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>Jim Savage calls the MaxDiff model of discrete choice a “mathematically incorrect model of a psychologically incoherent concept”<sup>1</sup>.</p>
<p>Despite this lovely dunk, and some wonderful <a href="https://discourse.mc-stan.org/t/post-on-ranked-random-coefficients-logit/7136/2">notes</a> explaining why MaxDiff’s not great, the model remains frequently used, most prominently in market research, as implemented in <a href="https://sawtoothsoftware.com/">Sawtooth</a>. Why is this?</p>
<p>Beyond the most obvious explanations like inertia, it’s also surprisingly hard to find a fully fleshed out explanation of alternatives online. In this post, I aim to remedy that, with sections for:</p>
<ol type="1">
<li>An introduction to MaxDiff and its limitations</li>
<li>An illustration of how these issues impact final model quality</li>
<li>Alternatives: rank-ordered logit with connected choice graphs, and rank-ordered logit with unknown middle options</li>
</ol>
<section id="introducing-maxdiff" class="level1">
<h1>Introducing MaxDiff</h1>
<p>If you’re reading this post, you likely have some familiarity with the basics of discrete choice models, but here’s a brief refresher<sup>2</sup>.</p>
<p>We want to build a model of how people make decisions among discrete options. This could be:</p>
<ul>
<li>A pollster asking which candidates or political parties each respondent favors</li>
<li>A marketing firm studying preferences amongst a variety of chocolate bars</li>
<li>A political scientist asking which message voters find most convincing</li>
</ul>
<p>One reasonable model<sup>3</sup> for this is to say that individual <img src="https://latex.codecogs.com/png.latex?i"> making choices among <img src="https://latex.codecogs.com/png.latex?j"> options can be inferred to have some underlying utility <img src="https://latex.codecogs.com/png.latex?%5Cmu"> from the available choices, and that while they usually choose their most preferred option according to their utility function, there is some degree of randomness. The basic model is then</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AU_%7Bij%7D%20=%20%5Cmu_%7Bij%7D%20+%20%5Cepsilon_%7Bij%7D%0A"></p>
<p>where the <img src="https://latex.codecogs.com/png.latex?%5Cmu_%7Bij%7D"> can have rich provenance (demographics and other individual traits, the context in which the decision is made, the other options available…) but is fixed, and there’s some <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_%7Bij%7D"> of randomness involved. To make this model easier to estimate, we assume that <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_%7Bij%7D"> has a <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Gumbel</a><sup>4</sup> distribution.</p>
<p>Under this model, the probability that individual <img src="https://latex.codecogs.com/png.latex?i"> chooses alternative <img src="https://latex.codecogs.com/png.latex?j"> from a choice set <img src="https://latex.codecogs.com/png.latex?C_i"> is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP_%7Bij%7D%20=%20%5Cfrac%7B%5Cexp(%5Cmu_%7Bij%7D)%7D%7B%5Csum_%7Bk%20%5Cin%20C_i%7D%20%5Cexp(%5Cmu_%7Bik%7D)%7D%0A"></p>
<p>This is the standard multinomial logit. We could further decompose this <img src="https://latex.codecogs.com/png.latex?%5Cmu_%7Bij%7D"> into a linear function of explanatory variables, but this is not central to my point, so I won’t build up notation for these.</p>
<p>Now, suppose we have a dataset where each individual <img src="https://latex.codecogs.com/png.latex?i"> has made a choice <img src="https://latex.codecogs.com/png.latex?y_i"> from their choice set <img src="https://latex.codecogs.com/png.latex?C_i">. The likelihood of observing this dataset under the multinomial logit model is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL%20=%20%5Cprod_i%20P_%7Biy_i%7D%20=%20%5Cprod_i%20%5Cfrac%7B%5Cexp(%5Cmu_%7Biy_i%7D)%7D%7B%5Csum_%7Bk%20%5Cin%20C_i%7D%20%5Cexp(%5Cmu_%7Bik%7D)%7D%0A"></p>
<p>The likelihood is the product of the choice probabilities for each individual’s observed choice <img src="https://latex.codecogs.com/png.latex?y_i">. We can estimate the parameters of the utility function <img src="https://latex.codecogs.com/png.latex?%5Cmu_%7Bij%7D"> by maximizing this likelihood function (or, more commonly, the log-likelihood). This is how the single best choice data is naturally incorporated into the likelihood function. Each observation contributes a term to the likelihood based on the probability of the chosen “best” alternative under the model.</p>
<p>While we can (and will) push the basic logit model of this further to include respondent and choice level covariates, multilevel components, and other improvements, let’s think about the pressures of gathering data here for a moment, since that’ll motivate the desire for something like MaxDiff.</p>
<p>To estimate this, we recruit participants and ask them to choose their favorite option amongst a given choice set. As a way to control the difficulty of making a choice while still gathering enough data, we can limit the size of the set (choose the best of 10 items, instead of 20), and repeat the choice task.</p>
<p>Getting respondents and getting them to stick through a bunch of choice tasks is hard though, and it’s only natural to wonder: can we extract more with each choice set? One option would be to ask the respondents to rank ALL the options at once, but if you have a large choice set that sounds exhausting.</p>
<p>What if we asked people to choose their best and worst choices each time? After all, people might not have strong preferences amongst the middling 18 chocolate bars, but the best and worst seem more memorable, and that’s only 1 more choice.</p>
<p>And now, what if we don’t want to entirely change up our likelihood to handle the new rich source of data? Instead, what if we just treat the worsts as the opposite of the bests, which makes a sort of sense, and simplifies the likelihood one hell of a lot:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AU_%7Bij%7D%5EW%20=%20-U_%7Bij%7D%5EB%20=%20-(%5Cmu_%7Bij%7D%20+%20%5Cepsilon_%7Bij%7D)%0A"></p>
<p>Under this assumption, the probability of individual <img src="https://latex.codecogs.com/png.latex?i"> choosing alternative <img src="https://latex.codecogs.com/png.latex?j"> as the worst is: <img src="https://latex.codecogs.com/png.latex?%0AP_%7Bij%7D%5EW%20=%20%5Cfrac%7B%5Cexp(-%5Cmu_%7Bij%7D)%7D%7B%5Csum_%7Bk%20%5Cin%20C_i%7D%20%5Cexp(-%5Cmu_%7Bik%7D)%7D%0A"></p>
<p>… Now we’ve stepped in it, and Jimmy is mad at us.</p>
<section id="psychologically-incoherent" class="level2">
<h2 class="anchored" data-anchor-id="psychologically-incoherent">Psychologically Incoherent</h2>
<p>What I’ve introduced is the core of the MaxDiff formulation of discrete choice, before bells and whistles are introduced. This has some deep problems though; let’s start with the “psychological” ones. Human decision-making is an incredibly complex, not always logical process, and we’ll always be losing significant fidelity in boiling it down into a model. Here though, I’ll focus on explaining a handful of breakdowns in the relationship between reality and the world of MaxDiff models that are particularly harmful.</p>
<p>First, let’s talk about <strong>symmetry</strong>. With the MaxDiff likelihood above, we’re treating the worst as equal and opposite to the best. For example, though, I don’t like Joe Biden as much as I dislike Trump<sup>5</sup>. I really really like the best pizza in Brookyln, but New York pizza is all New York pizza, so it only gets so bad. This might be a reasonable simplification in some cases, but it’s hard to argue that baking this into our model faithfully mirrors reality.</p>
<p>Also, we’re not only asking them to be symmetric, we’re sort of conjoining the best and worst choice, asking them to share the <strong>same utility scale</strong>. In other words, the factors that make an alternative more attractive for the best choice are assumed to make it equally less attractive for the worst choice. An easy example is something like health risks- “that sushi place gave me food poisoning” is very relevant to my choice of worst restaurant, but the moment I have to think about food safety, a restaurant isn’t really anywhere relevant on the “best” side of the spectrum for me. Again, you can probably think of a case where this is a fine approximation, but in an ideal world, we won’t force ourselves to weld our notions of best to our notions of worst.</p>
<p>Finally, the <strong>error variances</strong> being treated as the same should feel pretty strange. I’m much, much more consistent in my selection of “bests” than “worsts”- why would I spend a bunch of time deciding which opinion of 20 is the absolute worst and which is just 19th worst? People tend to be much less consistent, and frankly much less engaged, with their worst choices. Why would we bake this into our model?</p>
<p>Putting this all together, the simplification in estimation that comes with MaxDiff also leaks into how the model “sees” the decision maker, and it meaningfully distorts the map in a way that does not necessarily reflect the territory.</p>
</section>
<section id="mathematically-incorrect" class="level2">
<h2 class="anchored" data-anchor-id="mathematically-incorrect">Mathematically Incorrect</h2>
<p>Beyond calling the model psychologically incoherent, Savage also says the model is mathematically incorrect. This feels like a slightly stronger insult, and indeed it is, in the sense that the model is failing on its own terms.</p>
<p>How? Well, we’ve explicitly laid out a model with Gumby errors:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/doing_maxdiff_better/imgs/gumby_distribution.webp" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>Wait, no sorry sorry<sup>6</sup>. we’ve laid out a model with Gumbel errors <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_%7Bij%7D">. As a reminder, the Gumbel distribution is chosen for its mathematical convenience, as it leads to a closed-form expression for the choice probabilities in the logit model. the PDF is plotted below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/doing_maxdiff_better/imgs/Gumbel-Density.svg.png" class="img-fluid figure-img"></p>
<figcaption>Probability density function of the Gumbel distribution. Source: Wikimedia Commons, https://en.wikipedia.org/wiki/Gumbel_distribution</figcaption>
</figure>
</div>
<p>Here’s where the trouble starts: like we discussed above, the MaxDiff model forcibly inserts an element of symmetry into the error distributions of the choices. However, as you probably already noticed, the <del>Gumby</del> Gumbel distribution is not a symmetric distribution! This is fine in the case where we’re just reasoning about the best choice, but even on its own terms the MaxDiff formulation doesn’t quite make sense.</p>
</section>
</section>
<section id="the-defects-are-not-just-theoretical-or-cosmetic-an-illustration" class="level1">
<h1>The defects are not just theoretical or cosmetic: an illustration</h1>
<p>So I’ve shown some ways I claim that the MaxDiff model of discrete choice falls short, but how much do they really matter? As someone with an appreciation for the <a href="https://www.cambridge.org/core/books/foundations-of-agnostic-statistics/684756357E7E9B3DFF0A8157FB2DCECA">agnostic perspective on statistical modeling</a>, I think it’s important to not only show that there are theoretical senses in which a model might be flawed, but further prove that these technical blemishes harm model performance on metrics we care about.</p>
<p>To do this, let’s generate some synthetic data, and fit both the best-choice and MaxDiff models to it. For this section, I’ll be starting from Jim Savage’s best choice model and simulated data <a href="https://khakieconomics.github.io/2018/12/27/Ranked-random-coefficients-logit.html">here</a>, and then add in the “worst choice as negative best choice” logic afterwards.</p>
<p>Let’s start with the synthetic data. If the exact DGP isn’t very exciting to you, feel free to skim or skip this section at first. I’ll give an overview after the block.</p>
<details>
<summary>
<strong>Data Generation for Simulation Study</strong>
</summary>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(tidyverse)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'tidyverse' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'ggplot2' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'tibble' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'tidyr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'readr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'purrr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'dplyr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'stringr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'forcats' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'lubridate' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.3     ✔ readr     2.1.4
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.4.3     ✔ tibble    3.2.1
✔ lubridate 1.9.2     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(rstan)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'rstan' was built under R version 4.2.1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: StanHeaders</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'StanHeaders' was built under R version 4.2.1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>
rstan version 2.26.13 (Stan version 2.26.1)

For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)
For within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,
change `threads_per_chain` option:
rstan_options(threads_per_chain = 1)

Do not specify '-march=native' in 'LOCAL_CPPFLAGS' or a Makevars file

Attaching package: 'rstan'

The following object is masked from 'package:tidyr':

    extract</code></pre>
</div>
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(ibd)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'ibd' was built under R version 4.2.3</code></pre>
</div>
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(crossdes)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'crossdes' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: AlgDesign
Loading required package: gtools</code></pre>
</div>
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(combinat)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'combinat'

The following object is masked from 'package:utils':

    combn</code></pre>
</div>
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">options</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">mc.cores =</span> parallel<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">detectCores</span>())</span>
<span id="cb25-2"></span>
<span id="cb25-3"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">set.seed</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">604</span>)</span>
<span id="cb25-4"></span>
<span id="cb25-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Again, note that this code is just reproducing https://khakieconomics.github.io/2018/12/27/Ranked-random-coefficients-logit.html,</span></span>
<span id="cb25-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># although with a more real-world feeling number of respondents and</span></span>
<span id="cb25-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># and total choices. If this is taking uncomfortably long, feel free to scale</span></span>
<span id="cb25-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># down I, the results should reproduce just fine outside irresponsible I.</span></span>
<span id="cb25-9"></span>
<span id="cb25-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Number of individuals</span></span>
<span id="cb25-11">I <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">800</span></span>
<span id="cb25-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Number of tasks per individual</span></span>
<span id="cb25-13">Tasks <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb25-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Number of choices per task</span></span>
<span id="cb25-15">J <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb25-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Dimension of covariate matrix</span></span>
<span id="cb25-17">P <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb25-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Dimension of demographic matrix</span></span>
<span id="cb25-19">P2 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span></span>
<span id="cb25-20"></span>
<span id="cb25-21"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># demographic matrix</span></span>
<span id="cb25-22">W <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">matrix</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(I<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>P2), I, P2)</span>
<span id="cb25-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Loading matrix</span></span>
<span id="cb25-24">Gamma <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">matrix</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(P<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>P2), P, P2)</span>
<span id="cb25-25"></span>
<span id="cb25-26"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Show W * t(Gamma) to make sure it looks right</span></span>
<span id="cb25-27">W <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">t</span>(Gamma)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               [,1]          [,2]          [,3]         [,4]         [,5]
  [1,] -1.147325812   1.453389560 -0.1201172125  0.964840281 -4.537077496
  [2,]  0.274237431   2.755783152 -3.0698355631  1.205484626 -0.334220382
  [3,] -1.776487023  -3.676627003 -4.1725965643  2.232580311  0.030518687
  [4,]  1.303242055   2.796902290 -1.0818975319  2.213676892  0.460143743
  [5,] -0.038077610   1.013494581  1.5886972343 -0.357147508  0.735694540
  [6,] -0.609004672  -2.399734277 -2.8631005810 -1.385657832  0.701776800
  [7,] -0.003016024  -2.605408894 -0.0723969226 -0.315353540  2.563850493
  [8,]  2.091206040  -0.275019611  0.7615789884  0.979190201  2.017939901
  [9,] -1.601434736  -1.203864883 -1.8161995764  0.075095339 -0.086070281
 [10,]  2.181393983   0.010819374  3.0136121632  0.444287730  3.280880660
 [11,] -0.923930456   5.505541925  0.2091929622 -0.312569250 -3.015452664
 [12,]  2.076792020  -4.464111386 -0.7351549450 -1.968268039  4.015238464
 [13,] -3.104754729  -0.703419923 -2.9470505419  0.066057565 -3.054629868
 [14,] -0.880319086  -3.420345971 -0.0277780987 -2.667776798  1.830987262
 [15,]  3.226864181   0.529723362  1.5324612847 -1.871606991  2.964084007
 [16,] -0.930974513  -3.108637587 -1.3524866917 -2.109222757  2.365901280
 [17,] -1.310019841  -4.169604411 -2.2627166822 -1.266567496  1.043406974
 [18,]  3.161574540  -2.611419524  3.0503506088  0.227149338  5.338110678
 [19,] -4.312627339   2.514695771 -3.2054050106  1.419674971 -5.850313526
 [20,] -0.369833548  -0.850701972  0.0437976453 -0.814807451 -0.217777666
 [21,] -0.093495250  -2.056789230 -1.9104855848 -0.660730465  1.329483831
 [22,] -0.067175662   1.879783406  0.4375834364  1.740624948 -2.363759883
 [23,] -0.856613421  -3.888184893  0.6467183551 -1.229485027  1.794434607
 [24,]  0.687448612  -2.803472954  1.6679059947 -0.254895844  1.529721953
 [25,]  1.720909217  -6.504963652  2.0340556967 -0.116379708  4.536006412
 [26,]  0.208506170   0.882608635 -0.4012337183 -0.465793586 -0.879646390
 [27,]  0.112184349   1.525029004 -1.7386757284 -0.331554096 -0.164149516
 [28,] -2.119012895   2.134066676 -0.3907224457  0.074333553 -3.719055698
 [29,]  2.480228434   2.103264918  2.5432286975 -1.296866193  1.938367170
 [30,]  2.138337683  -0.926221470  2.0946608957  0.926451734  3.379799523
 [31,] -0.914561807  -6.540433106 -0.4179541821 -1.086444475  3.049195665
 [32,] -1.062292815  -1.300877435 -0.4361758507 -0.799892721 -0.335557689
 [33,]  0.245720595   3.073001625  0.6991710796 -0.074686140 -2.225227033
 [34,] -0.377272607  -0.353870328 -1.3458442750 -0.910899967 -0.565737827
 [35,] -0.295052509   0.091813224  2.0721980064  0.882913806  0.498928760
 [36,] -2.521616146   0.977427103 -4.3256955047  1.887656083 -4.298542059
 [37,]  0.080044948  -3.371503258 -0.2716704826  1.157519771  2.152985108
 [38,] -0.008685601  -0.656543690 -0.3087439318  0.093581253 -0.702426038
 [39,] -0.452865455   2.817498436  0.8123390925 -1.872157949 -2.724494438
 [40,] -2.485016731   4.433176832 -2.3819198326 -2.055012388 -3.777695095
 [41,]  0.919314701   1.311984501 -0.0900807513 -2.035894542  0.948855357
 [42,]  1.187100142   1.360097874  0.6672974682  0.964509720  1.670374710
 [43,] -1.581338997   1.866155832 -0.3658008112  0.905249003 -2.945159079
 [44,] -1.889823182  -3.081847141  1.0495166855 -0.385461308 -0.853659352
 [45,]  1.436584668  -0.518680614  0.5533374897 -2.208769780 -1.039659093
 [46,] -1.491045520   2.237414633 -1.3836291477 -4.248664708 -1.247598550
 [47,]  1.341838094   1.264542976 -0.4475810722 -2.563594967  1.510455298
 [48,]  2.743537922  -5.020213997  0.8264976930 -0.344415550  5.410768104
 [49,]  2.603513418   3.346058668  2.6413899936  3.800534752  0.640711970
 [50,] -1.139672402   7.052317591 -0.1562487781 -0.163534252 -7.507161898
 [51,] -1.893776574   2.040924415  0.3700712218  1.168897436 -3.361268354
 [52,] -2.766244964  -3.350457840 -2.2300027111  0.466177598 -1.176316863
 [53,]  1.051265095  -0.367123446  1.0517269291  0.408515674  1.643631211
 [54,] -1.236930061  -1.185028345 -1.3981317392  0.324433518 -0.908758321
 [55,]  0.340330445   1.845788495  1.8664202308  0.252825786 -1.005874480
 [56,]  3.918194976  -4.413687519  1.4843814220 -0.458038880  6.665507986
 [57,]  1.261571660   5.578593399  2.1264783330 -1.211710550 -2.051998156
 [58,]  0.132447017  -0.183630094 -1.3015783488 -3.174628410  0.761193274
 [59,]  0.177767767   0.642973124  0.5653449537  2.824666462 -0.775694646
 [60,] -3.016442920  -1.595111319 -2.9277032233 -1.767701996 -1.510719034
 [61,] -0.480377639   0.060127459 -0.0077221630  0.795443333 -1.187731088
 [62,] -3.543415601   0.429381240 -2.6984588659 -1.471647171 -4.774057565
 [63,]  0.286764561  -0.579039329 -0.4605559673  0.854811811  0.751781396
 [64,] -2.034091897  -0.263222388 -1.7213749996  1.194130069 -2.888655258
 [65,]  1.512810608   2.608794043 -0.0938323589  3.544357756 -0.277532923
 [66,]  3.255482897  -4.435563394  3.6355028985  3.623299587  6.258966684
 [67,] -0.412404419  -1.215755986  1.4495470425  0.076860781 -0.999827874
 [68,]  2.367336905  -0.781368179  3.1853606621  0.775734078  1.193184720
 [69,] -0.902277060  -2.145380764 -0.1414340913  0.229382106  0.120942905
 [70,]  0.584096490  -0.238561426 -1.8005325973 -0.193155113  0.556455753
 [71,]  2.283679546  -3.287338668  0.8685959457 -3.182096474  3.882607935
 [72,]  4.626043086  -3.939590808  4.9731979922 -0.596820342  6.225318515
 [73,]  1.951199285   1.150419025  1.5934541582  0.059301868  2.308793191
 [74,]  1.496241974  -6.335195985  1.2603894395  2.571418344  3.929775887
 [75,] -2.538895792  -2.766574835 -2.6999004387 -2.014100295 -0.403374526
 [76,]  1.778949680   2.650396428  2.0791506946 -0.037130488  0.534264533
 [77,] -1.212100728   2.442953752  0.1950404092 -0.610212638 -2.689368809
 [78,]  0.887260911  -6.483892589 -0.3126799591  0.856856722  5.702372152
 [79,] -0.627714623   2.483061948  2.6897637129  0.003376006 -2.631401234
 [80,]  0.402922728   0.441148139  0.6169147116  0.421889156  0.577734315
 [81,] -1.419340454   0.025800748 -0.0351987058 -1.712028649 -0.367138194
 [82,] -0.818013748  -0.418637206  0.0372427652  0.851869216 -1.507908724
 [83,] -2.220465012   3.144092238 -0.6640153584  0.748190825 -5.330762918
 [84,]  2.365394529  -0.192615206  2.4255379176  0.402529096  3.131663053
 [85,]  0.352101145  -1.892590125  1.5660426646  0.069608994  2.247204152
 [86,] -3.826769157  -0.266559135 -1.5387074416 -2.220052962 -3.165995840
 [87,] -0.572952327   1.218360954 -0.3820010251  1.427246541 -0.835182239
 [88,] -1.375655142   3.362327590 -1.0818254409 -0.046499489 -3.869823850
 [89,]  0.697157410   3.139312336  1.5479172301  1.177933414 -2.870252935
 [90,]  0.349982893   4.853684518  2.2167216829 -0.751651949 -3.435385031
 [91,]  2.689693021  -2.154542802  3.9248033242 -0.632343115  5.006995771
 [92,]  2.147461559  -3.566419990  0.2148003851  0.554312904  4.093823466
 [93,]  1.434951093  -1.436192351  3.5370133246 -0.821415867  0.049581963
 [94,]  1.776298329  -5.767818211  0.2352658101  3.618976060  2.999211025
 [95,] -0.223138997  -2.122979775 -0.7034167588 -0.924787202  1.518849012
 [96,]  1.023487867   4.395375208 -0.0827556764  2.367699021 -2.796291231
 [97,] -3.063696373  -1.642236719 -3.7699827007 -0.474634240 -3.936073520
 [98,] -0.629739909  -0.776520856  0.7733825328 -2.600767739  0.289085852
 [99,]  1.997778954  -6.260621815  2.1883804789 -0.807008086  4.849920887
[100,]  3.537479830   3.467595377  4.3234933259 -1.769656581 -0.132934189
[101,] -0.836774957  -0.349898988 -1.1197247128  0.662003198 -2.374277301
[102,] -1.714479293  -0.606878227 -2.0338281892  1.460501154 -1.049031311
[103,]  3.469213483  -3.382349105  4.1260182008 -1.662649743  6.293060752
[104,]  3.201526813   6.244561529  5.6044489592  1.553683445 -0.433072498
[105,] -2.484505799  -2.132311127 -4.6687197790  1.251026152 -1.060343818
[106,]  2.959983547  -0.381444310  2.9977317458  0.249857660  2.808447698
[107,]  0.666856614   0.444735394 -0.7317306334 -0.737839466  0.313371564
[108,] -1.384671399   0.982298534 -3.2105810866 -2.952758507 -1.680539934
[109,] -0.222921826  -0.988743910  1.6294818082 -1.481295193  0.173106656
[110,]  1.590595327   5.404892300 -0.7737593719  1.153015865 -1.599618363
[111,]  2.818008297   0.300213470  1.9323230304 -0.505861613  1.526842614
[112,] -1.150525193  -0.739456189 -1.2902837050  0.825635918 -1.473932172
[113,] -3.160246512  -1.609875092 -1.8634933032  0.507429702 -1.377127118
[114,]  1.853285765   0.838386270  0.4442075115  3.588161468  2.129641137
[115,] -0.038168723   4.152736587  0.8552937100  2.358162578 -2.091705045
[116,] -3.041824445   1.222596263 -2.1187971634 -0.632459855 -2.940498959
[117,]  0.005854236  -2.220440695  1.8771692699  1.855087160  1.698327886
[118,] -1.745411590   1.756340914  0.8794652293  0.049838402 -2.658977182
[119,]  2.558215172  -0.057773934  1.6972909350  1.980287155  1.595839232
[120,] -1.346182514  -4.261041304 -0.7180215449  0.124573680  1.589423937
[121,] -1.167097720   1.327505281 -0.6871473661  1.294948108 -2.497022633
[122,] -1.580601817  -3.915179576 -0.5468114295  1.327796569 -0.395895937
[123,]  3.543305534  -2.749472513  2.7333434678  0.913761437  5.013309927
[124,] -4.862137216   1.821961104 -6.6153447188 -2.772410920 -4.926524351
[125,] -2.565802516   0.048958460 -1.0522613918 -0.560930688 -2.272633542
[126,] -0.406992020   1.438129749  0.5620736673  1.970858739 -2.349174500
[127,] -0.728408129  -4.606261717 -1.9114059999  0.237351971  2.245823980
[128,]  0.954292400   0.240677679  2.0974612476  0.850151298 -1.883533392
[129,]  0.747923124   0.095019392 -1.2441459810 -0.510758282  1.608079453
[130,] -0.914430334   2.124500772  0.4885946238  2.999426585 -0.429998181
[131,] -0.359819540  -0.253552587 -0.1132966961 -0.200815617 -1.648253615
[132,] -0.139328268   7.034489159  0.6250800426 -0.170535689 -3.810217970
[133,]  0.953522385   1.884010369  1.1547927881 -0.312382764  1.379458936
[134,] -1.857727648  -1.146619917 -2.7326924813 -0.857607617 -0.595583835
[135,] -2.476550853   5.792733745 -0.3408842392  1.078955340 -5.819511378
[136,]  0.541670970  -3.180398748 -0.1610077968  2.305518553  2.575943006
[137,]  0.309640572   9.714652930  0.0938091071 -1.479894037 -3.092546972
[138,] -1.397122976   0.136784980 -3.3757715659 -1.426894182 -1.653748468
[139,] -2.612448104  -3.585558658 -2.6868253571 -0.171930483 -1.114902143
[140,]  0.352546035  -4.240933874 -1.0407189143  0.208588905  3.371469349
[141,] -3.668231027  -1.069134966 -3.8915683166 -1.000472226 -2.664830096
[142,]  2.553308174 -11.112539431  0.1167527872 -0.856026250  8.402730480
[143,] -0.424918290   1.624886199 -1.9578544984 -2.600295532 -0.600916472
[144,] -1.287314256   1.276821566 -0.9678671244 -1.268447260 -0.969383679
[145,] -0.632165862  -2.997690519  0.6401426790 -0.885560634  1.909204796
[146,]  0.532830858   1.332600922  2.0603421220 -0.416613225 -0.901738534
[147,]  1.540560940   3.561378335  1.8650690538 -0.818773162 -0.804894245
[148,]  0.629580556   2.245842790  2.5548795267  2.812633031 -1.467932558
[149,]  0.689517253  -4.412227181  1.1885964108 -1.431090347  3.518173141
[150,]  1.469975030  -3.505353628  2.2210507752 -1.623588521  4.614175476
[151,] -1.879405439   2.492590933 -0.8209443349  1.358453023 -1.932861038
[152,]  1.284802681  -1.603351763  2.5509952267  0.103186345  1.590995771
[153,] -0.518357312   1.520496894 -0.1049530572 -1.360796854 -1.002540873
[154,] -2.035583786   1.540903107 -1.6809522726 -0.798315062 -1.372671547
[155,]  1.365243210   1.856570645  3.0469864077  0.535801057  0.767015498
[156,]  1.700472147  -0.816657576  2.2121371353  0.271139362  1.990247881
[157,] -0.423109570   0.971688184  0.7216906450 -0.083891209  0.118230267
[158,] -0.841208862  -0.442723728  0.4199048893 -1.778977801 -0.867300281
[159,]  3.468085784   0.317782620  2.2703820574  1.988241109  1.442909750
[160,] -1.011212013  -2.540982305 -1.2813905508  0.781692721  0.124039233
[161,]  0.392894704  -5.317134588  0.7811303497  1.429310491  2.490071184
[162,] -1.931843597   4.907334086 -2.1455729645  2.647480778 -6.466075552
[163,] -3.781403832   4.017454293 -4.3496406091 -1.628341787 -6.628727306
[164,]  0.980492061   1.558050655 -1.1222253855 -1.024031550 -0.238917623
[165,] -3.620149394  -3.180970580 -4.6171890206 -1.784218351 -1.582808941
[166,] -1.789439979   0.421005361 -2.3126583691 -2.545659252 -1.124149187
[167,] -3.037035863   1.482902779 -4.0808936706  0.229985434 -3.715256993
[168,] -0.451051708  -3.739637759 -2.1926029239  0.569611062  1.474048658
[169,]  2.558105304   3.578045141  3.4994877056 -2.598877095  0.521487280
[170,] -2.668865263  -2.937307995 -3.6000880426  1.006330533 -2.308283910
[171,] -0.629253656   2.316638291  0.1959717326 -0.843487591 -0.458876760
[172,] -2.526830485   5.580101549 -2.1835854767  2.864041538 -6.282063257
[173,] -2.896383094   1.613015994 -2.1882028789  0.953678064 -2.812582555
[174,] -2.126136747  -1.408927322 -1.4901156366 -0.108056146 -0.810012782
[175,]  0.032191073   0.286540791  3.5306208729  0.461497680 -1.039204827
[176,]  2.879985756   0.305246777  1.5081647033 -0.688096990  3.467366126
[177,]  1.742609063  -0.591923248  0.5355973194 -2.221030509  1.982229934
[178,]  0.867383883  -1.223075770 -0.7517734651 -1.367410397  1.913453886
[179,] -0.104577213  -1.608611576  0.5709293862  0.332918428  2.004652769
[180,]  0.818823197  -3.553940675  2.3051189573  0.400908785  2.388338243
[181,] -1.963965677   0.810941534 -0.6998286687 -0.130454708 -2.936106038
[182,]  0.866753563   1.103379573  1.4701392916 -0.384914150 -0.719300450
[183,]  1.075088091   1.542711228  0.9604435703 -0.447268544  1.376065136
[184,] -2.717763276   3.616257801 -2.8156324972 -2.434903485 -5.123506644
[185,]  1.669532671   2.460479907  1.6832929387 -0.923080548  0.999847914
[186,] -4.648863959   2.461572524 -2.8060141910 -2.058874784 -6.390285650
[187,]  0.746116196  -5.709606201 -0.4536416437  1.684924215  2.310522161
[188,] -1.688720688   0.098756922 -1.7480798713 -1.557039792 -1.497607268
[189,] -1.363626484  -0.492397127 -1.8774761491  2.936029454 -1.015145119
[190,] -1.189492345  -3.536179736 -1.6973115124 -2.850060289 -0.150286136
[191,] -0.920743392   1.702088131 -1.2613004554 -0.128682949 -1.797273963
[192,]  3.029813673  -1.604138219  0.6448555084  0.614426166  3.527531324
[193,] -0.534149467  -6.264726120 -0.3782466940 -0.723594171  2.101432577
[194,] -3.824601917   2.815285338 -3.8322990385 -1.218507638 -5.082096098
[195,]  1.739799546  -2.621197389  1.2704575938 -0.771554377  3.023381451
[196,] -0.139306618   0.021946800 -1.0953285386 -0.273040643 -0.615705881
[197,]  1.242138598  -0.827290958  2.2803561329  2.090801097  0.281611843
[198,]  2.547365427  -0.718370934  1.7379140599 -1.554420533  3.651011434
[199,]  1.690492059   2.615566494 -0.3004369557  2.216552860 -0.028995586
[200,] -1.476849894  -0.210325358 -0.5894008516  0.917000417 -1.319248572
[201,] -0.449010798   0.731538415 -3.2950548238  0.585151802  0.499530338
[202,]  1.996812641  -2.538206587  1.4092494807 -0.851289275  2.972899004
[203,] -2.273057123   0.443327068 -1.5600480614 -1.187380822 -1.187891044
[204,] -3.039237390   1.231504931 -2.4029885110 -2.747038189 -4.058419031
[205,]  0.816166718   0.063374317  1.5774084078  4.547430572 -0.822935052
[206,] -0.814368598  -3.236086029  0.8560657174  1.310962088  0.753733122
[207,] -1.310726016   2.899045668 -1.1873692087  2.292840427 -4.459857007
[208,]  1.897888537  -3.360258420  0.9642859479  0.174503671  3.264650382
[209,] -1.377524064   5.147961000 -0.9819411254 -1.225044732 -4.161151863
[210,]  0.327902215  -0.698974873  0.7186816108 -0.694784244  0.234130780
[211,] -1.448902136   6.259048533 -1.2164213904 -1.411471142 -5.400895580
[212,] -0.522003051  -1.739029394 -0.8670067012 -2.770375227 -0.561036385
[213,]  2.673411988  -1.016495853 -0.6747520188  0.591249191  2.784038960
[214,] -1.215892977  -0.415808045 -1.0895045534  0.820895793 -2.250666618
[215,] -0.754155402   1.813781340 -1.0290878069 -0.609781967 -1.834208817
[216,] -0.807334091   7.068133310 -0.0821869415 -0.073573285 -2.995571770
[217,]  0.812653201   1.779177053 -0.2594524909  1.323654156 -1.095825872
[218,] -0.896751958  -0.361832540 -1.9362862156  1.795237499 -2.103083333
[219,]  0.209757854   3.583665799  0.2326097286 -1.566542797 -1.950016750
[220,] -0.607822241  -3.538951600 -2.7917321052 -0.442502962  0.283975904
[221,]  0.052146486   0.346397912  0.3416016254  1.463618957 -0.675893045
[222,] -5.310786249   5.767981616 -3.5079415103  0.916968336 -8.040401468
[223,] -0.691953186  -8.173689661 -0.3346364606 -0.215522285  3.140273580
[224,]  1.991069616  -0.596374018  0.5708604947  2.699472595  2.309430888
[225,]  0.078614759  -2.648703771  0.5062579158 -0.529760198  2.363780176
[226,]  2.175891410  -2.126548526  3.1736763034  1.294763122  2.463010180
[227,]  1.812951905   6.012398846  2.9967276141 -1.067353000 -3.450174183
[228,]  0.453049361  -0.232522850  1.4115287786 -1.736627297  0.507827544
[229,] -0.789007351   6.816253416  0.4974357963  1.120724203 -4.784708633
[230,] -1.575380204   1.172590311 -1.8607763401 -0.481842623 -3.691723827
[231,]  4.536526172  -3.156827344  1.9730953283  1.536107112  7.224333166
[232,]  0.304963349   0.036860067  0.8908635300 -1.785552557 -0.084320804
[233,]  0.551523434  -2.776311271  0.9790498414 -4.106160804  1.939161011
[234,]  0.703926169  -2.633192133  2.1855389175  0.668578013  2.884839927
[235,]  0.538887098  -2.163024200 -1.4539048566 -0.202130035  2.363363480
[236,]  1.367698124  -8.225190890  1.9844771734 -2.731895856  5.789233515
[237,]  2.824749194   4.321248513  1.8156925135 -0.427485763 -0.400294495
[238,]  1.393585533   1.480917362  0.7577597501 -1.357877401  1.070671309
[239,]  0.163552603  -0.578976550 -0.4883511487 -0.124428028  1.394408287
[240,] -0.920475884  -2.461520224 -1.0119835104  0.572654228  0.774039116
[241,]  1.982356203  -2.104508622  1.9643822426  1.842204669  3.949965471
[242,]  0.896462357  -1.484021354  0.5511371570 -1.295333668  2.809166041
[243,] -0.511007131  -0.747369394 -1.4982529998 -0.720837309 -0.049399521
[244,]  0.917626129  -3.291233076  1.3945606344  0.921575372  1.561545491
[245,]  0.839547974   1.650484012  0.1382287949  0.052416342 -0.932493792
[246,] -0.885494723   2.243508299 -0.5336227929  0.441883190 -1.775383435
[247,] -0.561973385   3.022758903 -0.7192179181 -0.457258703 -3.423247083
[248,] -1.305460710   5.253572824 -0.7021705409 -0.595639530 -3.786391834
[249,]  1.780132470   2.952712483  2.4939830867  1.475215234 -1.109003757
[250,] -1.356617662  -3.798825450 -2.3723579240 -2.649162422  2.065106628
[251,]  0.620675684  -1.259884049 -0.4700035428 -0.532646813  1.757537363
[252,] -0.058443036   3.106592500  2.7416651365  0.732194809  0.394657339
[253,] -1.159981538  -2.798949470 -0.8226539238 -2.607236880  0.988752042
[254,]  0.422641461  -0.864548204 -0.5650287922  1.409186249  0.432565093
[255,] -1.846547856   2.987349625 -2.4850219076  0.048490928 -2.421457945
[256,] -0.686255788   9.036875671  1.0756765000 -2.743353993 -4.496535647
[257,]  3.477634078   4.277376064  4.0902262869  0.433673400  1.815721651
[258,]  0.366882229  -6.055565213 -1.8476864156 -1.358202013  3.696759737
[259,] -1.692373702   5.386758317 -2.1524104913  1.672686577 -4.697762049
[260,]  1.943820084  -3.332214806  2.8575517713  1.140286571  3.554252967
[261,]  2.361391236  -2.146309725  2.4414752533  0.620583802  2.471858473
[262,]  2.451122721  -2.642804980 -0.3167921963  0.555782212  2.910926101
[263,] -2.562877130  -1.957868683 -3.2157912395  0.212888819 -0.522834488
[264,] -0.314433381   2.246267515 -0.7856006930 -0.394418069 -0.526038409
[265,]  0.288729938   0.222169307  1.6599138499 -1.376314073  0.862982408
[266,]  1.136783569   0.656235064  2.4824861239  1.964967849 -0.784760141
[267,] -2.824170801  -3.235570874 -2.1175974093 -0.094849037 -0.964841934
[268,] -3.277865848   1.760657537 -2.4134164484 -0.354930577 -5.195509392
[269,] -0.197985224   3.168330103 -0.1155721572  0.417135719 -1.760878324
[270,] -0.084495207  -0.421624898  1.3572956658 -1.773251038  1.509732778
[271,]  0.491427706  -3.095326065 -0.2069028117  0.995565149  2.247850372
[272,]  3.242559098   1.414395475  5.1585054446  3.069712127  2.476792126
[273,]  0.789003817   4.118414844 -0.2748804343  1.827961816 -4.126480903
[274,] -1.564744016  -0.701007429 -1.4063802825  0.597984003 -1.728985266
[275,] -0.746608048   1.346641439 -1.1332276864 -1.950216250  1.409067739
[276,] -0.250864970   2.249833478  0.2146301686 -1.452409000 -1.422875383
[277,]  0.999964906   1.542858584 -0.6411493364  1.202993774  0.896258261
[278,] -1.050198925  -3.792564575 -1.8759986065 -1.160351914  1.109446525
[279,] -1.060945275  -1.031792292 -0.3823303054  2.158079796 -1.483892572
[280,] -1.378020472   0.368215094 -3.2661952196 -1.638018888 -2.688433257
[281,]  0.153248293  -4.850542821 -0.5899299915 -0.725820418  2.630699167
[282,] -0.536571645   1.582055167  0.3709058639 -0.972421650 -1.913293489
[283,] -1.820060812   4.659002929 -1.1978200396  0.895666673 -4.961505430
[284,] -0.427099207   0.181793032 -1.7785365421  0.819952456 -1.050559958
[285,] -0.260361517  -1.423161826 -0.4064427527  2.121456521  0.169050890
[286,] -0.003748518   3.312654742 -1.1604807667 -0.417769596 -3.149985094
[287,]  2.812989352   1.619439565  4.0393513777 -0.788674282  2.080161055
[288,] -1.780721380   3.779178413 -2.1598868414 -0.536542168 -3.864738722
[289,] -2.735446228  -3.074115281 -1.5958466348 -0.747521239 -0.818903078
[290,]  0.337393852   1.543932843  0.6416575157  3.156805570 -1.568128255
[291,]  1.795486252   1.628566198 -0.4484571547 -3.143091163  2.399445758
[292,] -0.664075645  -2.415838300 -1.8822646136 -0.938905604 -0.272235763
[293,] -1.644891708   1.788543611 -0.1099643461 -1.799749507 -1.641883587
[294,] -1.835911684  -1.925047206 -3.4328655375 -2.462787270  0.475285171
[295,] -0.990776279  -1.765580053 -3.2955435297 -1.626794182  1.036678577
[296,] -0.245271457  -4.831363195 -1.5226459261 -0.043770371  3.666030816
[297,]  0.569967287   1.737752394  0.1355817010  1.510964361 -1.187605275
[298,] -0.299584538   3.360761771  1.1210215973 -1.395869183 -1.784654725
[299,]  0.477194454   2.443255325  0.5395754328  2.862167343 -0.420041862
[300,] -0.824156639  -4.132407143 -0.0219441275 -0.010716763  1.661706649
[301,] -1.260393289  -3.482049069 -0.8700705113 -0.682520467  2.323427297
[302,] -0.201296664  -1.116414363  1.7560019280 -1.767778904  0.200013786
[303,] -2.820823397   1.383841289 -1.5204703754 -3.530233489 -3.094894822
[304,]  2.219972811   3.649120217  2.9749625320 -0.055624671  0.307163272
[305,]  0.216853882  -0.665801975 -0.6269717748 -1.450179831  1.564789726
[306,] -4.800468856   5.478749919 -2.7897751169 -3.920517503 -6.904228425
[307,] -2.515283377  -2.592977459 -0.4688634456  0.244979591 -2.202451171
[308,]  1.197537457   2.379341310 -0.4595128053  2.286482608 -0.413465784
[309,] -2.012585043   2.838666030 -1.0447548720  0.552168827 -4.086716182
[310,]  2.358000477  -4.054649686  0.2669417685  1.543611910  3.295832475
[311,] -4.047714357  -4.408506285 -4.0891241465 -0.377377080 -0.866602945
[312,]  2.517577875   1.060988622  0.9718986985  1.153093903  0.714208479
[313,] -1.387150611  -0.871804702 -3.0397679104  2.696390974 -0.970477644
[314,] -0.631537950  -0.706169481 -2.6871867133 -1.471914053 -0.771659698
[315,]  2.297999881  -0.985575526  2.2902151774  1.684186562  2.943480911
[316,] -0.801397750  -5.527458296 -0.8356829415 -1.002822106  2.359299639
[317,]  3.081757791  -2.597842048  2.5077062967 -0.646799136  5.309396624
[318,]  1.526928857   1.827403311  1.5081841049 -0.087289039  0.497086648
[319,]  1.746192869  -3.156304504  2.1630106889 -2.775199500  2.833922316
[320,] -0.640844853  -1.707424612  2.6734105992 -1.223312014  1.593243152
[321,]  0.685432307  -2.761920328 -1.1438698229 -2.287135191  2.855302436
[322,] -3.580130836  -2.040506000 -5.2081651059 -2.635526086 -2.317890390
[323,] -0.008273563   2.950932294 -2.0929988845  0.024072140 -1.022442235
[324,] -2.341921052  -2.925617168 -0.8502426989  0.419269148 -2.282209548
[325,] -2.553772574  -6.352513160 -2.8675647885 -1.605145143  1.260437929
[326,] -2.058585249  -0.556104637 -1.7641074708  0.697550857 -0.925183763
[327,] -1.259480118   1.250441985 -2.8342113507  0.342850416 -0.568671546
[328,] -2.356427708   2.764884150  0.3517233016 -1.452263995 -5.334750436
[329,]  4.496090997   3.498766405  5.0683134336 -0.273712834  1.821998093
[330,]  1.035975859   1.903736935  0.0007916891  0.204415935  0.538509571
[331,] -2.451373154   5.259521405  0.2135237829 -1.624064203 -4.354078979
[332,]  0.043891910  -2.361961609 -0.7901052975  1.204027900  0.248944407
[333,] -2.532197966  -2.950513593 -1.8170632831  0.452354600  0.103487354
[334,] -1.690819506   2.680252951 -2.1882567675 -0.958951785 -4.087785035
[335,] -0.745221727   0.172301580 -0.7114413375 -1.295037090 -1.448698679
[336,] -0.828644750  -0.010770368  0.3800549781 -1.763020531 -0.307968315
[337,] -1.832777604   2.836506111 -1.4246544753 -1.184711778 -3.053256210
[338,]  1.519566763  -1.713892455  1.2092995136  2.039331146  2.340921972
[339,]  1.027249569   0.965102261  0.8566585044  0.424076590  3.098111147
[340,]  1.278320836   0.002497314  2.0690965640  0.766337606  1.453103988
[341,] -1.513215123  -5.131717998 -0.3821831180 -0.180453070  1.660833921
[342,]  0.359356037  -0.438468435  0.0917964465  0.163040652  0.562550194
[343,]  0.644615494   4.137694671  1.2838575261  0.147163915 -1.598879814
[344,]  0.213643335  -2.015199933  0.5159102127  1.464150195  1.377162392
[345,] -2.432923548   0.480195587 -1.8595872864  3.293856484 -3.005345228
[346,] -0.332333899  -0.482991292 -1.3185459178 -0.311970278 -1.687868685
[347,]  0.026409277   2.985440517  0.0160709439 -1.032682466 -0.632932543
[348,]  1.357084452   7.043885938  3.9079073029  1.740237359 -3.098172669
[349,]  0.456870932  -1.729319444 -0.4657653266 -0.414305785  0.725963334
[350,] -0.137666418  -2.158134292 -0.0741607676  0.144782912  2.753867844
[351,] -0.420192610  -7.374829689 -3.6695585011  0.860330078  4.311390698
[352,] -2.394323308  -5.389673624 -2.9626618241 -2.736825312  0.440184037
[353,] -0.068616531   3.292823390 -0.3383370646 -2.814942876 -1.256326086
[354,] -3.309667902   7.894702862 -1.7805857253 -0.084267066 -7.122762320
[355,]  1.314323070   3.444542770  1.1813573183  2.505931081 -0.398728562
[356,]  0.722143552  -4.790203071  0.4450831850  0.056212071  3.333812400
[357,] -0.771819515  -2.380335732 -1.2786225831 -2.341698298  0.332323311
[358,] -1.498312176  -5.273867306 -2.1380934069 -2.084250623  0.378591654
[359,]  0.299824084   1.449786014 -1.6976929883  3.306379370 -0.399401363
[360,]  2.329647757   1.038322514  1.6267597690 -0.684361602  1.293079293
[361,]  0.471356448   2.148515598  1.1252024542  0.546380605 -2.051023094
[362,] -1.250048799   0.678557307 -0.4973077146  1.576890530 -2.236894892
[363,] -1.514186757  -0.182337139 -0.7278413142 -3.240579937 -0.784419498
[364,] -2.996545580  -1.083828349 -2.5488364158  0.422588728 -1.995390088
[365,] -1.940556019   0.902307028 -1.3976860485 -1.367354015 -2.880138172
[366,] -2.791523173   4.076062975 -1.4399774454  1.573466276 -5.392101755
[367,] -2.408091034   1.299720336 -2.2852973584  1.881722586 -3.269797226
[368,]  3.820875226  -0.761538723  4.7306349561  0.674699068  3.557105530
[369,] -2.798760966   3.528654380 -3.0476683086  0.001778689 -4.604119147
[370,]  2.817791607   5.889432340  2.0295493274  0.719133960 -0.273473710
[371,] -0.126572242  -4.522054257  0.0357855299  0.008632871  2.232267623
[372,]  5.995531560  -2.265335206  4.8308268359 -0.629143304  7.143317426
[373,]  2.156928700   1.465282866  3.2409417105 -0.465831267  1.245576284
[374,]  1.030409310   2.948730452  1.9366168633  1.961540885  1.033447316
[375,]  0.432928558  -3.779201779 -0.7141907566 -0.221686496  1.742527060
[376,]  0.916526972   7.242238756  0.5544108669  0.105733108 -3.838443828
[377,] -3.487569245  -2.474051906 -2.4946733400  0.057406443 -2.208738315
[378,] -0.893835500   0.446854119 -0.9283223531  0.474138154 -1.091448085
[379,] -2.150241296   0.185050472 -0.6913233328  0.036837967 -2.644252901
[380,] -0.237253795   1.194272970 -0.4908948597  0.404721306 -1.693469179
[381,] -0.869537906   3.319607423 -2.2584913405  0.723497507 -2.060267399
[382,] -0.177616238  -2.339784868 -0.5337778997 -0.777606057  2.205508032
[383,] -0.565171052   0.666878155 -0.0340599129 -0.331408156 -1.545994058
[384,] -3.300993520  -0.380403127 -4.9918180665 -2.525357465 -0.335073758
[385,]  0.399802123  -1.475915971  0.0028480702 -4.508386865  2.821703958
[386,]  0.129954113   1.731369756 -0.0361766382  0.185206127 -1.513388943
[387,] -0.843443516  -0.637235310  0.5209086907  0.169651516 -0.741539078
[388,]  3.426952024  -4.500311904  1.0603295279  1.529183634  6.581931024
[389,]  3.565194231  -1.542060007  2.9750779269 -0.739484119  2.510948861
[390,] -3.280779964   4.062607431 -0.9053832876 -2.259670290 -3.966705459
[391,]  2.010960586   2.509440292  2.4796209672 -0.758585114  0.501385509
[392,] -0.346670385   0.293827742  0.8869619617  0.571785447  0.025925780
[393,] -0.176510929  -1.123325371  0.1672903444  0.387087930 -0.364143040
[394,] -0.836094906  -1.822918783 -0.1372034019 -1.030647288 -1.067025193
[395,] -2.959071753   6.044727186 -1.4066800674 -0.669100636 -5.948108958
[396,] -1.353431160  -2.943140168 -2.6669603898 -0.688079948 -1.448911292
[397,]  0.117004261   0.975015560  0.9054251362 -2.539219140  0.265730487
[398,]  0.356975915   0.029392264  0.3241238195 -0.785106534 -0.571822651
[399,] -2.023034288   3.893035331  1.9660923016 -1.645486441 -2.623749863
[400,] -0.500986882   2.321564694  1.2618729804 -1.308438505 -1.874042761
[401,]  0.404141821  -0.523101936 -0.6077818270  0.772714239  1.715825845
[402,] -0.640543501  -8.157576238 -2.8219361766  1.487057294  4.151240261
[403,] -1.800693181   4.116729169 -0.0851213698  1.566888798 -4.875282466
[404,]  3.185196501   0.520111507  1.0518299445  3.129502081  1.609747442
[405,] -1.020123600   3.049150439  1.1117514443 -1.340415412 -2.780888197
[406,]  4.309932927   0.386020583  3.4561104360 -0.445672046  5.746869838
[407,] -3.121702156   2.123491114 -2.5150757556 -0.568799765 -2.979108358
[408,] -0.015241285  -2.446383919 -1.1097423524 -0.041111957  0.976616685
[409,] -0.196127354   6.966414072 -0.0334626406  2.048691608 -2.865138005
[410,]  0.373525316  -1.171129072 -0.7893948354  2.384201274  0.382008279
[411,]  0.108900657  -0.810103386 -1.7045007460  1.370580639 -1.180496674
[412,]  0.831090826  -2.565610263  0.2406260185  0.591343194  0.566718511
[413,]  2.906847610  -3.508634991  1.2029936018  2.422803927  3.967051094
[414,] -0.596200885   0.544154904 -1.8685619219 -0.925784670 -1.460321718
[415,] -1.305128236  -0.900257464  0.4670259623  1.524989221 -2.103353381
[416,]  1.470058174   3.342565974  0.4359323038 -0.771130850 -0.205355886
[417,]  0.677411999   1.376181142  1.6843918604  2.231978526  1.214850686
[418,] -0.548564821  -2.625386782  1.6284479887 -0.261429189  1.460294863
[419,] -3.327350762   0.022108449 -2.3087874343 -0.999521916 -2.639165668
[420,]  1.519445791  -7.362959652  0.5153801542  0.170843358  5.700242905
[421,]  0.439385011   0.683238548 -0.1081730710  2.912977284  0.198953393
[422,]  3.173652638   1.334905516  3.0801652688  0.863362947  3.830287037
[423,]  1.554826821  -1.583934812  1.2402222574  2.529253528  2.020625776
[424,]  1.226966468  -3.622143929  1.0725571281 -0.727733812  3.196872654
[425,] -1.054943051  -1.483962006  0.2559551852 -3.665160226  0.457707432
[426,]  1.483368985  -2.455070045 -0.2739190190  0.600194768  0.555487717
[427,] -1.282941291   2.701874717 -0.4618195805  2.423807255 -2.688645367
[428,] -0.739740224  -2.300149467 -1.8634552325 -1.608391641  1.233255634
[429,]  0.879792731   1.779915334  1.3628628616  0.381397913  1.153251878
[430,]  1.206926607   0.351834514  0.5993600183 -0.530403360  0.770329841
[431,]  0.471988696  -2.079862164 -1.4782468514 -1.199637402  1.894200406
[432,]  1.816111523  -1.609570706  1.3713828636 -1.304475868  3.860225529
[433,]  2.032058115   3.007732914  3.1372808396 -0.096510729  0.831496344
[434,]  0.003572781  -2.103794423  0.9935247659  0.314942465  0.954295882
[435,]  0.304649305  -0.457526603  1.9407027989 -1.916293232 -0.401132173
[436,]  0.936449471   2.557489163 -0.3025254689  2.000114128 -1.721016640
[437,]  4.055775414  -3.852973083  2.1989533460 -1.639153845  5.480100733
[438,]  0.906832715   0.085631007  1.0367607106 -1.704632841  2.735549575
[439,] -0.538818434  -3.165449278 -1.5903444479 -1.561475929  1.469334707
[440,] -1.545381677   0.372687643 -2.0679884336 -1.482697067 -1.458572298
[441,]  2.075986634   3.432644046  0.3447383549 -0.926921160  1.047362044
[442,]  0.265103514   2.569231213  1.4780904026 -2.475381770 -1.099134945
[443,]  0.200342652  -1.177979470 -1.2539377069  0.586925064  0.337304403
[444,] -1.785079286  -2.712932332 -1.5051830322 -1.524511528  0.152819178
[445,] -1.035847660   3.769964122  1.1392004918  1.269082479 -3.010631109
[446,]  1.295976024   4.243907552 -0.3754817460 -1.254022767 -1.637106570
[447,]  1.017534385   1.791657397  2.0984124712 -0.960023146  0.434061837
[448,] -0.219033262   1.215302523  0.0187702129  0.588743204  0.831678318
[449,] -1.120190295  -0.104342017 -2.1823418697 -1.044332409 -1.851926295
[450,] -0.351316982  -0.148857987 -1.6688488575  2.332145883 -2.517597784
[451,] -0.749160343  -1.179728954  0.3399263399 -3.390386702  0.364363719
[452,]  3.632046943  -3.613266085  3.7538131006  1.194468037  4.472975588
[453,]  1.903897989   0.592007701  0.8835773391 -1.013941847  1.120218602
[454,]  1.223445726  -1.741374705  1.6617553337 -1.252918647  2.753773661
[455,] -0.941614169  -4.533435627 -1.8942575906  0.474935477  2.815084098
[456,]  1.637943688  -2.023704266  0.3759644672  0.409263248  1.726142271
[457,] -0.623481531   3.118016903  0.7347811503 -1.176778259 -0.106689134
[458,]  0.520036396  -1.158691092  0.4863931263 -1.568240430  0.960339971
[459,]  0.433968296  -0.446494166 -1.0810135302 -0.546000796  0.457577502
[460,]  1.413356244   5.442970300  0.4921085584  0.120897118 -1.811488549
[461,]  2.580662827   4.956397145  3.4975858006  1.850520862 -0.448516132
[462,] -2.084154746   2.581951723 -0.3829811570  0.994110721 -4.330909923
[463,] -3.758311554   0.006369473 -3.4385333584 -0.004529990 -2.090805455
[464,]  2.233372190  -1.747556571  2.3374873153  0.762338646  3.822333227
[465,] -2.421740707   3.513676229 -0.8337129443 -0.557430892 -3.593505048
[466,] -0.336660140   0.412802904 -2.3359097923 -0.160441130 -0.371951828
[467,] -1.465698569  -0.939923326 -3.0827039944 -0.380541565  0.125236603
[468,] -1.165735184   0.132146186 -1.6296004033  1.538743196 -1.302208920
[469,] -3.277372425  -0.035820041 -1.1058153964 -1.397956531 -3.358248544
[470,]  1.429328848   3.411462270  3.4115940841 -0.604048457  0.025718416
[471,]  2.453854564   4.084050610  1.8285919156  1.995518384 -1.156147694
[472,] -1.682814907   0.296375646 -0.4175517168  0.666486417 -2.056190672
[473,] -3.065298815   2.985699679 -2.7155226499  0.149130260 -4.254352864
[474,] -2.075750967  -1.188369057 -2.6244369968  0.980214350 -2.344367145
[475,]  5.755523575  -1.230789495  7.6875553303 -1.043490612  6.492020675
[476,]  0.654089682   2.155228795  0.4045567371 -0.941674482 -0.983062790
[477,]  0.693530007   2.202452402  0.5916435631  1.952856801 -2.337994314
[478,]  0.376746643   6.258486032  1.3007160622  2.038211211 -3.054021186
[479,]  2.312255171   3.246888455  3.4013795648  2.314332178  0.913937176
[480,] -2.659874345  -0.920508087 -2.6066703437 -1.455427321 -2.088144988
[481,]  2.442221589   0.165276924  2.7865796958  1.812285138  0.269527340
[482,] -0.143398850   2.358948143 -0.7263290326  0.074017618 -0.045361179
[483,]  0.096539487   1.065388304 -0.4350798345  1.028069222  0.649652264
[484,]  4.396060817   0.568427438  6.5438081039 -1.608908298  5.307983948
[485,]  1.034929454  -5.034442027  2.6886018106 -0.119318917  3.067543222
[486,]  1.261213579   2.140328931  0.4130152357 -1.252742441  0.431791809
[487,]  3.777735905   5.738368319  4.2797913614  0.336295263  0.900497936
[488,]  1.742453751  -0.509073903  0.8046036945  1.391903854  2.337276256
[489,]  0.598862123   5.602560176 -0.4892512706 -2.240561353 -1.647455187
[490,] -1.808966335   0.950544799 -0.4402703968 -1.114836792 -1.958751752
[491,]  1.147639951   3.045089771  0.6658656411 -0.468945869 -1.181528375
[492,] -0.302052105  -0.899231441 -0.9298090014  0.511887115 -0.149915393
[493,] -2.787062704   2.190348120 -0.8378460550  2.523854051 -5.293623579
[494,] -2.384163980   1.978987035 -1.5274853613 -0.027067227 -2.984903112
[495,] -2.339427452  -1.394737558 -2.9129292257 -0.592136919 -1.020572785
[496,]  2.591168272  -2.024848621  2.3191618940 -1.425546396  4.379194752
[497,] -2.315606888  -6.659263510 -0.7931744013  0.023099316  0.823265807
[498,] -1.479801643   0.199177540 -0.9951182020  2.950851013 -1.434877535
[499,] -2.598236434  -2.438550326 -2.3071029643 -1.614365594 -2.396126880
[500,]  4.684731619  -1.124806653  3.8999456391 -0.340925229  3.834046033
[501,]  1.718395624  -1.424069028 -1.1337578872 -1.438901085  2.011743809
[502,] -0.350617794  -0.650804759  0.4089715200  0.317384267 -0.475468421
[503,] -2.618672379   0.249212142 -2.7262094428 -3.266938767 -1.595143932
[504,]  1.160879361  -1.501700885 -0.3159113539  0.413675509  2.482799724
[505,] -1.058723481   1.772339794 -1.1449453640  0.528593060 -2.323751198
[506,]  1.461827652  -5.008462143  3.1742598518  0.293775466  4.729719631
[507,] -2.016149234  -0.963373341 -1.3853518956 -1.479719122 -0.591673555
[508,] -1.447306227   2.943607283 -2.5193851982  1.081540993 -4.028585710
[509,]  0.777620812   4.038731506 -1.2196686492 -0.948977677 -0.984959441
[510,] -2.274215743  -1.357999032  0.0348886473  0.920368579 -1.107955211
[511,] -3.147334257  -3.709682406 -2.2336661545 -1.154749624 -0.524728928
[512,]  2.178205062   0.118253465  2.5280841472  0.869509988  2.593017427
[513,]  0.416310461   3.263269275  1.0279936725  1.043265833 -1.982523273
[514,]  0.606487359  -1.736180222 -0.6890234684 -0.355851859  2.726844003
[515,] -0.504930141   4.796381794 -0.6793226240 -2.054837392 -3.945210391
[516,] -0.727309342   1.140961595  0.5812838120 -2.064489028 -1.053020530
[517,]  1.015979172   0.596699059  1.0168315394  0.095138489 -0.634886808
[518,]  2.177206805  -3.371566537  2.8686656322 -0.304577606  2.989458196
[519,] -5.452032484   4.376416265 -3.5403543505 -3.833492725 -6.587118634
[520,]  0.162546595   2.523109007  1.0413874246 -1.180434554  0.138107171
[521,]  1.001972200   2.148634614  0.3872183001  0.581784041 -1.770168648
[522,]  1.702542415   3.484408050  2.9648111772  3.069611588 -0.728037465
[523,]  1.402555873  -2.383322954 -1.4062534572  3.066911443  1.970978299
[524,] -2.307829558   2.975389109 -1.8844070926  0.445689800 -2.270805124
[525,]  0.668466457   4.649333212 -1.1470772328  0.349380962 -4.030893923
[526,] -1.927554531   1.176985649 -1.2100627011 -1.437482026 -2.242505659
[527,] -1.995456874   3.253252831 -0.9084933631 -0.242762166 -1.477475091
[528,] -3.251116635  -3.007832462 -3.3878926286 -2.243406869  0.226053791
[529,]  1.877642266   1.413479051  2.6512272462 -1.857131020  1.569857678
[530,] -1.049972421   0.020855932  0.2574370298 -0.343728751 -1.620806296
[531,]  2.834461246   5.016248333  3.5964275759  1.145922272 -0.453784061
[532,]  0.167344829  -3.509939864 -0.9309146546 -0.761886328  3.706009262
[533,] -3.049823154   2.236110310 -1.3448897266 -1.856894815 -3.485971384
[534,]  0.526536665   1.687006831  2.2215332469  2.672287465 -1.788753501
[535,]  1.633273340  -0.957197665  1.1389166145 -2.201998378  1.705893280
[536,] -1.139992739  -6.855312183 -1.2195266328 -0.200085706  2.581023693
[537,]  0.301942751  -0.029612354  0.1945668202  1.949447208  1.151611701
[538,]  1.290475345  -0.505408272  1.2702275229  0.092288252  1.431451260
[539,] -0.903461686  -1.702447760 -1.3842013519 -0.590125734  0.862797469
[540,] -0.172331315   5.331763579 -0.0909878019 -2.226112440 -2.545325375
[541,] -0.846052891  -2.613269793 -0.5303659582  1.921666687  0.906156217
[542,]  2.301965393   1.934757780  1.9001921097 -0.578429322  0.952667518
[543,] -1.002025349   0.588354242  0.8412176287 -0.513837550  0.637652298
[544,] -2.179999749  -3.750957639 -3.0202304397 -1.625441400 -0.663515299
[545,] -4.375896478  -0.333128177 -7.4313975399 -4.971850132 -4.514082469
[546,] -2.042611143  -0.607990125 -4.0177168229 -2.165745297 -1.679500922
[547,] -1.482734907  -1.986418551 -0.7001209317 -0.176834302  0.083835112
[548,] -1.638337586   3.242482956 -0.1670055422  0.740646580 -4.931943173
[549,] -0.833042627  -3.045476059 -1.6075459718  0.576270315  1.700108596
[550,] -0.273594983   1.778294975  0.3105959629  0.463975627 -0.711636790
[551,] -1.516525050  -2.271379529 -2.2436502280 -1.505621126  0.380675730
[552,] -1.638628048   2.175281219 -0.9591803903  0.014619932 -4.362384150
[553,] -1.170113159  -5.110500837  0.0323073667 -3.375816540  2.411445866
[554,]  1.673547179  -4.682467997  0.9679521071 -0.439571948  5.898395560
[555,] -0.689177059   5.406589849 -0.7006791297  0.009517968 -4.398153718
[556,]  1.709926311  -1.731562071  2.8129877774  1.263098166  2.317410080
[557,] -0.045110443  -2.094785463 -1.0889114387 -2.309107602  1.475913089
[558,]  3.846896184   0.442567230  3.9678876903  3.036868241  3.514141829
[559,] -0.690657480  -2.630552256 -0.8036164400  0.967667758  1.143878452
[560,]  2.480899813  -1.667020830  2.7925722470  1.663729333  1.489271092
[561,]  2.816367686  -1.821895999  3.7001080740  2.194194189  3.786378457
[562,]  1.527166114   3.330686868  1.1913812338 -1.102823796 -1.635001920
[563,] -0.993053446   1.719644698 -1.6143464201 -2.333644030 -0.484120492
[564,] -0.529606899   1.881044865 -1.1785461855  2.369653360 -3.317146095
[565,]  2.390283371   0.145878063  0.6613034887 -0.138057843  2.469828457
[566,]  1.129896823   2.585141888  2.7704905597  1.592002101  1.220857443
[567,]  1.785546031  -4.862551967  1.2474729044 -3.032824828  6.235628315
[568,] -0.359694320   1.086528394  2.0351272609 -1.946065522 -1.179936196
[569,] -1.460835217  -4.781142448  0.4166102951 -0.095297087  1.184300782
[570,]  1.180775426  -4.302735910  0.7851905185 -0.120296399  4.253901732
[571,]  0.824557804   1.079477161 -0.1133365775 -1.660113364  0.846366667
[572,] -0.439714482  -1.803001446 -0.0283155285  2.189670321  0.226888826
[573,] -0.396525959   2.966841234  1.6890565322 -1.492667968 -1.476711061
[574,]  0.640134316   2.062480839  0.6155279504  0.385553537  0.458440356
[575,]  0.435915340  -3.867260452 -0.3556649542  0.437707854  3.238045410
[576,] -1.586046425  -5.562491172 -2.3182650616  1.757738944  3.550290232
[577,]  3.734796245  -1.939343130  4.3159695238  0.923435123  4.448483743
[578,]  0.107885766  -4.096395609 -0.4058648615 -3.178922727  2.959180706
[579,]  0.292150660   6.650027940  1.7147024926  3.220320859 -2.993357343
[580,] -1.163388542  -0.490282694  1.7005644700 -2.278423975 -0.497236815
[581,] -0.254436813  -4.771251632 -0.1272173523 -2.161902706  3.328925880
[582,]  2.286309408  -4.989246450  3.7939482052 -0.939825615  4.845695279
[583,]  5.300738589   3.675560080  4.1071046641  1.124470977  3.068312370
[584,] -1.144907560  -4.627227342 -1.3567687296 -0.636048634  1.082324308
[585,]  1.435481821  -6.273809796 -0.4600575320  0.316032451  5.853268555
[586,] -0.044150599  -5.779164097 -2.4050619238  1.097357495  3.807099962
[587,] -0.884418342   1.609573560 -0.1043158725 -0.906490847 -0.420072700
[588,]  1.871024220   3.038820318  1.8631497672  0.083433549 -0.962048310
[589,]  1.597289267   2.152264536  1.3053984303  0.683902708 -0.215311968
[590,] -1.684722729   0.767191432 -2.2342991074 -0.587728084 -1.316599312
[591,]  0.901063871  -1.361110323  0.7593274019 -0.895492892  1.415411214
[592,]  0.060863767   2.326388765 -0.0247762495  1.103084471 -1.151651297
[593,] -1.253942822   2.222581918 -0.3786485312 -0.485735841 -1.549899438
[594,]  2.551567369   3.321406275  1.8981325762 -1.717846394  1.397453980
[595,] -1.666703405  -1.450441056 -1.6259547328 -1.121364885 -0.421868125
[596,]  0.985996824   1.598265899 -0.7873917128  1.118490957 -1.670996235
[597,] -2.054492328   3.100898453 -1.9596489969  1.536128709 -3.205598502
[598,]  0.614683422   9.865447526 -1.4644369565  2.428347437 -5.693914819
[599,] -3.969505747  -0.185765213 -5.1041285887 -1.198211593 -4.289183849
[600,] -1.249656772  -3.620582850 -3.0971775281 -2.169322322  0.332091834
[601,]  2.807177819  -7.311841229  2.4925955012  1.431159760  5.842353201
[602,]  1.697350714   4.211742396  3.3581603092  1.660672852 -1.513427914
[603,]  1.474316567  -0.172098358  1.5186415113 -1.564820007  2.200312026
[604,]  2.515899542   2.666596800  2.7525894498 -0.064088397  2.057706557
[605,] -0.686808814  -6.591696447 -3.9454376130  0.949991578  1.792121113
[606,]  2.443595233  -0.837717552  3.0713582766 -1.799166146  4.127095734
[607,] -0.062316217   4.331168021 -0.6765890736  0.389575183 -2.952638179
[608,]  2.392137234  -1.822984291  3.0146553005  2.315240582  1.412687147
[609,]  1.405128553   1.413225465  3.1081923843 -0.673972513  0.682742213
[610,]  1.598438524  -0.598902471  1.1318735989 -0.392652065  1.358472836
[611,]  2.945996741   0.219922255  3.3446295689  0.855022546  1.575001311
[612,]  2.441540333   1.432535916  1.6889227906 -0.041138774  0.517575220
[613,]  0.662791737  -4.206601401  1.7852459379 -2.960369668  2.335310826
[614,]  1.409769982   3.246361620  2.3872913793  0.879130992 -1.186169693
[615,] -3.713201200   1.554657929 -1.7953499868  2.890796982 -3.910438316
[616,] -0.367605547  -1.811389413 -1.4817665532 -1.057084934  0.358328835
[617,] -1.814956955   2.529764189 -2.6889061331 -0.867660171 -3.302865613
[618,] -2.250927305  -3.630320097 -3.3203844063  2.265118517 -0.567337065
[619,]  3.185804668   0.076784835  2.2794335947  0.598405700  3.012051340
[620,]  0.612485037  -3.639865941 -0.9873471322 -2.434328512  2.013078548
[621,] -2.666346316   3.408414503 -1.0137783546 -1.128261358 -4.107276833
[622,] -0.277152361  -1.555063943  1.5449620638 -2.096654772  1.628745241
[623,] -0.564162497  -0.794000344  0.9531111685  2.767378955 -0.320098658
[624,] -1.558464987  -0.680002369 -0.7668893262  2.199817044 -2.443047178
[625,]  0.962794783   4.184693190  1.4998903575  0.718249904 -1.584003578
[626,]  0.920494353   3.025654055  1.5866716514 -0.071368049 -0.537474981
[627,] -0.792606159   0.781473720 -1.2125693788 -1.549729192 -2.240582619
[628,] -1.272960629   2.143286753 -1.4465666848  1.017271451 -3.279670121
[629,]  0.300454258   7.698656926  0.3007005469  0.610739357 -3.616025233
[630,] -1.405415753  -3.151886648 -1.9268935361 -0.454877748  0.980769113
[631,]  4.912622179  -5.162062540  3.3426931963  0.003814061  8.589078840
[632,]  0.480629541  -2.536661228 -0.6980510994 -1.926167438  2.228495218
[633,] -0.590130968  -3.352279868 -0.0612257228 -2.107486348  1.111808882
[634,] -1.054843156  -4.083860206  0.3854095851 -4.061332503  1.328373369
[635,]  0.217405937  -2.777682942 -0.4668315493  0.663442913  2.787685290
[636,]  2.243994751   0.768431800  4.1809990679 -1.948713096  1.937764956
[637,] -0.273548320  -4.550628956  0.4570303741 -0.952786547  2.156153931
[638,]  1.576767296   1.948546702  1.6215404919  0.144540619  1.176143143
[639,]  0.120669640  -0.871217197  0.9920319593  1.927565469  0.609587247
[640,]  0.906207717  -2.128306809 -0.7146282865  0.304020170  0.781921269
[641,] -3.456747208   5.961674485 -2.7179913632 -1.618433491 -3.999603301
[642,] -0.325269685   0.867991765 -1.4770639820 -0.781287706  1.017122843
[643,]  0.853920969   3.209140303  1.2683764805  1.909054788 -1.809231827
[644,]  1.558159195  -0.014461332 -0.7652724412  0.363188258  1.694745774
[645,]  1.919514333  -4.360346040 -0.0733399533  2.738784661  4.016452586
[646,] -1.158946360   6.516551674 -1.2714695126  1.135282199 -3.741180003
[647,] -2.347792498   3.281993171 -2.3536491530 -0.013153782 -4.550046070
[648,]  0.153184558   2.657409893 -0.7834774795  0.586570719  0.264892327
[649,] -0.075421816  -0.775408755  0.9653533443  1.924853136  0.554536603
[650,] -1.954970366   1.060069478 -2.9805575173 -0.252732875 -2.188717240
[651,] -0.986370464  -0.191289347 -2.5261327038 -1.063989770 -2.287301636
[652,]  1.989891273   4.020737668  0.8597697458 -3.512436279  0.652772052
[653,] -0.655660850   1.482542245  0.0233291917 -0.131102960 -2.798189534
[654,]  2.269737641   4.741250449  3.3258732971  1.984517982 -2.110310246
[655,] -3.738487292   3.763709588 -0.5226914389 -0.845292758 -4.760786178
[656,]  2.892186440   7.797695345  2.0448110862  0.215256879 -1.971526044
[657,] -4.041574525   4.242700420 -4.2601662544 -0.228402007 -6.104531143
[658,] -1.271420268  -3.965343661 -2.1320814150  0.598475261  1.500802383
[659,]  1.480175536  -0.305564529  1.2536996029 -0.961733524  1.814771745
[660,] -0.817766506  -0.573934864 -1.0385565888  2.462949563  0.549626275
[661,] -1.731724241   2.143396807 -0.9526306498  0.945961229 -1.830294342
[662,]  0.585149091   1.461671951  0.7586155195  1.900448974  0.492164620
[663,] -0.495709609   0.795564790  1.4584312385  1.649826683 -1.043117067
[664,]  0.470755908   1.296015036 -0.3172529956  0.918461943 -0.546944680
[665,]  3.457516906   2.890464758  3.4268584058  2.073116618  0.514225756
[666,] -0.984814535   5.190223662 -0.3571826800 -1.048280531 -2.473581727
[667,] -1.985051719   3.423216273 -0.1113935773 -0.557473528 -2.640056157
[668,]  1.832628587  -0.117776083  0.0356676424 -0.790898757  1.020076957
[669,]  0.601267502  -4.358435023  0.7933610446 -0.396714879  3.265145308
[670,]  1.713910830  -4.031945959  0.1570854388  1.882177065  3.383697621
[671,] -1.307008249   0.038764740 -0.2964032984  0.191008923  1.078037819
[672,] -1.358350068   9.230426208 -1.9648817632 -0.483711310 -6.263034644
[673,]  0.018399244  -6.301507331  0.0794882448 -0.178218998  2.943565818
[674,]  1.915898072   3.436674845  2.0127391342  0.843292647  0.556229629
[675,] -0.432599053   2.920379926  0.7948193036  0.204437662 -1.689926517
[676,] -1.992099908  -4.525137984 -1.4350708342 -0.494388714  0.711151351
[677,]  1.242120300  -2.573578856  0.5977245908  1.282401111  2.942843917
[678,] -1.145095487  -1.678504543  1.3899078712  0.656994549  0.332112827
[679,] -0.015690354  -4.393341040 -0.4095047587 -1.014950847  1.609557688
[680,] -0.056740498   7.246872133  0.9277882301  0.417821650 -2.964592327
[681,]  2.863001528   5.815099590  4.2050426473  0.034795117  0.722651793
[682,] -0.633851638   0.325610917 -2.9581543536  1.453603804 -1.561752756
[683,]  0.559220504   3.795815984 -0.3743257713  1.907908674 -0.810082219
[684,]  4.290333916   3.038887492  5.4016954071  1.574921554  3.273358137
[685,] -0.186794961  -0.405063155 -1.2739476718 -1.339539254  0.516570883
[686,] -3.106455899   1.705549934 -0.1634416855  0.142874203 -2.439664505
[687,]  1.130368008  -0.403120147  2.7368152317 -0.872782742  2.257992772
[688,]  0.395712921  -0.467215972 -0.0383191199  1.596814717 -1.392959400
[689,] -0.442939192   3.166005875  0.8747197733 -1.350961189 -0.973520549
[690,]  0.931682377  -3.752253291 -0.4845000549 -0.939445655  4.636790750
[691,]  3.290438502  -1.017672977  3.6825304889  0.337282254  4.678899145
[692,] -0.129510032  -0.505534205 -1.0205582578 -0.851913840  0.400727476
[693,]  0.514182192  -0.034610868  0.4721612257  1.033964641  0.949413118
[694,] -3.789008591   5.568587405 -2.3942592927 -0.491288004 -6.019993440
[695,]  2.271818612  -3.803097061  2.2211626074  0.703865127  4.922844826
[696,] -1.802301919  -0.794704670 -2.6342686374  0.641338030 -0.967936776
[697,] -2.195866141   2.025110636 -4.3008245008  3.356712750 -3.704175123
[698,]  0.214279931   1.036613828  0.6056408562 -1.481286239 -0.424443406
[699,] -4.365172549   0.187278757 -3.6618565528 -1.111036203 -3.903103259
[700,] -2.323497343  -0.320817663 -3.4068608234  1.607645459 -2.465016854
[701,]  1.044393633  -0.428398260  2.9262018472  1.567778654  0.999212100
[702,]  3.013580982  -0.796684430  2.7426273978  0.782634764  3.935755587
[703,] -0.070239091  -0.912825248 -0.1166317544 -0.470468335  1.254705196
[704,] -0.059963352   1.973892108  2.5170468154 -0.517458996  0.944609703
[705,]  2.590800760   5.333389430  2.5248245784  1.098300755 -3.161284126
[706,] -1.937571459  -3.545243310 -1.5855075438  0.340579276  0.118113564
[707,] -0.887914411  -3.483447899  0.1522108967 -1.231990784  3.269438095
[708,] -1.993934244  -0.756633361 -0.9638599280 -0.295013342 -0.321252029
[709,]  1.360879924  -0.424875101  0.7919573152 -1.024809312 -0.740646124
[710,]  0.676872148  -7.387174185  0.3715712062 -0.274664031  5.728938905
[711,] -0.449568822   2.127826640 -0.6750041341  0.432909702 -1.223875281
[712,] -0.353136069  -1.243011625  1.2526274956 -0.723871149  2.453420748
[713,]  0.271822297  -2.757667594 -0.0268512381 -1.993731689  2.818838556
[714,]  1.744307788  -1.702890373  0.1769017192 -0.879694049  4.175886178
[715,]  0.845854851   3.529937516  1.7911764224  1.682764062 -2.805446422
[716,]  0.911507841   5.394251355  1.7834682476  0.909599944 -2.869235165
[717,] -1.589666866   1.066421762 -1.6788048738  0.233739233 -1.984749517
[718,]  0.072676669   0.620257578  0.9230352605  0.367181712  0.875466143
[719,]  2.733833857  -1.627313953  1.3777588314  1.796585327  3.816847293
[720,] -1.567208031  -5.512627795 -2.3589619366 -0.007309992  0.875208551
[721,] -2.198977014  -7.540483938 -2.2145260112 -4.026983014  3.076707518
[722,]  0.211824342  -2.961546107  0.3984818951 -0.127663410  0.949887745
[723,] -0.242084838  -5.963216409 -1.9744710270 -1.140494379  4.379596896
[724,]  1.450507863   3.824337152  1.5752727914 -1.464988140  0.230288676
[725,] -0.330011897   7.797341487  0.2243535543  0.339937203 -5.222482640
[726,]  0.967186902   1.027704996 -0.9749764273  0.679918304  1.173466260
[727,]  0.310260769  -3.555189209  1.6184844055  1.303855692  2.857119038
[728,]  1.499787743  -6.809780038  0.5587978299  0.110214460  5.836555775
[729,] -1.554679937   3.593510241  0.0369922555 -0.640463796 -3.605683613
[730,] -2.003823483  -2.581327508  0.6285289255  0.028555318 -2.207076533
[731,] -0.227649281   1.886149333 -2.0845628002 -0.684688475 -1.788037190
[732,] -1.582911001   1.720968334 -1.0879294626 -4.951521331 -2.197972639
[733,] -0.476707192   4.674850588  0.8798566315 -2.640304340 -1.952383371
[734,] -0.402301478   2.004681409  0.4213662970  2.062354522 -0.761396983
[735,]  2.376127983  -3.072144128  2.6275935762  0.357626250  4.638671921
[736,]  0.881755693   1.304887493  1.6105670523  0.101049133  1.091238517
[737,] -2.240316474  -1.251649511 -1.6553466360  0.551624995 -0.427749905
[738,] -0.248697575  -2.384506310 -1.2135470015 -0.653784618  0.314643520
[739,]  0.434364025  -5.812317943  0.0620217349  2.204676077  3.246828405
[740,] -1.511312333   5.426706244  0.0307704846 -1.667977429 -2.912716168
[741,]  0.378134117  -2.227811270  0.8673000798  1.490642741  1.959776491
[742,] -0.885010370   1.584634889 -0.7366072092 -1.557049495 -0.234626007
[743,]  1.037919328  -2.457155326  1.5768421021  2.019570436  3.975374884
[744,]  1.690845925  -3.764061109  3.1130911387  1.951504874  2.618796393
[745,]  1.176998570   2.569612740  2.4960547322 -0.097386131 -0.160344469
[746,]  2.659626184   3.976778884  1.7595454730  0.823788730 -0.133357933
[747,] -2.269270463  -0.427690104 -1.3528046972 -0.698679557 -1.751949714
[748,]  2.178331713   1.134928081  1.3621819551  0.274208634  1.500396212
[749,]  1.197797085   0.133440675 -0.0803064531 -0.351949701  0.619688874
[750,] -1.104484541  -1.084351911  0.7691604146 -0.247406131 -1.219955039
[751,] -0.329593596   3.939781632  0.9424645150  1.206475499 -2.867678260
[752,]  0.010858794   0.028398559 -0.2107277207 -1.885100717  0.001396571
[753,] -1.479616958   2.731647357 -2.4164217671 -1.014898113 -2.085992305
[754,]  0.195745915  -3.202733305  0.8415752331 -0.079580354  2.632328641
[755,]  0.191399766   5.355333889  0.8862173860  0.076566862 -2.965684458
[756,]  2.305663578   1.407666944  2.3718010880 -1.793925219  0.311560361
[757,]  0.439864986   2.351877113 -0.2832473031  1.614253890 -1.792755643
[758,]  0.240579365   1.352740126 -0.8273565995  1.926354533 -1.276892509
[759,]  2.008900463  -2.843462241  1.0854392540 -0.132149892  2.756883438
[760,]  1.707347249  -3.547018578  1.3319123670 -0.313639326  3.289679363
[761,] -2.306343178   2.134064878  0.7435479682  0.259133250 -3.731965137
[762,]  3.005897520   1.962585636  3.9217290971  2.106037662  2.003094511
[763,] -2.624116364  -3.637138243 -2.5080971034 -0.413621980 -1.376724426
[764,]  0.779825304  -1.660827361  0.2163319832 -1.504609014  2.115022659
[765,]  0.909121331   0.193773300  1.4828518438  1.124248351  0.604931725
[766,]  1.253796757  -7.152543640  0.9778368409  0.789846536  4.200889320
[767,] -0.411597350  -6.303594948  0.7468999138  4.766816000  2.984946174
[768,] -0.698006025   4.472618910  0.0041807399  2.023273101 -2.230073237
[769,]  0.610022588  -0.196092967  2.7154887360  1.335621975  1.704899592
[770,] -1.209949272  -1.565391353 -0.0571311044 -3.515267148  1.283125706
[771,]  1.750078702  -4.218703627  1.4554044927  1.299355291  3.078215404
[772,]  0.507448900   1.283153025  1.3666862658 -1.219305136 -0.996568105
[773,]  1.540318056  -0.633239937  1.1932147476  1.851560294  3.300948840
[774,]  1.529795364   1.285885393  1.2113249076  0.131788086  0.441395772
[775,]  3.260148533   4.869505397  3.9457963782  0.518121243  0.815770297
[776,]  0.186580617  -1.818970504  1.9457621155 -1.509474471  1.806481046
[777,] -0.686455425   0.133385108 -0.9887047819 -0.933247930 -1.654272574
[778,] -1.568897702   1.290683715 -0.8589525665  0.568819816 -1.983675957
[779,] -0.395123967   4.772802593 -0.4663853558 -1.336303626 -2.201785096
[780,]  2.819021073   3.495155571  5.2626436958  1.529406919 -1.054558292
[781,]  1.080017687  -1.055016411  0.0911826557  1.769497663  1.431831269
[782,] -1.041286385   2.029462474 -2.7268325315 -2.684900070 -1.238199439
[783,] -2.359415274   0.692818593 -2.8617585620  0.028006792 -3.094883315
[784,] -0.434176986   1.416043164 -1.3438242357 -3.381264451 -0.584957352
[785,]  1.502935017  -9.895691883 -0.2830694337  2.035493488  6.868280944
[786,]  2.271699898   2.253528334  1.4955602255 -0.351720562  0.230735265
[787,] -2.162031903  -0.934477157 -0.6172285853 -2.283534170 -0.848020956
[788,] -4.117280669   0.953345090 -2.0811057689 -3.282284031 -4.499898820
[789,]  0.520669030  -2.724844099 -0.5389557936 -0.408134873  2.321375000
[790,]  3.373968880   0.417418920  1.3588290420  0.726449764  1.927558995
[791,] -2.362530279   2.652603444 -2.6537653916 -1.873838603 -2.942545393
[792,]  1.325366584  -1.417318168  0.0337180707  0.503419175  2.077593583
[793,] -0.846683806  -3.028072687 -1.2302037198  2.615584927  0.481792688
[794,]  0.150650384   0.063978018  0.6704218708  0.803128273 -0.225200563
[795,] -2.085416032   2.423736711 -3.5296847657 -0.477660067 -3.757569896
[796,] -3.696173237   0.996584657 -2.5646429761 -0.329361570 -4.232942081
[797,] -0.851170496   0.165851719 -0.2157931645 -0.149200861 -0.345746086
[798,]  4.278500744   1.609834226  3.9205400300 -1.277134949  3.291661044
[799,] -1.747572181   4.054734706 -1.3492809247 -1.534892400 -3.305015726
[800,]  0.498296810   0.506639979  0.6839018920 -1.723128274  0.169399823</code></pre>
</div>
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Correlation of decisionmaker random slopes</span></span>
<span id="cb27-2">Omega <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">cor</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">matrix</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(P<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(P<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)), P<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, P))</span>
<span id="cb27-3"></span>
<span id="cb27-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Scale of decisionmaker random slopes</span></span>
<span id="cb27-5">tau <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">abs</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(P, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>))</span>
<span id="cb27-6"></span>
<span id="cb27-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Covariance matrix of decisionmaker random slopes</span></span>
<span id="cb27-8">Sigma <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">diag</span>(tau) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> Omega <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">diag</span>(tau)</span>
<span id="cb27-9"></span>
<span id="cb27-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Centers of random slopes</span></span>
<span id="cb27-11">beta <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(P)</span>
<span id="cb27-12"></span>
<span id="cb27-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Individual slopes</span></span>
<span id="cb27-14">beta_i <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> MASS<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mvrnorm</span>(I, beta, Sigma) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> W <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">t</span>(Gamma)</span>
<span id="cb27-15"></span>
<span id="cb27-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Again, quick plot to sanity check</span></span>
<span id="cb27-17"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">plot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.data.frame</span>(beta_i))</span></code></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/doing_maxdiff_better/better_maxdiff_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create X -- let's make this a dummy matrix</span></span>
<span id="cb28-2">X <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">matrix</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sample</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, Tasks<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>I<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>J<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>P, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">replace =</span> T), Tasks<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>I<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>J, P)</span>
<span id="cb28-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Each of the rows in this matrix correspond to a choice presented to a given individual</span></span>
<span id="cb28-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># in a given task</span></span>
<span id="cb28-5"></span>
<span id="cb28-6">indexes <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">crossing</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">individual =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>I, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>Tasks, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">option =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>J) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb28-7">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">row =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">n</span>())</span>
<span id="cb28-8"></span>
<span id="cb28-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Write a Gumbel random number generator using inverse CDF trick</span></span>
<span id="cb28-10">rgumbel <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>(n, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">mu =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">beta =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) mu <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> beta <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">runif</span>(n)))</span>
<span id="cb28-11"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mean</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rgumbel</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e6</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.576678</code></pre>
</div>
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Ok, now we need to simulate choices. Each person in each task compares each </span></span>
<span id="cb30-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># choice according to X*beta_i + epsilon, where epsilon is gumbel distributed. </span></span>
<span id="cb30-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># They return their rankings. </span></span>
<span id="cb30-4"></span>
<span id="cb30-5">ranked_options <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> indexes <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb30-6">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(individual, task) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb30-7">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">fixed_utility =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.numeric</span>(X[row,] <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.numeric</span>(beta_i[<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">first</span>(individual),])),</span>
<span id="cb30-8">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">plus_gumbel_error =</span> fixed_utility <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rgumbel</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">n</span>()),</span>
<span id="cb30-9">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">rank =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rank</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>plus_gumbel_error),</span>
<span id="cb30-10">         <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># We're going to use the order rather than the rank in the Stan part of the model</span></span>
<span id="cb30-11">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">order =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">order</span>(rank),</span>
<span id="cb30-12">         <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># And here we create a dummy vector for the best choice</span></span>
<span id="cb30-13">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">best_choice =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.numeric</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">n</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">which.max</span>(plus_gumbel_error)),</span>
<span id="cb30-14">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">worst_choice =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.numeric</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">n</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">which.min</span>(plus_gumbel_error))</span>
<span id="cb30-15">  )</span>
<span id="cb30-16"></span>
<span id="cb30-17"></span>
<span id="cb30-18">tt <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> ranked_options <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb30-19">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(individual, task) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb30-20">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarise</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">start =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">min</span>(row), </span>
<span id="cb30-21">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">end =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">max</span>(row)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb30-22">  ungroup <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb30-23">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task_number =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">n</span>())</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`summarise()` has grouped output by 'individual'. You can override using the
`.groups` argument.</code></pre>
</div>
</div>
</details>
<p>While this simulation isn’t exactly simple, let me pause to point out something I’ve not done. I have not made any choices here that are designed to make MaxDiff look poor. Since we want to use a model of realistic complexity, this has simulated structure such that it’ll benefit from the random coefficients formulation I’ll use in a moment. Similarly, this has load-bearing covariates at both the individual and choice level, since we would almost always have those in the real world. But the core utility function we’ve created for respondents boils down to simulating data under the assumptions of the multinomial choice model with Gumbel errors we’ve been discussing all along.</p>
<p>Let’s specify the best choice model in Stan<sup>7</sup>:</p>
<details>
<summary>
<strong>Best choice logit model</strong>
</summary>
<div class="cell">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1">best <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"// Again, note that this code is just reproducing https://khakieconomics.github.io/2018/12/27/Ranked-random-coefficients-logit.html</span></span>
<span id="cb32-2"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">data {</span></span>
<span id="cb32-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int N; // number of rows</span></span>
<span id="cb32-4"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int T; // number of inidvidual-choice sets/task combinations</span></span>
<span id="cb32-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int I; // number of Individuals</span></span>
<span id="cb32-6"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int P; // number of covariates that vary by choice</span></span>
<span id="cb32-7"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int P2; // number of covariates that vary by individual</span></span>
<span id="cb32-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int K; // number of choices</span></span>
<span id="cb32-9"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span></span>
<span id="cb32-10"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  vector&lt;lower = 0, upper = 1&gt;[N] choice; // binary indicator for choice</span></span>
<span id="cb32-11"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[N, P] X; // choice attributes</span></span>
<span id="cb32-12"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[I, P2] X2; // individual attributes</span></span>
<span id="cb32-13"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span></span>
<span id="cb32-14"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int task[T]; // index for tasks</span></span>
<span id="cb32-15"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int task_individual[T]; // index for individual</span></span>
<span id="cb32-16"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int start[T]; // the starting observation for each task</span></span>
<span id="cb32-17"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int end[T]; // the ending observation for each task</span></span>
<span id="cb32-18"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb32-19"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">parameters {</span></span>
<span id="cb32-20"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  vector[P] beta; // hypermeans of the part-worths</span></span>
<span id="cb32-21"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[P, P2] Gamma; // coefficient matrix on individual attributes</span></span>
<span id="cb32-22"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  vector&lt;lower = 0&gt;[P] tau; // diagonal of the part-worth covariance matrix</span></span>
<span id="cb32-23"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[I, P] z; // individual random effects (unscaled)</span></span>
<span id="cb32-24"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  cholesky_factor_corr[P] L_Omega; // the cholesky factor of the correlation matrix of tastes/part-worths</span></span>
<span id="cb32-25"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb32-26"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">transformed parameters {</span></span>
<span id="cb32-27"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  // here we use the reparameterization discussed on slide 30</span></span>
<span id="cb32-28"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[I, P] beta_individual = rep_matrix(beta', I) + X2 * Gamma' + z*diag_pre_multiply(tau, L_Omega);</span></span>
<span id="cb32-29"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb32-30"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">model {</span></span>
<span id="cb32-31"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  // create a temporary holding vector</span></span>
<span id="cb32-32"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  vector[N] log_prob;</span></span>
<span id="cb32-33"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span></span>
<span id="cb32-34"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  // priors on the parameters</span></span>
<span id="cb32-35"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  tau ~ normal(0, .5);</span></span>
<span id="cb32-36"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  beta ~ normal(0, .5);</span></span>
<span id="cb32-37"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  to_vector(z) ~ normal(0, 1);</span></span>
<span id="cb32-38"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  L_Omega ~ lkj_corr_cholesky(4);</span></span>
<span id="cb32-39"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  to_vector(Gamma) ~ normal(0, 1);</span></span>
<span id="cb32-40"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span></span>
<span id="cb32-41"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  // log probabilities of each choice in the dataset</span></span>
<span id="cb32-42"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  for(t in 1:T) {</span></span>
<span id="cb32-43"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    vector[K] utilities; // tmp vector holding the utilities for the task/individual combination</span></span>
<span id="cb32-44"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // add utility from product attributes with individual part-worths/marginal utilities</span></span>
<span id="cb32-45"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    utilities = X[start[t]:end[t]]*beta_individual[task_individual[t]]';</span></span>
<span id="cb32-46"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    </span></span>
<span id="cb32-47"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    log_prob[start[t]:end[t]] = log_softmax(utilities);</span></span>
<span id="cb32-48"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  }</span></span>
<span id="cb32-49"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span></span>
<span id="cb32-50"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  target += log_prob' * choice;</span></span>
<span id="cb32-51"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}"</span></span></code></pre></div>
</div>
</details>
<p>Much of this logic handles the multilevel component and covariates, but the real key is the last few lines where the likelihood is built. That’s where the problem will arise when we extend the above model into MaxDiff:</p>
<details>
<summary>
<strong>MaxDiff model</strong>
</summary>
<div class="cell">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1">best_worst <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"// Again, note that this code is starting from https://khakieconomics.github.io/2018/12/27/Ranked-random-coefficients-logit.html</span></span>
<span id="cb33-2"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">// I've just adapted to to include the worst choice</span></span>
<span id="cb33-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">data {</span></span>
<span id="cb33-4"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int N; // number of rows</span></span>
<span id="cb33-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int T; // number of inidvidual-choice sets/task combinations</span></span>
<span id="cb33-6"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int I; // number of Individuals</span></span>
<span id="cb33-7"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int P; // number of covariates that vary by choice</span></span>
<span id="cb33-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int P2; // number of covariates that vary by individual</span></span>
<span id="cb33-9"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int K; // number of choices</span></span>
<span id="cb33-10"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span></span>
<span id="cb33-11"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  vector&lt;lower = 0, upper = 1&gt;[N] choice; // binary indicator for choice</span></span>
<span id="cb33-12"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  vector&lt;lower = 0, upper = 1&gt;[N] worst_choice; // binary indicator for worst choice</span></span>
<span id="cb33-13"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[N, P] X; // choice attributes</span></span>
<span id="cb33-14"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[I, P2] X2; // individual attributes</span></span>
<span id="cb33-15"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span></span>
<span id="cb33-16"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int task[T]; // index for tasks</span></span>
<span id="cb33-17"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int task_individual[T]; // index for individual</span></span>
<span id="cb33-18"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int start[T]; // the starting observation for each task</span></span>
<span id="cb33-19"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int end[T]; // the ending observation for each task</span></span>
<span id="cb33-20"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb33-21"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">parameters {</span></span>
<span id="cb33-22"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  vector[P] beta; // hypermeans of the part-worths</span></span>
<span id="cb33-23"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[P, P2] Gamma; // coefficient matrix on individual attributes</span></span>
<span id="cb33-24"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  vector&lt;lower = 0&gt;[P] tau; // diagonal of the part-worth covariance matrix</span></span>
<span id="cb33-25"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[I, P] z; // individual random effects (unscaled)</span></span>
<span id="cb33-26"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  cholesky_factor_corr[P] L_Omega; // the cholesky factor of the correlation matrix of tastes/part-worths</span></span>
<span id="cb33-27"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb33-28"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">transformed parameters {</span></span>
<span id="cb33-29"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  // here we use the reparameterization discussed on slide 30</span></span>
<span id="cb33-30"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[I, P] beta_individual = rep_matrix(beta', I) + X2 * Gamma' + z*diag_pre_multiply(tau, L_Omega);</span></span>
<span id="cb33-31"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb33-32"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">model {</span></span>
<span id="cb33-33"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  // create a temporary holding vector</span></span>
<span id="cb33-34"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  vector[N] log_prob;</span></span>
<span id="cb33-35"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  vector[N] log_prob_worst;</span></span>
<span id="cb33-36"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span></span>
<span id="cb33-37"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  // priors on the parameters</span></span>
<span id="cb33-38"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  tau ~ normal(0, .5);</span></span>
<span id="cb33-39"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  beta ~ normal(0, .5);</span></span>
<span id="cb33-40"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  to_vector(z) ~ normal(0, 1);</span></span>
<span id="cb33-41"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  L_Omega ~ lkj_corr_cholesky(4);</span></span>
<span id="cb33-42"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  to_vector(Gamma) ~ normal(0, 1);</span></span>
<span id="cb33-43"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span></span>
<span id="cb33-44"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  // log probabilities of each choice in the dataset</span></span>
<span id="cb33-45"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  for(t in 1:T) {</span></span>
<span id="cb33-46"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    vector[K] utilities; // tmp vector holding the utilities for the task/individual combination</span></span>
<span id="cb33-47"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // add utility from product attributes with individual part-worths/marginal utilities</span></span>
<span id="cb33-48"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    utilities = X[start[t]:end[t]]*beta_individual[task_individual[t]]';</span></span>
<span id="cb33-49"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    </span></span>
<span id="cb33-50"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    log_prob[start[t]:end[t]] = log_softmax(utilities);</span></span>
<span id="cb33-51"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    log_prob_worst[start[t]:end[t]] = log_softmax(-utilities); // Here's the problem!</span></span>
<span id="cb33-52"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  }</span></span>
<span id="cb33-53"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span></span>
<span id="cb33-54"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  target += log_prob' * choice;</span></span>
<span id="cb33-55"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  target += log_prob_worst' * worst_choice;</span></span>
<span id="cb33-56"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}"</span></span></code></pre></div>
</div>
</details>
<p>Comparing the two, you can definitely see the implementation simplicity MaxDiff provides; we’re essentially just building out <code>log_prob_worst</code> and pulling it into the likelihood. Boom, done.</p>
<p>So how does this perform? Well, let’s fit the two and find out. The next code block has some plumbing to fit the models and extract results, so feel free to jump to the results plot below:</p>
<details>
<summary>
<strong>Fit the two models, and gather predictions to compare</strong>
</summary>
<div class="cell">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Again, note that this code is largely just reproducing https://khakieconomics.github.io/2018/12/27/Ranked-random-coefficients-logit.html,</span></span>
<span id="cb34-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># and then adding in details need to compare it to MaxDiff</span></span>
<span id="cb34-3"></span>
<span id="cb34-4">data_list_best_choice <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">N =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(X),</span>
<span id="cb34-5">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">T =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(tt),</span>
<span id="cb34-6">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">I =</span> I, </span>
<span id="cb34-7">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">P =</span> P, </span>
<span id="cb34-8">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">P2 =</span> P2, </span>
<span id="cb34-9">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">K =</span> J, </span>
<span id="cb34-10">                             <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">NOTE</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!! This is the tricky bit -- we use the order of the ranks (within task)</span></span>
<span id="cb34-11">                             <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Not the raw rank orderings. This is how we get the likelihood evaluation to be pretty quick</span></span>
<span id="cb34-12">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">choice =</span> ranked_options<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>best_choice,</span>
<span id="cb34-13">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X =</span> X, </span>
<span id="cb34-14">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X2 =</span> W, </span>
<span id="cb34-15">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>task_number, </span>
<span id="cb34-16">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task_individual =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>individual,</span>
<span id="cb34-17">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">start =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>start, </span>
<span id="cb34-18">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">end =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>end) </span>
<span id="cb34-19"></span>
<span id="cb34-20">data_list_best_worst <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">N =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(X),</span>
<span id="cb34-21">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">T =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(tt),</span>
<span id="cb34-22">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">I =</span> I, </span>
<span id="cb34-23">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">P =</span> P, </span>
<span id="cb34-24">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">P2 =</span> P2, </span>
<span id="cb34-25">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">K =</span> J,</span>
<span id="cb34-26">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">choice =</span> ranked_options<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>best_choice,</span>
<span id="cb34-27">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">worst_choice =</span> ranked_options<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>worst_choice,</span>
<span id="cb34-28">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X =</span> X, </span>
<span id="cb34-29">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X2 =</span> W, </span>
<span id="cb34-30">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>task_number, </span>
<span id="cb34-31">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task_individual =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>individual,</span>
<span id="cb34-32">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">start =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>start, </span>
<span id="cb34-33">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">end =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>end) </span>
<span id="cb34-34"></span>
<span id="cb34-35">compiled_best_choice_model <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stan_model</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model_code =</span> best)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in readLines(file.path(makevar_files)): incomplete final line found on
'C:\Users\timma\Documents/.R/Makevars.win'</code></pre>
</div>
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1">compiled_bw_choice_model <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stan_model</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model_code =</span> best_worst)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in readLines(file.path(makevar_files)): incomplete final line found on
'C:\Users\timma\Documents/.R/Makevars.win'</code></pre>
</div>
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1">best_choice_fit <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sampling</span>(compiled_best_choice_model, </span>
<span id="cb38-2">                            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> data_list_best_choice, </span>
<span id="cb38-3">                            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">iter =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">800</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: There were 15 divergent transitions after warmup. See
https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
to find out why this is a problem and how to eliminate them.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Examine the pairs() plot to diagnose sampling problems</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
</div>
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1">best__worst_choice_fit <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sampling</span>(compiled_bw_choice_model, </span>
<span id="cb43-2">                            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> data_list_best_worst, </span>
<span id="cb43-3">                            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">iter =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">800</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: There were 2 divergent transitions after warmup. See
https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
to find out why this is a problem and how to eliminate them.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Examine the pairs() plot to diagnose sampling problems</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
</div>
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Now make some predictions to compare</span></span>
<span id="cb48-2">best_choice <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.data.frame</span>(best_choice_fit, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">pars =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"beta_individual"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb48-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">gather</span>(Parameter, Value) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb48-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(Parameter) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb48-5">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarise</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">median =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">median</span>(Value),</span>
<span id="cb48-6">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">lower =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">quantile</span>(Value, .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">05</span>),</span>
<span id="cb48-7">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">upper =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">quantile</span>(Value, .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">95</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb48-8">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">individual =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">str_extract</span>(Parameter, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"[0-9]+(?=,)"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> parse_number,</span>
<span id="cb48-9">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">column =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">str_extract</span>(Parameter, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">",[0-9]{1,2}"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> parse_number) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb48-10">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">arrange</span>(individual, column) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb48-11">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">True value</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.numeric</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">t</span>(beta_i)),</span>
<span id="cb48-12">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Dataset =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Best Choice"</span>)</span>
<span id="cb48-13"></span>
<span id="cb48-14">best_worst_choice <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.data.frame</span>(best__worst_choice_fit, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">pars =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"beta_individual"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb48-15">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">gather</span>(Parameter, Value) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb48-16">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(Parameter) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb48-17">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarise</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">median =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">median</span>(Value),</span>
<span id="cb48-18">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">lower =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">quantile</span>(Value, .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">05</span>),</span>
<span id="cb48-19">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">upper =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">quantile</span>(Value, .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">95</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb48-20">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">individual =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">str_extract</span>(Parameter, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"[0-9]+(?=,)"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> parse_number,</span>
<span id="cb48-21">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">column =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">str_extract</span>(Parameter, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">",[0-9]{1,2}"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> parse_number) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb48-22">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">arrange</span>(individual, column) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb48-23">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">True value</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.numeric</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">t</span>(beta_i)),</span>
<span id="cb48-24">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Dataset =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MaxDiff"</span>)</span>
<span id="cb48-25"></span>
<span id="cb48-26"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Combine the datasets</span></span>
<span id="cb48-27">combined_data <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bind_rows</span>(best_choice, best_worst_choice)</span></code></pre></div>
</div>
</details>
<div class="cell">
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Plot results</span></span>
<span id="cb49-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(combined_data, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">True value</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> median, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> Dataset)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb49-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_linerange</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ymin =</span> lower, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ymax =</span> upper), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">alpha =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb49-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_point</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> median), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">alpha =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb49-5">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_abline</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">intercept =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">slope =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb49-6">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">labs</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Utility Estimates"</span>,</span>
<span id="cb49-7">       <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">title =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Utility Estimates from the Two Models"</span>,</span>
<span id="cb49-8">       <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">subtitle =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"With 90% CIs"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb49-9">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">scale_color_manual</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">values =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Best Choice"</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"blue"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MaxDiff"</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"red"</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb49-10">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">facet_wrap</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>Dataset) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb49-11">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">legend.position=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"none"</span>)</span></code></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/doing_maxdiff_better/better_maxdiff_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>To be clear, what’s plotted here are respondent level utilities for each choice. Concerningly, it doesn’t even seem clear that the best-worst choice model performs better here despite the extra data. The fact that we have to do this at all doesn’t bode well, but let’s compare the RMSE’s of the predictions versus the (simulated and thus known) truth to confirm:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Is the MaxDiff model actually better?</span></span>
<span id="cb50-2">combined_data <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb50-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(Dataset) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb50-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarize</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">RMSE =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sqrt</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mean</span>((<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">True value</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> median)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">^</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 2
  Dataset      RMSE
  &lt;chr&gt;       &lt;dbl&gt;
1 Best Choice 0.218
2 MaxDiff     0.333</code></pre>
</div>
</div>
<p>To say this more emphatically, MaxDiff gets twice as much data from each respondent, and it’s not even consistently the case that it is better<sup>8</sup>! And again, it’s not like I chose some obscure DGP where MaxDiff is uniquely poor, we’re working with data that aligns precisely with the model assumptions.</p>
<p>So to summarize, I’ve described some psychological and mathematical issues with MaxDiff. Then, via a comparison to a best choice only model, I showed that these differences are more than theoretical naggles: for pretty vanilla data, the MaxDiff model can actually perform worse than the best choice formulation.</p>
<p>It’s worth pointing out that we only get this type of precise comparison in a simulation study. On real world survey data with a similar DGP, you’d just see the models give fairly similar answers, with no reason to suspect the slight differences might actually be bad tweaks on MaxDiff’s part. With simulation, however, we can look closer, and the flaws I’ve discussed become increasingly apparent.</p>
</section>
<section id="alternative-1-rank-ordered-logits-with-connected-graphs-of-choices" class="level1">
<h1>Alternative #1: rank-ordered logits with connected graphs of choices</h1>
<p>Ok, so you get it. MaxDiff isn’t great. But respondents are still expensive, and you still have a choice modeling task to complete at work, and you really, really want to do better than just using the best choices. Maybe your boss thinks Sawtooth is fine since it’s been used for a long time. What’s the alternative?</p>
<p>In this section, I’ll lay out what we’ll call the rank-ordered logit with a connected graph of choices. If this sounds complicated, don’t worry; the model isn’t actually going to get much more complicated.</p>
<p>Instead, the plan is mostly to be cleverer about how we build our choice sets in the first place.</p>
<section id="explaining-the-connected-graphs" class="level2">
<h2 class="anchored" data-anchor-id="explaining-the-connected-graphs">Explaining the “connected graphs”</h2>
<p>What we really want, I’d argue, is full choice ranking data, for each respondent. But of course, respondents rapidly become unhappy and start satisficing when they are forced to rank 20 different largely similar options at the same time. How can we get the simplicity from the respondent POV of MaxDiff, but the rich ranking data we want?</p>
<p>The clever trick here is that by asking the right 3 item choice sets, and asking the survey taker to pick best and worst, we can collect every choice needed to construct a full ranking. We’ll then be able to simply fit a rank-ordered logit model to these data, which makes far fewer simplifying assumptions about how the choices and their utilities interact.</p>
<p>There are some downsides I’ll be transparent about here: for <img src="https://latex.codecogs.com/png.latex?K"> choices, you need each respondent to do <img src="https://latex.codecogs.com/png.latex?K-1"> choice tasks. If you have 20 options, this can get tiring fast, and you won’t really be gathering “extra” opinions per respondent anymore by doing the minimum <img src="https://latex.codecogs.com/png.latex?K-1"> rounds. Here’s a hand worked example of what this looks like for intuition:</p>
<details>
<summary>
<strong>Illustration: K-1 comparisons are needed</strong>
</summary>
<p>Let’s say we want to get someone’s ranking over 2024 presidential candidate choices, from the set {Biden,Trump,RFK,Stein}.</p>
<p>Choice Set 1:</p>
<p>Options: {Biden, Stein, RFK} Best choice: Biden Worst choice: RFK</p>
<p>Choice Set 2:</p>
<p>Options: {Stein, RFK, Trump} Best choice: Stein Worst choice: Trump</p>
<p>Choice Set 3:</p>
<p>Options: {Biden, Stein, Trump} Best choice: Biden Worst choice: Trump</p>
<p>Now, let’s reconstruct the full ranking from these choice sets:</p>
<p>From Choice Set 1, we know that Biden &gt; Stein &gt; RFK. From Choice Set 2, we know that Stein &gt; RFK &gt; Trump. From Choice Set 3, we know that Biden &gt; Stein &gt; Trump.</p>
<p>Combining these partial rankings, we can infer the full ranking: Biden &gt; Stein &gt; RFK &gt; Trump</p>
</details>
<p>You’ll notice that you can’t get the full ranking for e.g.&nbsp;4 options in any less than 3 sets, but you well could add more sets to gain some statistical efficiency if you so choose. Of course, in larger choice sets, you’ll probably end up closer to K-1 sets given respondent fatigue, but for smaller sets it’s a very helpful option to know exists.</p>
<p>Most of the innovation here was in the choice set design, but there’s some in the model too, so let’s turn to that. The likelihood for the Rank-Ordered Logit model just extends the intuition from the best-choice model to account for the full ranking.</p>
<p>For a given ranking <img src="https://latex.codecogs.com/png.latex?r"> of <img src="https://latex.codecogs.com/png.latex?J"> alternatives, the likelihood takes the form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(r)%20=%20%5Cprod_%7Bj=1%7D%5E%7BJ-1%7D%20%5Cfrac%7B%5Cexp(%5Cmu_%7Bij%7D)%7D%7B%5Csum_%7Bk=j%7D%5EJ%20%5Cexp(%5Cmu_%7Bik%7D)%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmu_%7Bij%7D"> is the utility of the alternative ranked <img src="https://latex.codecogs.com/png.latex?j">-th for individual <img src="https://latex.codecogs.com/png.latex?i">. Here that is in code; the key bit is the <code>rank_logit_lpmf</code> function.</p>
<details>
<summary>
<strong>Model 3: Rank-Ordered Logit for Connected Graphs</strong>
</summary>
<div class="cell">
<div class="sourceCode cell-code" id="cb52" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1">ranked <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"// Again, note that this code is just reproducing https://khakieconomics.github.io/2018/12/27/Ranked-random-coefficients-logit.html</span></span>
<span id="cb52-2"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">functions {</span></span>
<span id="cb52-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  real rank_logit_lpmf(int[] rank_order, vector delta) {</span></span>
<span id="cb52-4"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // We reorder the raw utilities so that the first rank is first, second rank second... </span></span>
<span id="cb52-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    vector[rows(delta)] tmp = delta[rank_order];</span></span>
<span id="cb52-6"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    real out;</span></span>
<span id="cb52-7"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // ... and sequentially take the log of the first element of the softmax applied to the remaining</span></span>
<span id="cb52-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // unranked elements.</span></span>
<span id="cb52-9"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    for(i in 1:(rows(tmp) - 1)) {</span></span>
<span id="cb52-10"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      if(i == 1) {</span></span>
<span id="cb52-11"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        out = tmp[1] - log_sum_exp(tmp);</span></span>
<span id="cb52-12"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      } else {</span></span>
<span id="cb52-13"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        out += tmp[i] - log_sum_exp(tmp[i:]);</span></span>
<span id="cb52-14"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      }</span></span>
<span id="cb52-15"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    }</span></span>
<span id="cb52-16"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // And return the log likelihood of observing that ranking</span></span>
<span id="cb52-17"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    return(out);</span></span>
<span id="cb52-18"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  }</span></span>
<span id="cb52-19"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb52-20"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">data {</span></span>
<span id="cb52-21"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int N; // number of rows</span></span>
<span id="cb52-22"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int T; // number of inidvidual-choice sets/task combinations</span></span>
<span id="cb52-23"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int I; // number of Individuals</span></span>
<span id="cb52-24"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int P; // number of covariates that vary by choice</span></span>
<span id="cb52-25"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int P2; // number of covariates that vary by individual</span></span>
<span id="cb52-26"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int K; // number of choices</span></span>
<span id="cb52-27"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span></span>
<span id="cb52-28"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int rank_order[N]; // The vector describing the index (within each task) of the first, second, third, ... choices. </span></span>
<span id="cb52-29"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  // In R, this is order(-utility) within each task</span></span>
<span id="cb52-30"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[N, P] X; // choice attributes</span></span>
<span id="cb52-31"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[I, P2] X2; // individual attributes</span></span>
<span id="cb52-32"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span></span>
<span id="cb52-33"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int task[T]; // index for tasks</span></span>
<span id="cb52-34"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int task_individual[T]; // index for individual</span></span>
<span id="cb52-35"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int start[T]; // the starting observation for each task</span></span>
<span id="cb52-36"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int end[T]; // the ending observation for each task</span></span>
<span id="cb52-37"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb52-38"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">parameters {</span></span>
<span id="cb52-39"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  vector[P] beta; // hypermeans of the part-worths</span></span>
<span id="cb52-40"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[P, P2] Gamma; // coefficient matrix on individual attributes</span></span>
<span id="cb52-41"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  vector&lt;lower = 0&gt;[P] tau; // diagonal of the part-worth covariance matrix</span></span>
<span id="cb52-42"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[I, P] z; // individual random effects (unscaled)</span></span>
<span id="cb52-43"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  cholesky_factor_corr[P] L_Omega; // the cholesky factor of the correlation matrix of tastes/part-worths</span></span>
<span id="cb52-44"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb52-45"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">transformed parameters {</span></span>
<span id="cb52-46"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  // here we use the reparameterization discussed on slide 30</span></span>
<span id="cb52-47"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[I, P] beta_individual = rep_matrix(beta', I) + X2 * Gamma' + z * diag_pre_multiply(tau, L_Omega);</span></span>
<span id="cb52-48"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb52-49"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">model {</span></span>
<span id="cb52-50"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  // priors on the parameters</span></span>
<span id="cb52-51"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  tau ~ normal(0, .5);</span></span>
<span id="cb52-52"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  beta ~ normal(0, 1);</span></span>
<span id="cb52-53"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  to_vector(z) ~ normal(0, 1);</span></span>
<span id="cb52-54"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  L_Omega ~ lkj_corr_cholesky(4);</span></span>
<span id="cb52-55"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  to_vector(Gamma) ~ normal(0, 1);</span></span>
<span id="cb52-56"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span></span>
<span id="cb52-57"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  // log probabilities of each choice in the dataset</span></span>
<span id="cb52-58"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  for(t in 1:T) {</span></span>
<span id="cb52-59"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    vector[K] utilities; // tmp vector holding the utilities for the task/individual combination</span></span>
<span id="cb52-60"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // add utility from product attributes with individual part-worths/marginal utilities</span></span>
<span id="cb52-61"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    utilities = X[start[t]:end[t]]*beta_individual[task_individual[t]]';</span></span>
<span id="cb52-62"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    rank_order[start[t]:end[t]] ~ rank_logit(utilities);</span></span>
<span id="cb52-63"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  }</span></span>
<span id="cb52-64"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}"</span></span></code></pre></div>
</div>
</details>
<p>Take a second to reason through the above code; we’re able to use all of the information a full ranking provides to us, without having to force any interrelationship between best and worst to exist that may or may not exist. Also, notice that there’s nothing here specifically for the “connected graph of choices” part of model- that’s all in the choice set generation, which we’ll do next.</p>
</section>
<section id="building-the-choice-sets" class="level2">
<h2 class="anchored" data-anchor-id="building-the-choice-sets">Building the choice sets</h2>
<p>Let’s try making the choice sets the new way, and then compare models again.</p>
<p>To keep things simple, we’ll generate our graph of options as a <a href="https://online.stat.psu.edu/stat503/lesson/4/4.7">balanced incomplete block design</a>. This is a quick way to get the connectedness property we want in a reasonably statistically efficient design. It’s worth noting though that the literature on designing maximally efficient discrete choice experiments is deep, and has some beautiful connections to the broader experimental design literature; here’s a footnote if you’re the sort of person who finds that exciting<sup>9</sup>.</p>
<details>
<summary>
<strong>Updated data generation for connected graph illustration</strong>
</summary>
<div class="cell">
<div class="sourceCode cell-code" id="cb53" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Number of individuals</span></span>
<span id="cb53-2">I <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">400</span></span>
<span id="cb53-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Number of tasks per individual</span></span>
<span id="cb53-4">Tasks <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb53-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Number of choices per task</span></span>
<span id="cb53-6">J <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb53-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Dimension of covariate matrix</span></span>
<span id="cb53-8">P <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb53-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Dimension of demographic matrix</span></span>
<span id="cb53-10">P2 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span></span>
<span id="cb53-11"></span>
<span id="cb53-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># demographic matrix</span></span>
<span id="cb53-13">W <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">matrix</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(I<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>P2), I, P2)</span>
<span id="cb53-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Loading matrix</span></span>
<span id="cb53-15">Gamma <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">matrix</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(P<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>P2), P, P2)</span>
<span id="cb53-16"></span>
<span id="cb53-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Show W * t(Gamma) to make sure it looks right</span></span>
<span id="cb53-18">W <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">t</span>(Gamma)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                [,1]         [,2]         [,3]          [,4]         [,5]
  [1,] -1.5412624799 -1.492174492 -5.116226558  -1.347807230  0.001574089
  [2,] -3.7257266234 -2.723139959 -2.822841903   0.101675159  0.763523848
  [3,]  0.4730049846  0.910864917  4.083111863  -1.471631667 -1.279559170
  [4,] -1.1289564758 -0.842181449 -2.937264693  -0.437928873  1.117150515
  [5,] -4.1905191154  3.318393965 -0.599327838  -3.310317907 -3.217285041
  [6,] -2.6492353829 -1.033830233 -2.811919648  -2.527213567  0.016064346
  [7,]  1.4591711093  4.353062595  4.524177128   3.791394053 -3.055228317
  [8,] -0.5661501940 -3.742740573 -5.053927670  -3.149040498  1.821531612
  [9,] -0.0569210566 -2.512887096  1.150352221  -4.158098423  2.544617312
 [10,]  0.8174140064 -3.257770914 -6.679072420   5.217851104  2.543820586
 [11,] -3.3644771332 -1.449694472 -5.814100769   0.034075107 -1.421669636
 [12,] -0.6646885105 -1.994607998  0.229234553  -2.930599125  2.334724287
 [13,] -0.4554012271  6.725575915  3.974224000   1.624482334 -2.525514532
 [14,] -0.4290414679 -2.914172693 -6.812881630   3.427975008  1.301192440
 [15,]  2.3451100762 -1.868093119  3.661664471   1.256928318  1.815057453
 [16,] -0.7424357061  1.314333858  2.326296328  -1.651657324 -1.242121310
 [17,] -1.1355535184 -1.554960793  3.500814111   2.138937785  0.399665190
 [18,] -2.4670221123 -2.350556927 -0.608124647   3.760988423  0.919302772
 [19,]  0.6336296989  1.098046588  2.083267335  -1.385890290 -0.900592817
 [20,] -0.1783054530 -1.782880847  1.301147942  -9.646402726  1.613337908
 [21,]  2.8588590755 -1.054297886  1.907116980  -7.060972023  1.038812638
 [22,] -0.2039825209 -1.247507832  2.470660505   1.239479888  1.106626373
 [23,] -3.4746506631 -3.508828446 -5.449731641  -7.648194070  2.333166463
 [24,]  2.5724355418 -0.160439092  4.824422518   4.478013435  0.809276982
 [25,] -0.9755464590  2.706429159  2.085153879  -7.002689457 -1.163424409
 [26,]  0.2742467555  2.783637461  0.487778649   1.538890117 -2.805469328
 [27,] -2.6327365951  0.647359172  1.006432011   0.690032677 -1.621858539
 [28,]  1.8763278673 -1.272990544  2.078108412  -5.243199305  2.082184979
 [29,]  2.2188968660  0.240736613 -0.815478862  -2.744060820 -0.048770516
 [30,] -3.9628733890 -0.113607772  0.224711894  -5.198084608 -1.114163049
 [31,] -4.2641345894  1.996915047  0.099624723   1.331299406 -1.516121432
 [32,] -0.5524263714  1.311462389 -1.214233177   2.584592840 -1.630199534
 [33,]  1.9944600720 -0.752170898  2.436853931  -5.344579832 -0.083796489
 [34,] -0.2059555289 -0.851039534  2.687940642  -6.656340585  1.886319532
 [35,] -1.7705938256  1.706629894  1.336601439  -5.630928568 -0.541385096
 [36,] -0.5273080110 -2.018241480 -0.700311999   3.621702144  0.279030488
 [37,] -2.7185644156  0.360535474 -0.169927022  -5.380951726  0.191448735
 [38,] -3.8600831761  3.791903631  0.447887021  -1.294971709 -1.569381142
 [39,]  5.2576883153 -4.031141475  0.981260876  -0.389974915  2.434063355
 [40,]  0.2264332470 -2.668998647 -2.151615699  -4.955313403  1.697963807
 [41,] -1.1162185151 -3.255401499 -3.544450808   1.238311086  2.634532420
 [42,] -1.4336601196 -2.100986861 -1.829304497  -6.780311417  2.468169302
 [43,] -3.2243263508  1.324538045 -0.541223217   3.753983082 -0.617814088
 [44,] -1.3758730861  1.959017116  5.831226366  -2.086105419 -0.270401213
 [45,] -1.9743016454  1.054005527  1.528013201  -4.542528278  1.105393765
 [46,]  3.0898340361  3.222170045  6.285250416  -0.762194443 -1.581173608
 [47,] -1.4055645007  2.087779424 -1.225676172   1.160761651 -0.050036984
 [48,]  0.4646083115 -0.981080358 -2.013141413  -1.061568791  0.095956075
 [49,]  0.6117466464  3.065568660 -1.186154449   1.919869983 -1.166239243
 [50,] -2.2382817344 -1.861037151 -1.984131835  -6.223047509  2.271733572
 [51,] -1.6750183655  0.314000156 -2.193940420   1.445680792  0.104323168
 [52,] -0.5424056953  4.635270387  5.493769693  -2.790746468 -0.647672683
 [53,] -1.0869464214  0.772149226 -0.457213500  -3.837057494  0.152893366
 [54,]  0.8314561763 -0.195856405 -2.802297421   9.629050659 -0.943798566
 [55,]  1.3873534192 -0.163905851  0.876675905   1.955158280  0.082625241
 [56,] -1.0028628371 -1.065792177 -0.387355992   2.934993534 -0.701583668
 [57,] -1.7069408645  1.963128354  2.492700571   0.324471935 -0.483430577
 [58,] -2.9952769643 -0.713819727 -7.860000192   5.633282644 -0.744285185
 [59,]  0.7948305572 -1.938533263 -1.219037953  -0.832381925  1.071975571
 [60,] -2.5497991144 -1.208014293 -4.972682949   1.382277683  1.349566702
 [61,]  1.2185172356  3.385639253  2.740549092  -0.303520107 -3.243991049
 [62,]  2.0346715861 -2.017907006 -1.661937529   1.712486110  0.269443568
 [63,]  1.1003817042 -0.069002060  4.935401334   0.300455176  2.408042531
 [64,] -0.6564698814 -0.826226916 -1.970112617  -6.712398580  0.958957912
 [65,]  0.3144406730  0.767761174  0.459991335   3.879362644  0.869803554
 [66,]  2.0929055159  0.854741777  5.230006519  -8.064751996 -0.145558505
 [67,]  0.7152519594  1.385720251 -2.925501025  -2.113992995 -1.654562613
 [68,] -1.0074155568 -0.430985382 -3.697355886   1.944809497  0.805377438
 [69,] -4.5877847402 -1.115382588 -6.601460733   6.511372347 -1.834392685
 [70,]  1.3149305093  1.375782792  2.276168002   4.572992536 -0.390324479
 [71,]  0.8831744784 -0.026936584  0.556239946   2.553588961  0.063788647
 [72,] -1.7266497964  0.112322933 -1.308367798  -5.016971355  0.921644440
 [73,]  1.7179999725 -1.072504568 -5.036695338   7.313133472 -0.605940628
 [74,] -2.5728198961 -0.860120812 -0.060575910  -0.999756268  1.768373949
 [75,] -0.5476467617 -1.159374665 -2.006579237  -2.268839513  0.187039974
 [76,]  0.1650960090 -3.708035880  0.102718618 -10.606112375  1.488022737
 [77,] -0.9721551093  2.158634003  3.189175876  -2.438725094 -1.106092489
 [78,]  0.4057151033  4.001283801  3.280511664  -6.940937119 -2.809459727
 [79,]  2.9478267878  1.135935148  2.142583188   0.664822137 -0.144571229
 [80,]  0.2755206984  0.472526312 -2.351192378  -1.849787822 -0.947808873
 [81,] -0.0248319352  0.814213514 -1.233446682  -5.156781210  1.299755378
 [82,]  0.1432628546  0.606484309  1.641764869  -2.981491335 -0.035143959
 [83,] -0.1959375904  4.032697198  4.292382205  -6.784462132 -1.637216987
 [84,] -1.0461641405  2.159511058  0.445951483   1.447644970 -1.506760901
 [85,]  0.3722287881  1.408425046  0.797829118  -3.724560916 -0.010446977
 [86,] -1.5351583649  2.038973232  1.638698528  -5.710825425 -0.304028004
 [87,]  1.7345228902  0.773871492 -2.567213936   0.384591360  0.002782755
 [88,] -1.2409675750  3.304432311  1.213745581  -4.811381380 -2.814723220
 [89,]  0.4792349258 -4.812218155 -5.104655158   0.764346408  1.267067517
 [90,] -2.8027120518 -1.429076516 -4.645677056  -0.247897534  0.077751243
 [91,]  4.8231762505  3.742086153 10.088221297  -2.527660071 -0.175154337
 [92,] -1.3797727159 -1.073837477 -2.111056183  -2.062980915  1.448070356
 [93,]  0.7100897071 -2.246621479 -1.580015393   4.568521557  1.621422544
 [94,]  4.8942011463  2.309683041  8.396684919  -4.541263246 -0.124288457
 [95,]  1.2013912261  2.895327974  2.410685637   1.870889346 -0.326469559
 [96,] -0.8505517239 -0.625527476 -2.527230492   7.491415972  1.899747380
 [97,] -1.5709793562 -1.079210748 -1.881011222   3.249166424 -0.313077176
 [98,] -3.5116383882  1.217197562 -5.823922097   1.055318646 -1.699281316
 [99,] -0.1567694031  1.792952395  1.768021069  -1.417357333  0.685322649
[100,]  0.9652400055 -3.179355675 -2.179006231  -3.020443444  1.655407424
[101,]  0.4823556858 -1.682962326 -1.131194413   1.254709438  1.020400258
[102,] -2.3044516832  4.097745443  2.977431366  -0.150843752 -1.990983859
[103,] -1.0175255260 -0.427489190  0.712764242  -2.452938001  1.702683003
[104,] -0.2698443257 -1.716970057  0.521238247   4.764269458  0.585221974
[105,]  2.5899921548  1.154353679  0.592179144   0.480689307 -0.342398771
[106,] -2.6282040844  0.781529018 -3.151733589   0.404017379 -0.855167573
[107,] -3.7578394431  0.725436982 -2.082676660   1.446067621 -0.210110152
[108,] -3.6568247106 -1.318774812 -3.388348305   3.349438898  0.545780946
[109,]  1.4037370745 -0.904889966  1.023022088  -2.192989050  0.481397270
[110,] -1.0884221704  2.806843203 -0.847539160  -0.818310380 -1.482521731
[111,]  6.6603289114 -4.923114094  0.894412761  -1.518693251  3.008703122
[112,] -0.9104537828 -1.758171794 -3.431275531   5.141225445  0.647030477
[113,] -1.5535052837  1.161424565  0.760838076  -7.247328990  1.499210539
[114,]  1.0682567477 -1.963708505 -0.148556728  -7.212397905  0.426371232
[115,] -0.0880081758  0.091873626  2.853263490  -4.913293403  1.406393441
[116,] -0.2691936928 -3.725975958 -3.560845295   5.169427116  2.031946658
[117,]  2.3775023933  0.892673643  4.204472259  -2.035742715 -0.659865801
[118,]  4.6448924786 -2.527037309 -0.616799447  -5.128486182  3.076479420
[119,] -4.3226389583  4.480860926 -0.974356555   6.303175192 -1.996697530
[120,] -1.2542342150  5.354119929  1.799319635   6.999319448 -3.252113365
[121,] -0.9852929611  0.951391968  1.551200330  -1.176931049 -1.308358452
[122,]  2.2852650200 -1.820923818 -0.555728479  -7.215712948  3.217758249
[123,]  1.1961343876  2.528461493  2.538794751   3.999815765 -2.261056170
[124,]  3.9434028987  0.468410063  2.953459433  -1.344569521 -0.650084090
[125,]  3.7113597666 -4.288455188 -3.817740127  -3.594567122  3.179976415
[126,]  0.1099306967 -2.651007315 -3.608223708  -6.043665129  1.578558040
[127,]  1.0696418393 -0.729894560  2.463588344  -0.522842823  0.040370391
[128,] -0.6787163713 -1.801860564 -3.739704396   7.603190616  0.554145723
[129,] -1.2859450363 -3.953714915 -3.736954229  -4.708744127  3.096942958
[130,] -0.3756811068 -1.153890578 -2.946557245  -4.158037571  0.176978925
[131,] -1.2186553868  2.246970408  2.212810159   3.881813134 -1.738799509
[132,]  1.9836235785 -2.887767397 -0.721256825  -2.384730879  2.013760760
[133,]  2.0292746447  3.028565720  7.656624845  -0.305502572 -1.068319678
[134,] -0.9594714878 -1.493083785 -1.351516594  -2.302979749  1.525383755
[135,] -1.9792088825 -1.850492713 -1.214298773   3.982149692  0.244041239
[136,] -0.4742228524 -2.106332820 -2.540659370  -3.294491828  0.530532763
[137,]  0.0571285304  0.232013344  1.960116037  -0.397719999  0.734578272
[138,]  0.5310229010 -1.094103849 -0.264782256  -3.190251020  2.244983085
[139,] -2.1888499907 -2.635693944 -0.988012506   3.885343352  1.547730404
[140,]  0.6508365425 -4.204347775  0.832578124  -4.900228760  2.863470736
[141,]  0.7740411320  2.823505148  1.409851042  -7.475820472  0.151679308
[142,]  0.0268653240  3.115058538  3.038754355  -2.264443676 -2.216430120
[143,] -0.0058500487  0.729084642  2.290477993  -0.513549995 -0.740090261
[144,] -3.3244178992 -2.863492582 -2.361741730  -3.665838545  0.580048164
[145,] -0.7783620259  3.129133473  2.223952122   2.255136322 -1.426592834
[146,]  1.8875741423 -0.617481752 -0.746104767  -0.419698149  1.588773517
[147,] -0.6827788185 -1.523238910 -1.341994806  -4.136552330  2.362416106
[148,]  2.1507516389  3.030654883  5.159838750  -3.983748678 -0.550696980
[149,] -1.6748774124 -0.288494688 -0.714806839  -9.996319038  1.140863331
[150,]  5.1522519392  0.307630449  7.003830438  -0.909847210  2.197515606
[151,]  1.9385032415 -2.249163256 -1.144024683  -2.044815308  2.281543871
[152,] -2.7899597051 -0.088987475  1.351830693   0.260738582 -0.709741001
[153,] -2.3066595278 -1.056895459 -1.638617662   2.749698665  0.754547895
[154,]  0.9800091577 -2.544119957 -3.086461348   4.519232301  2.606182930
[155,] -3.8686894949 -1.620129695 -2.351941041   0.602648533  0.714014301
[156,]  2.5383634852 -5.379374126 -0.716504320  -3.974143744  3.927922450
[157,]  1.1053159646 -0.422707780  2.008146213  -3.850246343  0.338358739
[158,]  0.3264388388 -0.593516866 -3.075237461   0.719912097 -0.112876982
[159,]  1.6012790692  1.812533336  1.797755581   3.660860044 -0.423047308
[160,]  1.2922500314 -0.459464352  2.215192655   1.352833069  1.206678403
[161,] -0.7622101570 -2.522655980 -4.213362587   4.912426613  0.103644226
[162,]  4.5404230491  0.721701168  1.555109280   1.299873582  0.628581632
[163,] -0.9540353397 -0.195694636 -0.161224837  -4.122842818  1.180071013
[164,] -3.6417478377 -0.394231940 -6.264626108  -0.608377097 -1.390711784
[165,]  2.1091447064  1.353748920  4.461868678   0.273458003  0.120112304
[166,] -0.0290936061  1.144045508  4.092093006  -1.936552910 -1.197377036
[167,]  1.4860203983 -0.584293520 -0.004069713   5.621312976  0.620950890
[168,]  0.3290043984 -1.612743964 -0.544021563  -1.438185235  0.963252517
[169,] -5.1572797410  5.401016313 -1.559269433  -0.231835231 -3.984857171
[170,] -2.6157717958  1.433195442  0.527952453   0.181755243 -1.578262619
[171,] -1.8740753120 -0.248012267  0.212349396  -3.833235112  1.451500183
[172,]  1.8811258923 -2.366310147  2.547098882  -2.288771052  1.765463673
[173,] -4.1492921663  4.586286028  1.142542347  -0.665391466 -2.760908221
[174,]  2.4120628060 -0.917798806  0.975102447  -3.880958877  1.111384476
[175,]  1.9592358643 -0.167929048  6.404296348  -0.358888539  1.558850196
[176,] -2.0423960871  0.433842229 -0.578914293  -4.341287710 -0.347192922
[177,] -1.4543777885  1.146837670  1.464567863  -3.091032963 -0.515891125
[178,] -1.4380563715  0.019447794 -0.579070961  -1.792870353 -0.197552068
[179,] -0.1278726908 -1.292790720 -1.737233273   0.038747398  0.376501230
[180,] -0.7135982395 -0.889820042 -3.973535998   4.333053174  0.152650807
[181,] -2.8937211604  0.041303667 -5.145715862  -0.765775591 -0.347524612
[182,] -0.6430920314 -0.295679308  3.406405768  -2.663172863 -0.159770649
[183,]  6.0267348568 -2.463655549 -0.494349697  -3.139696659  1.895277663
[184,]  0.7688866439 -1.255548051  0.727220543   1.427414584  1.339018973
[185,]  2.0103475737 -2.522123475  1.720728104  -3.390667013  2.680437375
[186,]  4.4206082338 -2.940748918 -0.949502862  -4.621107230  3.461719747
[187,]  5.4624956592 -3.767211808  0.194013035  -5.080256795  3.944792150
[188,] -0.2706198245 -1.174388056  0.220000788  -3.580691316  1.836674882
[189,]  0.6574342287 -0.266791033 -2.338009015  -1.313941916  1.072824906
[190,]  1.7708181838 -0.807595730  0.448269453  -0.264439364  2.080144818
[191,]  2.9277332197 -2.290685185  1.130965769  -2.648946888  2.294051234
[192,]  2.3376289738 -4.810455537 -5.743077235   2.245823198  1.551264919
[193,] -0.0445550487  2.050968440 -0.788438235  -2.000760923  0.049066720
[194,] -0.2721325117  3.543700004  3.880504277   0.071514588 -0.860674416
[195,] -2.8633225359  3.517464994  4.208164397  -2.116180407 -3.263312850
[196,]  5.3891735367 -3.600419049  2.771758646  -1.447716125  3.950849184
[197,] -2.2153679890 -2.406417057 -5.966472124   1.801221269 -0.122900748
[198,] -1.7246958740  0.743113811  3.653829564   1.135371579  0.820246965
[199,] -2.8245208248 -2.772037686 -2.803603884  -1.638859318  1.880857446
[200,] -2.1115753585  1.117852869  1.207609266  -0.182881622 -1.077794168
[201,] -0.0771998669 -4.656055217 -3.720279083  -0.971893333  2.461171395
[202,] -1.6639499127  3.480818549  3.186987522  -8.754646778 -0.317926593
[203,] -0.9280184504  1.506599081  0.646954668   1.382803939 -1.938449705
[204,] -0.4752444829 -1.275334331 -1.406909561   0.130797029 -0.995125045
[205,] -2.5620703689 -1.459616772 -4.390515016   3.062913186 -0.510308217
[206,] -1.6964887349  2.837466505 -0.641594375   2.783606274 -2.779898541
[207,] -4.6243368303  4.618496157  3.620502553   0.183556444 -2.999291378
[208,] -1.4746760840 -2.057579226 -4.030329668  -3.336698020 -1.132639487
[209,] -5.9629354032 -0.191445220 -4.329583840  -2.596478948 -0.689677710
[210,] -0.4764465345  1.745071235  1.635180190   1.322559406 -1.551122539
[211,]  0.7858118471  2.375928591  6.473000633  -5.145734534 -0.705885438
[212,] -2.0538425360  0.091292768  1.367583540  -4.274493629  1.190022621
[213,] -1.3686152481  4.686531589 -1.122755018   3.681627519 -3.869159447
[214,]  3.2086560459 -1.258015992 -0.460510159  -9.164499174  2.790891399
[215,] -3.7943660819  3.984004385 -2.011043112   0.804491187 -3.493218115
[216,] -1.6678732222  5.675634450  8.287417610  -5.213123717 -2.593581338
[217,] -1.1411340005  1.793941542 -0.732179612  -1.131659801 -2.240653511
[218,]  1.5877035866  1.407216793 -0.227111096   4.064433118 -0.970644315
[219,]  0.0213908464 -0.731075672 -1.702992361  -0.143091758  0.555208459
[220,]  1.2886190926  0.057318999  1.159205088 -11.900004402  1.339579101
[221,] -1.8874029196 -0.678092548 -1.922415493   2.536637609  0.176763838
[222,] -1.5481535790 -2.233664448 -3.931024438  -0.637148938  0.832523101
[223,] -3.1411791971  0.814817837 -1.791176437   0.067955883  0.770568438
[224,] -1.9656430756  3.230815079 -0.304634323   6.015425431 -1.824901702
[225,] -0.8718787657 -0.818840179  1.162383931  -0.708362934  1.397718748
[226,]  1.3325762625  1.445431300  2.659789287  -3.852118743 -2.155659613
[227,]  3.1841480716  2.102119921 -0.529331143   7.033242831 -2.707838078
[228,]  0.5196335301 -1.898114180 -2.264547089  -0.282825994  1.360109767
[229,] -1.6695109399  2.172579151  5.105986681   1.632536986  0.062691160
[230,] -2.1632159146  1.310763498 -2.434157235   1.286787544 -3.209746369
[231,]  0.0752715667 -0.240473372  0.164362746   0.331146101  0.134191803
[232,] -1.5988099760  5.448585338  4.785373130   0.368892045 -2.651398295
[233,]  2.4213864542 -2.152410691  4.724474115  -2.370211890  0.819671695
[234,]  1.2137433694 -4.215403271 -3.103534709  -9.346053778  3.678107036
[235,] -2.8397972037  3.825611752  1.307163761  -0.075750946 -3.080975910
[236,] -0.3262216246 -1.095678089 -2.925265561  -5.651569659  1.357939949
[237,]  1.2345872009  0.999166920  3.338740210   3.001784768 -0.994845288
[238,]  3.3868373207 -1.073254036  1.160564541  -0.373667439  2.246072143
[239,] -0.0004368311  1.984666603  2.678258421   0.044847876  0.317588987
[240,]  1.3609634388  2.096965468  1.260048693  -5.888458446  0.389243378
[241,]  3.2411279233 -1.891805623 -4.214081801   4.416902066  2.602978454
[242,] -2.7945577172 -1.104266271 -3.124331307  -6.694818023  2.103459667
[243,]  3.4899558567 -3.158850165  2.025287374   1.190410438  2.354619764
[244,]  2.8685019199  0.147372899  0.052249406  -1.686380913  1.520371056
[245,]  0.4407005088 -0.309341024  1.968090398   1.754090422  0.840552749
[246,]  0.4770725161 -2.379991071 -2.457669900   5.610080881  1.245976959
[247,] -1.3774356510 -2.182405551  0.136256314  -5.308583575  3.050991568
[248,] -2.3317120002  2.972877054 -0.592511418   0.757276508 -2.365377220
[249,] -1.2109381551 -0.268419650 -1.964015959   4.592277453  1.153552090
[250,]  8.2995536627 -4.848705764  3.341974461  -6.494704793  5.911996483
[251,]  4.4519126307 -1.222155245  1.973744988  -1.795958206  1.832629276
[252,] -2.8358609484  4.035047944  3.197290382   5.889419669 -2.793148700
[253,]  0.1735477201 -2.890729833 -4.033903460   0.763938445  1.679779840
[254,]  0.9916637745 -2.363646665  0.308022000   9.217571147  0.552854863
[255,]  0.1286702704 -1.753017494  2.990927854  -3.916214290  2.252696842
[256,] -3.0122367133  2.495467168  1.927867480   0.142405776 -1.786866625
[257,] -0.9432058286  0.143417466 -2.563617567   6.178052217  0.271914242
[258,]  1.0882938541  1.351837340  4.098870234  -2.012676795 -0.226826839
[259,]  2.5860141711 -0.289256836 -0.914675391  -1.664340430  0.556265495
[260,] -2.4457014342  2.178972355  2.153378187  -0.223719649 -2.402919174
[261,]  0.1457771480 -0.164694728  2.735616397   1.408696933  1.257969428
[262,] -4.7358857314  0.781087755  0.120120181  -0.730628058 -0.119145333
[263,]  1.8619444055 -1.678991032  2.643384992  -9.590256589  3.013505861
[264,]  1.1678683277  1.383399284  1.107692624  -3.070776033 -0.200076369
[265,] -2.0882424395  0.894465085 -0.915395936  -0.570762035 -0.015445036
[266,] -2.1141622513 -1.710927293 -0.077442988  -1.729499986  0.451725307
[267,]  0.3536386083  0.010927679  0.046937123  -3.813459412  0.933169264
[268,] -4.0197262095  2.369429440  3.584370545  -1.847007882 -0.871994427
[269,]  1.0997328660  7.747151983  8.261930139   2.629590525 -7.128945152
[270,] -2.2497716033 -1.121489710  0.060012025   6.045267586  0.699975151
[271,] -2.4918971173  3.872210353 -5.399106105   3.716266010 -4.443892725
[272,]  2.2895407874  2.331136532  6.253221578  -1.765980613 -1.448486227
[273,]  0.3567716687  1.533750574  1.325156379  -0.687018270 -0.554505501
[274,]  4.1093340293  1.144033063  3.077562557  -1.010487732  0.047011582
[275,]  0.7199603249 -0.671760402 -1.643340138  -1.805501803 -0.725343889
[276,] -1.1241502576  0.305943149 -0.579438747  -0.327705509 -0.312556120
[277,] -2.6235020669 -0.753496612 -3.889567627  -0.111757330  2.089658643
[278,] -3.3197459793 -2.312400949 -4.879802256  -1.007380467  1.064915835
[279,]  1.2467640802 -0.243609493  3.831151783  -7.446608384  2.017421902
[280,] -0.4810261503  2.628295900  2.882606665   0.577265948 -0.747689286
[281,]  3.4355030503  0.418218687  4.551389076   0.032214044  1.030933930
[282,]  1.5587065220 -4.354283522 -3.047823906  -2.085625730  1.985805211
[283,] -2.5532753739 -0.420570692  0.950776776  -3.315261795  0.398531703
[284,] -1.9458551434 -0.516262307 -1.760888750   7.229026143 -0.977479876
[285,] -1.9554143876 -2.000126858 -1.380702044  -1.325813108 -0.164382971
[286,]  0.3226550343 -2.561782501 -1.133588760  -0.321319492  2.262324510
[287,]  0.0502264452  4.046519646  0.817080464   8.984927525 -2.180837831
[288,] -1.7553511792  1.636438468  1.100463731   8.768995160  0.516296192
[289,] -0.0977606392 -2.793190676 -3.503937226  -2.445680922  0.710205286
[290,]  0.8037679703 -1.893508195 -3.339620655   4.013768293  1.187895707
[291,] -0.1548909875  1.760769330  1.282157811   0.645293941 -1.227291443
[292,]  2.9487681541  1.273837592  4.234409483  -3.464707154  1.703767268
[293,]  0.9599507814  6.051816624  4.268043832   5.554173771 -2.756390139
[294,] -2.1215692029  2.088479660 -0.091410396  -1.373266919 -0.537585980
[295,] -4.7531221693 -1.676852985 -4.542555984   1.773380186  0.433548877
[296,]  0.5419129330 -0.129916339  1.995913942   7.123535386 -0.074064828
[297,]  3.6950218515 -0.870476496  3.243938886  -4.182923198  2.795772909
[298,] -2.0809597737  0.208690990  2.273319084   1.423882574 -1.687377516
[299,]  1.4221278863 -0.501367710  0.035961812   4.122425866 -1.483101475
[300,] -0.3417691134 -2.116609045 -0.737975669   0.939547334  2.361158839
[301,]  1.3086492138  2.829847392  5.129402267   1.980408398 -2.009114492
[302,]  2.7637510247 -5.614449850 -6.235308012  -2.274170290  3.479506567
[303,] -2.0067482475  0.506826436  0.994977761   1.176581400  0.407421671
[304,] -2.3426833419  1.204946497  1.976441339  -9.014257342 -1.165731571
[305,] -1.7754009433  0.264859973 -2.897006697   0.267096243  0.229904567
[306,]  1.7069107567  0.263833742  0.731837028  -0.821499862 -1.201779921
[307,]  4.3459838333 -2.767812279  3.651435502  -0.558623218  1.882426894
[308,] -1.2561599715 -3.399856498 -6.893266020   2.980103189  0.458123764
[309,] -2.5895465056  6.232426601  2.036836456  -0.152024117 -3.451530867
[310,]  2.4490355263 -0.299455921  0.873034206  -3.625072865  1.641336088
[311,] -1.0024483978 -3.089324051 -4.911194845  -0.208256858  1.722434207
[312,]  0.3277856250  6.619762615 10.027982385   0.547909089 -5.352563263
[313,] -0.4138086439 -2.909200437 -6.022041961   2.853755441  1.568581140
[314,]  3.8143620300 -1.138003383  1.060894096  -3.343694692  2.986428684
[315,]  1.0389468466 -4.139732399 -2.299931804  -1.936714581  1.759737773
[316,] -2.8010586427  0.855179408 -3.937889107   3.922723463 -2.689481377
[317,]  6.0068667200 -2.452052067  0.926575212   4.950704980  1.312297273
[318,] -0.1843325608  1.364126137 -1.814461417   2.806065020 -0.993909431
[319,] -1.3714045030  0.556409910 -1.678003448  -1.641227464 -0.150295198
[320,]  0.3734495482 -3.813660303 -4.297412575  -2.989620528  2.972251935
[321,] -3.8111874424 -1.103618011 -5.718179987   0.038854005 -1.518296751
[322,] -0.4503106706 -1.593864412 -2.800863481  -1.191181804  2.734001096
[323,] -1.1646377032  1.377078821  3.748657213  -1.252972228 -0.567072995
[324,] -1.9196671897  0.240295466  1.163759700   3.036373295 -0.589744496
[325,] -2.4786493693  2.440088528  0.722633034   0.007714635 -1.832410270
[326,]  0.2759757669  1.803648954  4.229651552  -2.757308911 -0.119949920
[327,] -1.4406946784  0.280358344 -0.668566278   3.817194167 -1.048930730
[328,] -3.7195307994  0.132371450  0.048227163   6.582063532 -2.608982649
[329,] -0.0198730560  0.403507312  2.032030301  -2.353155918 -0.661874066
[330,]  2.6522478012 -0.875752249 -2.118692911  -8.584447666  0.676784210
[331,]  1.7482016944  3.463659667  5.083062612   0.407479241 -0.477145735
[332,]  3.2057224200  1.805907691  5.097592442   0.313955928 -0.194772102
[333,]  1.0830141534 -1.219515926 -0.606352500  -4.938554433  0.662739471
[334,]  2.1939255089  1.378546836  5.532263744  -4.825718910 -0.512513147
[335,]  0.2895851813  2.221719876  2.759203968  -1.138951352 -1.466324285
[336,] -1.6680114621  7.255253549  6.021719464   4.366479080 -4.849974098
[337,]  2.9149340641  1.813955090  3.099250059   2.831824079 -1.697545040
[338,] -1.3055925344 -3.543215074 -2.028013686   6.229299571 -0.004534920
[339,]  1.1292342049 -0.458117182  2.101925922  -1.413211931  1.020912005
[340,]  3.6242052353 -2.936378240 -4.912682226  -4.466661455  3.117054237
[341,] -0.6032583870  1.083878524  1.386151328  -6.760306755  0.273400085
[342,] -5.8408875246  0.364345708 -1.606113345  -3.941886283  0.297201502
[343,]  2.3528814653 -3.199928006 -0.156254785  -4.858713803  1.871573879
[344,]  2.9689024190 -1.871368564  0.907082987   1.068395241  1.253374176
[345,] -0.1994619398 -3.723697419 -3.951286919  -4.694055033  3.340486308
[346,] -1.1824969542 -1.223344393 -1.199173883   3.133309888 -0.561366430
[347,]  2.1900337315 -0.135932924  0.934022455  -1.823322418  0.871435934
[348,]  0.4574408621 -2.406204410 -0.073214145  -3.376441453  2.788976112
[349,] -1.9836893780 -4.130532452 -4.658974990   0.449849836  2.733680364
[350,] -1.1000589540  1.046733721 -1.399941834   8.164718767  0.014890627
[351,] -2.6052711423  4.492324992  4.856245845   1.956843650 -2.807793189
[352,] -2.0544991100  2.604083874 -2.878668632   3.878712133 -1.720694879
[353,]  4.0210815172 -1.419503975  6.110612202  -3.640316119  1.849353830
[354,]  2.4241853441 -0.508739794  2.508038096  -1.007456478  1.885966852
[355,]  0.2533916759 -0.509847528 -1.244787862   0.726972540  0.399856659
[356,] -4.3325638910  3.046534506  1.852349735  -7.777479756 -1.169035608
[357,] -3.2703336649  0.098420032 -2.445248917  -2.368231253 -0.377835768
[358,] -1.0002713655  4.085988836  0.689416392   1.569156500 -2.370921762
[359,] -2.7068700043  3.215339037  0.827195295  -1.834827563 -2.057384924
[360,]  1.9138198953 -0.156235204  3.225552439  -1.257812321  1.036676835
[361,]  2.0649131951  1.603782589  5.928903080  -1.478982412  0.402097546
[362,]  1.5732949929 -0.578526750  2.543265185  -0.820911660  0.355650098
[363,]  5.5018308327 -1.581824249  2.939668643   0.528774125  1.813284599
[364,] -1.8178895237 -0.837440767 -3.272859663  -0.524138343  0.163560944
[365,]  0.6379840448  1.488900809  1.950844328   0.150114299 -1.592521509
[366,]  5.1579950812 -3.064644101 -0.890625098   1.718199334  2.732690106
[367,] -4.9957252204 -0.782929898 -1.679901358   1.702519270 -0.206548879
[368,] -0.9901613421 -1.058866068 -2.217511692  -4.797444928  1.079949938
[369,]  1.3527372504 -6.651716236 -3.924733157   5.478559947  4.597974468
[370,]  0.3439111873 -0.114700380  0.475479632   2.040013425  0.165362378
[371,] -2.0698741776 -0.002093949 -2.623280008   0.402484692 -1.260062369
[372,] -3.6942055001  2.230241853 -0.745881858   0.408072506 -1.511184334
[373,]  2.6792189684  2.024161668  0.311133695   2.837114741 -1.228428290
[374,] -4.1200290084 -3.954575226 -6.182715237   4.540783902  1.141889815
[375,] -3.0389910195  2.127803778  1.473365199   4.749093504 -2.376674140
[376,]  2.9276471689  0.266925494  0.904247090   1.941260499 -0.437891765
[377,] -0.0996330446  3.480036806  1.227569879   2.408211004 -2.432264291
[378,]  5.6891190090 -2.271058567  1.273417213   1.822326223  0.172380537
[379,]  1.4151926928  4.432804917  2.225055529   0.604446965 -1.661457302
[380,]  4.6351507482 -2.861036630  0.581493898  -2.762629132  2.931101124
[381,] -2.0845835826  0.437190129 -4.311451776   1.695410225 -3.460968084
[382,]  0.5260692962  1.988186521  1.693206474   0.095309877 -0.654517664
[383,]  1.6234055938  0.249275099  2.935306962  -2.413443091  0.441335925
[384,] -0.0125945077  0.328927190 -1.000813898  -2.438906545 -0.335985361
[385,] -2.2971449989 -0.665226781 -2.276579138  -2.175669722 -1.186653499
[386,]  0.6265146542  4.274682751  2.014068285   3.072674562 -3.599539185
[387,]  1.2231314642  1.026097772  1.410984317   0.100134558 -0.538417996
[388,] -1.4290822752 -0.049352590  1.606991285  -1.730864779  0.473596109
[389,] -0.9372528372  1.615507190  1.480250427   5.063850969 -0.793673728
[390,]  1.8775016231  2.664194118  4.333083432  -0.301266972 -1.478078120
[391,] -2.8079047800 -0.583140754 -3.070339763  -4.734645850 -0.018427698
[392,] -1.3133931201 -4.057036154 -4.347763891   1.440233916  1.382953551
[393,] -1.5239131507 -0.239784368 -4.013799937   1.777625263 -1.034984876
[394,]  2.5665471713  1.542321872  1.205232466   3.159327011 -2.264593305
[395,]  3.3713851129 -3.331020990  0.488637965  -0.333368183  2.975326211
[396,]  3.8354075022  0.392123374  0.404967955   0.357983871 -0.791072036
[397,] -1.0800485176 -2.179510817  1.018442906  -3.379460578  1.394947831
[398,]  3.2589579784 -1.477800490  3.221959360  -2.584628239  0.968719704
[399,] -1.7802306370  3.391903955 -0.259946656   0.034371577 -2.626135392
[400,] -2.0346276380  0.687385242  0.123381126  -4.826722847 -0.954933517</code></pre>
</div>
<div class="sourceCode cell-code" id="cb55" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Correlation of decisionmaker random slopes</span></span>
<span id="cb55-2">Omega <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">cor</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">matrix</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(P<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(P<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)), P<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, P))</span>
<span id="cb55-3"></span>
<span id="cb55-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Scale of decisionmaker random slopes</span></span>
<span id="cb55-5">tau <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">abs</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(P, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>))</span>
<span id="cb55-6"></span>
<span id="cb55-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Covariance matrix of decisionmaker random slopes</span></span>
<span id="cb55-8">Sigma <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">diag</span>(tau) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> Omega <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">diag</span>(tau)</span>
<span id="cb55-9"></span>
<span id="cb55-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Centers of random slopes</span></span>
<span id="cb55-11">beta <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(P)</span>
<span id="cb55-12"></span>
<span id="cb55-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Individual slopes</span></span>
<span id="cb55-14">beta_i <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> MASS<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mvrnorm</span>(I, beta, Sigma) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> W <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">t</span>(Gamma)</span>
<span id="cb55-15"></span>
<span id="cb55-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Again, quick plot to sanity check</span></span>
<span id="cb55-17"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">plot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.data.frame</span>(beta_i))</span></code></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/doing_maxdiff_better/better_maxdiff_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb56" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb56-1">bibd <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">find.BIB</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">trt =</span> Tasks, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">b =</span> I<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>Tasks, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">k =</span> J)</span>
<span id="cb56-2"></span>
<span id="cb56-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The below function is not guaranteed to produce a valid BIBD; it may produce</span></span>
<span id="cb56-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># a close but imperfect design, which is sufficient for our purposes. I will</span></span>
<span id="cb56-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># however confirm it's connected, since we'd start to lose quite a lot of efficiency</span></span>
<span id="cb56-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># if it were not. </span></span>
<span id="cb56-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Returns 1 if the design is connected</span></span>
<span id="cb56-8"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">is.connected</span>(bibd)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb58" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create a mapping from BIBD options to attribute levels; idea is to make</span></span>
<span id="cb58-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># identically shaped inputs to our previous test, but using the conditions</span></span>
<span id="cb58-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># from the BIBD instead of sampling randomly for X.</span></span>
<span id="cb58-4">option_attributes <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">matrix</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sample</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>P, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">replace =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">nrow =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ncol =</span> P)</span>
<span id="cb58-5"></span>
<span id="cb58-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Function to convert BIBD task to attribute matrix</span></span>
<span id="cb58-7">bibd_to_attributes <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>(bibd_row) {</span>
<span id="cb58-8">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">matrix</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(</span>
<span id="cb58-9">    option_attributes[bibd_row[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], ],</span>
<span id="cb58-10">    option_attributes[bibd_row[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>], ],</span>
<span id="cb58-11">    option_attributes[bibd_row[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>], ]</span>
<span id="cb58-12">  ), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">nrow =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">byrow =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>)</span>
<span id="cb58-13">}</span>
<span id="cb58-14"></span>
<span id="cb58-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Generate X matrix based on BIBD</span></span>
<span id="cb58-16">X <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">do.call</span>(rbind, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">lapply</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(bibd), <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>(i) <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bibd_to_attributes</span>(bibd[i,])))</span>
<span id="cb58-17"></span>
<span id="cb58-18">indexes <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">crossing</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">individual =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>I, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>Tasks, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">option =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>J) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb58-19">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">row =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">n</span>())</span>
<span id="cb58-20"></span>
<span id="cb58-21"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Write a Gumbel random number generator using inverse CDF trick</span></span>
<span id="cb58-22">rgumbel <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>(n, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">mu =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">beta =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) mu <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> beta <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">runif</span>(n)))</span>
<span id="cb58-23"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mean</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rgumbel</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e6</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5771369</code></pre>
</div>
<div class="sourceCode cell-code" id="cb60" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Ok, now we need to simulate choices. Each person in each task compares each </span></span>
<span id="cb60-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># choice according to X*beta_i + epsilon, where epsilon is gumbel distributed. </span></span>
<span id="cb60-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># They return their rankings. </span></span>
<span id="cb60-4"></span>
<span id="cb60-5">ranked_options <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> indexes <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb60-6">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(individual, task) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb60-7">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">fixed_utility =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.numeric</span>(X[row,] <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.numeric</span>(beta_i[<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">first</span>(individual),])),</span>
<span id="cb60-8">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">plus_gumbel_error =</span> fixed_utility <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rgumbel</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">n</span>()),</span>
<span id="cb60-9">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">rank =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rank</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>plus_gumbel_error),</span>
<span id="cb60-10">         <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># We're going to use the order rather than the rank in the Stan part of the model</span></span>
<span id="cb60-11">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">order =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">order</span>(rank),</span>
<span id="cb60-12">         <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># And here we create a dummy vector for the best choice</span></span>
<span id="cb60-13">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">best_choice =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.numeric</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">n</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">which.max</span>(plus_gumbel_error)),</span>
<span id="cb60-14">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">worst_choice =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.numeric</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">n</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">which.min</span>(plus_gumbel_error))</span>
<span id="cb60-15">  )</span>
<span id="cb60-16"></span>
<span id="cb60-17"></span>
<span id="cb60-18">tt <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> ranked_options <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb60-19">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(individual, task) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb60-20">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarise</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">start =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">min</span>(row), </span>
<span id="cb60-21">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">end =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">max</span>(row)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb60-22">  ungroup <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb60-23">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task_number =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">n</span>())</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`summarise()` has grouped output by 'individual'. You can override using the
`.groups` argument.</code></pre>
</div>
</div>
</details>
<p>Now we have simulated data that represents a connected graph design with 10 total choices, and 15 choice sets per respondent. Each choice set contains 3 options, and each pair of choices appears together in at least one set across all respondents.</p>
</section>
<section id="evaluating-our-new-model" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-our-new-model">Evaluating our new model</h2>
<p>So now we’ve introduced the ranked choice logit model, and simulated data that fits the connected graph structure. How do the three models perform?</p>
<p>Again, some data plumbing in the drop down below, so feel free to jump to the results plots after it.</p>
<details>
<summary>
<strong>Comparing ROL to best choice, and MaxDiff models</strong>
</summary>
<div class="cell">
<div class="sourceCode cell-code" id="cb62" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1">data_list_best_choice <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">N =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(X),</span>
<span id="cb62-2">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">T =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(tt),</span>
<span id="cb62-3">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">I =</span> I, </span>
<span id="cb62-4">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">P =</span> P, </span>
<span id="cb62-5">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">P2 =</span> P2, </span>
<span id="cb62-6">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">K =</span> J, </span>
<span id="cb62-7">                             <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">NOTE</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!! This is the tricky bit -- we use the order of the ranks (within task)</span></span>
<span id="cb62-8">                             <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Not the raw rank orderings. This is how we get the likelihood evaluation to be pretty quick</span></span>
<span id="cb62-9">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">choice =</span> ranked_options<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>best_choice,</span>
<span id="cb62-10">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X =</span> X, </span>
<span id="cb62-11">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X2 =</span> W, </span>
<span id="cb62-12">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>task_number, </span>
<span id="cb62-13">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task_individual =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>individual,</span>
<span id="cb62-14">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">start =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>start, </span>
<span id="cb62-15">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">end =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>end)</span>
<span id="cb62-16"></span>
<span id="cb62-17">data_list_best_worst <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">N =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(X),</span>
<span id="cb62-18">                            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">T =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(tt),</span>
<span id="cb62-19">                            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">I =</span> I, </span>
<span id="cb62-20">                            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">P =</span> P, </span>
<span id="cb62-21">                            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">P2 =</span> P2, </span>
<span id="cb62-22">                            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">K =</span> J,</span>
<span id="cb62-23">                            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">choice =</span> ranked_options<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>best_choice,</span>
<span id="cb62-24">                            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">worst_choice =</span> ranked_options<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>worst_choice,</span>
<span id="cb62-25">                            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X =</span> X, </span>
<span id="cb62-26">                            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X2 =</span> W, </span>
<span id="cb62-27">                            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>task_number, </span>
<span id="cb62-28">                            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task_individual =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>individual,</span>
<span id="cb62-29">                            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">start =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>start, </span>
<span id="cb62-30">                            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">end =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>end)</span>
<span id="cb62-31"></span>
<span id="cb62-32">data_list_ranked_rcl <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">N =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(X),</span>
<span id="cb62-33">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">T =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(tt),</span>
<span id="cb62-34">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">I =</span> I, </span>
<span id="cb62-35">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">P =</span> P, </span>
<span id="cb62-36">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">P2 =</span> P2, </span>
<span id="cb62-37">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">K =</span> J, </span>
<span id="cb62-38">                             <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">NOTE</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!! This is the tricky bit -- we use the order of the ranks (within task)</span></span>
<span id="cb62-39">                             <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Not the raw rank orderings. This is how we get the likelihood evaluation to be pretty quick</span></span>
<span id="cb62-40">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">rank_order =</span> ranked_options<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>order,</span>
<span id="cb62-41">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X =</span> X, </span>
<span id="cb62-42">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X2 =</span> W, </span>
<span id="cb62-43">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>task_number, </span>
<span id="cb62-44">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task_individual =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>individual,</span>
<span id="cb62-45">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">start =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>start, </span>
<span id="cb62-46">                             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">end =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>end)</span>
<span id="cb62-47"></span>
<span id="cb62-48">compiled_best_choice_model <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stan_model</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model_code =</span> best)</span>
<span id="cb62-49">best_choice_fit <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sampling</span>(compiled_best_choice_model, </span>
<span id="cb62-50">                            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> data_list_best_choice, </span>
<span id="cb62-51">                            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">iter =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">800</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: There were 2 divergent transitions after warmup. See
https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
to find out why this is a problem and how to eliminate them.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Examine the pairs() plot to diagnose sampling problems</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
</div>
<div class="sourceCode cell-code" id="cb67" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb67-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compile and fit MaxDiff model</span></span>
<span id="cb67-2">compiled_maxdiff_model <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stan_model</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model_code =</span> best_worst)</span>
<span id="cb67-3">maxdiff_fit <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sampling</span>(compiled_maxdiff_model,</span>
<span id="cb67-4">                        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> data_list_best_worst,</span>
<span id="cb67-5">                        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">iter =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">800</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: The largest R-hat is NA, indicating chains have not mixed.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#r-hat</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
</div>
<div class="sourceCode cell-code" id="cb71" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb71-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compile and fit ROL model</span></span>
<span id="cb71-2">compiled_rol_model <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stan_model</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model_code =</span> ranked)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in readLines(file.path(makevar_files)): incomplete final line found on
'C:\Users\timma\Documents/.R/Makevars.win'</code></pre>
</div>
<div class="sourceCode cell-code" id="cb73" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1">rol_fit <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sampling</span>(compiled_rol_model,</span>
<span id="cb73-2">                    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> data_list_ranked_rcl,</span>
<span id="cb73-3">                    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">iter =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">800</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: There were 1 divergent transitions after warmup. See
https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
to find out why this is a problem and how to eliminate them.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Examine the pairs() plot to diagnose sampling problems</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: The largest R-hat is NA, indicating chains have not mixed.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#r-hat</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
</div>
<div class="sourceCode cell-code" id="cb79" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb79-1">best_choice <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.data.frame</span>(best_choice_fit, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">pars =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"beta_individual"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb79-2">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">gather</span>(Parameter, Value) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb79-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(Parameter) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb79-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarise</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">median =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">median</span>(Value),</span>
<span id="cb79-5">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">lower =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">quantile</span>(Value, .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">05</span>),</span>
<span id="cb79-6">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">upper =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">quantile</span>(Value, .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">95</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb79-7">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">individual =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">str_extract</span>(Parameter, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"[0-9]+(?=,)"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> parse_number,</span>
<span id="cb79-8">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">column =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">str_extract</span>(Parameter, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">",[0-9]{1,2}"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> parse_number) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb79-9">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">arrange</span>(individual, column) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb79-10">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">True value</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.numeric</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">t</span>(beta_i)),</span>
<span id="cb79-11">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Dataset =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Best Choice"</span>)</span>
<span id="cb79-12"></span>
<span id="cb79-13">maxdiff_choice <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.data.frame</span>(maxdiff_fit, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">pars =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"beta_individual"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb79-14">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">gather</span>(Parameter, Value) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb79-15">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(Parameter) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb79-16">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarise</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">median =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">median</span>(Value),</span>
<span id="cb79-17">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">lower =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">quantile</span>(Value, .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">05</span>),</span>
<span id="cb79-18">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">upper =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">quantile</span>(Value, .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">95</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb79-19">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">individual =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">str_extract</span>(Parameter, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"[0-9]+(?=,)"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> parse_number,</span>
<span id="cb79-20">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">column =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">str_extract</span>(Parameter, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">",[0-9]{1,2}"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> parse_number) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb79-21">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">arrange</span>(individual, column) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb79-22">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">True value</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.numeric</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">t</span>(beta_i)),</span>
<span id="cb79-23">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Dataset =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MaxDiff"</span>)</span>
<span id="cb79-24"></span>
<span id="cb79-25">rol_choice <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.data.frame</span>(rol_fit, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">pars =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"beta_individual"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb79-26">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">gather</span>(Parameter, Value) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb79-27">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(Parameter) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb79-28">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarise</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">median =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">median</span>(Value),</span>
<span id="cb79-29">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">lower =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">quantile</span>(Value, .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">05</span>),</span>
<span id="cb79-30">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">upper =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">quantile</span>(Value, .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">95</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb79-31">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">individual =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">str_extract</span>(Parameter, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"[0-9]+(?=,)"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> parse_number,</span>
<span id="cb79-32">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">column =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">str_extract</span>(Parameter, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">",[0-9]{1,2}"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> parse_number) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb79-33">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">arrange</span>(individual, column) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb79-34">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">True value</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.numeric</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">t</span>(beta_i)),</span>
<span id="cb79-35">         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Dataset =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Rank-Ordered Logit"</span>)</span>
<span id="cb79-36"></span>
<span id="cb79-37"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Combine all results</span></span>
<span id="cb79-38">all_results <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bind_rows</span>(best_choice, maxdiff_choice, rol_choice)</span></code></pre></div>
</div>
</details>
<div class="cell">
<div class="sourceCode cell-code" id="cb80" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb80-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(all_results, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">True value</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> median, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> Dataset)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb80-2">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_linerange</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ymin =</span> lower, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ymax =</span> upper), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">alpha =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb80-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_point</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> median), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">alpha =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb80-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_abline</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">intercept =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">slope =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb80-5">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">labs</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Utility Estimates"</span>,</span>
<span id="cb80-6">       <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">title =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Utility Estimates from the Three Models"</span>,</span>
<span id="cb80-7">       <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">subtitle =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"With 90% CIs"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb80-8">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">scale_color_manual</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">values =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Best Choice"</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"blue"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MaxDiff"</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"red"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Rank-Ordered Logit"</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"green"</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb80-9">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">facet_wrap</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>Dataset) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb80-10">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">legend.position=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bottom"</span>)</span></code></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/doing_maxdiff_better/better_maxdiff_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>At a squint, rank-ordered logit &gt; best choice &gt; MaxDiff as I’ve argued. Notice though that unlike the MaxDiff model, both best choice and rank-ordered logit look unbiased; the ROL model simply seems more efficient. Let’s check the RMSE to confirm:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb81" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb81-1">all_results <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb81-2">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(Dataset) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb81-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarize</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">RMSE =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sqrt</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mean</span>((<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">True value</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> median)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">^</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 3 × 2
  Dataset             RMSE
  &lt;chr&gt;              &lt;dbl&gt;
1 Best Choice        0.483
2 MaxDiff            0.556
3 Rank-Ordered Logit 0.417</code></pre>
</div>
</div>
<p>The ordering is as we’d expect from the plots.</p>
<p>If I have free choice over the design of a choice experiment, this sort of connected graph design, analyzed with the rank-ordered logit is what I’d generally prefer to use. Designing the choice graphs to make this work well isn’t particularly hard with modern software, and the rank ordered logit model is widely available, not just in a sophisticated Stan model like I cribbed from Jim Savage here, but also in <a href="https://www.stata.com/manuals13/rrologit.pdf">Stata</a>, and other commonly used languages.</p>
<p>Hopefully the theoretical discussion and simulation study so far are persuasive: you can and should aim to do better than the MaxDiff model of discrete choice.</p>
</section>
</section>
<section id="alternative-2-a-backup-plan--rank-ordered-logits-with-unknown-middle-preferences" class="level1">
<h1>Alternative #2: a backup plan- rank-ordered logits with unknown middle preferences</h1>
<p>There’s still an interesting question left here: what to do if you already ran an experiment designed without connectedness in mind, or your survey platform won’t easily allow you to make a connected design?</p>
<p>It’s not my preferred choice if I have control over design, but in this scenario we can use a Rank-Ordered Logit with unknown middle preferences.</p>
<section id="allowing-unknown-options-in-rank-ordered-logit-models" class="level2">
<h2 class="anchored" data-anchor-id="allowing-unknown-options-in-rank-ordered-logit-models">Allowing unknown options in rank-ordered logit models</h2>
<p>Let’s reuse the example above where we ranked my preferences over 2024 presidential candidates. The complete ordering for me is: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctextcolor%7Bblue%7D%7B%5Ctext%7BBiden%7D%7D%20%3E%20%5Ctextcolor%7Bgreen%7D%7B%5Ctext%7BStein%7D%7D%20%3E%20%5Ctextcolor%7Byellow%7D%7B%5Ctext%7BRFK%7D%7D%20%3E%20%5Ctextcolor%7Bred%7D%7B%5Ctext%7BTrump%7D%7D%0A"> If we collected data from me though by only asking my best and worse choices, we’d have: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctextcolor%7Bblue%7D%7B%5Ctext%7BBiden%7D%7D%20%3E%20???%20%3E%20???%20%3E%20%5Ctextcolor%7Bred%7D%7B%5Ctext%7BTrump%7D%7D%0A"> To keep using a rank-ordered model here, we’d need to figure out how to reason about the middle two options being unknown. <a href="https://www.jstor.org/stable/270983">Allison and Christakis, 1994</a> discuss various ways to handle this, but the one that seems most promising and that we’ll discuss operates as if the true ranking exists, but is unknown to the modeler. That seems pretty reasonable, since we’re not saying the middle preferences have to be strong, just that people likely aren’t totally ambivalent between the two in general. To handle this, we’ll first enumerate the possible ways the middle options could pan out. Here’s there’s just two: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctextcolor%7Bblue%7D%7B%5Ctext%7BBiden%7D%7D%20%3E%20%5Ctextcolor%7Bgreen%7D%7B%5Ctext%7BStein%7D%7D%20%3E%20%5Ctextcolor%7Byellow%7D%7B%5Ctext%7BRFK%7D%7D%20%3E%20%5Ctextcolor%7Bred%7D%7B%5Ctext%7BTrump%7D%7D%20%5Cvert%0A%5Ctextcolor%7Bblue%7D%7B%5Ctext%7BBiden%7D%7D%20%3E%20%5Ctextcolor%7Byellow%7D%7B%5Ctext%7BRFK%7D%7D%20%3E%20%5Ctextcolor%7Bgreen%7D%7B%5Ctext%7BStein%7D%7D%20%3E%20%5Ctextcolor%7Bred%7D%7B%5Ctext%7BTrump%7D%7D%0A"> Then, to construct a likelihood, we’ll marginalize over all the possible middle options. Since the two ways the middle options could shake out are mutually exclusive, that implies: <img src="https://latex.codecogs.com/png.latex?%0AP(%5Ctextcolor%7Bblue%7D%7BB%7D%3E%5Ctextcolor%7Bgreen%7D%7BS%7D%3E%5Ctextcolor%7Byellow%7D%7BR%7D%3E%5Ctextcolor%7Bred%7D%7BT%7D%20%5Ctext%7B%20or%20%7D%20%5Ctextcolor%7Bblue%7D%7BB%7D%3E%5Ctextcolor%7Byellow%7D%7BR%7D%3E%5Ctextcolor%7Bgreen%7D%7BS%7D%3E%5Ctextcolor%7Bred%7D%7BT%7D)%20=%20P(%5Ctextcolor%7Bblue%7D%7BB%7D%3E%5Ctextcolor%7Bgreen%7D%7BS%7D%3E%5Ctextcolor%7Byellow%7D%7BR%7D%3E%5Ctextcolor%7Bred%7D%7BT%7D)%20+%20P(%5Ctextcolor%7Bblue%7D%7BB%7D%3E%5Ctextcolor%7Byellow%7D%7BR%7D%3E%5Ctextcolor%7Bgreen%7D%7BS%7D%3E%5Ctextcolor%7Bred%7D%7BT%7D)%0A"></p>
<p>Now, let’s express this using the notation from Allison and Christakis (1994), which will help us handle the subscripts more easily. For a given individual <img src="https://latex.codecogs.com/png.latex?i">, let <img src="https://latex.codecogs.com/png.latex?%5Cmu_%7Bij%7D"> be the deterministic utility of the alternative ranked <img src="https://latex.codecogs.com/png.latex?j">-th. The probability of observing a particular complete ranking <img src="https://latex.codecogs.com/png.latex?r"> (where <img src="https://latex.codecogs.com/png.latex?r"> is one of the possible rankings consistent with the observed partial ranking) is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(r)%20=%20%5Cprod_%7Bj=1%7D%5E%7BJ-1%7D%20%5Cfrac%7B%5Cexp(%5Cmu_%7Bij%7D)%7D%7B%5Csum_%7Bk=j%7D%5EJ%20%5Cexp(%5Cmu_%7Bik%7D)%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?J"> is the total number of alternatives. For our example with unknown middle preferences, the likelihood becomes:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL(%5Ctextcolor%7Bblue%7D%7BB%7D%3E%5Ctextcolor%7Bgreen%7D%7BS%7D,%5Ctextcolor%7Byellow%7D%7BR%7D%3E%5Ctextcolor%7Bred%7D%7BT%7D)%20=%20%5Cfrac%7B%5Cexp(%5Cmu_%7Bi%5Ctextcolor%7Bblue%7D%7BB%7D%7D)%7D%7B%5Csum_%7Bj%20%5Cin%20%7B%5Ctextcolor%7Bblue%7D%7BB%7D,%5Ctextcolor%7Bgreen%7D%7BS%7D,%5Ctextcolor%7Byellow%7D%7BR%7D,%5Ctextcolor%7Bred%7D%7BT%7D%7D%7D%20%5Cexp(%5Cmu_%7Bij%7D)%7D%20%5Ccdot%20%5Cleft%5B%20%5Cfrac%7B%5Cexp(%5Cmu_%7Bi%5Ctextcolor%7Bgreen%7D%7BS%7D%7D)%7D%7B%5Cexp(%5Cmu_%7Bi%5Ctextcolor%7Bgreen%7D%7BS%7D%7D)%20+%20%5Cexp(%5Cmu_%7Bi%5Ctextcolor%7Byellow%7D%7BR%7D%7D)%20+%20%5Cexp(%5Cmu_%7Bi%5Ctextcolor%7Bred%7D%7BT%7D%7D)%7D%20+%20%5Cfrac%7B%5Cexp(%5Cmu_%7Bi%5Ctextcolor%7Byellow%7D%7BR%7D%7D)%7D%7B%5Cexp(%5Cmu_%7Bi%5Ctextcolor%7Bgreen%7D%7BS%7D%7D)%20+%20%5Cexp(%5Cmu_%7Bi%5Ctextcolor%7Byellow%7D%7BR%7D%7D)%20+%20%5Cexp(%5Cmu_%7Bi%5Ctextcolor%7Bred%7D%7BT%7D%7D)%7D%20%5Cright%5D%20%5Ccdot%20%5Cfrac%7B%5Cexp(%5Cmu_%7Bi%5Ctextcolor%7Bred%7D%7BT%7D%7D)%7D%7B%5Cexp(%5Cmu_%7Bi%5Ctextcolor%7Bred%7D%7BT%7D%7D)%7D%0A"></p>
<p>The central terms are the key to the unknown ranking model<sup>10</sup>. This lets us fit a ranked model and avoid MaxDiff, even with unknown middle options.</p>
<p>Of course, this approach isn’t without issues. Obviously, with a bit more foresight on design we could’ve learned more from the middle options while still asking respondents to make two choices per task. Putting that aside, the number of unknown middle options grows as <img src="https://latex.codecogs.com/png.latex?J_%7Bunknown%7D!">, which makes calculating all the likelihood terms a real, real pain in the ass. If we re-do our initial example with 10 total alternatives, the middle 8 will be unknown, and there will be 40,320 permutations.</p>
</section>
<section id="building-the-rol-with-unknown-middle-preferences-in-stan" class="level2">
<h2 class="anchored" data-anchor-id="building-the-rol-with-unknown-middle-preferences-in-stan">Building the ROL with unknown middle preferences in Stan</h2>
<p>I’ll present two versions of the ROL with unknown ties model. First, the full marginal likelihood model described above, and then the Efron approximation you can use when the number of unknown middle options gets unwieldy. We’ll build towards comparing MaxDiff, ROL with known middle options, and the two ROL with unknown middle options all together.</p>
<p>To make it easier to follow logic for many middle options, we’ll use <img src="https://latex.codecogs.com/png.latex?J%20=%205"><sup>11</sup>. The initial block below just sets up our DGP again with that <img src="https://latex.codecogs.com/png.latex?J">, so feel free to skip over it.</p>
<details>
<summary>
<strong>Setting up sim study with J = 5</strong>
</summary>
<div class="cell">
<div class="sourceCode cell-code" id="cb83" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb83-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Number of individuals</span></span>
<span id="cb83-2">I <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">400</span></span>
<span id="cb83-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Number of tasks per individual</span></span>
<span id="cb83-4">Tasks <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb83-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Number of choices per task</span></span>
<span id="cb83-6">J <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb83-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Dimension of covariate matrix</span></span>
<span id="cb83-8">P <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb83-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Dimension of demographic matrix</span></span>
<span id="cb83-10">P2 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span></span>
<span id="cb83-11"></span>
<span id="cb83-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># demographic matrix</span></span>
<span id="cb83-13">W <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">matrix</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(I<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>P2), I, P2)</span>
<span id="cb83-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Loading matrix</span></span>
<span id="cb83-15">Gamma <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">matrix</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(P<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>P2), P, P2)</span>
<span id="cb83-16"></span>
<span id="cb83-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Show W * t(Gamma) to make sure it looks right</span></span>
<span id="cb83-18">W <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">t</span>(Gamma)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               [,1]        [,2]         [,3]         [,4]         [,5]
  [1,] -1.216579571  2.70873392   4.76010662 -4.382515769 -0.937908838
  [2,] -1.387844888  0.17885653  -1.33873062  2.035207807 -2.473143410
  [3,] -1.974087987  2.54974157  -0.05054604  0.473104518 -0.319766538
  [4,] -0.038572162 -0.12696491  -4.25409082  2.960893246  1.404494080
  [5,] -1.973286116  1.83777720   3.11764558 -1.396922278 -3.347534740
  [6,]  0.021811383  0.13485559  -0.55249459 -2.480132938  1.291705586
  [7,] -0.695194602 -2.79267967  -4.67942726 -5.346484029 -1.963929107
  [8,]  0.172634748  0.36643230  -2.25234689  1.168163720  1.368807721
  [9,] -0.077259388 -0.59355347  -3.31794394  5.211427489 -0.505453554
 [10,]  0.360707691 -2.45773636  -8.34096274  3.798035117  0.199426789
 [11,]  2.750266966 -1.82184235   0.48818279  4.393474270  1.757772220
 [12,] -0.679066320  1.31976351  -2.30763550  1.889675707  2.789800339
 [13,]  1.356207516 -0.77427314   6.61269431 -3.815191591 -2.175364307
 [14,]  0.766761499 -0.34906173   4.66706502  1.696090489 -2.510595159
 [15,] -0.996255136  0.67730930   0.16777477  1.597980539 -1.645200273
 [16,]  3.340993722 -2.72091034  -2.83588751  1.817107395  4.048968295
 [17,]  0.351522713 -1.02381470  -7.91429219 -0.228757153  4.374795113
 [18,]  1.240716479 -2.03866079  -5.71359396  4.494115105  0.583872144
 [19,]  0.058400326  0.04797676  -0.33386413  1.875720886 -0.491040485
 [20,]  0.977746395 -0.83279447  -3.17861331 -3.326265046  3.883902047
 [21,] -1.582000171  2.31009955   4.24136353 -1.551488120 -1.241483096
 [22,] -0.514563884  0.57884167   0.47157613  2.094924274 -0.644648908
 [23,] -2.406167342  3.98769875   1.32632682 -3.453997963  2.294923969
 [24,]  1.183554922 -2.37631996   2.49497064 -5.268382542 -2.512550749
 [25,]  0.189164826 -0.17422938   0.07025894  2.769579833 -1.050418189
 [26,] -1.060474995  1.25949950  -4.62147738  0.029562404  3.640131139
 [27,] -1.429353764  1.12960654  -0.20956355 -0.968738060 -1.075509890
 [28,]  2.468893040 -2.27091117  -2.04595658  5.960297725  1.714721598
 [29,] -0.918459168  0.59887699  -3.84139586  0.994688172  2.235764032
 [30,] -1.686497217  1.20746453  -5.77564749  0.768937050  1.798329665
 [31,] -1.188033561  0.26178445  -2.40960031 -7.464547340  1.120329318
 [32,]  1.862992252  0.44641916   1.03165331  7.273582356  1.929000017
 [33,] -1.127011252  0.91077583  -0.07401327  4.276614591 -0.781270603
 [34,] -0.181426645  2.52195817   7.55051619 -2.290981931 -0.329052100
 [35,]  2.085501801 -2.58577267   2.52995115 -4.481252700 -0.342047830
 [36,]  0.701709411  0.16019537   1.43646575  0.278467356  0.510498483
 [37,] -0.671479819  3.47314033   5.36198511 -1.884920932  2.215536088
 [38,] -2.454845069  1.12652917  -6.99112053 -0.392976205  0.478820446
 [39,]  1.730178777 -0.89022809  -1.05486418  1.620248890  3.278902366
 [40,] -1.663282979  1.39908216  -5.21125371 -2.110969958  3.436185997
 [41,]  1.797840016 -1.86535129   2.52832408 -4.538488314 -0.079499900
 [42,]  1.038719451  0.26498961   3.26465934  0.599748301 -0.033729624
 [43,]  2.297279750 -1.58429404  -0.23695673  1.194527815  2.776576843
 [44,] -0.071852717  0.98513223  -4.43777834  2.495348624  4.374783717
 [45,] -1.543892918  1.97199815  -0.16631939  0.487586063 -0.283565322
 [46,]  0.426131807  1.97736196  -0.12517925  4.346262148  3.219351178
 [47,]  2.265043041 -2.09171520  -2.00845806 -0.068751460  1.797022682
 [48,]  0.567383299 -0.29024877   0.29224939  3.676598240 -0.621641327
 [49,] -0.176345124  0.44107718   4.89232266  1.126347219 -3.471926737
 [50,] -0.844702953  0.82349247  -0.49221873  0.307763571 -0.623136310
 [51,] -2.053192939  1.97084844   1.01393733 -1.086254711 -1.565288540
 [52,]  0.229281338  1.41228892  -0.90349963  4.346329529  3.349979294
 [53,]  0.984497495 -1.03927697  -4.25912636  2.922199666  2.442073417
 [54,] -3.091970408 -0.37952713  -5.18425564 -5.550823338 -3.153001017
 [55,] -0.131057602  1.20946429   0.82598928 -0.378467734  1.800731826
 [56,] -0.213103075 -1.65817438  -7.01258138  2.978287225  1.032440847
 [57,]  0.061283927 -1.69156643  -4.14396688  0.070556213 -0.722425082
 [58,] -0.049374049  2.57088819   8.30599698  0.601156956 -1.283911885
 [59,]  0.230893788  1.62922104   0.47314110  1.782321613  2.990915322
 [60,] -0.512382496  2.97945064  10.02622119 -0.832924450 -1.240592177
 [61,] -0.366173333  0.11557829  -3.87356417 -3.233602444  2.639237665
 [62,]  0.869895687  0.76576412  -0.43681650  4.001248944  3.176463018
 [63,]  0.473061889 -1.33483784   0.09255220 -1.490319783 -1.070715599
 [64,] -2.356754497  3.15982518  -0.99853340  0.735565698  0.936753056
 [65,]  0.201285345  1.41296717   8.27357369 -1.729195660 -2.741729085
 [66,] -0.401052793  0.86582842   4.54618171 -0.609548824 -1.579299240
 [67,] -2.863339391  1.71691761  -2.78441312 -0.955181086 -2.655232845
 [68,]  0.495327367  2.11081603   2.97899529  1.879336443  2.974633301
 [69,] -1.850276454  3.08091326   7.85568998 -2.790804301 -3.194963747
 [70,]  2.423489585 -3.07931789  -0.42103632 -2.065518997  0.143903961
 [71,]  0.587496372 -1.70882465  -0.38448361 -2.391568619 -1.013952113
 [72,] -2.223867231  3.84043486  -0.42335340  1.339277799  2.881342856
 [73,] -0.393369848  1.08856200   0.04034497 -1.685582733  2.328821949
 [74,] -0.201767104  0.44338176   1.24175513  0.246185021 -0.464638520
 [75,] -0.125690238  1.59907552   4.11853652 -3.969937744  0.674956172
 [76,]  2.573570274 -2.34340084  -5.26258972  3.990105924  3.116787863
 [77,] -2.367190587  2.90678698   2.14895611 -2.924219309 -0.294068412
 [78,]  0.223591758 -1.98841403  -1.81488639  0.441586505 -2.120663842
 [79,]  1.702386507 -1.65439290  -2.24642333  1.899063074  1.869864722
 [80,]  0.623283342 -2.37705031  -8.44270273 -0.320001760  2.197975787
 [81,] -1.135596092  1.93515262   2.26114534  0.187201581  0.053034881
 [82,]  0.992150277 -0.24651493   2.72928530  4.394954888 -0.835769166
 [83,]  0.888353672 -1.08083208   2.31564466 -3.506438335 -0.689231950
 [84,] -0.459142320 -1.96891848  -4.27152444 -1.673405034 -2.078976051
 [85,]  0.387188000 -1.59618300  -3.06711152 -1.776670111 -1.013240084
 [86,]  0.885856928  0.72663880   1.62617603  2.411960084  2.159386219
 [87,] -0.684864533 -0.52311775  -1.20279391 -1.366036983 -0.844771471
 [88,] -1.401212210  1.84508195  -3.17911088  1.012386180  2.650271632
 [89,]  4.381356302 -4.38588832  -2.11858977  4.122712243  1.924108586
 [90,] -2.468107679  3.08092735   0.73055995 -0.696269187  0.248239755
 [91,]  1.544479345 -1.85119287   2.16967643 -1.169893642 -0.130018928
 [92,] -2.474136352  1.86369601   2.15308368 -5.887644964 -3.080379017
 [93,]  1.397847763 -4.57065704  -9.73244829  2.421842468 -0.196002338
 [94,]  0.277850377  1.89699566   4.13484494  1.285801323  1.516673935
 [95,]  1.250203436 -1.88929158   1.13290415 -0.002303534 -1.038764649
 [96,] -0.520735336 -0.99436594  -4.46646725 -3.325818707  1.530780110
 [97,] -0.324184690  1.46353273   8.31126950 -4.562419476 -1.593689494
 [98,]  2.122994966 -3.51104869  -2.83203901 -0.025147839 -0.602620051
 [99,] -0.909697998  0.91702620   5.66086627  4.051043931 -4.876377716
[100,]  6.274110792 -6.90420006   1.42984899  2.353994272 -1.258770227
[101,] -3.438792759  1.56280262  -2.66053873 -3.811375308 -3.053074780
[102,] -1.694283234  0.85737290   0.91128272 -5.916134525 -0.433233922
[103,]  0.767207857 -0.89209510  -0.67522547 -0.845311186  0.339411959
[104,] -2.019318045  0.57742565  -1.93601609 -6.291971534  0.780497784
[105,] -1.929664677  4.55633777   7.38181491 -1.545012882  0.598312053
[106,]  0.682596844 -1.36802393  -1.71570849  2.368412757 -0.824857464
[107,]  0.876771519  0.99417037   1.88084167  5.150760527  1.850837612
[108,] -0.478065674  0.06722172   0.72906151 -1.793858351 -0.998983931
[109,]  3.148440674 -4.31802122   0.61109511  2.957494584 -2.521667099
[110,]  2.332325771 -3.41243063  -2.76281048 -0.308891070  0.517807936
[111,]  0.336806962  0.74171587   4.00147266 -0.117554886  0.218758744
[112,]  0.814612027 -0.48055430   0.38107544  1.051405077  0.079344009
[113,]  0.871134226 -1.14226771  -4.50155972 -1.529495949  2.723019057
[114,]  0.137997442  1.27417976  -0.52469212  0.935740984  2.552960788
[115,] -0.648607764  0.11710088   0.02121035 -0.038607345 -1.216682510
[116,] -0.411028381 -0.90852190  -3.52641943  1.598940129 -0.516388614
[117,] -0.533756717  1.09532411  -0.88668492 -1.491040358  0.735452080
[118,] -1.980191898  1.88182486  -2.70821992 -0.018012907  0.951254003
[119,]  1.484721941 -2.72138813  -2.99899864  3.714298943 -1.517649519
[120,] -0.279801031  0.15004612   0.78295515 -2.799122401  0.529306273
[121,]  2.521325745 -1.35081784   2.96996259  6.224844877 -0.655644812
[122,] -1.480463796  0.50282676  -1.32456803 -1.239108858 -0.886213725
[123,] -1.624486450  1.94913686   2.37968703  2.420097545 -1.428893752
[124,] -1.527492091  0.97270068  -2.92029912  0.152333844 -0.004997683
[125,]  0.110434607  0.81278610   3.24195170 -1.064361658  1.410746492
[126,] -0.080000347  0.42826225   2.59413550 -0.459459342 -0.586792140
[127,]  0.547175594 -2.01856994  -2.89988873 -1.832675271 -0.825523035
[128,] -0.220517419  0.82742692  -3.81056790  3.203059592  3.124891282
[129,] -0.440891654  2.52287285   5.39249625  1.078463945 -0.135110820
[130,] -2.090212047  1.47588635  -1.14286569  0.364415762 -1.159887965
[131,]  1.850219452 -3.52137692  -3.48196465 -3.311741555 -0.621943492
[132,]  1.812685259 -3.66820240   0.13990832  0.178667514 -3.709772974
[133,]  2.149510053 -1.61372123  -1.36428389 -0.080034562  3.544327896
[134,] -3.512844529  1.92659074  -5.61335705 -2.810951778  0.590229368
[135,] -2.896442106 -0.16840446  -5.03397850 -4.010771531 -3.729821885
[136,] -1.462333934 -1.63124024 -10.53648443 -3.664277060  1.420516356
[137,]  0.123193012  0.62010426   2.22876695  5.535365497 -1.178103868
[138,] -0.447352794 -0.49550143  -2.34934094  0.043305837 -1.050683271
[139,]  0.777457961 -0.18369536   5.21940396  0.677642015 -2.921129496
[140,] -2.312645218  3.31992302   2.23428037 -0.430973044  0.627576312
[141,] -1.500505811  1.34191676  -1.39205461  4.276350043 -1.388592031
[142,] -1.644217751  0.89374531   1.85432683 -9.325877489  0.029215124
[143,]  2.492114502 -2.71347132   1.15043127  0.913041660 -0.202423836
[144,]  0.408491669 -0.45222392   2.49349757 -2.531882415 -0.583093674
[145,]  0.182001420 -0.09272287  -1.43622392  1.943633243  0.425441117
[146,] -0.200408115 -2.52057517  -2.66381884 -5.411873939 -2.069164138
[147,]  0.035500446 -0.74051771  -3.92860362 -1.140613560  1.611409944
[148,] -0.646451266  0.88598255   3.01446603 -0.261463123 -2.195667977
[149,] -0.604713208  0.28230107  -0.81855013 -1.644805266  0.613821271
[150,] -2.107599687  1.90104058   1.88421609 -2.502571513 -2.060945188
[151,] -0.445518203 -0.55285880   0.38645170 -4.375456386 -1.244496527
[152,]  0.611360026 -0.83432704   3.36008137 -0.192024893 -1.369100762
[153,]  2.187110654 -0.20694617   1.38949758  4.113188578  2.756647523
[154,] -0.966812387  2.28392503   4.66521209 -4.916234095  0.270545511
[155,]  3.352008963 -3.09523778   1.19336681  6.771299365 -0.196799459
[156,]  0.518269302 -0.94897508  -1.79771941 -3.346219475  1.478154285
[157,]  1.987866782 -1.24563138  -0.06903570  5.355418317  1.277002150
[158,]  0.646448796 -0.09152440  -1.87568735 -3.543165816  3.585933350
[159,] -1.753567835  0.70240524  -2.84150293 -4.589372444  0.232967254
[160,] -2.315575731  2.61993301  -1.49042411  2.540972803  0.118385287
[161,] -0.821325531  0.01187968   0.75625448 -0.987762785 -2.599761936
[162,] -1.014568008 -0.88657609  -3.68394711 -3.833012602  0.040662540
[163,]  1.551996580  0.08601909  -0.53501162 -1.717490878  3.400728365
[164,]  0.639023853  1.58916639   3.29352178 -0.001243981  1.568302699
[165,]  2.443656175 -0.66648288   2.82710839  2.960306749  1.512583562
[166,]  0.359712946  1.98484048   5.35210133 -1.663816196  1.982797708
[167,]  0.949191424 -1.47522732  -0.97214673  1.467734123 -0.659636087
[168,]  0.128632745  1.11502559   2.66918688  0.632348742  1.766940746
[169,] -1.800453294  3.35248183   3.17538074  3.272052453 -0.566755837
[170,] -1.308135066  0.33724790  -3.16685871 -1.774634812 -1.854095021
[171,] -0.723454249  1.02899463  -4.46146335  0.014200403  1.943952229
[172,] -2.063358646  1.42117869   0.18854613  3.471249906 -2.257953478
[173,] -1.301690233 -0.85373707  -5.09426395 -2.655800117 -1.550760916
[174,]  1.020703164 -1.36402370   3.18561817 -1.036293935 -2.420110210
[175,]  2.813693451 -4.28902809  -2.98174492  0.387300463 -0.728057270
[176,] -0.420182732  1.04435321   4.88886138  0.716124067 -1.448363331
[177,]  0.905492114 -3.25620231  -2.29877884  1.293200856 -3.298427077
[178,]  1.100650877 -0.64417128  -4.98579805  0.378166681  3.709196577
[179,] -1.137135392  2.25819965   3.61087018 -1.888399094  0.245940323
[180,] -2.280841595  1.41123787   1.74442659 -3.949310420 -2.305304559
[181,] -3.453812367  3.07655760   3.03993861 -6.202806321 -2.289204640
[182,]  2.196224048 -1.93258726  -1.75273338  0.832002819  2.840487887
[183,] -0.466221996  0.93065594  -0.06278044 -0.286582429  0.387354535
[184,] -2.736953838  2.44698209  -3.34028763 -1.568613996  0.178060678
[185,]  0.338263316  0.27677515  -1.88495779  3.976503405  1.011432756
[186,]  0.352564211 -1.04960926   0.06498237 -4.971245841 -0.274408408
[187,]  0.353004909  1.30518613   3.36813818  2.097370887  1.687837945
[188,] -1.244044162  1.39750194   2.70351108 -4.474848748  0.081778980
[189,] -1.700211152  0.44036192   2.15526818 -1.917432605 -4.003434265
[190,] -2.035177750  2.80155670   1.73064581 -1.214388917 -0.342158093
[191,] -5.547532550  3.93837340  -0.89920337 -7.335106519 -2.717992031
[192,] -1.460221692 -0.88501486  -5.87681497 -2.942708166  0.161341351
[193,]  0.961722529  1.38991107   8.80286438  2.616002913 -2.557853005
[194,]  2.214347745 -3.30445066  -1.22762469  1.522488858 -1.473894369
[195,]  1.620065716 -0.24054816   2.64295364  4.218583696  0.185096720
[196,] -1.227408842  0.88577361   0.26581336 -2.368281471 -0.881839493
[197,]  0.155338667 -0.14250861  -0.04645994  1.851947399 -0.037051356
[198,] -0.459061858  0.10462473  -1.87390984 -6.411140822  1.400857892
[199,] -0.694520689 -0.75712013  -7.36435276 -2.112017736  2.686898461
[200,] -3.344892271  3.47399781   2.79600716 -7.833607529 -0.478585229
[201,] -0.624971473 -0.32298286  -0.85635111 -3.924491443 -1.753538802
[202,] -0.884240085  4.08358830   8.52976885  2.728276595 -0.711998265
[203,]  0.700727008  0.15711997   0.61410121  2.551085992  0.827746481
[204,]  0.154278809 -1.79824642  -1.61683584 -5.037611373  0.374662705
[205,] -1.553180327  0.06141386   0.02890125 -4.136029963 -2.306099645
[206,] -3.181219859 -0.10972461  -8.20897666 -7.181658517 -0.258450399
[207,] -0.935133061  1.64282905   1.38589823 -0.729840238 -0.939128474
[208,]  1.831114475 -3.18325489  -4.13411906 -3.052935685  1.611732248
[209,]  1.144287310 -1.18933032   0.25302956 -2.663265708  0.772068708
[210,] -1.192231085  2.89415632   2.80890942 -1.706775755  0.919187866
[211,] -2.127404572  2.92488162   3.99033240 -6.393980384 -0.489896206
[212,]  1.145706132 -3.05915541  -4.10092831  0.698566290 -1.634464284
[213,]  0.750871800 -0.19491301   2.21466881  1.186712509  0.056557752
[214,] -3.629641646  0.34568417  -7.70226572 -4.141671149 -1.450793015
[215,]  1.277073903 -0.47998170   2.44622180 -2.375967798  1.399450367
[216,] -1.323042253  2.70358618   6.13059942 -4.514457224  0.078674836
[217,] -1.135750942  2.59192130  -2.10956157  0.602795480  4.253393005
[218,] -3.368382702  2.75990269  -2.67294345  1.068057641 -1.064839655
[219,] -1.988724767  0.75419550  -6.53140106  0.568113529  1.095115740
[220,]  1.042723619  0.19362201   2.42654374 -0.379203170  1.471769839
[221,]  0.820984114  0.83863596   6.04199664 -2.950220617  0.560736353
[222,] -0.210110178  1.71668452   0.44095658  1.227485429  1.827329030
[223,] -1.898618493  0.58316976   0.89084458 -5.425560570 -2.469987647
[224,] -1.620885336  2.92562800  -0.17190273 -2.266670565  3.181762083
[225,]  0.610960272  0.24091255   4.74621105 -1.569507610 -1.107369937
[226,] -3.324280691  3.95035733  -2.84756708 -2.478605530  1.822246870
[227,]  1.756425567 -2.31421852   1.97047380  1.608573183 -1.981915765
[228,]  1.085629522 -0.86739460   1.14967813 -0.851353677  1.213899980
[229,]  0.323756244  0.32831042   0.51457663  0.242940047  0.756571048
[230,] -0.311368031 -2.01731011  -3.51802420  0.200516501 -3.112481140
[231,] -1.518342585  1.44484379   2.13177564 -2.986985935 -0.783918987
[232,] -0.391705239  0.95189901  -3.92140869  0.431564688  3.921098644
[233,]  0.721321652 -0.16985848   0.26390365  4.392385471  1.083101715
[234,] -1.402181909  2.16169081   0.34730757  0.490046231  1.681774948
[235,]  2.797610502 -3.42982768  -0.99541882  2.240535190  1.952869078
[236,]  1.318374734 -0.94690250  -1.46075166 -1.716701396  2.807917815
[237,] -0.061978116 -0.48865435  -2.71922956  1.658286057 -0.208860255
[238,] -0.890303261 -1.70362355  -4.48914722 -0.030959319 -1.903352757
[239,]  1.386722376 -1.58175522   0.25129130  3.572145386 -1.291378785
[240,] -1.248373856  0.12209949  -7.40771738  3.517288952  1.783639576
[241,]  1.781306979 -1.89436762   1.42316317  2.100995962 -1.019021475
[242,]  0.867286598 -0.62990603   3.39713229 -3.880467978 -0.960963954
[243,]  0.701634129 -1.95931361  -5.74527247  4.010412586  0.733770740
[244,] -0.134160594  1.93498642   2.25789212 -0.663664345  3.136978786
[245,]  1.117007661  0.29218005   2.81089264  1.214930094  0.237181962
[246,] -0.163571270  1.93487838   1.48168037 -0.729958627  2.467306500
[247,]  1.603204798 -0.73166534   0.45776690  1.914726717  1.787368387
[248,]  0.627389803  0.08311801   1.92324503  2.658739210 -0.128082588
[249,] -0.292158526  0.14471305  -3.09897158 -1.574686221  2.229693399
[250,]  0.328585858 -1.10220297  -3.24398541 -0.829486390  1.412927069
[251,]  1.926458310 -0.84355496   3.46890080 -0.439768296  0.549839717
[252,] -0.725891219  0.40191246   2.98260968 -2.044653413 -3.133427408
[253,] -0.607710450 -2.53413954  -5.23505164 -4.637132882 -2.274741628
[254,] -2.103698405  2.45940378   3.02075799  1.675357997 -3.414551325
[255,] -0.225586982 -0.74327401  -1.10489255 -2.423700186 -1.133981234
[256,] -1.604960926  1.36813407  -1.60813907 -0.949498150  0.709507901
[257,]  1.436250841 -2.83379540  -2.91200010  0.867792631 -0.735860864
[258,]  1.164890750 -2.08355821  -3.23762097  5.661142963 -0.661255696
[259,]  1.988364179 -0.77004509   5.15399151 -0.903078216  0.167115604
[260,] -1.225337766  1.88887393   1.74264731  0.200001622  1.176704936
[261,]  0.446192467 -2.94805584  -1.43018066  2.470910491 -3.684637601
[262,]  2.689912114 -3.55287803  -0.85687441  1.478310197 -1.916540828
[263,]  0.497087041  0.54601295   5.93671045  4.739477965 -3.166752723
[264,] -1.922235722  3.61427560   4.12208927 -3.997272387  0.870646609
[265,]  0.479593685  1.58293686  -1.62042309  1.642360320  3.652031839
[266,] -1.556813174 -0.12538057  -8.30717223 -0.696003898  2.028122085
[267,] -0.108715879 -0.52979050  -2.50209847 -0.267681968 -0.572181294
[268,] -0.637718044 -0.54873407  -2.18622631  0.853850723 -1.294362187
[269,]  0.841023435  1.02531000   8.07091569 -1.049451519 -1.317437270
[270,] -1.285653245  0.45332510  -2.70923676 -0.217927181  0.135499640
[271,]  1.873753743 -1.04406539   1.96500080  7.457488606 -1.005369102
[272,]  0.609731558  0.28139867  -0.61275710 -0.166049408  2.733017433
[273,] -0.085750174  1.57430475   0.42435688 -1.776067221  4.190441367
[274,]  1.995328491 -2.74723881  -0.90621214  2.655604905 -0.636786698
[275,] -1.311980016 -0.11403795  -2.79788257  0.288676841 -2.523783377
[276,]  1.157722347 -1.58713396   1.32378126 -2.308566633 -0.488762765
[277,] -0.680674484 -0.53362250   0.61458752 -1.950861031 -3.011816539
[278,] -1.508111302  1.84634875  -0.24850290 -1.388872437 -0.708279462
[279,]  0.917503289  0.96166425   6.45625313 -0.182669522 -0.220772529
[280,] -0.131286923 -0.65849683  -1.66142904  1.015546067 -1.419135638
[281,] -2.443279508  2.66289820   1.28109032  0.364107106 -1.592415823
[282,]  2.077899985 -0.76467584   1.47480264  0.643675012  1.893380333
[283,] -0.677435882 -0.29153531  -3.72835302 -6.037722023  2.001394917
[284,] -1.279819023  1.88379607   3.05194091 -0.490761649 -0.965105154
[285,]  5.098147669 -3.04861785   2.83630296  4.004104008  1.592949264
[286,] -1.903442479  2.67486798   0.53053863 -1.888524436  0.533943337
[287,] -0.633666489  1.45793600   0.45360214 -0.400256970  2.784971434
[288,]  1.617040194 -0.61473643   0.72557184  2.590827271  0.195715452
[289,] -0.010099896  0.70009949  -2.04898449 -2.372586626  2.940754443
[290,] -0.758466253 -0.48994911  -6.68373824 -0.728579617  1.152924537
[291,]  1.235268927 -2.30048119  -2.03212533  2.378610230 -0.342972200
[292,] -1.234680132  1.87490211   7.83511682 -3.137706948 -3.077840850
[293,] -1.065001551 -0.54736858  -4.00568004 -3.120151894 -0.375687936
[294,] -0.088914755 -1.49473951  -0.62687055 -0.829804933 -2.209448012
[295,] -2.452828385  2.87682621   2.79190494 -4.384403451  0.447660634
[296,] -1.815351822  1.10199581   3.05352825 -4.649083891 -2.726953307
[297,]  1.807925994 -0.82115147   2.74251540 -0.188802930  1.020885383
[298,] -0.304032193 -0.16499358  -2.83130623  1.465999505  0.225503227
[299,]  0.595156556  0.27531047   1.74968004  5.562974983 -1.218959658
[300,] -0.763819064 -0.61622846 -11.71098900  1.589104858  3.932059637
[301,]  0.639933505 -1.81503418  -2.43195844 -0.217595465 -1.126743674
[302,]  3.155992792 -4.57037450  -3.64536760  4.577310069  0.175771257
[303,] -1.008786047  1.85252057   2.41666541  2.494205033 -1.644322533
[304,]  3.518387518 -3.99715383  -2.17088824  4.668459463  0.864170284
[305,]  1.717473997 -0.83654832   2.82867799 -2.553167247  2.378354565
[306,] -0.619120218  0.33686585   1.87208476  1.272511313 -1.727945095
[307,] -1.327377273  0.86678874  -0.47689028 -3.685773952 -0.528349320
[308,] -0.065950168  1.27758478   1.09711040 -0.961355781  1.694910998
[309,]  2.225957982 -2.16242976   0.81520148  1.791493691 -0.655169707
[310,]  2.372180630 -2.08577892   1.07245185  6.944274274 -0.690476707
[311,] -2.130678842  0.23813717  -7.51433918  2.291649218  0.400852748
[312,]  2.055920153 -1.92132979   4.74415103  1.870492910 -2.619274650
[313,] -1.943762615  3.77706214   9.63962012 -2.527886084 -2.678707584
[314,]  1.238412229 -0.68566472   2.55744451 -4.252015758  1.573804912
[315,]  2.646871703 -1.40081743   1.11923652  1.509227382  3.663564547
[316,] -3.486481935  0.88476557  -9.58250491 -3.637342623 -0.216403930
[317,] -1.602333107  2.78259843   2.78994948  0.990263816  0.313001420
[318,] -0.307297235 -0.76359911   3.91262819 -2.436084599 -3.771644110
[319,]  0.288983224  0.46790521   1.57282948 -1.754741300  1.440563446
[320,] -2.421072687 -0.55206787  -4.56546659 -2.178445148 -2.468911057
[321,] -1.034854908  1.33252193  -2.08407344 -3.341912773  2.444401857
[322,]  1.633508075 -2.35602794  -1.44990418  2.626797324  0.111481807
[323,]  0.751325653  0.31340754   2.75955008  3.420124310  0.278456093
[324,]  0.206698627  1.24130440   6.19058401 -0.936110397 -0.398753881
[325,] -3.999313478  4.23136601   4.23354049 -1.109452465 -3.766558150
[326,] -1.221379278  1.26541092   3.35560689  0.792217212 -1.856982287
[327,]  0.821707720 -1.85461571   0.04879872 -1.448323303 -0.693378083
[328,] -0.457003201 -0.09517645   0.05509922  1.716085229 -0.994111704
[329,]  1.518681942 -2.16611196   0.03196824  0.402142846 -2.149190593
[330,] -0.078764424  0.27447215   1.56343127 -3.223689459  0.228276401
[331,]  0.980184655  0.78893354   3.79765664  3.312416250  0.445732298
[332,]  2.175787136 -1.59825511   2.27583867  2.675454583 -0.941815234
[333,] -0.114824474 -1.46088070  -4.01256181 -4.266632911  0.596618046
[334,] -2.921921643  3.18307789  -0.10599158 -6.069361405  0.610689254
[335,] -1.712972554  3.80925754   4.86194068  3.218113121 -0.781627330
[336,]  0.619001087  0.41338944   4.95658515 -1.864141925  0.515941006
[337,] -0.606500971  2.25167419  11.16270929 -1.375347895 -2.775000999
[338,]  2.217958423 -0.41597303   3.85393115  9.174158953 -1.050081050
[339,]  2.085405179 -2.77380391  -2.10605650  0.517492202  0.652220023
[340,]  4.726543562 -2.23943913   5.78876139  3.278022792  2.123194126
[341,] -2.193801620  2.65936214   0.22722917  2.884829879 -0.057843555
[342,]  1.255253026 -2.62868053  -1.92930055 -5.822755593  0.629723738
[343,]  0.592399016 -1.40224298  -3.47257230  3.027561170 -0.716042834
[344,]  0.119966099 -1.96907792  -5.54392885  0.759160936 -1.262958907
[345,] -0.807795329  1.82916917   1.37180367 -0.700631781  0.612713211
[346,]  0.341164978 -0.22263240   1.28607058  3.159470306 -1.695398467
[347,] -0.467760081  1.37992781  -0.94029216 -1.174450403  2.502551226
[348,] -3.621849483  3.93868838   2.67555854  2.924786944 -3.018185572
[349,] -0.913433944  0.62292682  -5.38362835 -0.850070228  2.900332115
[350,]  2.369180780 -0.85399411  -1.56552181  1.395627624  4.604564518
[351,] -1.141166310  0.95922259  -1.20961032  2.810004557 -1.511761224
[352,] -1.957407586  2.16001640   0.98674034  0.057197504 -1.254935414
[353,] -0.059016915 -0.16286272   0.90454263  6.127529440 -3.587639624
[354,]  0.808748491 -4.21643192  -8.72608317 -1.279902761 -0.427942708
[355,] -0.309413314  1.58825665  -0.49292967 -1.210027705  2.167031756
[356,] -0.768731216 -0.36588248  -4.31732159 -0.609272753  1.072888543
[357,]  0.349265394 -0.63556690   3.03726666 -3.261147769 -1.440456428
[358,]  2.806627413 -2.84371830   1.26787649  2.411274187 -0.074980811
[359,]  0.205507415 -1.52870237  -2.46121902  0.215091466 -0.797082997
[360,]  1.149334108 -0.35483037   3.55963929  1.520816293 -1.541252796
[361,] -0.155231927  1.60073649   5.24594146 -5.101487678  0.566700250
[362,]  0.231085596  0.58465030  -0.31466202  0.133151575  2.456692048
[363,]  3.642889860 -1.24894240   6.77847665  2.612198913  0.060178000
[364,]  0.774405977  2.68802212   8.15859749  4.250771280  0.261355750
[365,] -1.842675006  0.96892505  -7.17223221  0.229415332  2.089404412
[366,]  1.798663460 -2.20271984  -0.40030975 -3.556785981  0.346707086
[367,] -3.198417808  2.28570286  -0.38243932 -4.793175982 -1.654825708
[368,]  3.356017638 -2.35434104   2.22901307 -0.224784910  3.168800151
[369,] -3.553630052  4.31139430  -1.78588198 -1.002743066  1.227130059
[370,] -0.053067836 -0.19655825   0.36167390 -0.389082192  0.186775247
[371,] -2.368857135  1.78347438  -2.77362299 -3.875264111  1.109180794
[372,]  1.904740742 -0.85812158   5.94417551  5.310516904 -2.106634047
[373,] -3.040036703  3.99715013   3.41579992 -5.886436531  0.258341193
[374,] -0.106209707  0.44482084  -2.47023872 -2.133275165  4.163105678
[375,] -1.033846218  0.61666593  -2.99052465  5.670501210  0.650497560
[376,]  0.622611719 -0.32746819  -1.11740659 -3.374015641  2.497470897
[377,]  1.317136510  1.04065530   5.07221361  1.845771103  3.061267426
[378,] -1.304341112  0.34187656   0.91246585 -3.000075358 -3.844585775
[379,] -0.291800494  1.67054416   5.56087556 -0.243651127 -0.751876164
[380,] -0.007734198  0.70998738  -1.00341784 -0.767996485  1.194602777
[381,] -0.456955104 -0.02919313   3.31263972 -0.290289319 -3.290770359
[382,]  0.881149914 -0.08303452   6.65092611  0.307703104 -1.776583847
[383,]  0.557504127 -0.42963564  -2.87069730  5.210829241  1.158852013
[384,]  2.264710077 -2.51315957   1.13151312 -0.421917337 -0.481671154
[385,] -1.653540298  3.16512800   4.51837513 -6.796621649  0.567640704
[386,] -1.506421789  2.40131247   0.50289595 -5.357770945  3.222042011
[387,]  0.375983176  1.09372189   3.08551851 -2.546430501  1.389371108
[388,] -1.841582929  1.30092823   5.11999852 -5.904575086 -3.193939475
[389,] -0.648636743  1.22265140  -1.34515387  1.266765545  2.162066617
[390,] -2.551332163  2.51976416  -0.49036230 -4.012865729 -0.635121968
[391,] -1.000019881  1.42347471   3.56327428  2.905751797 -1.897738426
[392,] -0.655573167 -0.57785985  -1.78788252 -2.973397562 -0.203445633
[393,] -1.984303232  0.44053236  -6.05663671  0.851814514 -1.305492880
[394,] -1.914193717  3.22628735   1.51544383 -2.402711887  1.437765858
[395,] -1.075368511 -0.57129014  -0.51760917 -4.396319609 -1.566183356
[396,] -3.259144372  2.89437278  -5.10500864  0.773307666  0.813983812
[397,]  3.251940596 -3.04554432  -2.40066236  5.767109852  2.916763413
[398,]  0.307578711 -0.64129445  -5.87574163 -0.753198949  2.493047187
[399,]  1.033377609  0.63334602   1.72819503  0.960919556  3.077539059
[400,]  1.270341353 -2.39520487  -0.27432219 -2.138915024 -2.486122999</code></pre>
</div>
<div class="sourceCode cell-code" id="cb85" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb85-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Correlation of decisionmaker random slopes</span></span>
<span id="cb85-2">Omega <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">cor</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">matrix</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(P<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(P<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)), P<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, P))</span>
<span id="cb85-3"></span>
<span id="cb85-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Scale of decisionmaker random slopes</span></span>
<span id="cb85-5">tau <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">abs</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(P, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>))</span>
<span id="cb85-6"></span>
<span id="cb85-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Covariance matrix of decisionmaker random slopes</span></span>
<span id="cb85-8">Sigma <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">diag</span>(tau) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> Omega <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">diag</span>(tau)</span>
<span id="cb85-9"></span>
<span id="cb85-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Centers of random slopes</span></span>
<span id="cb85-11">beta <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(P)</span>
<span id="cb85-12"></span>
<span id="cb85-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Individual slopes</span></span>
<span id="cb85-14">beta_i <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> MASS<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mvrnorm</span>(I, beta, Sigma) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> W <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">t</span>(Gamma)</span>
<span id="cb85-15"></span>
<span id="cb85-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Again, quick plot to sanity check</span></span>
<span id="cb85-17"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">plot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.data.frame</span>(beta_i))</span></code></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/doing_maxdiff_better/better_maxdiff_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb86" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb86-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create X -- let's make this a dummy matrix</span></span>
<span id="cb86-2">X <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">matrix</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sample</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, Tasks<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>I<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>J<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>P, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">replace =</span> T), Tasks<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>I<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>J, P)</span>
<span id="cb86-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Each of the rows in this matrix correspond to a choice presented to a given individual</span></span>
<span id="cb86-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># in a given task</span></span>
<span id="cb86-5"></span>
<span id="cb86-6">indexes <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">crossing</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">individual =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>I, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>Tasks, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">option =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>J) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb86-7">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">row =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">n</span>())</span>
<span id="cb86-8"></span>
<span id="cb86-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Write a Gumbel random number generator using inverse CDF trick</span></span>
<span id="cb86-10">rgumbel <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>(n, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">mu =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">beta =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) mu <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> beta <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">runif</span>(n)))</span>
<span id="cb86-11"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mean</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rgumbel</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e6</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5779057</code></pre>
</div>
<div class="sourceCode cell-code" id="cb88" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb88-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Ok, now we need to simulate choices. Each person in each task compares each </span></span>
<span id="cb88-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># choice according to X*beta_i + epsilon, where epsilon is gumbel distributed. </span></span>
<span id="cb88-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># They return their rankings. </span></span>
<span id="cb88-4"></span>
<span id="cb88-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Modify the ranked_options data frame to include all necessary information</span></span>
<span id="cb88-6">ranked_options <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> indexes <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb88-7">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(individual, task) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb88-8">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(</span>
<span id="cb88-9">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">fixed_utility =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.numeric</span>(X[row,] <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.numeric</span>(beta_i[<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">first</span>(individual),])),</span>
<span id="cb88-10">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">plus_gumbel_error =</span> fixed_utility <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rgumbel</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">n</span>()),</span>
<span id="cb88-11">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">true_rank =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rank</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>plus_gumbel_error),</span>
<span id="cb88-12">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">true_order =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">order</span>(true_rank),</span>
<span id="cb88-13">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">observed_order =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">case_when</span>(</span>
<span id="cb88-14">      true_order <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> J <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> J,  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Worst choice</span></span>
<span id="cb88-15">      true_order <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Best choice</span></span>
<span id="cb88-16">      <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Tie all middle orders</span></span>
<span id="cb88-17">    ),</span>
<span id="cb88-18">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">best_choice =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.numeric</span>(true_order <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb88-19">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">worst_choice =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.numeric</span>(true_order <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> J)</span>
<span id="cb88-20">  )</span>
<span id="cb88-21"></span>
<span id="cb88-22">tt <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> ranked_options <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb88-23">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(individual, task) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb88-24">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarise</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">start =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">min</span>(row), </span>
<span id="cb88-25">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">end =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">max</span>(row)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb88-26">  ungroup <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb88-27">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task_number =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">n</span>())</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`summarise()` has grouped output by 'individual'. You can override using the
`.groups` argument.</code></pre>
</div>
</div>
</details>
<p>Ok, onto new material; we’ll generate a “permutation matrix” of all the ways the <img src="https://latex.codecogs.com/png.latex?J%20-%202"> middle options can occur, which we’ll iterate over to calculate all the possible likelihoods.</p>
<p>For example, here with <img src="https://latex.codecogs.com/png.latex?J%20=%205">, the permutation matrix will have the following 6 permutations:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb90" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb90-1">n_tied <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> J <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>  </span>
<span id="cb90-2">permutations <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">permn</span>(n_tied)</span>
<span id="cb90-3">permutation_matrix <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">do.call</span>(rbind, permutations)</span>
<span id="cb90-4">n_permutations <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(permutation_matrix)</span>
<span id="cb90-5"></span>
<span id="cb90-6"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">print</span>(permutation_matrix)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    1    3    2
[3,]    3    1    2
[4,]    3    2    1
[5,]    2    3    1
[6,]    2    1    3</code></pre>
</div>
</div>
<p>Calculating these outside R is convenient in two senses. First, it’s more efficient- Stan isn’t really designed or equipped to work with permutations like this as they get more involved. Second, it can be helpful for intuition to mentally step through this matrix of permutations and consider the calculation needed for each entry, imagining it growing rapidly with the number of unknown middle options.</p>
<p>Here’s the Stan input data and model:</p>
<details>
<summary>
<strong>ROL with unknown middle preferences</strong>
</summary>
<div class="cell">
<div class="sourceCode cell-code" id="cb92" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb92-1">stan_data_rol_ties <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(</span>
<span id="cb92-2">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">N =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(X),</span>
<span id="cb92-3">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">T =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(tt),</span>
<span id="cb92-4">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">I =</span> I, </span>
<span id="cb92-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">P =</span> P, </span>
<span id="cb92-6">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">P2 =</span> P2, </span>
<span id="cb92-7">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">K =</span> J, </span>
<span id="cb92-8">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">rank_order =</span> ranked_options<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>observed_order,</span>
<span id="cb92-9">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X =</span> X, </span>
<span id="cb92-10">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X2 =</span> W, </span>
<span id="cb92-11">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>task_number, </span>
<span id="cb92-12">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task_individual =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>individual,</span>
<span id="cb92-13">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">start =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>start, </span>
<span id="cb92-14">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">end =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>end,</span>
<span id="cb92-15">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">n_tied =</span> n_tied,</span>
<span id="cb92-16">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">permutations =</span> permutation_matrix,</span>
<span id="cb92-17">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">n_permutations =</span> n_permutations</span>
<span id="cb92-18">)</span>
<span id="cb92-19"></span>
<span id="cb92-20">rol_with_unknown_middle_options <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb92-21"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">functions {</span></span>
<span id="cb92-22"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  real rank_logit_ties_lpmf(int[] y, vector delta, int[,] permutations, int n_permutations, int n_tied) {</span></span>
<span id="cb92-23"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    int K = rows(delta);</span></span>
<span id="cb92-24"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    vector[K] sorted_delta = delta[y];</span></span>
<span id="cb92-25"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    real out = 0;</span></span>
<span id="cb92-26"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    </span></span>
<span id="cb92-27"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // Handle known best</span></span>
<span id="cb92-28"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    out += sorted_delta[1] - log_sum_exp(sorted_delta);</span></span>
<span id="cb92-29"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    </span></span>
<span id="cb92-30"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // Compute normalizing factor once</span></span>
<span id="cb92-31"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    real normalizing_factor = log_sum_exp(sorted_delta[2:K]);</span></span>
<span id="cb92-32"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    </span></span>
<span id="cb92-33"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // Handle tied middle ranks</span></span>
<span id="cb92-34"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    real perm_sum = 0;</span></span>
<span id="cb92-35"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    for (p in 1:n_permutations) {</span></span>
<span id="cb92-36"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      real perm_ll = 0;</span></span>
<span id="cb92-37"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      for (i in 1:n_tied) {</span></span>
<span id="cb92-38"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        int idx = permutations[p, i];</span></span>
<span id="cb92-39"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        perm_ll += sorted_delta[1+idx];</span></span>
<span id="cb92-40"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      }</span></span>
<span id="cb92-41"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      perm_sum += exp(perm_ll - n_tied * normalizing_factor);</span></span>
<span id="cb92-42"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    }</span></span>
<span id="cb92-43"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    out += log(perm_sum / n_permutations);</span></span>
<span id="cb92-44"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    </span></span>
<span id="cb92-45"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    return out;</span></span>
<span id="cb92-46"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  }</span></span>
<span id="cb92-47"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb92-48"></span>
<span id="cb92-49"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">data {</span></span>
<span id="cb92-50"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int N; // number of rows</span></span>
<span id="cb92-51"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int T; // number of individual-choice sets/task combinations</span></span>
<span id="cb92-52"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int I; // number of Individuals</span></span>
<span id="cb92-53"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int P; // number of covariates that vary by choice</span></span>
<span id="cb92-54"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int P2; // number of covariates that vary by individual</span></span>
<span id="cb92-55"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int K; // number of choices</span></span>
<span id="cb92-56"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span></span>
<span id="cb92-57"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int rank_order[N]; // The vector describing the index (within each task) of the first, second, third, ... choices. </span></span>
<span id="cb92-58"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[N, P] X; // choice attributes</span></span>
<span id="cb92-59"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[I, P2] X2; // individual attributes</span></span>
<span id="cb92-60"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span></span>
<span id="cb92-61"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int task[T]; // index for tasks</span></span>
<span id="cb92-62"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int task_individual[T]; // index for individual</span></span>
<span id="cb92-63"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int start[T]; // the starting observation for each task</span></span>
<span id="cb92-64"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int end[T]; // the ending observation for each task</span></span>
<span id="cb92-65"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span></span>
<span id="cb92-66"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int&lt;lower=2&gt; n_tied;</span></span>
<span id="cb92-67"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int&lt;lower=1&gt; n_permutations;</span></span>
<span id="cb92-68"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int permutations[n_permutations, n_tied];</span></span>
<span id="cb92-69"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb92-70"></span>
<span id="cb92-71"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">parameters {</span></span>
<span id="cb92-72"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  vector[P] beta; // hypermeans of the part-worths</span></span>
<span id="cb92-73"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[P, P2] Gamma; // coefficient matrix on individual attributes</span></span>
<span id="cb92-74"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  vector&lt;lower = 0&gt;[P] tau; // diagonal of the part-worth covariance matrix</span></span>
<span id="cb92-75"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[I, P] z; // individual random effects (unscaled)</span></span>
<span id="cb92-76"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  cholesky_factor_corr[P] L_Omega; // the cholesky factor of the correlation matrix of tastes/part-worths</span></span>
<span id="cb92-77"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb92-78"></span>
<span id="cb92-79"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">transformed parameters {</span></span>
<span id="cb92-80"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[I, P] beta_individual = rep_matrix(beta', I) + X2 * Gamma' + z * diag_pre_multiply(tau, L_Omega);</span></span>
<span id="cb92-81"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb92-82"></span>
<span id="cb92-83"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">model {</span></span>
<span id="cb92-84"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  tau ~ normal(0, .5);</span></span>
<span id="cb92-85"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  beta ~ normal(0, 1);</span></span>
<span id="cb92-86"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  to_vector(z) ~ normal(0, 1);</span></span>
<span id="cb92-87"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  L_Omega ~ lkj_corr_cholesky(4);</span></span>
<span id="cb92-88"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  to_vector(Gamma) ~ normal(0, 1);</span></span>
<span id="cb92-89"></span>
<span id="cb92-90"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  for(t in 1:T) {</span></span>
<span id="cb92-91"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    vector[K] utilities;</span></span>
<span id="cb92-92"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    utilities = X[start[t]:end[t]] * beta_individual[task_individual[t]]';</span></span>
<span id="cb92-93"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    rank_order[start[t]:end[t]] ~ rank_logit_ties(utilities, permutations, n_permutations, n_tied);</span></span>
<span id="cb92-94"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  }</span></span>
<span id="cb92-95"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb92-96"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span></code></pre></div>
</div>
</details>
<p>Summing up all the possible middle options should be fairly transparent, but if you’re interested in the <code>log_sum_exp</code> tricks I’m using to speed this up, see this footnote<sup>12</sup>.</p>
<p>To reinforce the intuition here, this is choosing to handles unknown middle options with maximum precision given we know very little about them, at (usually) great computational cost. Before we compare this to MaxDiff, and see if it is meaningfully more accurate with the same data, I want to also introduce an approximation to this full marginal likelihood that won’t choke even on big choice sets.</p>
</section>
<section id="j-is-pretty-bad-is-there-an-approximation" class="level2">
<h2 class="anchored" data-anchor-id="j-is-pretty-bad-is-there-an-approximation">J! is pretty bad; is there an approximation?</h2>
<p>As we’ve seen, calculating all possible orderings for unknown middle preferences can quickly become computationally intensive as the number of choices increases. Fortunately, there’s a clever approximation that can help us out. This approximation comes to us via a deep connection between discrete choice models and survival analysis. To gesture at the analogy here- tied times in survival models function like our unknown middle options, and the survival analysis literature has decades of experience handling this type of tie given how common they are in survival data<sup>13</sup>. We’ll look at my favorite of these, the Efron approximation, originally developed for tied times in Cox proportional-hazards models.</p>
<p>Mathematically, the Efron approximation for our rank-ordered logit with unknown middle preferences looks like this:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL(%5Cmu)%20=%20%5Cprod_%7Bi=1%7D%5EN%20%5Cfrac%7B%5Cexp(%5Cmu_%7Bij%7D)%7D%7B%5Csum_%7Bj%20%5Cin%20C_i%7D%20%5Cexp(%5Cmu_%7Bij%7D)%7D%20%5Ccdot%20%5Cprod_%7Bk=2%7D%5E%7BJ-2%7D%20%5Cfrac%7B%5Cexp(%5Cmu_%7Bik%7D)%7D%7B%5Csum_%7Bj%20%5Cin%20C_i%7D%20%5Cexp(%5Cmu_%7Bij%7D)%20-%20%5Cfrac%7Bk-1%7D%7BJ-2%7D%20%5Csum_%7Bl%20%5Cin%20M_i%7D%20%5Cexp(%5Cmu_%7Bil%7D)%7D%0A"></p>
<p>Where <img src="https://latex.codecogs.com/png.latex?C_i"> is the full choice set for individual <img src="https://latex.codecogs.com/png.latex?i">, <img src="https://latex.codecogs.com/png.latex?M_i"> is the subset of <img src="https://latex.codecogs.com/png.latex?C_i"> containing the tied (unranked) middle alternatives, and <img src="https://latex.codecogs.com/png.latex?J"> is the total number of alternatives in <img src="https://latex.codecogs.com/png.latex?C_i">. I recognize this is super dense, and that grokking it will require a bit of reading that’s outside the scope of this post; here’s a footnote with some helpful references<sup>14</sup>.</p>
<p>Intuitively, this approximation attempts to more continuously approximate the full marginal likelihood we just discussed without calculating every possible ranking. It does this by adjusting the denominator of the likelihood to account for the tied middle alternatives in a stepwise fashion.</p>
<p>Here’s the code implementation, which to me gives a lot more intuition than the equation above:</p>
<details>
<summary>
<strong>ROL with Efron approximation of unknown middle options</strong>
</summary>
<div class="cell">
<div class="sourceCode cell-code" id="cb93" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb93-1">efron_model <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb93-2"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">functions {</span></span>
<span id="cb93-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  real rank_logit_efron_lpmf(int[] y, vector delta, int n_tied) {</span></span>
<span id="cb93-4"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    int K = rows(delta);</span></span>
<span id="cb93-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    vector[K] sorted_delta = delta[y];</span></span>
<span id="cb93-6"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    real out = 0;</span></span>
<span id="cb93-7"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    </span></span>
<span id="cb93-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // Handle known best</span></span>
<span id="cb93-9"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    out += sorted_delta[1] - log_sum_exp(sorted_delta);</span></span>
<span id="cb93-10"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    </span></span>
<span id="cb93-11"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // Handle tied middle ranks using Efron approximation</span></span>
<span id="cb93-12"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    real tied_sum = log_sum_exp(sorted_delta[2:(K-1)]);  // Changed to log_sum_exp</span></span>
<span id="cb93-13"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    real normalizing_factor = log_sum_exp(sorted_delta[2:K]);</span></span>
<span id="cb93-14"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    for (i in 1:n_tied) {</span></span>
<span id="cb93-15"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      real d = (i - 1.0) / n_tied;</span></span>
<span id="cb93-16"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      real adjusted_factor = log_diff_exp(normalizing_factor, log(d) + tied_sum);</span></span>
<span id="cb93-17"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      out += sorted_delta[1+i] - adjusted_factor;</span></span>
<span id="cb93-18"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    }</span></span>
<span id="cb93-19"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    </span></span>
<span id="cb93-20"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // Handle known worst</span></span>
<span id="cb93-21"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    // out += sorted_delta[K] - log_sum_exp(sorted_delta[2:K]);</span></span>
<span id="cb93-22"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    </span></span>
<span id="cb93-23"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    return out;</span></span>
<span id="cb93-24"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  }</span></span>
<span id="cb93-25"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb93-26"></span>
<span id="cb93-27"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">data {</span></span>
<span id="cb93-28"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int N; // number of rows</span></span>
<span id="cb93-29"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int T; // number of individual-choice sets/task combinations</span></span>
<span id="cb93-30"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int I; // number of Individuals</span></span>
<span id="cb93-31"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int P; // number of covariates that vary by choice</span></span>
<span id="cb93-32"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int P2; // number of covariates that vary by individual</span></span>
<span id="cb93-33"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int K; // number of choices</span></span>
<span id="cb93-34"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span></span>
<span id="cb93-35"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int rank_order[N]; // The vector describing the index (within each task) of the first, second, third, ... choices. </span></span>
<span id="cb93-36"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[N, P] X; // choice attributes</span></span>
<span id="cb93-37"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[I, P2] X2; // individual attributes</span></span>
<span id="cb93-38"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span></span>
<span id="cb93-39"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int task[T]; // index for tasks</span></span>
<span id="cb93-40"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int task_individual[T]; // index for individual</span></span>
<span id="cb93-41"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int start[T]; // the starting observation for each task</span></span>
<span id="cb93-42"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int end[T]; // the ending observation for each task</span></span>
<span id="cb93-43"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  </span></span>
<span id="cb93-44"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  int&lt;lower=2&gt; n_tied; // number of tied ranks</span></span>
<span id="cb93-45"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb93-46"></span>
<span id="cb93-47"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">parameters {</span></span>
<span id="cb93-48"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  vector[P] beta; // hypermeans of the part-worths</span></span>
<span id="cb93-49"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[P, P2] Gamma; // coefficient matrix on individual attributes</span></span>
<span id="cb93-50"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  vector&lt;lower = 0&gt;[P] tau; // diagonal of the part-worth covariance matrix</span></span>
<span id="cb93-51"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[I, P] z; // individual random effects (unscaled)</span></span>
<span id="cb93-52"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  cholesky_factor_corr[P] L_Omega; // the cholesky factor of the correlation matrix of tastes/part-worths</span></span>
<span id="cb93-53"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb93-54"></span>
<span id="cb93-55"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">transformed parameters {</span></span>
<span id="cb93-56"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  matrix[I, P] beta_individual = rep_matrix(beta', I) + X2 * Gamma' + z * diag_pre_multiply(tau, L_Omega);</span></span>
<span id="cb93-57"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb93-58"></span>
<span id="cb93-59"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">model {</span></span>
<span id="cb93-60"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  tau ~ normal(0, .5);</span></span>
<span id="cb93-61"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  beta ~ normal(0, 1);</span></span>
<span id="cb93-62"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  to_vector(z) ~ normal(0, 1);</span></span>
<span id="cb93-63"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  L_Omega ~ lkj_corr_cholesky(4);</span></span>
<span id="cb93-64"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  to_vector(Gamma) ~ normal(0, 1);</span></span>
<span id="cb93-65"></span>
<span id="cb93-66"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  for(t in 1:T) {</span></span>
<span id="cb93-67"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    vector[K] utilities;</span></span>
<span id="cb93-68"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    utilities = X[start[t]:end[t]] * beta_individual[task_individual[t]]';</span></span>
<span id="cb93-69"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    rank_order[start[t]:end[t]] ~ rank_logit_efron(utilities, n_tied);</span></span>
<span id="cb93-70"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  }</span></span>
<span id="cb93-71"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb93-72"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span></code></pre></div>
</div>
</details>
<p>The beauty of the Efron approximation is that it works remarkably well in practice, especially compared to alternatives. As discussed for example in <a href="https://pubmed.ncbi.nlm.nih.gov/9333345/">Hertz-Picciotto and Rockhill (1997)</a>, the Efron method tends to outperform other approximations, particularly when dealing with moderate to heavy ties (read: unknown middle rankings).</p>
</section>
<section id="evaluating-rol-with-unknown-middle-preferences-against-maxdiff-and-rol" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-rol-with-unknown-middle-preferences-against-maxdiff-and-rol">Evaluating ROL with Unknown Middle Preferences against MaxDiff and ROL</h2>
<p>So now we’ve got 3 candidate models for best/worst choice data- MaxDiff, ROL calculating the full marginal likelihood for the middle options, and ROL using the Efron approximation. I’ll also add in the ROL without ties to give a sense of the information lost when just collecting best and worst choices, though of course my main goal is compare the quality of the 3 models on best/worst choice data.</p>
<p>The next chunk just sets up the comparison between the four models on data with <img src="https://latex.codecogs.com/png.latex?J%20=%205">, so feel free to jump to the summary plot after it.</p>
<details>
<summary>
<strong>4 way Comparison Setup</strong>
</summary>
<div class="cell">
<div class="sourceCode cell-code" id="cb94" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb94-1">compiled_maxdiff_model <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stan_model</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model_code =</span> best_worst)</span>
<span id="cb94-2">compiled_rol_ties_model <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stan_model</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model_code =</span> rol_with_unknown_middle_options)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in readLines(file.path(makevar_files)): incomplete final line found on
'C:\Users\timma\Documents/.R/Makevars.win'</code></pre>
</div>
<div class="sourceCode cell-code" id="cb96" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb96-1">compiled_efron_model <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stan_model</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model_code =</span> efron_model)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in readLines(file.path(makevar_files)): incomplete final line found on
'C:\Users\timma\Documents/.R/Makevars.win'</code></pre>
</div>
<div class="sourceCode cell-code" id="cb98" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb98-1">compiled_rol_model <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stan_model</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model_code =</span> ranked)</span>
<span id="cb98-2"></span>
<span id="cb98-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Prepare data for Stan models</span></span>
<span id="cb98-4">stan_data_maxdiff <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(</span>
<span id="cb98-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">N =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(X),</span>
<span id="cb98-6">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">T =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(tt),</span>
<span id="cb98-7">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">I =</span> I,</span>
<span id="cb98-8">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">P =</span> P,</span>
<span id="cb98-9">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">P2 =</span> P2,</span>
<span id="cb98-10">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">K =</span> J,</span>
<span id="cb98-11">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">choice =</span> ranked_options<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>best_choice,</span>
<span id="cb98-12">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">worst_choice =</span> ranked_options<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>worst_choice,</span>
<span id="cb98-13">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X =</span> X, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X2 =</span> W, </span>
<span id="cb98-14">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>task_number, </span>
<span id="cb98-15">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task_individual =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>individual,</span>
<span id="cb98-16">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">start =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>start, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">end =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>end</span>
<span id="cb98-17">)</span>
<span id="cb98-18"></span>
<span id="cb98-19">stan_data_efron <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(</span>
<span id="cb98-20">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">N =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(X),</span>
<span id="cb98-21">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">T =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(tt),</span>
<span id="cb98-22">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">I =</span> I,</span>
<span id="cb98-23">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">P =</span> P,</span>
<span id="cb98-24">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">P2 =</span> P2,</span>
<span id="cb98-25">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">K =</span> J,</span>
<span id="cb98-26">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">rank_order =</span> ranked_options<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>observed_order,</span>
<span id="cb98-27">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X =</span> X, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X2 =</span> W, </span>
<span id="cb98-28">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>task_number, </span>
<span id="cb98-29">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task_individual =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>individual,</span>
<span id="cb98-30">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">start =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>start, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">end =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>end,</span>
<span id="cb98-31">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">n_tied =</span> J <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># number of tied ranks</span></span>
<span id="cb98-32">)</span>
<span id="cb98-33"></span>
<span id="cb98-34">stan_data_rol <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(</span>
<span id="cb98-35">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">N =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(X),</span>
<span id="cb98-36">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">T =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(tt),</span>
<span id="cb98-37">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">I =</span> I,</span>
<span id="cb98-38">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">P =</span> P,</span>
<span id="cb98-39">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">P2 =</span> P2,</span>
<span id="cb98-40">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">K =</span> J,</span>
<span id="cb98-41">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">rank_order =</span> ranked_options<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>true_order,  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Use true_order for full ranking</span></span>
<span id="cb98-42">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X =</span> X, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X2 =</span> W, </span>
<span id="cb98-43">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>task_number, </span>
<span id="cb98-44">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">task_individual =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>individual,</span>
<span id="cb98-45">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">start =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>start, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">end =</span> tt<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>end</span>
<span id="cb98-46">)</span>
<span id="cb98-47"></span>
<span id="cb98-48">fit_maxdiff <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sampling</span>(compiled_maxdiff_model, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> stan_data_maxdiff, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">iter =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">800</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
</div>
<div class="sourceCode cell-code" id="cb101" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb101-1">fit_rol_ties <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sampling</span>(compiled_rol_ties_model, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> stan_data_rol_ties, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">iter =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">800</span>)</span>
<span id="cb101-2">fit_efron <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sampling</span>(compiled_efron_model, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> stan_data_efron, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">iter =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">800</span>)</span>
<span id="cb101-3">fit_rol <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sampling</span>(compiled_rol_model, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> stan_data_rol, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">iter =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">800</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#bulk-ess

Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
</div>
<div class="sourceCode cell-code" id="cb103" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb103-1">normalize_utilities <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>(utilities) {</span>
<span id="cb103-2">  (utilities <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mean</span>(utilities)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sd</span>(utilities)</span>
<span id="cb103-3">}</span>
<span id="cb103-4"></span>
<span id="cb103-5">process_results <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>(fit, dataset_name) {</span>
<span id="cb103-6">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.data.frame</span>(fit, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">pars =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"beta_individual"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb103-7">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">gather</span>(Parameter, Value) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb103-8">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(</span>
<span id="cb103-9">      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">individual =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">str_extract</span>(Parameter, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"[0-9]+(?=,)"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">parse_number</span>(),</span>
<span id="cb103-10">      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">column =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">str_extract</span>(Parameter, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">",[0-9]{1,2}"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">parse_number</span>()</span>
<span id="cb103-11">    ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb103-12">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(individual) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb103-13">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Value_normalized =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normalize_utilities</span>(Value)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb103-14">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(individual, column) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb103-15">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarise</span>(</span>
<span id="cb103-16">      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">median =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">median</span>(Value_normalized),</span>
<span id="cb103-17">      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">lower =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">quantile</span>(Value_normalized, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>),</span>
<span id="cb103-18">      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">upper =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">quantile</span>(Value_normalized, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.95</span>),</span>
<span id="cb103-19">      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">.groups =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'drop'</span></span>
<span id="cb103-20">    ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb103-21">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ungroup</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb103-22">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(</span>
<span id="cb103-23">      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">True_value =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.numeric</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">t</span>(beta_i)),</span>
<span id="cb103-24">      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Dataset =</span> dataset_name</span>
<span id="cb103-25">    ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb103-26">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(individual) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb103-27">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">True_value_normalized =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normalize_utilities</span>(True_value)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb103-28">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ungroup</span>()</span>
<span id="cb103-29">}</span>
<span id="cb103-30"></span>
<span id="cb103-31">results_maxdiff <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">process_results</span>(fit_maxdiff, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MaxDiff"</span>)</span>
<span id="cb103-32">results_rol_ties <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">process_results</span>(fit_rol_ties, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ROL with ties"</span>)</span>
<span id="cb103-33">results_efron <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">process_results</span>(fit_efron, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Efron Approximation"</span>)</span>
<span id="cb103-34">results_rol <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">process_results</span>(fit_rol, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ROL (no ties)"</span>)</span>
<span id="cb103-35"></span>
<span id="cb103-36">all_results <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bind_rows</span>(results_maxdiff, results_rol_ties, results_efron, results_rol)</span>
<span id="cb103-37"></span>
<span id="cb103-38"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Calculate and print RMSE; couldn't be bothered to find one in a package</span></span>
<span id="cb103-39">calculate_rmse <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>(data) {</span>
<span id="cb103-40">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sqrt</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mean</span>((data<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>True_value_normalized <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> data<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>median)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">^</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))</span>
<span id="cb103-41">}</span>
<span id="cb103-42"></span>
<span id="cb103-43">rmse_comparison <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(</span>
<span id="cb103-44">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Model =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MaxDiff"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ROL with ties"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Efron Approximation"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ROL (no ties)"</span>),</span>
<span id="cb103-45">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">RMSE =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(</span>
<span id="cb103-46">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">calculate_rmse</span>(results_maxdiff),</span>
<span id="cb103-47">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">calculate_rmse</span>(results_rol_ties),</span>
<span id="cb103-48">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">calculate_rmse</span>(results_efron),</span>
<span id="cb103-49">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">calculate_rmse</span>(results_rol)</span>
<span id="cb103-50">  )</span>
<span id="cb103-51">) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb103-52">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">arrange</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">desc</span>(RMSE))</span>
<span id="cb103-53"></span>
<span id="cb103-54"></span>
<span id="cb103-55">fourway_comparison_plot <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(all_results, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> True_value_normalized, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> median, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> Dataset)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb103-56">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_linerange</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ymin =</span> lower, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ymax =</span> upper), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">alpha =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb103-57">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_point</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">alpha =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb103-58">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_abline</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">intercept =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">slope =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">linetype =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"dashed"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb103-59">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">labs</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Normalized True Utility"</span>,</span>
<span id="cb103-60">       <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Normalized Estimated Utility"</span>,</span>
<span id="cb103-61">       <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">title =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Comparison of Model Performances"</span>,</span>
<span id="cb103-62">       <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">subtitle =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"With 90% credibility intervals"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb103-63">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">scale_color_manual</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">values =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MaxDiff"</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"blue"</span>, </span>
<span id="cb103-64">                                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ROL with ties"</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"green"</span>,</span>
<span id="cb103-65">                                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Efron Approximation"</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"red"</span>,</span>
<span id="cb103-66">                                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ROL (no ties)"</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"purple"</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb103-67">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme_minimal</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb103-68">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">facet_wrap</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>Dataset, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ncol =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb103-69">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">legend.position =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"none"</span>,</span>
<span id="cb103-70">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">plot.title =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">element_text</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">hjust =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>),</span>
<span id="cb103-71">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">plot.subtitle =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">element_text</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">hjust =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>))</span></code></pre></div>
</div>
</details>
<p>Before we show the comparison plot, here’s a minor technical caveat. The natural scale of the utilities the two unknown handling models produce is different than that of the MaxDiff and ROL without ties models. To plot them usefully against one another, I thus have to normalize the utilities. This makes it a bit harder to compare these plots to the last few, but unfortunately its necessary for a valid comparison.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb104" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb104-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">print</span>(fourway_comparison_plot)</span></code></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/doing_maxdiff_better/better_maxdiff_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>So roughly from the plot: ROL without unknowns &gt; ROL with full marginal likelihood for unknowns ~ ROL with Efron Approximation for Unknowns &gt; MaxDiff. Let’s sanity check that by looking at RMSEs.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb105" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb105-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">print</span>(rmse_comparison)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                Model      RMSE
1             MaxDiff 0.4370070
2       ROL with ties 0.2507489
3 Efron Approximation 0.2499740
4       ROL (no ties) 0.1625324</code></pre>
</div>
</div>
<p>I flagged this briefly above, but to say it again: <img src="https://latex.codecogs.com/png.latex?J%20=%205"> has few unknown middle options, meaning these are fairly favorable conditions for the Efron approximation. We’d expect higher <img src="https://latex.codecogs.com/png.latex?J"> to widen the gap between the full marginal likelihood and the Efron approximation.</p>
<p>So practically, the takeaway here is that you can do better than modeling with MaxDiff even if you designed your discrete choice experiment to just collect best/worst choice data. If you have the time and/or your number of alternatives isn’t super large, you can do better with the full marginal likelihood model. If you want a workable and fast approximation, the Efron model still looks meaningfully better than MaxDiff.</p>
<p>Beyond giving a better option for existing MaxDiff data, I’ll admit I had a secondary motive in wanting to explore these “unknown middle choices” style models. I’ve sometimes seen MaxDiff discussed as an imperfect modeling strategy that becomes desirable because it gives access to the best/worst choice data collection strategy.</p>
<p>To flesh out that claim, the idea is that MaxDiff has known flaws that aren’t ideal, but a study only needing to collect best and worst options hits a sweet spot of respondent experience, simplicity to setup, and statistical efficiency (via being able to repeat many choice sets). Better a slightly worse model on significantly more informative-dense data, since design trumps analysis.</p>
<p>This is probably the type of argument for MaxDiff I’m most sympathetic to, in the sense that I do genuinely do believe choice experiments place a lot of burden on respondents in a way that may bias results. Perhaps selecting only the best and worst option meaningfully helps with that. Further, I might even find it plausible that if you need to compare a larger than ideal set of options, there might be value in asking a bunch of best/worst of 10 choice sets instead of trying to do tons of best of threes to enable the full ROL with ties option I mentioned above. Even if you think this way though, the ROL with unknown middle options models I’ve just shown are likely the more accurate model for the design you find significant value in. <strong>You can’t justify using MaxDiff via the designs it enables.</strong></p>
</section>
</section>
<section id="conclusions" class="level1">
<h1>Conclusions</h1>
<p>In summary:</p>
<ol type="1">
<li>There are some significant psychological and mathematical limitations entailed in using a MaxDiff model of discrete choice.</li>
<li>These issues with the MaxDiff model are more than theoretical, and can cause significant harm to the quality of model predictions.</li>
<li>If you have control over the design of the choice experiment, you can do much better than MaxDiff by designing the choice sets in a connected graph, and then using a rank-ordered logit. This gets most of the benefit of ROLs more generally, while preserving the relatively low burden respondent experience of just choosing a best and worst choice.</li>
<li>Even if you don’t have control over the design of the experiment, you can still outperform MaxDiff using ROL models that correctly handle unknown middle choices. Crucially, the existence of these models negates the argument for using MaxDiff based solely on its compatibility with best/worst choice designs.</li>
</ol>
<p>In writing this post, I’ve aimed to both give intuition and concrete examples of models that I would’ve benefited from when I was initially trying to find alternatives to the MaxDiff formulation of discrete choice. My hope is gathering both the objections to MaxDiff and alternatives in one place can make it a bit easier for others to pivot to the better models available.</p>
<p>There are plenty of ways you can extend what I’ve done in this post, and I don’t want to claim this is in any way some definitive model for discrete choice. Just to name a few- one could improve the priors in the models, design much more efficient graphs of options, compare the efficiency of connected graphs to ROL with unknowns for large choice sets via simulation, or even implement these models in something like <a href="https://blackjax-devs.github.io/blackjax/">Blackjax</a> with a Gibbs sampler to make them blazing fast. My primary goal here has been to show that the MaxDiff likelihood isn’t a great foundation for discrete choice.</p>
<p>Thanks for reading!</p>
</section>
<section id="bluesky-comments" class="level1">
<h1>Bluesky Comments</h1>
<div class="bluesky-comments">

</div>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>The smiley face is threatening: it’s feels like it’s there to make sure MaxDiff stays down :-D↩︎</p></li>
<li id="fn2"><p>If you want more of an in-depth introduction, here are some resources I’d recommend. First, Jim Savage’s <a href="https://khakieconomics.github.io/2019/03/17/Logit-models-of-discrete-choice.html">blog post series</a> is great, and does a fantastic job of explaining how our assumptions about choicemaking map onto the math. I also benefited from reading Glasgow’s <a href="https://www.cambridge.org/core/elements/abs/interpreting-discrete-choice-models/676EC2C0F19A3B6D932E12CC612872BC">Interpreting Discrete Choice Models</a>, which builds up the basics of choice models a bit more slowly. Finally, if you want something that goes much more in detail, Kenneth Train’s <a href="https://eml.berkeley.edu/books/choice2.html">Discrete Choice Models with Simulation</a> goes into great mathematical detail about the models.↩︎</p></li>
<li id="fn3"><p>I won’t rehash more subtle implications of this basic model here; see the footnote above for references. Instead, I’ll mostly pull up assumptions as they become relevant for discussing MaxDiff.↩︎</p></li>
<li id="fn4"><p>Other options are possible, but less common, and would take us too far afield, so I’ll skip explaining the choice of Gumbel versus other distributions here.↩︎</p></li>
<li id="fn5"><p>President Biden is great, it’s just hard to compete with the comically malignant and incompetent by being solid and stabilizing.↩︎</p></li>
<li id="fn6"><p>Unlike the blog post about left-handed kangaroos drinking fosters I’m working on, this post was not primarily motivated by the urge to ask DALL-E to create this.↩︎</p></li>
<li id="fn7"><p>I won’t re-explain the full details of the model here, since the original source already does a fantastic job. If you haven’t already read the original post series, it’s well worth taking the time to do so now, as there are a lot of moving pieces here- setting sensible priors for this model, incorporating covariates logically, and setting this up to run efficiently in Stan.↩︎</p></li>
<li id="fn8"><p>I will note that if you’re following along at home, sampling variability means that for some realizations of this DGP + these models at much lower N, MaxDiff performs just slightly better. But “barely better in a fraction of runs” isn’t really a good outcome from gathering twice as much data.↩︎</p></li>
<li id="fn9"><p>A great place to start reading about efficient design of discrete choice experiments is the literature review section of <a href="https://www.jstatsoft.org/article/view/v096i03">Traets et al, 2020</a>. For more on the desirable properties of designs like the BIBD I actually chose, Raghavarao’s 1988 book <strong>Constructions and Combinatorial Problems in Design of Experiments</strong> is wonderful. Part of what I find appealing about this broader literature is that results not just from the ’80s, but stretching back a century or more remain relevant for building good choice experiments. While the last decade or so has seen numerous new techniques pop up that squeeze out additional bits of accuracy in the right conditions, the core challenge remains interwoven with the combinatorics of experimental design that were recognized by the earliest statisticians.↩︎</p></li>
<li id="fn10"><p>Just for fun, notice what happens if there’s only 1 middle term. It’s your friendly neighborhood complete rank-ordered logit, since there’s only 1 way 1 option can occur!↩︎</p></li>
<li id="fn11"><p>To be transparent, note that J = 5 isn’t a totally neutral comparison ground between the full marginal likelihood model and the Efron approximation. The Efron will get somewhat worse as the number of middle options grows.↩︎</p></li>
<li id="fn12"><p>Woo optimization gang! Ok, so the idea here is similar to this block in the ROL without unknowns model, where at each choice we normalize by subtracting out <code>out += tmp[i] - log_sum_exp(tmp[i:])</code>, but it has to function differently given the unknowns. More specifically, the right normalizing factor for all the middle <img src="https://latex.codecogs.com/png.latex?J%20-2"> options is <code>log_sum_exp(sorted_delta[2:K])</code> for each possible <em>permutation</em>, not each possible middle option. So as the number of permutations grows, we can save a bunch of compute mostly through pre-computing the normalizing factor, but also by doing <code>n_tied * normalizing_factor</code> instead of subtracting <code>n_tied</code> times per permutation.↩︎</p></li>
<li id="fn13"><p>See the Allison/Christakis paper above for a more fleshed out version of this comparison. It’s very elegant, but I don’t think there’s a way to do that analogy justice without a massive detour in this post. Similarly, I’d love to walk through the reasoning that leads to the Efron approximation and how it compares to competitors, but this post is already pretty long.↩︎</p></li>
<li id="fn14"><p>I really appreciate <a href="https://myweb.uiowa.edu/pbreheny/7210/f15/notes/11-5.pdf">this set of slides</a> for pumping intuitions about different approaches to approximating the middle options. These <a href="https://public.websites.umich.edu/~yili/lect4notes.pdf">course notes</a> are also super helpful, especially if you want to see the Cox proportional hazards model discussed more in depth before you try to understand the tie handling in those terms. Finally of course, the <a href="https://www.jstor.org/stable/2286217">original paper</a> is great, but very very dense.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2024,
  author = {Timm, Andy},
  title = {Better Discrete Choice Modeling Through the Rank Ordered
    Logit},
  date = {2024-08-06},
  url = {https://andytimm.github.io/posts/doing_maxdiff_better/better_maxdiff.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2024" class="csl-entry quarto-appendix-citeas">
Timm, Andy. 2024. <span>“Better Discrete Choice Modeling Through the
Rank Ordered Logit.”</span> August 6, 2024. <a href="https://andytimm.github.io/posts/doing_maxdiff_better/better_maxdiff.html">https://andytimm.github.io/posts/doing_maxdiff_better/better_maxdiff.html</a>.
</div></div></section></div> ]]></description>
  <category>discrete choice</category>
  <category>Bayes</category>
  <guid>https://andytimm.github.io/posts/doing_maxdiff_better/better_maxdiff.html</guid>
  <pubDate>Tue, 06 Aug 2024 04:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/doing_maxdiff_better/imgs/gumby_distribution.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Kangaroos, Foster’s, and E.T. Jaynes</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/kangaroos_fosters_jaynes/kangaroos_fosters_jaynes.html</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/kangaroos_fosters_jaynes/imgs/fosters_kangaroo.webp" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Ahead of starting a new gig next week (wooo!), I was reviewing a bit about multilevel models, and came across this fascinating footnote in Richard McElreath’s <a href="https://xcelab.net/rm/statistical-rethinking/">Statistical Rethinking</a>:</p>
<blockquote class="blockquote">
<p>See Jaynes (1986) for an entertaining example concerning the beer preferences of left-handed kangaroos. There is an updated <a href="https://bayes.wustl.edu/etj/articles/cmonkeys.pdf">1996 version</a> of this paper available online.</p>
</blockquote>
<p>I’m not sure what it is with Bayesians and wild, fantastic adventures in the footnotes<sup>1</sup>, but obviously I couldn’t not chase this one down. Also, the paper didn’t immediately pop up when I googled “beer preferences of left-handed kangaroos”, so hopefully this post gets a few people where they’re going.</p>
<p>I mostly started this post to have a reason to DALL-E a (left-handed) kangaroo drinking a Foster’s, but the more serious content of the post discusses different conceptions of maximum entropy, and how useful the idea is in building prior distributions for Bayesian inference.</p>
<section id="why-on-earth-are-we-talking-about-kangroos-beer-preferences" class="level1">
<h1>Why on earth are we talking about kangroos’ beer preferences?</h1>
<p>To be clear, we’re actually talking about the <em>joint distribution of beer preferences of kangaroos and their handedness</em>. Why?</p>
<p>In the time-honored tradition of taking a joke and continuing to run with it until it’s funny, E.T. Jaynes is extending an example due to Steve Gull<sup>2</sup> to discuss the properties of <a href="https://en.wikipedia.org/wiki/Principle_of_maximum_entropy">maximum entropy</a> priors:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/kangaroos_fosters_jaynes/imgs/problem_setup.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Extending analysis of this scenario then provides a tutorial in prior specification, improving the prior slowly to ensure it reflects all we know about the example.</p>
<p>For example, the first piece of intuitive prior knowledge that Jaynes shares is that kangaroos are (for most practical purposes) indivisible<sup>3</sup>. Constraining acceptable priors to ones with integer solutions tweaks the problem and corresponding reasonable priors a bit.</p>
<p>From there, the article provides a nice example of a difficult Bayesian task: taking our priors seriously, and seeing how we feel about what they imply. For example, when taken to large N<sup>4</sup>, the simplest maximum entropy prior starts to become remarkably confident about the unknown proportions <img src="https://latex.codecogs.com/png.latex?p"> above. The question that arises is whether our intuition about <img src="https://latex.codecogs.com/png.latex?p"> were poor, or whether we have hidden prior information to incorporate (i.e: do we know that kangaroos are likely to be related given they are drawn from the same genetic pool and environment)?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/kangaroos_fosters_jaynes/imgs/hidden_kangaroo_knowledge.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">As I often ask myself…</figcaption>
</figure>
</div>
<p>With the ensuing sermon<sup>5</sup>, Jaynes makes the case that maximum entropy remains a useful, logical tool for building priors. Interestingly, this argument has a different presentation of maxent than more recent pieces that place these priors alongside reference, Jeffreys, or invariance-based priors<sup>6</sup>. Jaynes instead lays out a presentation that feels more like a workflow, with maximum entropy itself providing a starting point.</p>
</section>
<section id="but-first-more-kangaroo-facts" class="level1">
<h1>But first, more kangaroo facts</h1>
<p>Just like our understanding of Bayesian inference has advanced since the 1980s, so has our understanding of kangaroo handedness.</p>
<p><a href="https://www.cell.com/current-biology/fulltext/S0960-9822(15)00617-X#articleInformation">Giljov et al.&nbsp;(2015)</a> study several members of the broader family of macropod<sup>7</sup> marsupials, and find that bipedal macropod marsupials show “population-level left-forelimb preference”, confirming Gull’s observations above.</p>
<p>This was studied by observing the natural behavior of the different species in a variety of common tasks, like bipedal/tripedal<sup>8</sup>/quadrupedal stances for feeding, self-grooming, and accepting a nice cold can of Foster’s<sup>9</sup>.</p>
<p>Ok, so Gull was correct they’re broadly left-handed. How accurate was his 65% observation? Here we need to be careful: there are a variety of macropod marsupials, which display different degrees of left-handedness<sup>10</sup>. As Jaynes notes, Gull was not particularly specific in his formulation of this problem:</p>
<blockquote class="blockquote">
<p>Although there are several species of kangaroos with size varying from man to mouse, we assume that Gull intended his problem to refer to the man sized species (who else could stand up at a bar and drink Foster’s?)</p>
</blockquote>
<p>This seems reasonable enough, but to split unnecessary hairs even further, there are two different types of “man sized” kangaroos in the study- <a href="https://en.wikipedia.org/wiki/Red_kangaroo">red kangaroos</a> and <a href="https://en.wikipedia.org/wiki/Eastern_grey_kangaroo">eastern grey kangaroos</a><sup>11</sup><sup>12</sup>. Digging into the <a href="https://www.cell.com/cms/10.1016/j.cub.2015.05.043/attachment/6b90006c-b1dc-4683-97dd-03dabd0fdebc/mmc1">appendix</a> to find the raw data since the main paper only reports statistical test results, 80% of red kangaroos are left-handed, and 68% of eastern grey kangaroos are<sup>13</sup>. Perhaps Gull was observing his kangaroos in the eastern part of Australia, which (unsurprisingly) is where eastern grey kangaroos can be found.</p>
<p>As much as we now stand on the shoulders of giants when it comes to Bayesian inference and kangaroo handedness, I wasn’t able to find any work on the beer preferences of kangaroos. That said, I see no reason to doubt Gull’s 65% estimate, so let’s move on to talking about priors.</p>
</section>
<section id="ok-some-actual-thoughts-about-priors" class="level1">
<h1>Ok, some actual thoughts about priors</h1>
<p>In grad school, I was lucky to take a Bayesian inference course from Ben Goodrich- he came down from Columbia to teach at NYU. I spent some time in his office hours over a few weeks asking questions about maximum entropy.</p>
<p>As I remember it, his position was that maximum entropy could be a useful concept for building some intuition about probability, but that it wasn’t a good way to land on priors. Richard McElreath’s take in Statistical Rethinking is similar, if a bit more enthusiastic on the intuition building point- chapter 10 develops the concept at length for this purpose, but not encouraging it as a tool for making priors.</p>
<p><personal initial="" thoughts=""></personal></p>
<p>&lt;How is what Jaynes saying a bit richer than that?&gt;</p>
<p><examples like="" the="" indivisible="" kangaroos=""></examples></p>
<p><is this="" still="" a="" useful="" perspective=""></is></p>
<p><outro encouragment="" to="" read="" the="" paper=""></outro></p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><a href="https://dansblog.netlify.app/">Dan Simpson’s</a> are particularly incredible.↩︎</p></li>
<li id="fn2"><p>Who I have to imagine Australians were thrilled with for this novel joke. I’m sure if the dropbear or ʇxǝʇ uʍop ǝpᴉsdn jokes existed back then this problem would have been about the preferred reading angles of drop bears or something instead.↩︎</p></li>
<li id="fn3"><p>If one did divide a kangaroo, would it still have a preference for Fosters? I suppose it would depend on the type of division, but I have to imagine most types of division would leave the kangaroo desiring something a bit stronger.↩︎</p></li>
<li id="fn4"><p>umber of kangaroos.↩︎</p></li>
<li id="fn5"><p>his wonderful term, not mine.↩︎</p></li>
<li id="fn6"><p>For example, how the <a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations#general-principles">Stan Prior Choice Recommendations</a> page, or <a href="https://www.mdpi.com/1099-4300/19/10/555">Gelman, Simpson, and Betancourt (2017)</a> which I’ll discuss later in the post.↩︎</p></li>
<li id="fn7"><p>See <a href="https://en.wikipedia.org/wiki/Macropodidae">Macropodidae</a>- basically all the cute, friend-shaped ones, including kangaroos, wallabies, sugar gliders, quokkas, and wallaroos (which I just learned of, and am happy to report exist).↩︎</p></li>
<li id="fn8"><p>Three limbs, not the tail, sadly.↩︎</p></li>
<li id="fn9"><p>Ok, ok fine. The authors restricted their attention to “natural, not artificially evoked, behaviors”, and that condition is what presumably ruled out handing any of them a Foster’s.↩︎</p></li>
<li id="fn10"><p>The motivating causal theory here is that handedness seems to be more common in primarily bipedal species, according the <a href="https://www.smithsonianmag.com/science-nature/kangaroos-are-lefties-and-can-teach-us-about-human-handedness-180955630/">lead author</a>. This connects this work to a larger, interesting debate on whether handedness is a uniquely human or primate trait.↩︎</p></li>
<li id="fn11"><p>There are also two other species referred to as kangaroos, the <a href="https://en.wikipedia.org/wiki/Western_grey_kangaroo">western grey kangaroo</a>, and the <a href="https://en.wikipedia.org/wiki/Antilopine_kangaroo">antilopine kangaroo</a>. Note that the term “kangaroo” is a <a href="https://en.wikipedia.org/wiki/Paraphyly">paraphyletic</a> grouping, and seems to be based on size, so, for example, I found some references to the antilopine kangaroo being referred to as a wallaby or wallaroo. I was thus unable to verify Jaynes’ claim of mouse-sized kangaroos, and presume he was thinking of other Macropodidae. Fortunately, neither of the other two species should undermine our confidence in the study’s ability to tell us about the man sized kangaroos capable of standing up at a bar and drinking: while both the eastern and red kangaroo can easily stand over a bar, with male heights often observed roughly around 6’7” and 5’11” respectively, the western grey kangaroo only reaches typical heights of 4’3” (unclear if this is males only, or all of them), and the antilopine kangaroo males only reaching 3’9”. If we instead restricted our attention to just the <a href="https://letmegooglethat.com/?q=typical+height+of+a+bar">typical height of a bar</a> however, and take a typical bar height to be 42”, we might have some further decisions to make, as both species would (for the tallest members) only narrowly be taller than the bar, and might not be able to comfortably drink at one. In addition, we would need to take into consideration the bartender’s willingness to serve such small patrons…↩︎</p></li>
<li id="fn12"><p>Second footnote to say all average male heights in the first footnote are just from the linked Wikipedia pages. I’m in a footnote for a footnote here, which in turn is referring to a paper I found to validate a comment in a 28 year old paper referencing a 41 year old anecdote. If you nitpick me on the roo heights with more precise information about the distribution of their heights, I’ll laugh at you, but I will update the text. If you argue whether we care about length of the roo (including tail), not height, I’ll just laugh at you.↩︎</p></li>
<li id="fn13"><p>I’m taking them at their word on whether each kangaroo is left-handed or not based on counting the number of left vs.&nbsp;right pawed observations of each action. You can find the relevant data and make your own conclusions in tables S3/S4 of the appendix, or use the summary of handedness by task I’ve included here.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div id="quarto-reuse" class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2024,
  author = {Timm, Andy},
  title = {Kangaroos, {Foster’s,} and {E.T.} {Jaynes}},
  date = {2024-02-06},
  url = {https://andytimm.github.io/posts/kangaroos_fosters_jaynes/kangaroos_fosters_jaynes.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2024" class="csl-entry quarto-appendix-citeas">
Timm, Andy. 2024. <span>“Kangaroos, Foster’s, and E.T. Jaynes.”</span>
February 6, 2024. <a href="https://andytimm.github.io/posts/kangaroos_fosters_jaynes/kangaroos_fosters_jaynes.html">https://andytimm.github.io/posts/kangaroos_fosters_jaynes/kangaroos_fosters_jaynes.html</a>.
</div></div></section></div> ]]></description>
  <category>priors</category>
  <category>stupid Bayesian stuff</category>
  <guid>https://andytimm.github.io/posts/kangaroos_fosters_jaynes/kangaroos_fosters_jaynes.html</guid>
  <pubDate>Tue, 06 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/kangaroos_fosters_jaynes/imgs/fosters_kangaroo.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Who Forms Voting Habits?</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Is Voting Habit Forming 2/voting_habits_heterogenity.html</link>
  <description><![CDATA[ 




<p>This is a draft of a paper I wrote in grad school on whether formation of voting habits exhibits any heterogeneity by race, party, or gender, using data from the Florida voter file. Putting it online since I’ve shown the draft to a few folks at this point and someone mentioned it’d be nice to have it publicly available.</p>
<p><strong>TL;DR:</strong> It doesn’t seem like there is any strong heterogeneity by race, party, or gender in voting habit formation, although there’s some evidence that non-affiliated voters might form habits less readily. This is encouraging as a Democrat; if our folks struggled to form voting habits easily, the task of expanding the electorate would be much harder. Due to the complexity of methods needed to study this problem, I have a considerable amount of uncertainty over both the true effect sizes of habit formation and the “causes of effects” - whether what’s happening is really habit<sup>1</sup>.</p>
<p>Happy to share replication materials if asked, just reach out (the data is too large for github).</p>
<hr>
<section id="abstract" class="level1">
<h1>Abstract</h1>
<p>In this paper, I provide a first test of whether voting habit formation exhibits heterogeneity by race, gender, or party affiliation. To do this, I leverage data from the 2018 Florida voter file, leveraging the fuzzy discontinuity in votes cast created in downstream elections by some young adults being “just 18” or just ineligible in a given upstream election. While prior work claims the instrument created by this discontinuity would be too weak to usefully study demographic heterogeneity, I show it is similarly precise to numerous estimates interpreted in prior work. From this analysis, two main findings emerge despite the considerable limitations of the instruments. First, there don’t appear to be major differences in CACEs by race or by gender in Florida compliers across different election types. Second, there is some suggestion that non-party affiliated compliers have slightly lower CACEs than their major party-affiliated peers in presidential election pairs, although the mechanism for this is unclear. The additional covariates also facilitate additional robustness checks compared to earlier work, and my results are robust to a wide variety of alternative specifications, although they are sensitive to shifts in bandwidth.</p>
</section>
<section id="motivation" class="level1">
<h1>Motivation</h1>
<p>Is voting habit forming? If so, for whom, and in what types of elections? Political scientists have long agreed that individual differences in voter turnout appear to persist over time, and that voting in prior elections is the most predictive characteristic of those who vote in future elections (e.g: Fraga, 2019; Hersh, 2015; Gerber et al., 2003; Rosenstone and Hansen, 1993; Verba &amp; Nie, 1972). There is also strong observational support for the idea that those who vote early on in life tend to continue on a path of voting more frequently than those who were absent in such early elections. For example, Franklin and Hobolt (2011) have found that there is strong variation in European voters’ subsequent turnout dependent on whether they first became eligible to vote in a European Parliament (lower turnout and salience) or national (higher turnout and salience) election. Both the consistency with which voting predicts voting and the meaningfully stronger voting trajectory of those who vote early in life heavily suggest a habit effect, but stop short of finding a causal link. As an example of a common alternative explanation, perhaps individuals who develop frequent voting habits simply have inherent interest in politics, which drives their involvement.</p>
<p>If we believe that there may be a habit effect on voting in an initial in future elections, how can we estimate a causal effect? Of course, we cannot use a randomized experiment: we cannot ethically randomize some citizens to vote, or more concerningly, randomize some citizens to not do so. The body of literature on this causal question is much smaller, and began to develop in earnest only 15 years ago. The main observational strategies used to estimate such causal effects involve instrumental variable approaches, utilizing randomized experiments in the “upstream” election which attempt to mobilize voters in the treatment group as an instrument to estimate the Complier Average Causal Effect (CACE) in subsequent “downstream” elections. For example, Bedolla and Michelson (2012), utilizing a series of experiments that aimed to improve turnout in minority voters in California, find overall that voting in the upstream election results in a 23-percentage point increase in probability to vote in subsequent elections for compliers. However, these designs are frequently limited by somewhat weak instruments and low overall sample sizes, as increasing turnout through campaign intervention is extremely difficult and expensive per voter reached, especially in non-white and younger populations (Gerber &amp; Green, 2012; Fraga, 2019).</p>
<p>The weakness of these instruments has prompted the development of another major identification strategy, using fuzzy regression discontinuity (FRD) designs, leveraging the fact that being just barely 18 or just too young to vote on the upstream election day should set voters on very different voting trajectories if such a habit effect exists. The pioneering paper in this vein is Meredith (2009), which leverages the full California voter file as a dataset. Meredith argues that while the turnout effect of being “just 18” in the upstream election is likely smaller than most campaign interventions, the opportunity to work with a total of 20 million voters generates many more compliers overall than small upstream experiments can reasonably could expect to. Stepping back, both of these estimation strategies only study and make causal inferences about narrow slices of the broader population we’d like to understand. In an ideal world, research would allow us to understand the habit formation of all citizens eligible to vote, and give us rich and actionable knowledge about any heterogeneity in habit formation by demographic or geographic characteristics. Instead, both designs have tended to focus on registered voters in states with richer data in their voter files. In the case of the upstream-experiment-as-IV studies, this is because the papers often leverage earlier papers in experimental political science. These papers usually prefer to target voters because the voter file provides a natural list of potential targets whose turnout can then be easily be tracked after the experiment (Gerber &amp; Green, 2012). Also, turnout effects of campaign-style interventions tend to be much higher for registered voters than the unregistered ineligible, likely due to both individual differences between voters and non-voters and the additional complications of needing to register as well as vote (Gerber et al, 2003). Thus, in order to use the upstream-experiment-as-IV paradigm to explore behavior of the broader eligible voter population, researchers would have to invest in extremely large scale and high treatment effect interventions in order to have a strong enough instrument for downstream estimation. Given sufficient investment, however, this strategy could feasibly allow for the estimation of CACEs for any subpopulation where a reasonable number of compliers can be created. The FRD design approach faces a slightly different set of problems with regards to potential populations of causal inference. First, the design unavoidably only can study voters- the voter file only tracks registered voters, and no equivalent file exists for the unregistered. Second, the CACE is only identified right at the cutoff- that is, the causal impact of being precisely old enough to vote on election day in the upstream election. There is no guarantee that we can extrapolate these estimates to the broader and more theoretically interesting population of all 18-year olds who are eligible to vote. Finally, in thinking about the problem of possible heterogeneous treatment effects, few states provide both the birthdate field needed as the cutoff and demographic information in their voter files, whereas it is possible to gather that information during many experimental treatments.</p>
<p>More recent work like Dinas (2012) and Coppock &amp; Green (2016) has been concerned with heterogeneity in habit forming across election types and states. Both papers find that habits tend to form most strongly across similar types of elections- for instance primary voting causing future primary voting. Coppock &amp; Green, using data from all 15 states whose voter files are sufficient for the FRD design, find significant variability in state habit formation, although they are uncertain of the mechanisms underlying this. Heterogeneity by demographic characteristics has been a longtime aspiration of this body of literature, which has been stymied by the weakness of both campaign and the “just eligible to vote” interventions as instruments. Coppock &amp; Green end their 2016 paper on the note that precise estimates of such heterogeneity will likely require new experiments of ambitious scale, and that this must be the future direction of voting habit research. While I agree that any fine-grained variation will likely require such an approach to explore, I will argue there is still some capacity to explore heterogeneous habit formation remaining in the voter file approach.</p>
<p>In this paper, I provide a first test of whether CACE estimates exhibit heterogeneity by a variety of demographic characteristics. To do this, I leverage data from the 2018 Florida voter file, in a fashion similar to Coppock and Green (2016). Unlike Coppock and Green, however, I retain and use 9 additional demographic covariates from the voter file concerning gender, party affiliation, and race. From this analysis, two main findings emerge despite the limitations of the instruments. First, there don’t appear to be major differences in CACEs by race or by gender in Florida compliers across different election types. Second, there is some suggestion that non-party affiliated (sometimes known as independent<sup>2</sup>) compliers have slightly lower CACEs than their major party-affiliated peers in presidential election pairs, although the mechanism for this could be either parties turning out their voters or features of the independent compliers themselves. The additional covariates also facilitate additional robustness checks compared to earlier work, and my results are robust to a wide variety of alternative specifications, although they are sensitive to shifts in bandwidth.</p>
</section>
<section id="data-and-descriptive-statistics" class="level1">
<h1>Data and Descriptive Statistics</h1>
<p>I work with the June 2018 release of the Florida voter file, a uniquely demographically rich voter file. States are required by the Help America Vote Act to make available to the public individual level data on every registered voter in their state, although states vary considerably in how much information about each voter they provide (Hersh, 2015). Given that Coppock &amp; Green (2016) were able to work with 15 states, it might be concerning that I am only working with one. The authors work with all states where the information needed to estimate the discontinuity are available, specifically birth date and voting eligibility (to rule out felons and other ineligible, formerly registered citizens). Of these 15 states, however, only Florida is one of the nine states that include mandatory, self-supplied racial information in their voter file. Thus, while other states could be used for examining heterogeneity across party identification or gender, only Florida allows us to look at all three groups of demographic variables. I chose to work with the 2018 data (giving results back to 2006) for two reasons: first, given the file only includes 10 years of historical voting data, this leaves more overlap with Coppock &amp; Green’s data gathered in 2013 for crosschecking results. Second, the 2010-2020 voting history data would at this point only include 2 presidential elections, whereas my selection includes 2008, 2012, and 2016, allowing for the estimation of 2 separate CACEs by pairs of presidential years.</p>
<p>There were two major steps in cleaning this data. First, I removed all voters who requested a public records exemption. These are voters who have a legal reason to have their information hidden from the public, for example domestic violence survivors or those who have restraining orders filed against others. Accepted exemptions still have their voting history listed, but all demographic and geographic information is lost, thus rendering the redacted data unhelpful for my purposes. I also checked for any seemingly impossible votes cast (for example, voting underage), but in this edition of the file no such problems appeared to occur compared to earlier editions of the file.</p>
<p>Second, I group the roughly 13 million Florida voters by birthdate cohort, calculating summaries of each cohort’s voting behavior and demographic characteristics. These are then filtered to the 1.9 million voters who turned 18 or almost did in the last 10 years. This grouping sidesteps the potential problems arising from the fact the voter file only includes people registered to vote. While there is not a complete list of just eligible and just ineligible 17 and 18-year olds, through this cohort grouping, we can work with the upstream/downstream votes cast by cohorts above and below the eligibility threshold instead. Unfortunately, this also adds some additional complexity to interpreting results, as most potential violations of the fuzzy regression discontinuity design assumptions would occur at the individual level. As noted earlier, this also narrows the population of causal inference: we can only reason about registered voter compliers, not the broader adult population. Unlike earlier work, I also compute the proportion of each race, gender, and party in each cohort, which both allow for examining heterogeneity and act as additional controls in estimation. A last question of data quality that might be concerning is a possibility of attrition, or of movement across states between election pairs. If, for instance, voters move out of Florida, but are not removed from the voter file, they would appear not to vote in subsequent elections, biasing the CACEs downward. Alternatively, voters might move into Florida but not correctly have their prior state’s voting history brought with them to the Florida file. Coppock &amp; Green explored this question extensively in their appendix, using a private vendor’s data on net inflows and outflows of voters between states. Their overall conclusion is that the net change in the Florida voting population is small, and that even their more extreme estimates of net change to the voter file would not bias the CACE estimates much. While I do not have access to their vendor’s data, and thus am not able to show that net inflow remained the same between 2013 and 2018, I can think of little reason Florida would experience a large change in it’s young adult voting population between those years due to movement between states. More broadly, this concern also applies to upstream-experiment-as-IV designs, as almost all of these use the voter file first as a list of people to target, and second as a way to check voting history (and thus experiment effect sizes) after the fact. Unfortunately, as Coppock &amp; Green show, the potential bias from this depends on a complicated interaction of whether inflow or outflow is higher, and the voting histories of those who leave. All of this makes it hard to predict if final CACE estimates are biased up or down in total, if such an effect were to exist.</p>
<p>Below in table 1 is a demographic profile of the sample after the filtering to those who turned 18 in the last 10 years. First, we can see that there are a large number of total votes cast, making it plausible that a sufficient number of compliers exist for somewhat precise estimates. It is fortunate that Florida is simultaneously the third most populous state in the United States, and has rich demographic data as well. As expected, presidential years have much higher turnout. I can combine the different year totals for a variety of different estimates: two presidential to presidential year estimates, 2 midterm to midterm year estimates, and 2 presidential to midterm year estimates. Each of these have theoretical value, as they represent different types of habit formation as I will discuss later. Also, the proportion of each demographic group can help calibrate our expectations around precision of estimates. With Asians being 2% of this population, it’s unrealistic to expect particularly precise estimates for them, but we should be able to make more headway for the other racial, gender, and party groups.</p>
<table class="table">
<thead>
<tr class="header">
<th>variable</th>
<th>proportion or count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>female</td>
<td>53.01%</td>
</tr>
<tr class="even">
<td>male</td>
<td>42.35%</td>
</tr>
<tr class="odd">
<td>unknown gender</td>
<td>4.61%</td>
</tr>
<tr class="even">
<td>white</td>
<td>52.83%</td>
</tr>
<tr class="odd">
<td>black</td>
<td>19.17%</td>
</tr>
<tr class="even">
<td>hispanic</td>
<td>18.86%</td>
</tr>
<tr class="odd">
<td>asian</td>
<td>2.07%</td>
</tr>
<tr class="even">
<td>other race</td>
<td>7.07%</td>
</tr>
<tr class="odd">
<td>democrat</td>
<td>40.36%</td>
</tr>
<tr class="even">
<td>republican</td>
<td>28.06%</td>
</tr>
<tr class="odd">
<td>no party</td>
<td>30.86%</td>
</tr>
<tr class="even">
<td>other party</td>
<td>0.73%</td>
</tr>
<tr class="odd">
<td>voted 2006</td>
<td>65,754</td>
</tr>
<tr class="even">
<td>voted 2008</td>
<td>501,582</td>
</tr>
<tr class="odd">
<td>voted 2010</td>
<td>202,207</td>
</tr>
<tr class="even">
<td>voted 2012</td>
<td>878,111</td>
</tr>
<tr class="odd">
<td>voted 2014</td>
<td>468,721</td>
</tr>
<tr class="even">
<td>voted 2016</td>
<td>1,467,375</td>
</tr>
</tbody>
</table>
<p>My outcome of interest is the number of votes cast in the downstream election, for example, the 2012 presidential general election. The “treatment” is having voted in the upstream presidential election, for example the 2008 election. The instrument is eligibility to participate in (for example) the 2008 election, which is determined by being 18 on election day, not 18 by the registration deadline as is sometimes commonly believed. The forcing variable is simply the cohort’s number of days above or below turning 18 on the upstream election day- a negative number in the field indicates the number of days older than 18 they are, whereas a positive number indicates the days remaining to turn 18. Finally, to account for seasonal and day of the week birth trends which subsequently influence total votes cast by each cohort, like Coppock and Green I include a lagged downstream vote total for the birthdate cohort one year younger. This is particularly important for this estimation strategy given that the several month period just before election day tends to have the most children born each year (Mulligan, 2012).</p>
<p>As a final descriptive presentation below in figure 1 and 2 are two example plots of the 60 discontinuities I leverage. In both plots, overall downstream votes cast appear slightly higher for those who were just eligible, although the trend is much clearer in the 2008-2012 example. This is a good first illustration of the potential of the design. The red line indicates the actual eligibility cutoff. As I will discuss later in the robustness section, there seems to be some indication that young Floridians are confused about the difference between being 18 by election day (red line), and being 18 by the last day to register to vote for the general election (blue line). This is partially the failure of the Florida Division of Elections, as their online materials don’t explain the distinction well. This may also explain why my results are sensitive to variation in bandwidth- a sharp change in behavior appears to occur 29 days before the actual eligibility deadline as well.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Is Voting Habit Forming 2/images/fig_1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure 1: Example Discontinuity 1</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Is Voting Habit Forming 2/images/fig_2.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure 2: Example Discontinuity 2</figcaption>
</figure>
</div>
</section>
<section id="estimand" class="level1">
<h1>Estimand</h1>
<p>Given this data, we can estimate a Complier Average Causal Effect (CACE). Define <img src="https://latex.codecogs.com/png.latex?D"> to be the number of votes cast in the upstream election, and <img src="https://latex.codecogs.com/png.latex?Y"> to be votes in the downstream election. As a reminder, one cannot assign <img src="https://latex.codecogs.com/png.latex?Y">, one can only leverage the encouragement to do so created by being just eligible, which I label <img src="https://latex.codecogs.com/png.latex?Z%20%E2%88%88%20%5B0,1%5D">, with 0 being too young, and 1 being old enough to vote in 2008 respectively. The forcing variable, days above or below being old enough to vote in 2008, I label <img src="https://latex.codecogs.com/png.latex?T">. Lagged refers to lagged downstream vote total for the birthdate cohort one year older, used to eliminate other temporal trends. For individual birthdate cohorts, I use subscript <img src="https://latex.codecogs.com/png.latex?i">’s.</p>
<p>The estimand is thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Clim%20_%7BT%20%5Cdownarrow%200%7D%20%5Csum_1%5EN%5Cleft%5BY_i%20%5Cmid%20D_%7B1%20i%7D=1%5Cright%5D-%5Clim%20_%7BT%20%5Cuparrow%200%7D%20%5Csum_1%5EN%5Cleft%5BY_i%20%5Cmid%20D_%7B1%20i%7D=0%5Cright%5D"></p>
<p>This is the additional number of votes in the downstream year we would expect across cohorts if all voters did vote in the upstream one, beyond the number if each cohort’s subjects had not voted in the upstream, among only the compliers, subjects who vote if and only if they receive the “encouragement” of being eligible in the upstream. This CACE is identified only at the cutoff, which as discussed above considerably restricts our ability to generalize these findings to the full voting or even young voter population. Following Coppock and Green, I present these as expected proportions of increase, ie .06 or 6% increase in upstream turnout, as this makes it easier to compare to earlier work. This must be restricted to compliers only because they are the only group in our analysis whose behavior changes just above or below being 18 on election day. For example, the encouragement provided by becoming eligible does not influence the behavior of “never takers”, those who would never vote. Our encouragement cannot create a difference in votes cast for this group or any other that is not the compliers: being just above or below the eligibility threshold only potentially alters the observed outcome of the compliers.</p>
</section>
<section id="methods" class="level1">
<h1>Methods</h1>
<p>I use linear 2 stage least squares (2SLS) regressions to estimate these CACEs:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%20&amp;%20D=%5Calpha_0+%5Calpha_1%20Z+%5Calpha_2%20T+%5Calpha_3%20T%20*%20Z+%5Calpha_4%20%5Ctext%20%7B%20Lagged%20%7D+%20%5Ctext%20%7B%20Controls%20%7D+%5Cvarepsilon(1)%20%5C%5C%20&amp;%20Y=%5Cbeta_0+%5Cbeta_1%20D+%5Cbeta_2%20T+%5Cbeta_3%20T%20*%20D+%5Cbeta_4%20%5Ctext%20%7B%20Lagged%20%7D+%20%5Ctext%20%7B%20Controls%20%7D+%5Cvarepsilon(2)%5Cend%7Baligned%7D"></p>
<p>With our fuzzy regression discontinuity estimate of the CACE as <img src="https://latex.codecogs.com/png.latex?%CE%B2_1">. <img src="https://latex.codecogs.com/png.latex?T"> is centered at 0, removing the need to complicate this definition by including the interaction term. I have 9 different control variables for each birthdate cohort, which can be thought of as divided into three blocks: gender (% female/% male), race (% White, %Black, %Asian, %Hispanic), and party (% Democrat,% Republican,% No Party Affiliation). When estimating a subset CACE that is part of a block, I do not include any control variables from that block. For instance, with the data subset to only Hispanics to estimate the Hispanic CACEs, % White, Black, and Asian are all 0, and thus excluded. My main results are estimated with a 365-day bandwidth around the upstream election and using first order polynomials, as suggested by bandwidth selection algorithms (Imbens &amp; Kalyanaraman, 2009). In the robustness checks section, I describe how varying choices of bandwidth and controls influence these estimates.</p>
<p>I estimate 60 different CACEs in total. I have 2 estimates for each of type of election pair (presidential-presidential, midterm-midterm, and presidential-midterm), which I then estimate for 10 different subsets: one for each of the 9 controls, plus the overall Florida estimate as a point of comparison. These are carried out with the AER R package for estimation, and using the rdrobust R package for access to bandwidth selection algorithms. All code is available in the appendix.</p>
</section>
<section id="assumptions" class="level1">
<h1>Assumptions</h1>
<p>In order for the fuzzy regression discontinuity design to correctly identify and estimate the CACE, 5 different assumptions must be met. I discuss each in turn.</p>
<p><strong>Exclusion:</strong> Is there any path other than through D through which the “treatment” of being just eligible in the upstream election could affect propensity to vote in the downstream election? Absent a history of actually participating, it is unlikely that someone who becomes eligible slightly earlier would be more likely to be targeted by a campaign to turn out. We can, however, imagine a subject who knew they would be eligible paying more attention in the upstream contest than someone who knew they would not be, which could spark more interest by the subsequent election. Similarly, a subject who missed voting in their first presidential election but was eligible might be more motivated in the downstream election for a fear of missing out again (“I missed voting for Obama, but won’t do so again”). With all such paths that imagine a more engaged citizen for their first presidential election, however, one has to imagine that most such citizens would want to put that energy towards voting in the first election. Thus, while we have no way to rule out such backdoor paths from being just eligible the upstream election to voting in the downstream, we have to imagine such cases would be relatively rare.</p>
<p><strong>Effectiveness of the Instrument:</strong> While just being eligible alone likely has a relatively weak effect on subsequent turnout, given that we are working at the scale of a full state voter file, we can be confident we have generated a reasonable number of compliers to at least detect strong effects for many subsets.</p>
<p><strong>Monotonicity:</strong> A defier in this design would be someone who votes despite not quite being old enough, or vice versa. Given that this constitutes a felony, and a hard to commit one with little benefit, we can be extremely confident in this form of the assumption. Alternatively, someone could choose to not vote only if just eligible, which again implies an extremely unlikely stance towards election law.</p>
<p><strong>Ignorability:</strong> Here, we need to be clear that we mean ignorability within some cutoff of being just eligible, given our covariates: <img src="https://latex.codecogs.com/png.latex?Y(1),Y(0)%20%E2%8A%A5Z%20%7C%20x,x%20%CF%B5%20(C-a,C+a)">. It seems plausible that being a month above or below 18 on election day shouldn’t create any other major changes in a 17 or 18 year old’s potential outcomes. However, as we get farther from the eligibility threshold it becomes more plausible that the groups could diverge through, for example, differences in maturity or differences in education due to birthdate.</p>
<p><strong>Cutoff and forcing variable determined independently:</strong> It seems unlikely that either Election Day or someone’s recorded birthday could be moved to help a subject vote earlier. Federal Election Day has been fixed in the United States since 1945. Further, it seems exceedingly unlikely that a parent or hospital administrator would attempt to modify a birth certificate simply to allow their child to vote 1 year earlier.</p>
<p><strong>Estimation:</strong> As we saw in the plots above, there seems to be little non-linear trend in Y above or below the cutoff, simplifying modeling <img src="https://latex.codecogs.com/png.latex?E%5BY%7CX%5D">. Also, while there might be some concern with day of the week, seasonal, or other temporal trends in the number of dates on a given birthdate and thus the number of expected votes there, including the year lagged variable should arguably be sufficient to model them, provided any trends aren’t particularly unique to 1990 births. Compared to prior work, my 9 controls make it much easier to believe that we can model <img src="https://latex.codecogs.com/png.latex?E%5BY%7CX%5D"> precisely.</p>
<p><strong>SUTVA:</strong> Lastly, there is little reason to believe that the encouragement to vote provided to one subject by turning 18 close to election day could influence another subject. While we can imagine students influencing each other’s politics, it seems implausible that a friend turning 18 close to election day alone could change a student’s propensity to vote meaningfully.</p>
<p>Outside of the questions of strength of instrument which I discuss next in detail, most of these assumptions don’t seem to face any major violations that we should be particularly worried about.</p>
<section id="instrument-effectiveness" class="level2">
<h2 class="anchored" data-anchor-id="instrument-effectiveness">Instrument Effectiveness</h2>
<p>Given that strength of instruments is a major concern in this body of research, and given prior work has posited CACEs from subset estimation like I use would be too imprecise to really understand heterogeneity, it is important to give a precise characterization of my instrument’s strength. This can be easily done using just the first stage of my 2SLS estimates above, effectively presenting how many downstream votes we expect our instrument to generate for a variety of specifications. In table 2 below, the first estimate is the number of votes we believe to be generated by our instrument (the coefficient on <img src="https://latex.codecogs.com/png.latex?%CE%B1_1">) for the 2008-2012 pair. Past work has been comfortable presenting this level of result as sufficiently precise. Next is the 2010-2014 pair, which prior work has also interpreted extensively. Restricting my data to blacks who are only 20% of the voting population is instructive- while fewer votes are of course generated, we can still expect an amount of precision in black presidential pairs similar to that in the full population in midterms. Hispanic voters show a similar trend given a similar share of the population, and white voter estimates will be even more precise as they are over half of Floridian voters. It is also important to show where this strategy breaks down- with the Asian (2% of population) subset, our estimates will be very imprecise, even for the presidential pair, and effectively useless for the midterms. Stepping back, given there is no prior knowledge about heterogeneity of CACEs by race, party, and gender, even these relatively coarse instruments are capable of showing any strong trends that exist, or ruling out ones that do not.</p>
<table class="table">
<thead>
<tr class="header">
<th>Subgroup</th>
<th>Estimate</th>
<th>SE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>All Florida, 2008-2012</td>
<td>37.14</td>
<td>5.02</td>
</tr>
<tr class="even">
<td>All Florida, 2010-2014</td>
<td>8.743</td>
<td>2.743</td>
</tr>
<tr class="odd">
<td>Black, 2008-2012</td>
<td>9.674</td>
<td>1.44</td>
</tr>
<tr class="even">
<td>Black, 2010-2014</td>
<td>1.908</td>
<td>0.745</td>
</tr>
<tr class="odd">
<td>Asian, 2008-2012</td>
<td>0.822</td>
<td>0.313</td>
</tr>
<tr class="even">
<td>Asian, 2010-2014</td>
<td>0.619</td>
<td>0.185</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<p>To organize presenting this collection of CACEs, I group the estimates into the 3 types of election pairs for discussion, after which I step back to interpret trends across them.</p>
<p>Below in Figure 3 and Table 3 are presidential-presidential pair CACEs, along with standard errors and 95% confidence intervals. These represent the most precise estimates I am able to produce, as the largest number of votes are cast in presidential elections, thus generating the largest number of compliers to work with. As a first validation of these estimates, the 2008-2012 pair estimates for all of Florida are nearly identical to Coppock &amp; Green’s estimates despite using more controls. The standard errors are somewhat smaller, however, as expected. Also consistent with prior research, all these estimates are consistently positive. Looking at the 2 pairs of estimates by gender, there doesn’t seem to be any heterogeneity I have the precision to detect. In the party estimates however, the non-party affiliated voters seem to see noticeably lower CACEs. With the 4 pairs of race estimates, we more strongly feel the limitations of such a weak instrument- the Asian estimates aren’t precise enough to inspire much confidence, but there’s no sign of massive racial disparity in habit forming. The lower CACE for non-party voters between presidential elections seems consistent with multiple findings in prior literature on both the general enthusiasm for voting of non-party affiliated voters (Leighley &amp; Nagler, 2014), and campaigns strong preference to target registered members of their own party with turnout interventions in party registration states (Hersh, 2015). On the other hand, some prior work has theorized that significant racial disparities might exist in voting habit formation. For instance, Bedolla &amp; Micheleson (2012) posited that minority groups might more weakly form voting habits than whites, making the overall CACE a combination of a higher white effect, and a lower non-white one. There is little evidence of such a pattern in my estimates- if one did exist, it would have to be quite small to be consistent with my results. Thus, even with imprecise estimates, we have made some headway in answering theoretical questions around heterogeneity of voting habit.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Is Voting Habit Forming 2/images/fig_3.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure 3: CACE Estimates for Presidential Pairs</figcaption>
</figure>
</div>
<table class="table">
<colgroup>
<col style="width: 16%">
<col style="width: 21%">
<col style="width: 20%">
<col style="width: 21%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Subset</th>
<th>2008-2012 Est</th>
<th>2008-2012 SE</th>
<th>2012-2016 Est</th>
<th>2012-2016 SE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>FL</td>
<td>0.114</td>
<td>0.016</td>
<td>0.129</td>
<td>0.021</td>
</tr>
<tr class="even">
<td>White</td>
<td>0.124</td>
<td>0.020</td>
<td>0.074</td>
<td>0.030</td>
</tr>
<tr class="odd">
<td>Black</td>
<td>0.136</td>
<td>0.023</td>
<td>0.100</td>
<td>0.024</td>
</tr>
<tr class="even">
<td>Asian</td>
<td>0.168</td>
<td>0.062</td>
<td>0.154</td>
<td>0.071</td>
</tr>
<tr class="odd">
<td>Hispanic</td>
<td>0.126</td>
<td>0.025</td>
<td>0.136</td>
<td>0.033</td>
</tr>
<tr class="even">
<td>Democrat</td>
<td>0.141</td>
<td>0.017</td>
<td>0.137</td>
<td>0.023</td>
</tr>
<tr class="odd">
<td>Republican</td>
<td>0.135</td>
<td>0.022</td>
<td>0.125</td>
<td>0.027</td>
</tr>
<tr class="even">
<td>No Party</td>
<td>0.066</td>
<td>0.024</td>
<td>0.071</td>
<td>0.029</td>
</tr>
<tr class="odd">
<td>Male</td>
<td>0.141</td>
<td>0.017</td>
<td>0.116</td>
<td>0.023</td>
</tr>
<tr class="even">
<td>Female</td>
<td>0.108</td>
<td>0.019</td>
<td>0.130</td>
<td>0.023</td>
</tr>
</tbody>
</table>
<p>Moving on to the Midterm CACEs in Figure 4 and Table 4, we lose precision due to the smaller number of compliers possible in a midterm election. Given Florida is a battleground state, and has been frequently considered the tipping point in past presidential elections, we could expect these estimates to reflect a much lower election spending estimate of habit formation. With so few Asian midterm voters, the confidence intervals of the estimates stretched as low as -.5 and as high as 1.1, the latter of which is an impossible value. They are thus removed from the graph to make the other numbers easier to interpret. Many of the confidence intervals no longer exclude zero, but the point estimates are still all positive. While there still isn’t much sign of a gender or racial disparity, there isn’t a gap in midterm CACEs for non-party affiliated voters that we have precision to detect.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Is Voting Habit Forming 2/images/fig_4.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure 4: Midterm CACEs</figcaption>
</figure>
</div>
<table class="table">
<colgroup>
<col style="width: 16%">
<col style="width: 21%">
<col style="width: 20%">
<col style="width: 21%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Subgroup</th>
<th>2006-2010 Est</th>
<th>2006-2010 SE</th>
<th>2010-2014 Est</th>
<th>2010-2014 SE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>FL</td>
<td>0.069</td>
<td>0.033</td>
<td>0.095</td>
<td>0.038</td>
</tr>
<tr class="even">
<td>White</td>
<td>0.110</td>
<td>0.037</td>
<td>0.089</td>
<td>0.045</td>
</tr>
<tr class="odd">
<td>Black</td>
<td>0.100</td>
<td>0.093</td>
<td>0.167</td>
<td>0.073</td>
</tr>
<tr class="even">
<td>Asian</td>
<td>-0.225</td>
<td>0.206</td>
<td>0.666</td>
<td>0.213</td>
</tr>
<tr class="odd">
<td>Hispanic</td>
<td>0.127</td>
<td>0.074</td>
<td>0.234</td>
<td>0.089</td>
</tr>
<tr class="even">
<td>Democrat</td>
<td>0.085</td>
<td>0.046</td>
<td>0.162</td>
<td>0.054</td>
</tr>
<tr class="odd">
<td>Republican</td>
<td>0.144</td>
<td>0.050</td>
<td>0.089</td>
<td>0.045</td>
</tr>
<tr class="even">
<td>No Party</td>
<td>0.038</td>
<td>0.061</td>
<td>0.083</td>
<td>0.076</td>
</tr>
<tr class="odd">
<td>Male</td>
<td>0.074</td>
<td>0.042</td>
<td>0.104</td>
<td>0.045</td>
</tr>
<tr class="even">
<td>Female</td>
<td>0.088</td>
<td>0.042</td>
<td>0.125</td>
<td>0.048</td>
</tr>
</tbody>
</table>
<p>Lastly, in Figure 5 and table 5 below are the CACE estimates for the Presidential-Midterm pairs, or dropoff estimates. Given that the compliers are generated in a presidential election, but the electoral environment downstream is one of a midterm, we should expect these estimates to have precision closer to the first pair of estimates, but also reflect that the downstream votes occur in a lower salience midterm environment. While overall and race trends are similar to the last two graphs, there is again some suggestion of a slightly lower CACE for non-party affiliated voters in the dropoff pairs. Also, there is some suggestion that in the 2008-2010 pair, female CACES are much lower than male ones, although the pattern doesn’t repeat in 2012-2014, suggesting it may simply be a function of noise.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Is Voting Habit Forming 2/images/fig_5.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure 5: Dropoff CACEs</figcaption>
</figure>
</div>
<table class="table">
<colgroup>
<col style="width: 16%">
<col style="width: 21%">
<col style="width: 20%">
<col style="width: 21%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Subgroup</th>
<th>2008-2010 Est</th>
<th>2008-2010 SE</th>
<th>2012-2014 Est</th>
<th>2012-2014 SE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>FL</td>
<td>0.084</td>
<td>0.006</td>
<td>0.123</td>
<td>0.008</td>
</tr>
<tr class="even">
<td>White</td>
<td>0.089</td>
<td>0.007</td>
<td>0.111</td>
<td>0.013</td>
</tr>
<tr class="odd">
<td>Black</td>
<td>0.094</td>
<td>0.009</td>
<td>0.104</td>
<td>0.013</td>
</tr>
<tr class="even">
<td>Asian</td>
<td>0.123</td>
<td>0.030</td>
<td>0.139</td>
<td>0.032</td>
</tr>
<tr class="odd">
<td>Hispanic</td>
<td>0.060</td>
<td>0.010</td>
<td>0.089</td>
<td>0.014</td>
</tr>
<tr class="even">
<td>Democrat</td>
<td>0.081</td>
<td>0.006</td>
<td>0.118</td>
<td>0.011</td>
</tr>
<tr class="odd">
<td>Republican</td>
<td>0.112</td>
<td>0.011</td>
<td>0.140</td>
<td>0.013</td>
</tr>
<tr class="even">
<td>No Party</td>
<td>0.065</td>
<td>0.008</td>
<td>0.089</td>
<td>0.012</td>
</tr>
<tr class="odd">
<td>Male</td>
<td>0.104</td>
<td>0.007</td>
<td>0.124</td>
<td>0.010</td>
</tr>
<tr class="even">
<td>Female</td>
<td>0.074</td>
<td>0.007</td>
<td>0.118</td>
<td>0.010</td>
</tr>
</tbody>
</table>
<p>Stepping back, the major trends here are limitations due to the instrument strength, lack of major disparities across gender or race, and some suggestion that non-party affiliated voters may form less habit than Democrats or Republicans.</p>
<p>Encouragingly, there don’t seem to be any large and persistent differences in habit formation by race or gender across all these types of elections. If minorities formed voting habit less than Whites, this would imply an even more severe uphill battle to close racial disparities in voting (Fraga, 2019). While I lack the ability to detect any fine-grained differences, even ruling out large racial or gender disparities is a novel contribution to the study of voting habit given the challenges this literature has faced around weak instruments. For example, if the overall Florida presidential CACE of .11 in the 2008-2012 was comprised of a .15 White CACE and .07 CACE for all other groups, my estimates would likely be precise enough to reflect that. Thus, while some researchers like Bedolla &amp; Micheleson (2012) have posited deep racial disparities in other areas of voting behavior like turnout extend to voting habit formation, that happily doesn’t seem to be the case, at least for Floridian compliers influenced to vote by being just 18 on an upstream election day. It is also possible, of course, that these trends aren’t a reflection of pure habit, but instead are a result of the democratic party working to turn out non-whites, diminishing whatever non-campaign related habit pattern might exist.</p>
<p>By party, there does seem to be some suggestion that non-party affiliated voters CACEs are considerably lower than party-affiliated voters. Multiple theories of voting behavior could explain the variation in these estimates across different types of elections. For instance, perhaps the party-affiliated CACEs are higher in presidential pairs because of massive presidential campaign spending in downstream years. In midterm years, Florida is still a purple state, but we would not expect the same magnitude of party expenditure there, given that not every gubernatorial, senate, or congressional midterm election is strongly contested. This would shrink the partisan vs.&nbsp;non-partisan gap. On the other hand, this trend could simply be consistent with work like Leighley &amp; Nagler (2014), which shows convincingly that non-party affiliated voters are less engaged and interested in politics. The dropoff estimates being more or less between the two other types does nothing to resolve this causes of effects debate. While the future of voting habit research may lie in large scale experiment-as-IV programs due to the challenges my work and prior work have face with our instrumental variables, working with Florida’s demographics-rich voter file data has provided a initial rough answer on what types of strong heterogeneity exists in voting habit formation.</p>
</section>
<section id="robustness-checks" class="level1">
<h1>Robustness Checks</h1>
<p>Beyond allowing me to estimate heterogeneous CACEs, retaining more information from the Florida voter file than previous work allows me to explore a greater variety of robustness checks as well. In order, I discuss including or excluding control variables, bandwidth variation, jumps in any of the covariates at the discontinuity, and a falsification check leveraging the fact that some cohorts of voters turned 18 not in a federal election. Of these, only bandwidth raise any concern.</p>
<p>First, as a simple check of the robustness, one might wonder if inclusion of my 9 new controls significantly changes any of the CACE estimates I present. I compared my estimates presented in the results section against both the same estimates excluding the control blocks in equations (1) and (2), and my full FL estimates to those presented by Coppock and Green. In both cases, the point estimates moved little, although the standard errors were reduced slightly as expected. This result also lends some credibility to earlier work like Coppock &amp; Green’s, and Dinas (2012)’s; if my estimates were sensitive to inclusion of controls, their work likely would be as well, calling into question the viability of the FRD estimation in states that don’t provide such rich demographic data. As suggested by Coppock &amp; Green however, the major source of variation does seem to be temporal variation, which is accounted for with the lagged variable. Code carrying out these comparisons can be found in the appendix.</p>
<p>Bandwidth provides more of a concern for my estimates: while bandwidth selection algorithms suggest the full 365-day bandwidth presented regardless of metrics to optimize for, or polynomial used, the estimates shrink towards zero as smaller and smaller bandwidths around the upstream election are used to estimate the CACE. Across all possible metrics to optimize for, kernels, and up to order 3 polynomial, all bandwidth selection algorithms formulations I tried using the rdRobust R package agreed that using the full 365 bandwidth was the best choice. As an illustration of the shrinking CACEs across varying bandwidths, in table 6 I present the 2008-2012 (presidential) election pair CACE point estimates at 5 different possible bandwidths. While the unweighted average of these subset estimates is of course not substantively meaningful, I include it as an aid in seeing the trend. A similar pattern occurs for all the other 5 election pairs I estimate- a full matrix of such estimates for all elections (also including their SEs) is in the appendix.</p>
<table class="table">
<colgroup>
<col style="width: 17%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th>State</th>
<th>365</th>
<th>180</th>
<th>90</th>
<th>60</th>
<th>30</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AR</td>
<td>0.200</td>
<td>0.150</td>
<td>0.140</td>
<td>0.010</td>
<td>0.100</td>
</tr>
<tr class="even">
<td>CT</td>
<td>0.160</td>
<td>0.140</td>
<td>0.140</td>
<td>0.110</td>
<td>0.110</td>
</tr>
<tr class="odd">
<td>IA</td>
<td>0.080</td>
<td>0.030</td>
<td>0.030</td>
<td>-0.050</td>
<td>-0.030</td>
</tr>
<tr class="even">
<td>IL</td>
<td>0.080</td>
<td>0.050</td>
<td>0.020</td>
<td>0.010</td>
<td>0.040</td>
</tr>
<tr class="odd">
<td>FL</td>
<td>0.112</td>
<td>0.090</td>
<td>0.050</td>
<td>0.030</td>
<td>-0.010</td>
</tr>
<tr class="even">
<td>KY</td>
<td>0.080</td>
<td>0.080</td>
<td>0.050</td>
<td>0.050</td>
<td>0.080</td>
</tr>
<tr class="odd">
<td>MO</td>
<td>0.160</td>
<td>0.150</td>
<td>0.140</td>
<td>0.130</td>
<td>0.110</td>
</tr>
<tr class="even">
<td>MT</td>
<td>0.110</td>
<td>0.080</td>
<td>0.030</td>
<td>-0.010</td>
<td>-0.080</td>
</tr>
<tr class="odd">
<td>NJ</td>
<td>0.150</td>
<td>0.120</td>
<td>0.090</td>
<td>0.030</td>
<td>0.010</td>
</tr>
<tr class="even">
<td>NV</td>
<td>0.170</td>
<td>0.140</td>
<td>0.090</td>
<td>0.000</td>
<td>-0.010</td>
</tr>
<tr class="odd">
<td>NY</td>
<td>0.070</td>
<td>0.020</td>
<td>-0.030</td>
<td>-0.060</td>
<td>-0.030</td>
</tr>
<tr class="even">
<td>OK</td>
<td>0.140</td>
<td>0.110</td>
<td>0.090</td>
<td>0.010</td>
<td>0.040</td>
</tr>
<tr class="odd">
<td>OR</td>
<td>0.110</td>
<td>0.070</td>
<td>0.060</td>
<td>0.010</td>
<td>-0.070</td>
</tr>
<tr class="even">
<td>PA</td>
<td>0.120</td>
<td>0.080</td>
<td>0.010</td>
<td>-0.050</td>
<td>-0.040</td>
</tr>
<tr class="odd">
<td>RI</td>
<td>0.110</td>
<td>0.140</td>
<td>0.050</td>
<td>0.030</td>
<td>-0.150</td>
</tr>
<tr class="even">
<td><strong>Average</strong></td>
<td><strong>0.123</strong></td>
<td><strong>0.097</strong></td>
<td><strong>0.065</strong></td>
<td><strong>0.017</strong></td>
<td><strong>0.005</strong></td>
</tr>
</tbody>
</table>
<p>The distinction between what Coppock and Green present (precision-weighted overall CACEs across states) and my table 7 (varying bandwidth in their data for individual states) is crucial for comparing their cases to mine. It seems that all states Coppock and Green studied share this shrinkage pattern, even ones with higher quality election administration than Florida’s. Similarly, Bandwidth selection algorithms run on individual states in Coppock and Green’s data also support wide bandwidths as I found. And finally, most other states, when plotted, show a spike at where the registration deadline would typically be, around 30 days before election day. All this taken together, it seems likely all work leveraging this discontinuity has and will struggle with the odd combination of sensitivity to bandwidth, but bandwidth selection algorithms preferring use of all data. I chose to present 365 days in my primary results because the bandwidth selection algorithms suggested it, but have tried to provide enough information in this paper and the appendix that readers more convinced by a smaller bandwidth could understand the implications of such a belief on voting habit. Usually, when the CACE estimates shrink towards 0 as the bandwidth is reduced, that suggests that no true effect exists. However, given how consistently upstream-experiment-as-IV approaches have produced positive CACEs suggesting a habit effect, and given how prior voter file work has a similar pattern as the bandwidth shrinks, I believe the confusion around eligibility to vote or register is the most likely cause of this finding.</p>
<p>My additional demographic variables also allow me to show that no covariate jumps at this discontinuity. If for example, whites disproportionately had birthdays just before election day compared to other races, that might indicate that the CACE was a reflection of different demographic profiles on either side of the cutoff, rather than effect of becoming just 18 near an election as we intended. Looking at Figure 6 below, there doesn’t seem to be any strong sign of a covariate jump at the discontinuity.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Is Voting Habit Forming 2/images/fig_6.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure 6: Covariates don’t seem to jump at 2008 discontinuity; further similar plots can be found in appendix.</figcaption>
</figure>
</div>
<p>Of course, a small variation across the bandwidth might be too small to see in these smaller plots, but still large enough to bias my results. More formally, we can test this using regression, by seeing whether the eligibility indicator <img src="https://latex.codecogs.com/png.latex?Z"> is a strong predictor of each covariate <img src="https://latex.codecogs.com/png.latex?Y"> in the equation below:</p>
<p><img src="https://latex.codecogs.com/png.latex?Y=%5Cbeta_0+%5Cbeta_1%20Z+%5Cbeta_2%20T+%5Cbeta_3%20T%20*%20Z+%5Cvarepsilon"></p>
<p>I show the <img src="https://latex.codecogs.com/png.latex?%CE%B2_1"> estimates for the 2008 discontinuity below in table 8- as we would expect, eligibility in 2008 is a poor tool for predicting all demographic covariates- all the coefficients are effectively zero, and none are statistically significant.</p>
<table class="table">
<colgroup>
<col style="width: 26%">
<col style="width: 25%">
<col style="width: 23%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th>Covariate</th>
<th><img src="https://latex.codecogs.com/png.latex?%5Cbeta_1"></th>
<th>SE <img src="https://latex.codecogs.com/png.latex?%5Cbeta_1"></th>
<th>p-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>White</td>
<td>-0.000012</td>
<td>0.000016</td>
<td>0.437300</td>
</tr>
<tr class="even">
<td>Black</td>
<td>0.000009</td>
<td>0.000012</td>
<td>0.492000</td>
</tr>
<tr class="odd">
<td>Hispanic</td>
<td>-0.004892</td>
<td>0.002987</td>
<td>0.102000</td>
</tr>
<tr class="even">
<td>Asian</td>
<td>0.000775</td>
<td>0.001098</td>
<td>0.480000</td>
</tr>
<tr class="odd">
<td>Democrat</td>
<td>-0.000025</td>
<td>0.000013</td>
<td>0.055877</td>
</tr>
<tr class="even">
<td>Republican</td>
<td>-0.001402</td>
<td>0.003630</td>
<td>0.699000</td>
</tr>
<tr class="odd">
<td>No Party</td>
<td>0.000021</td>
<td>0.000011</td>
<td>0.064430</td>
</tr>
<tr class="even">
<td>Female</td>
<td>-0.000011</td>
<td>0.000012</td>
<td>0.388040</td>
</tr>
<tr class="odd">
<td>Male</td>
<td>0.000010</td>
<td>0.000012</td>
<td>0.421000</td>
</tr>
</tbody>
</table>
<p>A final type of sensitivity analysis is to check whether the cohorts who turned 18 in a non-federal election year show no CACE the next following election. If we are correct in assuming that the causal effects observed in the results above are indeed caused by being “just 18” in the upstream year, then no such effects should be present for those who became “just 18” in a year like 2011 or 2007 for example. However, estimating these fake effects using the full FRD design would be challenging, as our true instrument is weak enough already, and an incorrectly specified instrument like being just eligible in the wrong year would certainly be even weaker.</p>
<p>Thus, to compare the two, I estimate two different intent to treat (ITT) effects using the regression below:</p>
<p><img src="https://latex.codecogs.com/png.latex?Y=%5Cbeta_0+%5Cbeta_1%20Z+%5Cbeta_2%20T+%5Cbeta_3%20T%20*%20Z+%5Cvarepsilon"></p>
<p>Where <img src="https://latex.codecogs.com/png.latex?Y"> is now the 2012 vote total in both cases, but the days to eligibility and eligibility indicator are switched from 2008 (correct) and 2007 (falsified) numbers. While these shouldn’t be interpreted as proper causal estimates of the effect of habit, in table 9 we receive a point estimate for <img src="https://latex.codecogs.com/png.latex?%CE%B2_1"> of roughly zero in the falsified case, with a similar precision to the correctly specified ITT one.</p>
<table class="table">
<colgroup>
<col style="width: 34%">
<col style="width: 21%">
<col style="width: 20%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th>Year Eligible</th>
<th>Beta1</th>
<th>SE</th>
<th>P-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2008 (real)</td>
<td>37.150</td>
<td>5.028</td>
<td>0.000</td>
</tr>
<tr class="even">
<td>2007 (fake)</td>
<td>2.375</td>
<td>5.068</td>
<td>0.403</td>
</tr>
</tbody>
</table>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<p>The study of habit formation remains significantly encumbered by the strength of the instruments available to us. While I generally agree with Coppock and Green that the future of this research will likely require bolder and more ambitious experiments to leverage as instrumental variables, in this paper I have shown that some rough insights into habit heterogeneity can still be garnered from voter file approaches. While my data are only sufficient to study very large effects, I was able to provide some evidence of non-party affiliated voters forming habit at lower rates than their party affiliated peers between presidential election pairs, and some evidence against previously hypothesized disparities in habit formation by race.</p>
<p>Whether these trends exist outside Florida, or outside the very narrow population of causal inference at eligibility discontinuities remains to be seen. Some additional headway on heterogeneity of habit formation between party and non-party registrants is likely possible using the voter file approach, as several other state files include this information. It would be particularly interesting to compare battleground and non-battleground states, given the patterns I found in presidential pair CACEs in Florida. The questions around habit formation by race will likely require the alternative upstream experiment approach, as Florida is unique in providing full birthdate and race information on its voters.</p>
<p>Numerous vendors who usually work for political campaigns also make available voter files that are often referred to as “augmented”, in the sense that they contain modeled race, or work to better link voters in their movement across states. This is a particularly promising approach for reducing the effects of movement between states on CACE estimates of habit formation, to see whether Coppock &amp; Green’s argument it doesn’t matter much was correct. Leveraging a modeled race field in a similar approach to could be a third approach for studying racial heterogeneity in voting habits. However, there are a number of practical and conceptual challenges to this. First, there is a question of correct interpretation; these scores are usually presented on a 1-100 scale, but cannot be correctly interpreted as a probability, instead being a relative ranking of how likely each individual is to be that race. Second, these modeled fields are not usually presented with measures of uncertainty around their point estimate. In studying one such augmented file that does have a probability interpretation, Hersh (2015) finds that the models rarely reach certainty beyond 80% likely to be a given race for the vast majority of voters. All these points make it hard to conceptualize how to properly propagate uncertainty due to race being modeled rather than self-reported forward into subsequent estimates. Thus, if the uncertainty was propagated forward, it would likely considerably inflate the standard errors, making such an approach unlikely to provide the more precise estimates than that possible with Florida’s uniquely rich demographic data. Still, such approaches could allow for all 50 states to be studied, if more precisely, allowing for potentially powerful meta-analysis findings.</p>
<p>There also seems to be considerable room to understand the puzzling combination of bandwidth selection algorithms preferring to use all possible data, and CACEs shrinking to zero as less data is used. One way to better understand this problem might be to see if this pair of trends persists across states with different registration deadlines, for example in states with same day registration that is well advertised like Minnesota. Other estimation strategies could also be considered. Perhaps the day to day variation in vote totals by birthdate can be reduced by some of aggregation into several day periods. Or perhaps treating the registration as an alternative cutoff would result in a different bandwidth recommendation or different shrinkage pattern.</p>
<p>Causes of effects observed remain a thorny problem for both approaches to estimating these CACEs. While it seems clear that upstream voting does cause downstream voting to some degree, is this a result of genuine habit formation, or is it a result of campaigns targeting those who have voted before? While a crude tool for understanding this, future work could consider seeing if the CACEs from either estimation approach show a relationship to Federal Elections Commission spending data in a given geography. Still, given the decentralized structure of most federal elections, with numerous PACs, party organizations, and advocacy groups developing separate and hard to track programs, this sort of approach likely cannot finely distinguish between the two. A broader perspective is that both campaign effects and any genuine habit forming are will remain a part American politics; without the development of a new strategy for ruling out one possible mechanism, we may need to be content with treating causal effects observed as a mix of habit and campaign work.</p>
</section>
<section id="works-cited" class="level1">
<h1>Works Cited</h1>
<p>Andersen, Kristi. 1994. “Mobilization, Participation, and Democracy in America. By Steven J. Rosenstone and John Mark Hansen. New York: Macmillan, 1993. 333p. $15.00.” American Political Science Review 88(3): 771–771.</p>
<p>Bedolla, Lisa García, and Melissa R. Michelson. 2012. Mobilizing Inclusion: Transforming the Electorate through Get-Out-the-Vote Campaigns. Yale University Press. www.jstor.org/stable/j.ctt32bj9c (December 13, 2019).</p>
<p>Coppock, Alexander, and Donald P. Green. 2016. “Is Voting Habit Forming? New Evidence from Experiments and Regression Discontinuities: IS VOTING HABIT FORMING?” American Journal of Political Science 60(4): 1044–62. Dinas, Elias. 2012. “The Formation of Voting Habits.” Journal of Elections, Public Opinion and Parties 22(4): 431–56.</p>
<p>Fraga, Bernard L. 2019. “Candidates or Districts? Reevaluating the Role of Race in Voter Turnout.” American Journal of Political Science 60(1): 97–122.</p>
<p>Gerber, Alan S., Donald P. Green, and Ron Shachar. 2003. “Voting May Be Habit-Forming: Evidence from a Randomized Field Experiment.” American Journal of Political Science 47(3): 540–50.</p>
<p>Green, Donald P., and Alan S. Gerber. 2015. Get Out the Vote: How to Increase Voter Turnout. Third edition. Washington, D.C: Brookings Institution Press.</p>
<p>Hersh, Eitan. 2015. Hacking the Electorate: How Campaigns Perceive Voters. Cambridge University Press. Hobolt, Sara Binzer, and Mark N. Franklin. 2011. “The Legacy of Lethargy: How Elections to the European Parliament Depress Turnout.” http://cadmus.eui.eu//handle/1814/19980 (December 13, 2019).</p>
<p>Imbens, Guido, and Karthik Kalyanaraman. 2009. Optimal Bandwidth Choice for the Regression Discontinuity Estimator. National Bureau of Economic Research. Working Paper. http://www.nber.org/papers/w14726 (December 13, 2019).</p>
<p>Meredith, Marc. 2009. “Persistence in Political Participation.” Quarterly Journal of Political Science 4(3): 187–209.</p>
<p>Mulligan, Chris. 2012. “Births by Day of Year.” http://chmullig.com/2012/06/births-by-day-of-year/ (May 10, 2020).</p>
<p>Verba, Sidney, and Norman H. Nie. 1987. Participation in America: Political Democracy and Social Equality. University of Chicago Press.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>It’s hard to TL;DR why I’m unsure about causes of effects, but essentially the methods don’t give us much ability to differentiate the “direct” effect of voting in one electing on voting in the next, versus anything that initial vote sets in action before the following election. For example, a campaign attempting to turn you out to vote would be impossible to distinguish from the direct effect of habit using this approach. Given that non-affiliated voters generally get less targeted turnout outreach, one plausible story is that voting doesn’t so much form habits as opt you into political contact of numerous types because you are seen as a “likely voter”.↩︎</p></li>
<li id="fn2"><p>I avoid the term independent for these voters because many states have independent or independence parties, which is a common source of confusion.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div id="quarto-reuse" class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2023,
  author = {Timm, Andy},
  title = {Who {Forms} {Voting} {Habits?}},
  date = {2023-12-18},
  url = {https://andytimm.github.io/posts/Is Voting Habit Forming 2/voting_habits_heterogenity.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2023" class="csl-entry quarto-appendix-citeas">
Timm, Andy. 2023. <span>“Who Forms Voting Habits?”</span> December 18,
2023. <a href="https://andytimm.github.io/posts/Is Voting Habit Forming 2/voting_habits_heterogenity.html">https://andytimm.github.io/posts/Is
Voting Habit Forming 2/voting_habits_heterogenity.html</a>.
</div></div></section></div> ]]></description>
  <category>Causal Inference</category>
  <guid>https://andytimm.github.io/posts/Is Voting Habit Forming 2/voting_habits_heterogenity.html</guid>
  <pubDate>Mon, 18 Dec 2023 05:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Is Voting Habit Forming 2/images/fig_3.png" medium="image" type="image/png" height="90" width="144"/>
</item>
<item>
  <title>My talk on Regularized Raking at NYOSPM</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/NYSOPM_talk_regularized_raking/NYOSPM_talk.html</link>
  <description><![CDATA[ 




<p>I recently gave a talk on <strong>Regularized Raking</strong> at the <a href="https://nyhackr.org/index.html">New York Open Statistical Programming Meetup</a>.</p>
<p>Here is the abstract:</p>
<blockquote class="blockquote">
<p>Raking is among the most common algorithms for producing survey weights, but it is often opaque what qualities of the resulting weights set are prioritized by the method. This is especially true when practitioners turn to heuristic methods like trimming to improve weights. After reviewing the basic raking algorithm and showing some examples in R, I’ll show that survey weighting can also be understood as an optimization problem, one which allows for explicit regularization. In addition to providing a conceptually crisp view of what (vanilla) raking optimizes for, I’ll show that this regularized raking (implemented via the rsw python package) can allow for more fine-grained control over weights distributions, and ultimately more accurate weighted estimates. Examples will be drawn from US elections surveys.</p>
</blockquote>
<p>The slides and reproduction materials can be found here: <a href="https://github.com/andytimm/Regularized-Raking">https://github.com/andytimm/Regularized-Raking</a>. It looks like the presentation on stream froze for a bit in the middle part of the talk, so you may want to pop the slides open to follow along.</p>
<p>For anyone else in the New York area, the meetup is a great group of smart folks working in a bunch of interesting industries- come join us sometime.</p>
<p>The recording is below:</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/qeGltVhozNI" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div id="quarto-reuse" class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2023,
  author = {Timm, Andy},
  title = {My Talk on {Regularized} {Raking} at {NYOSPM}},
  date = {2023-12-05},
  url = {https://andytimm.github.io/posts/NYSOPM_talk_regularized_raking/NYOSPM_talk.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2023" class="csl-entry quarto-appendix-citeas">
Timm, Andy. 2023. <span>“My Talk on Regularized Raking at
NYOSPM.”</span> December 5, 2023. <a href="https://andytimm.github.io/posts/NYSOPM_talk_regularized_raking/NYOSPM_talk.html">https://andytimm.github.io/posts/NYSOPM_talk_regularized_raking/NYOSPM_talk.html</a>.
</div></div></section></div> ]]></description>
  <category>surveys</category>
  <category>weighting</category>
  <guid>https://andytimm.github.io/posts/NYSOPM_talk_regularized_raking/NYOSPM_talk.html</guid>
  <pubDate>Tue, 05 Dec 2023 05:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/NYSOPM_talk_regularized_raking/image/December_2023_Meetup_Card.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Variational Inference for MRP with Reliable Posterior Distributions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Variational MRP Pt7/variational_mrp_pt7.html</link>
  <description><![CDATA[ 




<p><strong>Note:</strong> Since writing this post, <a href="https://mc-stan.org/docs/2_33/cmdstan-guide/pathfinder-intro.html#pathfinder-intro">Pathfinder</a> (described below as well in Section&nbsp;5) has become available in CmdStan, which poses a problem for this concluding post to the series.</p>
<p>More specifically, the running example (a simple-ish MRP model), was chosen to be too complex to get a good approximation with a simple mean-field or full-rank approximation as implemented by <code>rstanarm</code>, but Pathfinder is both significantly faster than what’s below, and quite capable here.</p>
<p>This is a bit awkward, but is informative. I generally still endorse my thoughts below about how to make variational inference work for much more complex models. That said, the series example now lives in an awkward middle ground where the most basic methods fail, but middle of the road options like Pathfinder can outperform complex methods like those I spent the series introducing.</p>
<p>Another point worth emphasizing here is that the methods below are not only inefficient compared to Pathfinder, they produce worse posteriors with regards to uncertainty! This highlights that more complex tools for variational inference are not uniformly better for all problems, even if something like the methods below might still be in my toolbox for more challenging models.</p>
<hr>
<p>This is the final post in my series about using Variational Inference to speed up complex Bayesian models, such as Multilevel Regression and Poststratification. Ideally, we want to do this without the approximation being of hilariously poor quality.</p>
<p>The last few posts in the series have explored several different major advances in black box variational inference. This post puts a bunch of these tools together to build a pretty decent approximation that runs ~8x faster than MCMC, and points to some other advances in BBVI I haven’t had time to cover in the series.</p>
<p>The other posts in the series are:</p>
<ol type="1">
<li>Introducing the Problem- Why is VI useful, why VI can produce spherical cows</li>
<li>How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li>Problem 1: KL-D prefers exclusive solutions; are there alternatives?</li>
<li>Problem 2: Not all VI samples are of equal utility; can we weight them cleverly?</li>
<li>Problem 3: How can we get deeply flexible variational approximations; are Normalizing Flows the answer?</li>
<li>Problem 4: How can we know when VI is wrong? Are there useful error bounds?</li>
<li><strong>(This post)</strong>: Putting it all together</li>
</ol>
<section id="cutting-to-the-chase" class="level1">
<h1>Cutting to the chase</h1>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt7/plots/CI_plot.png" class="img-fluid" style="width:100.0%"></p>
<p>To cut to the chase, the new and improved variational approximation is looking pretty, pretty good!</p>
<p>Like with the simpler mean-field and full-rank models from earlier in the series, this has the medians basically correct, but we also have reasonable uncertainty estimation too. First, the state distributions are much more smooth and unimodal- no more “lumpy” distributions with odd spikes of probability that make no sense as a model of public opinion. Further, the approximation is more consistent: while there’s still some variation state to state in how closely VI matches MCMC, pretty much all states are reasonable.</p>
<p>Certainly, we’re still to some degree understating the full size of MCMC’s credible interval. Considering this model runs in an hour and change versus MCMC’s 8 hours on 60,000 datapoints (!), this feels pretty acceptable. As I’ll write a bit more about later, there are a few ways to trade compute and/or runtime to fill out the CI’s as well.</p>
<p>Last time we look at a variational approximation in post 2, we found a dot plot was a significantly more revealing visual, which made it clear how bad the first try at VI in the series was. How does that look here?</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt7/plots/dot_plot.png" class="img-fluid" style="width:100.0%"></p>
<p>Again, pretty solid- no more weird spikes, and the concentration of mass looks pretty comparable (if a bit compressed) versus MCMC. VI is now much more uncertain about the same states as MCMC, and no longer shows any signs of degenerate optimization to fit data points. Nice!</p>
<p>Finally, how are the diagnostics we learned in the last post? The <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is .9, which is at least much improved<sup>1</sup>. The approximation is good enough that the Wasserstein bounds aren’t tight enough to inform us much about any issues<sup>2</sup>, although we should be a bit careful in trusting them giving that <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> (recall: the Wasserstein bounds aren’t super reliable when <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is high). In one sense, none of the diagnostics here are “great”, but this is pretty typical of my experience with BBVI for non-trivial models. We’re almost always losing something from the true posterior, and these diagnostics are not sufficiently fine-grained to differentiate important from unimportant losses.</p>
</section>
<section id="what-worked-here" class="level1">
<h1>What worked here?</h1>
<p>So the caption above gives some hints, but what all is in this model?</p>
<p>To fit this variational approximation, I’m using Agrawal, Domke, and Sheldon’s <a href="https://github.com/abhiagwl/vistan/tree/master">vistan</a>, which is a companion python package to their great paper <a href="https://proceedings.neurips.cc/paper/2020/file/c91e3483cf4f90057d02aa492d2b25b1-Paper.pdf">Advances In Black-Box VI</a>.</p>
<p>Here’s a footnote with more implementation details<sup>3</sup>, but for purposes of this post, I’ll just reference the parameters of the main setup function here:</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">vistan.algorithm(</span>
<span id="cb1-2">    vi_family <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rnvp"</span>,</span>
<span id="cb1-3">    full_step_search <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb1-4">    full_step_search_scaling <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb1-5">    step_size_exp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,</span>
<span id="cb1-6">    step_size_exp_range <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>],</span>
<span id="cb1-7">    step_size_base <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>,</span>
<span id="cb1-8">    step_size_scale <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">4.0</span>,</span>
<span id="cb1-9">    max_iters <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>,</span>
<span id="cb1-10">    optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'adam'</span>,</span>
<span id="cb1-11">    M_iw_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb1-12">    M_iw_sample <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>,</span>
<span id="cb1-13">    grad_estimator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"STL"</span>,</span>
<span id="cb1-14">    per_iter_sample_budget <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>,</span>
<span id="cb1-15">    fix_sample_budget <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb1-16">    evaluation_fn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"IWELBO"</span>,</span>
<span id="cb1-17">    rnvp_num_transformations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>,</span>
<span id="cb1-18">    rnvp_num_hidden_units <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>,</span>
<span id="cb1-19">    rnvp_num_hidden_layers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,</span>
<span id="cb1-20">    rnvp_params_init_scale <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb1-21">)</span></code></pre></div>
</div>
<p>Let’s talk about:</p>
<ol type="1">
<li>Normalizing Flows</li>
<li>Importance Sampling</li>
<li>Optimization</li>
<li>Sampling Budgets</li>
</ol>
<section id="normalizing-flows" class="level3">
<h3 class="anchored" data-anchor-id="normalizing-flows">Normalizing Flows</h3>
<p>First, this is using a <a href="https://arxiv.org/abs/1605.08803">Real NVP</a> normalizing flow, with a fairly small (<code>10</code>) number of transformation layers, each of which is using a pretty shallow neural net (<code>2 layers, 32 hidden units</code>). This helps make the approximating distribution complex enough to handle our model. I didn’t find much benefit from either adding more transform layers or making the neural nets deeper- that sort of makes sense, since the jump from mean-field or full-rank VI to using a normalizing flow at all is a fairly big one in terms of model representation capacity.</p>
</section>
<section id="importance-sampling" class="level3">
<h3 class="anchored" data-anchor-id="importance-sampling">Importance Sampling</h3>
<p>Second, this is using importance sampling only at the sampling stage, and with just <code>10</code> IW samples per returned sample. I didn’t find much benefit from importance weighted training<sup>4</sup> here, although for relatively more complex models than this I’ve found it to sometimes matter. As a point of comparison, in their paper linked above, Agrawal et al.&nbsp;find that about 20% of models actually get <em>worse</em> with IW-Training, and only ~10% get improvements, although those improvements seem to be clustered in more complex models and can be fairly significant.</p>
<p>At least for this particular model, using 10 importance samples per returned sample during provided most of the benefit of importance weighting, although pushing this higher to 100 or 200 helped fill out the outcome distribution’s tails a bit (more on this in a bit).</p>
</section>
<section id="optimization" class="level3">
<h3 class="anchored" data-anchor-id="optimization">Optimization</h3>
<p>Finally, many of the parameters here are about optimization, which we haven’t talked about in this series too much yet, despite it being critical to good performance.</p>
<p>First, Agrawal et al.&nbsp;found that variational inference is reasonably sensitive to optimization hyperparameters like the step size the Adam optimizer uses- to handle this, they suggest initial runs at few different step sizes (in <code>step_size_exp_range</code>), selecting the best one via an ELBO average over the whole optimization trace. This model benefits quite a lot from this, and Agrawal et al.’s results see strong improvements for ~25-30% of models using this adjustment.</p>
<p>Second, they use the <a href="https://arxiv.org/abs/1703.09194">Sticking the Landing</a> (<code>STL</code>) gradient estimator, which I’ll link out to later, but is essentially removing the score term from the gradient to reduce it’s variance.</p>
<p>A final optimization hyperparameter here is the number of iterations (both for that step search procedure and the final run)- I found <code>1000</code> was more than sufficient for optimization of this model, with going up to 2000 iterations not getting us much benefit, but going down to 500 leading to a heavily over-dispersed posterior.</p>
</section>
<section id="a-sample-budget" class="level3">
<h3 class="anchored" data-anchor-id="a-sample-budget">A sample budget?</h3>
<p>Since their paper is ultimately a bakeoff, Agrawal et al.’s paper touches on the theme of a sample budget again and again, and it’s a great concept to consider here. Essentially, a sample or computation budget is somewhat <strong>fungible</strong>: given an amount of run time, compute available, etc, we can, for example, trade a larger <code>max_iters</code> for using (more) importance weighted training, or use fewer iterations for (more) importance weighting our samples.</p>
<p>How to best spend this budget is an open question and fairly model dependent- here, I decided not to present the best model I could possibly fit with variational inference, but the best one I could get to fit in ~1h. The point of this series is to fit something nearly as good fast, not a different, more complex way but also in 8h.</p>
<p>Next, I’ll talk through some ideas for what could improve the model further at greater compute and wall time cost.</p>
</section>
</section>
<section id="what-i-might-change-next" class="level1">
<h1>What I might change next</h1>
<p>The major shortfall of the current approximation (assuming we continue to mostly care about state level estimates) is the overly narrow credible intervals. If I did want to improve the accuracy of this model, what might I consider next?</p>
<p>The low hanging fruit here would be to use some combination of more samples and/or more importance weighted samples per retained sample. In my testing this produces credible intervals about halfway between what I showed above and the MCMC ones, at the cost of another hour of runtime. It might be possible to go further than this and get closer to the MCMC interval, but likely suffers from quite harsh diminishing returns. For some applications though, that might be the right trade off to make!</p>
<p>As I already mentioned above, I didn’t find much benefit from more training iterations, importance weighted training, or making the RNVP component deeper. Thus, if I wanted to really invest a lot of time to improve this further, my next step might be to consider fitting another model in parallel to ensemble with this one. For example, perhaps an objective like the CUBO that tends to emphasize coverage would be worth combining with this one either via multiple importance sampling or more simplistic model averaging.</p>
</section>
<section id="what-i-might-change-for-other-models" class="level1">
<h1>What I might change for other models</h1>
<p>A logical next question: for models in general, which situations suggest tweaking which hyperparameters? While this is a really hard question, I’ll offer some tentative thoughts:</p>
<p><strong>The variational approximation is unable to represent the complexity of my model:</strong> In this situation, I’ve had the best luck increasing the complexity of my normalizing flow. Just like increasing the number of transforms helped in our ring density approximation example in post 6, more transforms and/or a deeper neural net within each transformation seems to be the most straightforward way I’ve found to improve representation capacity of variational inference. For the most complex models, perhaps changing the flow type will be necessary or efficient, but I’ve had surprisingly good luck just scaling up RNVP.</p>
<p>Like I mentioned above for this series’ specific model, I’ve had pretty meh results with IW-Training. I’ve had 1-2 models actually really benefit, but it hurts as often as it helps it seems, so it’s not something I reach for first anymore.</p>
<p><strong>The approximation is close, but it misses some minor aspect the posterior:</strong> In this type of situation, like the one above, adding more samples or using more importance weighting to produce the samples has worked well. In my experience, diminishing returns on the number of importance samples kick in faster than on the number of full model samples. I rarely see benefits beyond a couple hundred importance samples, but more draws often continue to provide benefits well into the thousands sometimes. Keep in mind that having trained a VI model, sampling is often orders of magnitude faster than MCMC: “just sample more” is much less time consuming advice to take than with MCMC.</p>
<p><strong>I can’t get the model to meaningfully converge:</strong> This often looks like the result we got in the second post in the series with mean-field and full-rank Variational Inference. Like with that post, there’s a variety of reasons this can happen. If you’re using mean-field or full-rank for a complex model, there’s a good chance you just need a normalizing flow or otherwise more complex approximating distribution to get any sort of useful convergence.</p>
<p>If you’re using something complex to make the approximation and you still see massively under/over-dispersed posteriors, then consider broadening the step size search, or grid searching a bit over the other Adam hyperparameters. Unlike with models based on deeper neural nets, I haven’t ever really seen a variational approximation plateau on loss for a long time and make a breakthrough; it’s pretty reasonable to trust early stopping and try to find something that actually gets optimization traction early on.</p>
<p><strong>There’s a whole part of the posterior entirely missing:</strong> I’ve only really seen this with highly multi-modal posteriors, but sometimes a single ELBO based model will only meaningfully cover a single mode in a parameter you care about. In this case, I’ve found a few smaller models averaged/MIS’d together to be the simplest solution- a single model that covers all modes is often quite hard given the mode seeking behavior of the ELBO that I discussed in post 3. Trying a different loss here is an option, but for more complex posteriors, I often struggle to get convergence with the CUBO.</p>
<p>This is by no means a authoritative list, but hopefully this set of suggestions for the most common issues I’ve had with variational inference in a variety of applied models is helpful. I’d also highly recommend the <a href="https://proceedings.neurips.cc/paper/2020/file/c91e3483cf4f90057d02aa492d2b25b1-Paper.pdf">Advances In Black-Box VI</a> paper mentioned above for more practical guidance of this type.</p>
</section>
<section id="sec-other-algos" class="level1">
<h1>Other things I didn’t cover</h1>
<p>Variational Inference is a decent sized research area, so I couldn’t cover everything in this series. As a way to wrap up, I want to gesture at some other papers that are worthwhile, but weren’t worth a full post in this series.</p>
<p><strong>Better Optimization for Variational Inference:</strong> Besides the work on step search in <a href="https://proceedings.neurips.cc/paper/2020/file/c91e3483cf4f90057d02aa492d2b25b1-Paper.pdf">Advances In Black-Box VI</a>, there are two really good papers on improving the underlying gradients we optimize on in variational inference. First, the <a href="https://arxiv.org/abs/1703.09194">Sticking the Landing</a> gradient estimator removes the score term in the total gradient with respect to the variational parameters. The result is a still unbiased<sup>5</sup>, but lower variance gradient estimator which helps a lot with both normalizing flow and simpler approximation fitting. Second, for when using importance weighted training, there’s the <a href="https://arxiv.org/abs/1810.04152">doubly reparameterized gradient (DReG)</a> estimator, which is both lower variance and unbiased via a clever (second) application of the reparameterization trick.</p>
<p><strong>Pathfinder:</strong> Coming soon to <code>brms</code> and Stan in general, <a href="https://arxiv.org/pdf/2108.03782.pdf">Pathfinder</a> is an attempt at a variational inference algorithm by Lu Zhang, Andrew Gelman, Aki Vehtari, and Bob Carpenter<sup>6</sup>. It uses a quasi-Newtonian optimization algorithm, and then samples from (a MIS combination of) Gaussian approximations along the optimization. There’s a ton of clever work here to make this incredibly fast, and comparatively parallel versus other variational algorithms. For even moderately complex posteriors, it’s blazingly fast, and quite accurate.</p>
<p>My only problem with it is that for more challenging posteriors, I’ve found it a little limited: it doesn’t seem to have the representation capacity possible with normalizing flows, which unfortunately is necessary for most of the non-blog-post-series-examples applications I use VI for. Like so many papers with these authors though, there’s a ton here that’s deeply insightful and more broadly applicable knowledge here, even if you need normalizing flows for your work.</p>
<p><strong>Boosting Variational Inference:</strong> Given how dominant boosting based algorithms are in basic machine learning, applying boosting to variational inference definitely had my attention when I first saw papers like <a href="https://arxiv.org/abs/1906.01235?utm_source=pocket_saves">Universal Boosting Variational Inference</a>. What’s weird about this strain of papers though is that there don’t see to be any public replication materials that sufficiently implement this for me to test it out, and there are no comparative bake off versus other serious, modern variational inference algorithms I can find.</p>
<p>This may be a lack of familiarity on my part (and I’m most uncertain about boosting VI of everything I’ve discussed in this series), but the vibes of this sub-literature feel off to me. Why is no one releasing a public, serious implementation of this<sup>7</sup>? If the authors claims about performance and ease of use are true, this pattern of “lots of papers, little code” is even weirder. Again, I’m super uncertain here, but after investing some time to dig here, this corner of the variational inference literature is surprisingly hard to engage with, and so I haven’t really made the time yet.</p>
<p>Thanks for reading this end to the series! Writing it as I learned more and more about variational inference has been incredibly helpful to me, and hopefully it has been useful to you as well.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>This is still not a “good” (&lt; .7) <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D">, but as we saw in the last post, <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is not a infallible metric, especially if you are interested in just a summary or two of the posterior where any defects it suggests may not be relevant. My guess is that to get <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> down, we’d need to move much further along in fully fleshing out the tails of the approximation, but even then we might still have issues given the dimensionality of this posterior.↩︎</p></li>
<li id="fn2"><p>This happens to me a lot, where these bounds don’t really tell me anything unless a model won’t even pass a basic smell test. If you have some counterexamples, I’d love to see them!↩︎</p></li>
<li id="fn3"><p>To fit this, I first used brms’s <code>make_standata</code> on both the cleaned survey responses and the ~12,000 bin matrix we want to post-stratify on to get data to pass to Stan, which I just saved out as JSON files to read back in later. Then, I used <code>make_stancode</code> to extract the basic Stan model, and added in logic to produce predictions onto the post-stratification matrix. You can see that modified Stan model <a href="https://github.com/andytimm/vistan_mrp_predictions">here</a>. After that, it was fairly easy to pass these inputs into vistan in python. One quick final note: vistan is quite sensetive to the version of python you use, so I recommend making a virtual environment with python 3.8 or 3.9.↩︎</p></li>
<li id="fn4"><p><code>evaluation_fn = "IWELBO"</code> is just a quirk of their syntax, where with <code>M_iw_train = 1</code>, it’s equivalent to not doing IW-weighted training at all.↩︎</p></li>
<li id="fn5"><p>See the paper for a longer explanation for why it’s still unbiased, but essentially this term has expectation zero. for some samples it may not be zero, but it’s sufficient for the unbiasedness of the broader gradient that it have expectation zero.↩︎</p></li>
<li id="fn6"><p>See the paper for a longer explanation for why it’s still unbiased, but essentially this term has expectation zero. for some samples it may not be zero, but it’s sufficient for the unbiasedness of the broader gradient that it have expectation zero.↩︎</p></li>
<li id="fn7"><p>Beyond this <a href="https://pyro.ai/examples/boosting_bbvi.html">toy example</a> in Pyro, I can’t find much. Again, prove me wrong if you know of something.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div id="quarto-reuse" class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2023,
  author = {Timm, Andy},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2023-07-12},
  url = {https://andytimm.github.io/posts/Variational MRP Pt7/variational_mrp_pt7.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2023" class="csl-entry quarto-appendix-citeas">
Timm, Andy. 2023. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> July 12, 2023. <a href="https://andytimm.github.io/posts/Variational MRP Pt7/variational_mrp_pt7.html">https://andytimm.github.io/posts/Variational
MRP Pt7/variational_mrp_pt7.html</a>.
</div></div></section></div> ]]></description>
  <category>MRP</category>
  <category>Variational Inference</category>
  <guid>https://andytimm.github.io/posts/Variational MRP Pt7/variational_mrp_pt7.html</guid>
  <pubDate>Wed, 12 Jul 2023 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Variational Inference for MRP with Reliable Posterior Distributions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Variational MRP Pt6/variational_mrp_pt6.html</link>
  <description><![CDATA[ 




<p>This is section 6 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.</p>
<p>The general structure for this post and the posts around it will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in these four posts will go a long way towards more robust variational inference. I’ll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.</p>
<p>In the <a href="https://andytimm.github.io/posts/Variational%20MRP%20Pt5/variational_mrp_5.html">last post</a> we looked at normalizing flows, a way to leverage neural networks to learn significantly more expressive variational families in a way that adapt to specific problems.</p>
<p>In this post, we’ll explore different diagnostics for variational inference, ranging from simple statistics that are easy to calculate as we fit our approximation to solving the problem in parallel with MCMC to compare and contrast. Some recurring themes will be aiming to be precise about what constitutes failure under each diagnostic tool, and providing intuition building examples where each diagnostic will fail to do anything useful. While no single diagnostic provides strong guarantees of variational inference’s correctness on their own, taken together the tools in this post broaden our ability to know when our models fall short.</p>
<p>The rough plan for the series is as follows:</p>
<ol type="1">
<li>Introducing the Problem- Why is VI useful, why VI can produce spherical cows</li>
<li>How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li>Problem 1: KL-D prefers exclusive solutions; are there alternatives?</li>
<li>Problem 2: Not all VI samples are of equal utility; can we weight them cleverly?</li>
<li>Problem 3: How can we get deeply flexible variational approximations; are Normalizing Flows the answer?</li>
<li><strong>(This post)</strong> Problem 4: How can we know when VI is wrong? Are there useful error bounds?</li>
<li>Putting the workflow all together</li>
</ol>
<section id="looking-at-our-loss-function" class="level1">
<h1>Looking at our loss function</h1>
<p>One logical place to start with diagnostics is to discuss what we can and can’t infer from our optimization objectives like an ELBO or CUBO.</p>
<p>In training a model with variational inference some common stopping rule choices are either to just run optimization for a fixed number of iterations, or to stop when relative changes in the loss have slowed, indicating convergence of the optimization to a local minimum. So we can at least look at changes in the ELBO/CUBO/other loss to know if our approximation has hit a local minimum yet.</p>
<p>Unfortunately, that’s about all monitoring the loss can tell us. Recall that an unknown, multiplicative constant exists in <img src="https://latex.codecogs.com/png.latex?p(z,x)%20%5Cpropto%20p(z%7Cx)"> that changes as reparameterize our model; thus, we can’t compare two different models on the same objective and expect their ELBO or similar loss values to be comparable. So the typical ML strategy of “which model achieves lower loss” is pretty much out here.</p>
<p>Also, the loss values themselves aren’t particularly meaningful: there’s no way to interpret a given ELBO as indicating a good approximation, for example. This generally stems from our bounds being bounds, not directly optimizing the quantity we want to optimize. While they’re definitely degenerate cases, there are even some fun counter examples I’ll show in a second where you can make the ELBO/CUBO arbitrarily low, while still allowing the posterior mean or standard deviation to be arbitrarily wrong!</p>
</section>
<section id="the-majesty-of-hatk" class="level1">
<h1>The majesty of <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"></h1>
<p>So if we can’t just look at our loss, what can we look at? One broadly applicable diagnostic tool is <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D">, which we already introduced in the post on using importance sampling to improve variational inference.</p>
<p>As a several sentence refresher, Pareto smoothed importance sampling (PSIS) proposes to stabilize importance ratios <img src="https://latex.codecogs.com/png.latex?r(%5Ctheta)"> used in importance sampling by modeling the tail of the distribution as a generalized Pareto distribution:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7B%5Csigma%7D%20%5Cleft(1%20+%20k%5Cfrac%7Br%20-%20%5Ctau%7D%7B%5Csigma%7D%20%5Cright)%5E%7B-1/k-1%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Ctau"> is a lower bound parameter, which in our case defines how many ratios from the tail we’ll actually model. <img src="https://latex.codecogs.com/png.latex?%5Csigma"> is a scale parameter, and <img src="https://latex.codecogs.com/png.latex?k"> is a unconstrained shape parameter.</p>
<p>To see how this provides a natural diagnostic for importance sampling, it’s useful to know that importance sampling depends on how many moments <img src="https://latex.codecogs.com/png.latex?r(%5Ctheta)"> has- for example, if at least two moments exist, the vanilla IS estimator has finite variance (which is obviously required, but no guarantee of performance since it might be finite but massive). The GPD has <img src="https://latex.codecogs.com/png.latex?k%5E%7B-1%7D"> finite fractional moments when <img src="https://latex.codecogs.com/png.latex?k%20%3E%200">. <a href="https://arxiv.org/abs/1507.02646">Vehtari et Al. (2015)</a> show through extensive theoretical digging and simulations that PSIS works fantastically when <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D%20%3C%20.5">. and acceptably if <img src="https://latex.codecogs.com/png.latex?.5%20%3C%20%5Chat%7Bk%7D%20%3C%20.7">. Beyond <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D%20=%20.7"> there the number of samples needed rapidly become impractically large.</p>
<p>Why should we think <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is a relevant diagnostic for variational inference? <a href="https://arxiv.org/abs/1511.01437">Chaterjee and Draconis (2018)</a> showed that for a given accuracy, how big our number of samples <img src="https://latex.codecogs.com/png.latex?S"> needs to be for importance sampling more broadly depends on how close <img src="https://latex.codecogs.com/png.latex?q(x)"> is to <img src="https://latex.codecogs.com/png.latex?p(x)"> in KL distance- we need to satisfy <img src="https://latex.codecogs.com/png.latex?log(S)%20%5Cgeq%20%5Cmathbb%7BE%7D_%7B%5Ctheta%20%5Csim%20q(x)%7D%5Br(%5Ctheta)log(r(%5Ctheta))%5D"> to get reasonable accuracy. So a good <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> indicates importance sampling is feasible, which in turn indicates that <img src="https://latex.codecogs.com/png.latex?q(x)"> is likely close to <img src="https://latex.codecogs.com/png.latex?p(x)"> in KL Divergence- exactly what we’re hoping to get at!</p>
<p>Fleshing out the use of <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> as a VI diagnostic was done by <a href="https://arxiv.org/abs/1802.02538">Yao et al.&nbsp;(2018)</a>, who generally show that high values of <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> do generally map onto posterior approximations with variational inference being quite poor. This is really useful, and generally maps well on to my experience- if <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is bigger than .7, you probably need to go back to the drawing board on how you’re fitting your VI.</p>
<p>What I want to stress though, is that the inverse isn’t broadly true- a low <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> isn’t necessarily a guarantee the VI approximation is good. Let’s look at a couple different ways this can happen.</p>
<section id="problem-case-1-importance-sampling-neq-direct-variational-inference" class="level2">
<h2 class="anchored" data-anchor-id="problem-case-1-importance-sampling-neq-direct-variational-inference">Problem Case 1: Importance sampling <img src="https://latex.codecogs.com/png.latex?%5Cneq"> direct variational inference</h2>
<p>We should keep in mind that <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is ultimately a diagnostic tool for importance sampling, and in cases where the needs of importance sampling and simple variational inference diverge, <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> can give a misleading answer.</p>
<p>Let’s re-use an example from the importance sampling post to illustrate this. What happens if we approximate the red distribution below with the green one?</p>
<div class="cell">

</div>
<div class="cell">

</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">mixture <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-2">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"red"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> mean_seeking_kl), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"green"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggtitle</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The green approximation is great for IS, terrible on its own"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt6/variational_mrp_pt6_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The green distribution here is a prime candidate to importance sample to approximate the red one- it coves all the needed mass, and we can massively down weight the irrelevant points in the center. On the other hand, this’d be a really, really bad variational approximation to use raw, since it has a ton of mass between the two modes which will blow up our loss. Because the needs of PSIS-based estimators and unadjusted VI diverge, <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is low, but the approximation would be pretty bad:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">importance_ratios <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tibble</span>(</span>
<span id="cb2-2"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">q_x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>),</span>
<span id="cb2-3"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">p_x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)),</span>
<span id="cb2-4"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ratios =</span> (.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)))<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>))</span>
<span id="cb2-5"></span>
<span id="cb2-6">psis_result <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">psis</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(importance_ratios<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>ratios),</span>
<span id="cb2-7">                       <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">r_eff =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span>)</span>
<span id="cb2-8"></span>
<span id="cb2-9">psis_result<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>diagnostics<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>pareto_k</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -1.737515</code></pre>
</div>
</div>
<p>So our <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> says everything is beautiful, but in reality it’s really only a happy time for PSIS, not the raw VI estimator. This ultimately isn’t the most concerning failure mode: if you do the work to calculate <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D">, you’re pretty much ready to use PSIS to improve your variational inference anyway. That said, this should provide intuition that <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> isn’t in general super well equipped to tell you much about non-IS augmented VI.</p>
</section>
<section id="problem-case-2-hatk-is-a-local-diagnostic" class="level2">
<h2 class="anchored" data-anchor-id="problem-case-2-hatk-is-a-local-diagnostic">Problem Case 2: <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is a local diagnostic</h2>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> inherits a common issue with most KL Divergence adjacent metrics: it’s ultimately something we evaluate locally, so if there’s a part of the posterior totally unknown to our <img src="https://latex.codecogs.com/png.latex?q(x)">, it won’t be able to tell you what you’re missing.</p>
<p>We already used 1 example from the importance sampling post, so let’s keep that moving. What do you think will happen with <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> with the green approximation below that misses a whole mode?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">mixture <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-2">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"red"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> mode_seeking_kl), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"green"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggtitle</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"We're missing a whole mode here"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt6/variational_mrp_pt6_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>If you guessed <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> will say everything is perfect when it’s not, you’re correct:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">second_importance_ratios <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tibble</span>(</span>
<span id="cb5-2"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">q_x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200000</span>,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),</span>
<span id="cb5-3"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">p_x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)),</span>
<span id="cb5-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Notice: these density calls are at the points defined by q(x)!</span></span>
<span id="cb5-5"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ratios =</span> (.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)))<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb5-6"></span>
<span id="cb5-7">psis_result_2 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">psis</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(second_importance_ratios<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>ratios),</span>
<span id="cb5-8">                       <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">r_eff =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span>)</span>
<span id="cb5-9"></span>
<span id="cb5-10">psis_result_2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>diagnostics<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>pareto_k</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.07343881</code></pre>
</div>
</div>
<p>That’s… not great. Since we evaluate the importance ratio and thus eventually <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> at the collection of values in <img src="https://latex.codecogs.com/png.latex?q(x)">, the diagnostic has no real way to know we’re missing an entire mode, and unlike in the above case there’s no easy fix here.</p>
<p>Another interesting question this example raises is what happens in high dimensions, where it’s much less intuitive what “missing one or several modes” looks like. Just by increasing the sd of the normal <img src="https://latex.codecogs.com/png.latex?q(x)"> a little in the example, we see a sudden, large increase in <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D">;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1">third_importance_ratios <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tibble</span>(</span>
<span id="cb7-2"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">q_x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200000</span>,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>),</span>
<span id="cb7-3"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">p_x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)),</span>
<span id="cb7-4"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ratios =</span> (.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)))<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))</span>
<span id="cb7-5"></span>
<span id="cb7-6">psis_result_3 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">psis</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(third_importance_ratios<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>ratios),</span>
<span id="cb7-7">                       <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">r_eff =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1">psis_result_3<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>diagnostics<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>pareto_k</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.70381</code></pre>
</div>
</div>
<p>similar sudden shifts in <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> can frequently occur as you increase the dimension of a posterior you’re approximating- intuitively, the mass you do and don’t know about becomes much harder to keep track of in high dimensions and for complex posteriors. This can lead to <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> being a bit less stable than you’d like over different initializations or other slight modifications of a VI model, with this pattern being common both in my own applications and documented in several papers like <a href="https://arxiv.org/abs/2302.12419">Wang et al.&nbsp;(2023)</a>’s testing.</p>
</section>
<section id="problem-case-3-hatk-is-a-joint-posterior-level-tool" class="level2">
<h2 class="anchored" data-anchor-id="problem-case-3-hatk-is-a-joint-posterior-level-tool">Problem Case 3: <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is a joint posterior level tool</h2>
<p>A final, more conceptual problem with <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> that <a href="https://arxiv.org/abs/1802.02538">Yao et al.&nbsp;(2018)</a> point out is that it’s ultimately a diagnostic of the joint posterior, not the specific marginal or summary statistic you may ultimately care about.</p>
<p>Variational inference is hard: we often know that the overall posterior approximation is deeply flawed, but it may be up to the task of representing some metrics we care about correctly enough. For example, in the MRP example I introduced earlier in the series, the mean-field variational inference fit was reasonable at representing the state-level means, but garbage at pretty much anything related to uncertainty. The <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> from that model was greater than 2, so we clearly know the broader posterior approximation was poor, but <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> might be a false positive sign if what you really care about was just the means. For the most complicated posteriors, we should expect to spend a lot of time in this feeling of “some parts of the posterior may be good enough”, so this is a useful trap to know about.</p>
<p>…Let’s step back for a second. Since I introduced <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> as a diagnostic with a bunch of cases where it falls short in surprising ways, I do want to emphasize it is a very useful <em>heuristic</em> diagnostic tool in general. Large <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> tells you something is very likely wrong with your joint posterior, and that’s generally practically helpful information. Where we need to be cautious is in inferring whether the wrongness <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> picks up on is something we care about, and also in remembering that low <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> doesn’t provide guarantees of correctness.</p>
</section>
</section>
<section id="wasserstein-bounds" class="level1">
<h1>Wasserstein Bounds</h1>
<p>So we’ve seen some limitations of using our objectives and <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> as diagnostics. Let’s break out some fun propositions and examples from <a href="https://arxiv.org/abs/1910.04102">Huggins et al.&nbsp;(2020)</a> to really drive home the need for a bound that actually makes guarantees on errors of posterior summaries we care about like means and variances.</p>
<hr>
<section id="proposition-3.1-arbitrarily-poor-mean-approximation" class="level3">
<h3 class="anchored" data-anchor-id="proposition-3.1-arbitrarily-poor-mean-approximation"><strong>Proposition 3.1 (Arbitrarily Poor Mean Approximation):</strong></h3>
<p>For any t &gt; 0, there exist (A) one dimensional, unimodal distributions <img src="https://latex.codecogs.com/png.latex?q"> and <img src="https://latex.codecogs.com/png.latex?p"> such that <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BKL%7D(q%20%5Cmid%20p)"> &lt; 0.9 and <img src="https://latex.codecogs.com/png.latex?%5Cleft(m_%7Bq%7D-m_p%5Cright)%5E2%3Et%20%5Csigma_p%5E2">, and <img src="https://latex.codecogs.com/png.latex?(B)"> one-dimensional, unimodal distributions <img src="https://latex.codecogs.com/png.latex?q"> and <img src="https://latex.codecogs.com/png.latex?p"> such that <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BKL%7D(q%20%5Cmid%20p)%20%3C%200.3"> and <img src="https://latex.codecogs.com/png.latex?%5Cleft(m_%7Bq%7D-m_p%5Cright)%5E2%3Et%20%5Csigma_%7Bq%7D%5E2">.</p>
</section>
<section id="proposition-3.2-arbitrarily-poor-variance-approximation" class="level3">
<h3 class="anchored" data-anchor-id="proposition-3.2-arbitrarily-poor-variance-approximation"><strong>Proposition 3.2 (Arbitrarily Poor Variance Approximation):</strong></h3>
<p>For any <img src="https://latex.codecogs.com/png.latex?t%20%5Cin(1,%20%5Cinfty%5D">, there exist one-dimensional, mean-zero, unimodal distributions <img src="https://latex.codecogs.com/png.latex?q"> and <img src="https://latex.codecogs.com/png.latex?p"> such that <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BKL%7D(q%20%5Cmid%20p)%3C0.12"> but <img src="https://latex.codecogs.com/png.latex?%5Csigma_p%5E2%20%5Cgeq%20t%20%5Csigma_%7Bq%7D%5E2">.</p>
<hr>
<p>So, these have some fairly scary names, huh? We can make the KL divergence pretty small, but have arbitrarily bad mean and variance approximation. How to do this? Given the examples here have to be unimodal and 1-D, they can’t be that, that weird, right?</p>
<hr>
</section>
<section id="proposition-3.1-example" class="level3">
<h3 class="anchored" data-anchor-id="proposition-3.1-example"><strong>Proposition 3.1 Example:</strong></h3>
<p>Let Weibull <img src="https://latex.codecogs.com/png.latex?(k,%201)"> denote the Weibull distribution with shape <img src="https://latex.codecogs.com/png.latex?k%3E0"> and scale 1. For (A), for any <img src="https://latex.codecogs.com/png.latex?t%3E0">, we can choose <img src="https://latex.codecogs.com/png.latex?k=k(t),%20p=%5Coperatorname%7BWeibull%7D(k,%201)">, and <img src="https://latex.codecogs.com/png.latex?q=%20%5Coperatorname%7BWeibull%7D(k%20/%202,1)">, where <img src="https://latex.codecogs.com/png.latex?k(t)%20%5Csearrow%200"> as <img src="https://latex.codecogs.com/png.latex?t%20%5Crightarrow%20%5Cinfty">. We can just exchange the two distributions for (B).</p>
</section>
<section id="proposition-3.2-example" class="level3">
<h3 class="anchored" data-anchor-id="proposition-3.2-example"><strong>Proposition 3.2 Example:</strong></h3>
<p>For any <img src="https://latex.codecogs.com/png.latex?t%3E0"> we let <img src="https://latex.codecogs.com/png.latex?h=h(t),%20p=%5Cmathcal%7BT%7D_h"> (standard <img src="https://latex.codecogs.com/png.latex?t"> distribution with <img src="https://latex.codecogs.com/png.latex?h"> degrees of freedom), and <img src="https://latex.codecogs.com/png.latex?q=%5Cmathcal%7BN%7D(0,1)"> (standard Gaussian), where <img src="https://latex.codecogs.com/png.latex?h(t)%20%5Csearrow%202"> as <img src="https://latex.codecogs.com/png.latex?t%20%5Crightarrow%20%5Cinfty">.</p>
<hr>
<p>I won’t show it here, but the Huggins et al.&nbsp;paper provides a similar proposition and example for <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> divergences and the CUBO bound we discussed earlier in the series, and the example has pretty much the same form.</p>
<p>Whenever presented with a “counterexample” to things working properly like this, it’s worth asking how broad the case’s applicability is: often counterexamples reside in the land of extremes, and we should be cautious in interpreting the result’s negative implications too broadly. There’s certainly some of that going on here, in the sense that usually a lower ELBO/CUBO will give some (if not perfect) traction in improving our posterior estimates of mean and variance. The intuitive point here though is that <strong>the bounds we optimize give no rigorous guarantees of posterior summaries we care about</strong>, even in the loosest sense.</p>
<p>To get some better guarantees like this, <a href="https://arxiv.org/abs/1910.04102">Huggins et al.&nbsp;(2020)</a> propose bounds on the the mean and uncertainty estimates that arise from variational inference, which leverage the Wasserstein distance. In addition to providing actual<sup>1</sup> bounds on quantities we care about, these bounds come at very reasonable computational cost, as they are readily computable from the bounds we already have (ELBO and CUBO) plus some additional Monte Carlo estimation and quick analytic calculation.</p>
<p>Let’s first discuss what the Wasserstein distance is, and then discuss the fairly involved path from our existing estimates to actually calculating the bounds.</p>
</section>
<section id="whats-a-wassterstein" class="level2">
<h2 class="anchored" data-anchor-id="whats-a-wassterstein">What’s a Wassterstein?</h2>
<p>The <img src="https://latex.codecogs.com/png.latex?p">-Wasserstein distance between <img src="https://latex.codecogs.com/png.latex?%5Cxi"> and <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BW%7D_p(%5Cxi,%20p):=%5Cinf%20_%7B%5Cgamma%20%5Cin%20%5CGamma(%5Cxi,%20p)%7D%5Cleft%5C%7B%5Cint%5Cleft%5C%7C%5Ctheta-%5Ctheta%5E%7B%5Cprime%7D%5Cright%5C%7C_2%5Ep%20%5Cgamma%5Cleft(%5Cmathrm%7Bd%7D%20%5Ctheta,%20%5Cmathrm%7Bd%7D%20%5Ctheta%5E%7B%5Cprime%7D%5Cright)%5Cright%5C%7D%5E%7B1%20/%20p%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?%5CGamma(%5Cxi,%20p)"> is the set of couplings between <img src="https://latex.codecogs.com/png.latex?%5Cxi"> and <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
<p>As a quick note on notation, I’m overloading <img src="https://latex.codecogs.com/png.latex?p"> here a bit; given I’ve used <img src="https://latex.codecogs.com/png.latex?p"> for our target posterior all series, I’m not going to switch that now, and calling it anything other than a <img src="https://latex.codecogs.com/png.latex?p">-Wasserstein distance would just be confusing to anyone who’se seen this distance before.</p>
<p>This looks a lot more involved than something like the KL Divergence. For example, the fact that we have an infinum over something complicated looking suggests this’ll be a real pain to calculate. As we’ll see in a second, Huggins et al.&nbsp;don’t actually seek to calculate it or approximate it, they seek to bound it <sup>2</sup>.</p>
<p>Before we get there though, let’s seek to understand the distance and it’s properties a little better.</p>
<p>A good way to start unpacking this is to consider the optimal transport problem. Given some probability mass <img src="https://latex.codecogs.com/png.latex?%5Cxi(%5Ctheta)"> on a space <img src="https://latex.codecogs.com/png.latex?X">, we wish to transport it such that it is transformed into the distribution <img src="https://latex.codecogs.com/png.latex?p(%5Ctheta)">. To provide physical intuition, this is often formulated as a problem of moving an equal amount of dirt/earth in pile <img src="https://latex.codecogs.com/png.latex?%5Cxi(%5Ctheta)"> to make pile <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Ctheta)">- hence the name commonly used in several disciplines, the <a href="https://en.wikipedia.org/wiki/Earth_mover%27s_distance">Earthmovers Distance</a>.</p>
<p>Let’s say we have some non-negative cost function for moving mass from <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> to <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B%5Cprime%7D">, <img src="https://latex.codecogs.com/png.latex?c(%5Ctheta,%5Ctheta%5E%7B%5Cprime%7D)">. A single transport plan for moving from <img src="https://latex.codecogs.com/png.latex?%5Cxi(%5Ctheta)"> to <img src="https://latex.codecogs.com/png.latex?p(%5Ctheta%5E%7B%5Cprime%7D)"> is a function <img src="https://latex.codecogs.com/png.latex?%5Cgamma(%5Cxi,%20%5Cpi)"> which describes the amount of mass to move at each point. If we assume <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is a valid joint probability mass with marginals <img src="https://latex.codecogs.com/png.latex?%5Cxi%5Ctheta)"> and <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Ctheta%5E%7B%5Cprime%7D)"> <sup>3</sup>, then the infinitesimal mass we transport from <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> to <img src="https://latex.codecogs.com/png.latex?%5Ctheta%7B%5Cprime%7D"> is <img src="https://latex.codecogs.com/png.latex?%5Cgamma(%5Ctheta,%20%5Ctheta%5E%7B%5Cprime%7D)%20d%5Ctheta%20d%5Ctheta%5E%7B%5Cprime%7D">, with cost</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cint%20%5Cint%20c(%5Ctheta,%5Ctheta%5E%7B%5Cprime%7D)%20%5Cgamma(%5Ctheta,%20%5Ctheta%5E%7B%5Cprime%7D)%20d%5Ctheta%20d%5Ctheta%5E%7B%5Cprime%7D%20=%20%5Cint%20c(%5Ctheta,%5Ctheta%5E%7B%5Cprime%7D)%20d%20%5Cgamma(%5Ctheta,%5Ctheta%5E%7B%5Cprime%7D)%0A"></p>
<p>Finally getting close to something that looks like our Wasserstein distance. There are many such plans, but the one we want, the solution to the optimal transport problem, is the one with minimal cost out of all such plans.</p>
<p>One last point to cover to define this: what’s our cost? If the cost here is the <img src="https://latex.codecogs.com/png.latex?p">-distance between our <img src="https://latex.codecogs.com/png.latex?%5Ctheta">s, then this is the p-Wassterstein distance.</p>
<p>What are some properties of this distance? I already mentioned a major downside (this looks nasty to estimate in general, and indeed it is). What are the upsides of this?</p>
<p>Unlike the KL or <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> divergences we’ve looked at before, the Wasserstein distance takes into account the metric on the underlying space! Let’s unpack that by again drawing on the optimal transport problem for intuition. The Wasserstein distance takes into account not only the differences in the values or probabilities assigned to different points in the distributions but also the actual “spatial”<sup>4</sup> arrangement of those points.</p>
<p>This is a incredibly useful property because the summaries of the posterior we care about in general also rely on the underlying metric. This is basically how the arbitrarily poor mean and variance examples above work; they exploit the lack of use of an underlying metric. That allows Huggins et al.&nbsp;to derive one of the key results of the paper</p>
<table class="table">
<colgroup>
<col style="width: 6%">
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Theorem 3.4.</strong> If <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BW%7D_1(q,%20p)%20%5Cleq%20%5Cvarepsilon"> or <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BW%7D_2(q,%20p)%20%5Cleq%20%5Cvarepsilon">, then <img src="https://latex.codecogs.com/png.latex?%5Cleft%5C%7Cm_%7Bq%7D-m_p%5Cright%5C%7C_2%20%5Cleq%20%5Cvarepsilon"> and <img src="https://latex.codecogs.com/png.latex?%5Cmax%20_i%5Cleft%7C%5Cmathrm%7BMAD%7D_%7Bq,%20i%7D-%5Cmathrm%7BMAD%7D_%7Bp,%20i%7D%5Cright%7C%20%5Cleq%202%20%5Cvarepsilon">.</td>
</tr>
<tr class="even">
<td>If <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BW%7D_2(q,%20p)%20%5Cleq%20%5Cvarepsilon">, then for <img src="https://latex.codecogs.com/png.latex?S%20:=%20%5Csqrt%7Bmin%20(%5Cleft%5C%7C%5CSigma_%7Bq%7D%5Cright%5C%7C_2,%20%5Cleft%5C%7C%5CSigma_p%5Cright%5C%7C_2)%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cmax%20_i%5Cleft%7C%5Csigma_%7Bq,%20i%7D-%5Csigma_%7Bp,%20i%7D%5Cright%7C%20%5Cleq%20%5Cvarepsilon"> and <img src="https://latex.codecogs.com/png.latex?%5Cleft%5C%7C%5CSigma_%7Bq%7D-%5CSigma_p%5Cright%5C%7C_2%3C2%20%5Cvarepsilon(S+%5Cvarepsilon)">.</td>
</tr>
</tbody>
</table>
<p>A similar type of result holds for the difference between expectations of any smooth function, so this result is somewhat extensible with additional work.</p>
<p>This is a nice improvement over the KL or <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> divergences as far as an a diagnostic, since we have some guarantees of correctness where we had literally none. I’ll return to how tight these are bounds in practice in a bit, since that’s entangled with how we can actually estimate them in the variational inference use case.</p>
</section>
<section id="bounds-of-bounds-via-bounds" class="level2">
<h2 class="anchored" data-anchor-id="bounds-of-bounds-via-bounds">Bounds of Bounds via… Bounds!</h2>
<p>One contribution of the Huggins at al.&nbsp;paper is the above result, but where things get even more impressive is that they find a reasonable and practical way to bound these quantities. It’s certainly not simple, but it works.</p>
<p>Here’s the plan to get real bounds on our posterior summaries in full:</p>
<ol type="1">
<li>Use the ELBO and CUBO to to bound the KL and <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> divergences.</li>
<li>Use tail properties of the distribution <img src="https://latex.codecogs.com/png.latex?q"> to get bounds on the Wasserstein distance through the KL and <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> divergences.</li>
<li>Finally, bound posterior summaries using the Wasserstein bounds.</li>
</ol>
<p>That’s a lot of layers of bounding, and it’s reasonable to wonder why this is needed and whether the bounds are usefully tight after such transformations. One key reason this type of bounding is so involved is that we’re using a set of scale-invariant distances to bound a scale-dependent one- we need to incorporate some notion of scale into the bounding process to make it work.</p>
<p>To do this, define the moment constants <img src="https://latex.codecogs.com/png.latex?C_p%5E%7B%5Cmathrm%7BPI%7D%7D(%5Cxi)"> and <img src="https://latex.codecogs.com/png.latex?C_p%5E%7B%5Cmathrm%7BEI%7D%7D(%5Cxi)">. For <img src="https://latex.codecogs.com/png.latex?p%20%5Cgeq%201">, <img src="https://latex.codecogs.com/png.latex?%5Cxi"> is p-polynomially integrable if <img src="https://latex.codecogs.com/png.latex?%0AC_p%5E%7B%5Cmathrm%7BPI%7D%7D(%5Cxi):=2%20%5Cinf%20_%7B%5Ctheta_0%7D%5Cleft%5C%7B%5Cint%5Cleft%5C%7C%5Ctheta-%5Ctheta_0%5Cright%5C%7C_2%5Ep%20%5Cxi(%5Cmathrm%7Bd%7D%20%5Ctheta)%5Cright%5C%7D%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D%3C%5Cinfty%0A"> and that <img src="https://latex.codecogs.com/png.latex?%5Cxi"> is p-exponentially integrable if <img src="https://latex.codecogs.com/png.latex?%0AC_p%5E%7B%5Cmathrm%7BEI%7D%7D(%5Cxi):=2%20%5Cinf%20_%7B%5Ctheta_0,%20%5Cepsilon%3E0%7D%5Cleft%5B%5Cfrac%7B1%7D%7B%5Cepsilon%7D%5Cleft%5C%7B%5Cfrac%7B3%7D%7B2%7D+%5Clog%20%5Cint%20e%5E%7B%5Cepsilon%5Cleft%5C%7C%5Ctheta-%5Ctheta_0%5Cright%5C%7C_2%5Ep%7D%20%5Cxi(%5Cmathrm%7Bd%7D%20%5Ctheta)%5Cright%5C%7D%5Cright%5D%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D%3C%5Cinfty%0A"></p>
<p>Next, with the assumption that the variational approximation <img src="https://latex.codecogs.com/png.latex?q"> has polynomial (respectively, exponential) tails, our next result provides a bound on the <img src="https://latex.codecogs.com/png.latex?p">-Wasserstein distance using the <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2">-divergence (respectively, the KL divergence).</p>
<p>This is saying we require at least polynomial, and ideally exponential moments for <img src="https://latex.codecogs.com/png.latex?q"> and <img src="https://latex.codecogs.com/png.latex?p">, which isn’t that strenuous of a requirement. Then:</p>
<table class="table">
<colgroup>
<col style="width: 6%">
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Proposition 4.2.</strong> If <img src="https://latex.codecogs.com/png.latex?p"> is absolutely continuous w.r.t. to <img src="https://latex.codecogs.com/png.latex?q"> then <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BW%7D_p(q,%20p)%20%5Cleq%20C_%7B2%20p%7D%5E%7B%5Cmathrm%7BPI%7D%7D(q)%5Cleft%5B%5Cexp%20%5Cleft%5C%7B%5Cchi_2(p%20%5Cmid%20q)%5Cright%5C%7D-1%5Cright%5D%5E%7B%5Cfrac%7B1%7D%7B2%20p%7D%7D%0A"> and <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BW%7D_p(q,%20p)%20%5Cleq%20C_p%5E%7B%5Cmathrm%7BEI%7D%7D(q)%5Cleft%5B%5Cmathrm%7BKL%7D(p%20%5Cmid%20q)%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D+%5C%7B%5Cmathrm%7BKL%7D(p%20%5Cmid%20q)%20/%202%5C%7D%5E%7B%5Cfrac%7B1%7D%7B2%20p%7D%7D%5Cright%5D%0A"></td>
</tr>
</tbody>
</table>
<p>A reasonable question here: does using the KL and <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> as part of building the bounds inherit KL/<img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2">’s arbitrarily poor posterior summaries? Nope! I won’t reproduce here, but the counter examples shown above for these divergences on their own no longer work to make our estimates arbitrarily wrong.</p>
<p>Next step: how do we use the ELBO and CUBO to bound the KL and <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> terms in the proposition above above?</p>
<p>We first define for any distribution <img src="https://latex.codecogs.com/png.latex?%5Ceta">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathrm%7BH%7D_%5Calpha(%5Cxi,%20%5Ceta):=%5Cfrac%7B%5Calpha%7D%7B%5Calpha-1%7D%5Cleft%5C%7B%5Coperatorname%7BCUBO%7D_%5Calpha(%5Cxi)-%5Coperatorname%7BELBO%7D(%5Ceta)%5Cright%5C%7D%0A"></p>
<p>Then we get:</p>
<table class="table">
<colgroup>
<col style="width: 6%">
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Lemma 4.5.</strong> For any distribution <img src="https://latex.codecogs.com/png.latex?%5Ceta"> such that <img src="https://latex.codecogs.com/png.latex?p"> is absolutely continuous w.r.t. <img src="https://latex.codecogs.com/png.latex?%5Ceta">- <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathrm%7BKL%7D(p%20%5Cmid%20q)%20%5Cleq%20%5Cchi_%5Calpha(p%20%5Cmid%20q)%20%5Cleq%20%5Cmathrm%7BH%7D_%5Calpha(q,%20%5Ceta)%0A"></td>
</tr>
</tbody>
</table>
<p>By combining the lemma above and proposition from 4.2 earlier, we can bound the Wasserstein distance finally! To do this, we need all of <img src="https://latex.codecogs.com/png.latex?C_p%5E%7B%5Cmathrm%7BPI%7D%7D(%5Cxi)">, <img src="https://latex.codecogs.com/png.latex?C_p%5E%7B%5Cmathrm%7BEI%7D%7D(%5Cxi)">, CUBO, and ELBO. All of these are efficiently calculable much of the time (we’ll get to when it’s not soon), and largely result from things we were already calculating as promised. Great! Our combined result:</p>
<hr>
<p><strong>Theorem 4.6.</strong> For any <img src="https://latex.codecogs.com/png.latex?p%20%5Cgeq%201"> and any distribution <img src="https://latex.codecogs.com/png.latex?%5Ceta">, if <img src="https://latex.codecogs.com/png.latex?p"> is absolutely continuous w.r.t. <img src="https://latex.codecogs.com/png.latex?q">, then <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BW%7D_p(q,%20p)%20%5Cleq%20C_%7B2%20p%7D%5E%7B%5Cmathrm%7BPI%7D%7D(q)%5Cleft%5B%5Cexp%20%5Cleft%5C%7B%5Cmathrm%7BH%7D_2(q,%20%5Ceta)%5Cright%5C%7D-1%5Cright%5D%5E%7B%5Cfrac%7B1%7D%7B2%20p%7D%7D%0A"> and <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BW%7D_p(q,%20p)%20%5Cleq%20C_p%5E%7B%5Cmathrm%7BEI%7D%7D(q)%5Cleft%5B%5Cmathrm%7BH%7D_2(q,%20%5Ceta)%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D+%5Cleft%5C%7B%5Cmathrm%7BH%7D_2(q,%20%5Ceta)%20/%202%5Cright%5C%7D%5E%7B%5Cfrac%7B1%7D%7B2%20p%7D%7D%5Cright%5D%20.%0A"></p>
<hr>
<p>This basically completes the process, other than some discussion of how to actually compute the quantities needed for the bounds above. The ELBO and CUBO we can compute as introduced previously in the series.</p>
<p>They have a good practical suggestion for a workflow given their bounds rely on CUBO, and thus on Monte Carlo estimation<sup>5</sup>. Before proceeding any further with a VI approximation, they suggest using <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D%20%3C%20.7"> as an initial diagnostic of the approximation, before calculating their bounds or leveraging importance sampling or PSIS to improve the estimate. Their point is that if <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> is high, the Monte Carlo work involved in generating their bounds will be unreliable at reasonable sample sizes, and thus you won’t be able to gaurantee the bounds are useful.</p>
<p>A final computational point you may be wondering about is how to calculate the moment constants, <img src="https://latex.codecogs.com/png.latex?C_p%5E%7B%5Cmathrm%7BPI%7D%7D(%5Cxi)"> and <img src="https://latex.codecogs.com/png.latex?C_p%5E%7B%5Cmathrm%7BEI%7D%7D(%5Cxi)">. They provide a helpful example showing how to do this when <img src="https://latex.codecogs.com/png.latex?q"> is a T distribution, and so the moments used are analytically calculable. What about when this isn’t possible? They suggest one can reasonably do this by fixing <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> and <img src="https://latex.codecogs.com/png.latex?%5Ctheta_0"> (for example, setting <img src="https://latex.codecogs.com/png.latex?%5Ctheta_0"> at the mean of the distribution), and sampling from <img src="https://latex.codecogs.com/png.latex?q">, which seems reasonable on a bit of reflection. The main reason this is worth bringing up: you don’t need a <img src="https://latex.codecogs.com/png.latex?q"> with easy to calculate moments to make this work, which was a worry when I first saw the moment constants.</p>
<p>So the final, combined workflow<sup>6</sup> they suggest is:</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt6/algo.png" class="img-fluid" style="width:50.0%"></p>
<p>This was a very, very long derivation, but hopefully walking through why we would want a distance with a sense of scale and how to calculate it helped build your intuition around variational inference.</p>
</section>
<section id="so-whats-the-bad-news-about-this-diagnostic" class="level2">
<h2 class="anchored" data-anchor-id="so-whats-the-bad-news-about-this-diagnostic">So what’s the bad news about this diagnostic?</h2>
<p>While these bounds are genuinely useful, let’s talk through some caveats and limitations of this diagnostic tool.</p>
<p>First, the bound really only is trustworthy when we can reliably estimate the CUBO, as I discussed above. Fortunately, as Higgins et al.&nbsp;note, we have an affordable way to check this in <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D">. Of course, we then take on the responsibility of finding a solution where we can trust <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D">, one which doesn’t fall into any of the blind spots the algorithm has that I discussed above. If we want to leverage the bounds, we’re also sort of forced to find a variational family that works well with the CUBO bound, even if an ELBO-based solution might work better. None of this is insurmountable, and much of the time my experience is that you probably need to change your variational family anyway if the <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> for the CUBO optimized approximation is greater than .7.</p>
<p>The second issue here is tightness of the bounds. As you might expect, the whole “bounds via bounds of bounds” thing can result in fairly loose bounding behavior. Wang et al.&nbsp;(2023) show some informative examples where the bounds are anywhere from 10-1000x (!) too conservative. For example, here’s an example of theirs over Neal’s funnel:</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt6/funnel_example.png" class="img-fluid" style="width:60.0%"></p>
<p>The <img src="https://latex.codecogs.com/png.latex?W%5E2"> based bounds (stars) here are 10-100x times too large compared to the true values (dashed lines). In my experience, this example and others from the paper aren’t pathological examples- the bounds are frequently this loose, especially for high dimensional and complex posteriors. Exactly what drives the achieved tightness of the bounds is fairly opaque to me; there are several stages of bounds, and it’s not really been possible to pinpoint the source of problems when the bounds are particularly loose.</p>
<p>However, this isn’t to say the bounds aren’t useful, far from it. In practice, especially on variance or covariance parameters, when things go off the rails, they often <em>really go off the rails</em>. If your variational family is nowhere near up to fitting a model, knowing you’re not within a few OOMs of reasonable values can actually be pretty helpful<sup>7</sup>. Also, these bounds provide some rigorous sense of approximation error where we previously had none, so in that sense this is a big step forward, even if they are loose.</p>
</section>
</section>
<section id="mcmc-based-diagnostics-whats-old-is-new-again" class="level1">
<h1>MCMC based diagnostics; what’s old is new again</h1>
<p>So <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> and Wasserstein bounds are both useful, but don’t tell us nearly as much as we really want to know, or do it as reliably as we’d hope. When the show isn’t going well, play the greatest hits: can we go back to using MCMC?</p>
<p>…Ok yeah, this feels like a bit of moving the goalposts, and in some sense it is. I motivated this whole series by saying I have models I’d like to fit where MCMC was impractically slow. But if you really want your variational approximation to look like what MCMC gives you, and do it over some summary of the posterior that isn’t a top-level mean, variance, or other simple summary, it may be the only responsible thing to do to break out your markov chains again. And things aren’t as bad as they sound here; there are a couple of ways to sanity check your VI with MCMC without committing to days of runtime.</p>
<section id="mcmc-can-be-practically-useful-even-when-its-slow" class="level2">
<h2 class="anchored" data-anchor-id="mcmc-can-be-practically-useful-even-when-its-slow">MCMC can be practically useful even when it’s slow</h2>
<p>Probably the most obvious way to use MCMC to sanity check variational inference is to not sanity check every model, just the occasional one. For example, say I was going to fit something like our MRP model to a running poll, and re-run the inferences weekly or daily. We can pretty reasonably make the leap from assuming if the variational approximation compares favorably to MCMC in the first such fit, the subsequent ones will also fit alright given the underlying data doesn’t fundamentally change in some way.</p>
<p>This is a pretty practical way to get the benefits of VI with the comfort MCMC gives you, as long as MCMC fits in a manageable time window. It’s just important to make sure to set up a realistic process where you spot check the occasional fit along the way, or perhaps retest with MCMC for other questions or shifts in the respondent pool that might plausibly break your model. This is the best strategy I’ve found for validating variational inference so far; even if an MCMC run can take a week or more, as long as it only has to happen once in a while that’s a totally reasonable price of admission.</p>
</section>
<section id="giving-mcmc-an-environment-to-succeed" class="level2">
<h2 class="anchored" data-anchor-id="giving-mcmc-an-environment-to-succeed">Giving MCMC an environment to succeed</h2>
<p>…But what if you’re using variational inference because MCMC will not finish at all? This is totally possible if you have millions of data points, and/or a complex model to estimate. In this case, you may need to give MCMC a good environment to succeed in.</p>
<p>A simple way to do this is to subsample the full data if that’s the choking point. For example, if I have a model that I want to use variational inference to fit on millions of rows, I can reasonably infer most of the time that 100k observations will still tell me at least something useful. By fitting the subsampled data with both MCMC and variational inference, we can make sure that at least at that scale the fits align.</p>
<p>Another, more challenging, but perhaps more efficient way to test when MCMC is unworkably slow on the full data is through data simulation. By simulating data that contains some of the core features I’m hoping my model will understand, and only simulating a moderate amount of it, I can see if VI can capture those features the same way MCMC can. For example, if I think understanding immigration survey question responses is a complex interaction of race, education, and location, I can simulate data which has the patterns I believe exist, and see if VI does meaningfully worse than VI for that covariance structure in the model. This approach or something close to it is something mentioned by David Shor in several of his <a href="https://www.youtube.com/watch?v=maddf8Emzds">talks</a> about Blue Rose Research’s Bayesian models which they scale to hundreds of millions of observations.</p>
</section>
<section id="taddaa" class="level2">
<h2 class="anchored" data-anchor-id="taddaa">TADDAA</h2>
<p>A final newer and more efficient way to gut check VI with MCMC is through <a href="https://arxiv.org/abs/2302.12419">Wang et al.&nbsp;(2023)</a>’s TArgeted Diagnostic for Distribution Approximation Accuracy algorithm, or TADDAA<sup>8</sup>. TADDAA provides a relatively compute efficient way to bound the error of VI via MCMC. They have two main motivations for this paper: first, that many existing VI diagnostics penalize approximations that are bad in any way, not necessarily just the ways you care about most (hence, TArgeted). Realistically, for complex models, it’s not a question of <em>if</em> a variational approximations are worse than MCMC, it’s a question of <em>how</em>. Second, they note that the Wasserstein bounds we discussed earlier are often so loose as to be impractical. Again, true.</p>
<p>These are both sensible points, but both are really properties of MCMC, not their algorithm, so the real juice in the paper is how they bound the error efficiently. Their strategy is to fit a variational inference model, draw values, and then start many short chains of MCMC at those points. If the variational approximation to the target distribution is good, we shouldn’t expect the MCMC running for a while to change much- the points should already be in highly plausible parts of parameter space. If the approximation is less good, then if the MCMC is setup well, the chains should move towards more correct values. Even if the chains don’t reach stationarity, this can be used to provide a lower bound on the amount of approximation error a variational approximation has.</p>
<p>Before I discuss a few technical details, visually the idea is:</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt6/taddaa cartoon.png" class="img-fluid" style="width:80.0%"></p>
<p>If things aren’t too poorly setup for MCMC, we can reasonably assume that the blue distribution (MCMC modified) will be between the red one (original VI posterior) and the black (true posterior). Neat!</p>
<p>In practice, this can work pretty well as you’d expect, and provides much tighter bounds than Wasserstein bounds given it’s a flavor of MCMC. Repeating the plot example I used earlier, notice how much closer the solid lines from the TADDAA bounds are to the dashed ones (ground truth) than the stars (denoting Wasserstein bounds):</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt6/funnel_example.png" class="img-fluid" style="width:60.0%"></p>
<p>I won’t wade too, too far into implementation details here as there a lot of them, but I do want to give enough information to discuss compute cost legibly. A first point worth raising is that “how many chains/iterations do I need” is shown by the paper to be a function of the accuracy you want on the bound- this adaptivity and ability to calculate what number of iterations you need ahead of time is convenient. Second, they’re using a lot of fairly sophisticated techniques in MCMC all together to make this efficient- strong sampling algorithms like MALA/HMC/Barker, preconditioning, and inter-chain adaptation (INCA) to adapt proposal parameters across the chains together. That’s both a great thing (this paper taught me about several new techniques around MCMC I didn’t previously know), and a bad one (to implement this for broader use there is a LOT of work to implement TADDAA efficiently<sup>9</sup>).</p>
<p>In their tests, all of this heavy machinery buys them some fairly impressive speed: implementing TADDAA takes from 2-18% of the gradient evaluations that the actual variational approximation takes. It’s a little opaque to me how that translates into wall time- on the one hand the many little chains are parallizeable, on the other if the number of the iterations is large that’d be the major driver of actual time this takes to run. Still, given it’s never several times VI’s compute needs like more generic MCMC would be in their tests, this seems pretty promising.</p>
<p>This paper is only a few months old, so I should be clear I’ve only had a bit of time to digest and play with the algorithm. If a more robust implementation became available, and the computational efficiencies they suggest are real for complex posteriors too, then this will be a fantastic new tool. They are absolutely right on the point that targeted diagnostics are valuable, and it seems like this is a way forward to getting bounds on the most relevant posterior summaries efficiently. As is, the time to set this up isn’t worth the effort versus letting simpler MCMC comparisons run for longer.</p>
</section>
</section>
<section id="conclusions" class="level1">
<h1>Conclusions</h1>
<p>So let’s take stock of the state of variational inference diagnostics. In terms of fast to calculate solutions, there are several tools that can help us detect quite poor approximations in <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> and Wasserstein bounds, but both have significant limitations. Each help detect some classes of posterior issues, but not others, and it’s a little opaque when we can strongly feel that their answers are reliable, even using both together. These feel worth running, but I’m not sure any of them are load bearing yet.</p>
<p>Using MCMC to validate variational approximations currently feels pretty necessary to me- the other metrics currently available can go wrong in numerous subtle and not-so-subtle ways. Of course, as this post hopefully shows, there are a lot of ways to make that validation price cheaper, from only testing one in several runs in a family of model, to using synthetic data to check results align between MCMC and VI at lower N sizes, to exploring new tools like TADDAA.</p>
<p>This post concludes my theoryposting streak. In the next post we’ll see if all the improvements to variational inference we’ve worked through in the past few posts buy us a better end product. We’ll finally return to trying to fit our MRP model better.</p>
<p>Thanks for reading. The code for this post can be found <a href="https://github.com/andytimm/andytimm.github.io/blob/main/posts/Variational%20MRP%20Pt6/variational_mrp_pt6.qmd">here</a>.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Alternatively, <em>not arbitrarily wrong</em>.↩︎</p></li>
<li id="fn2"><p>No, we’re not winding up to define another bound like the CUBO or ELBO and optimize it like you might think. There’s an interesting little sub-literature on making calculating the Wasserstein distance (really, approximations of it) efficiently enough that we could use it for scalable Bayesian inference tasks like variational inference. I haven’t looked back at this literature in a few years, but <a href="https://arxiv.org/abs/1508.05880">Srivastava et al.&nbsp;(2018)</a> is at least a starting point in this literature if you’re interested.↩︎</p></li>
<li id="fn3"><p>For further intuition, think about how this requirement maps onto our earth moving scenario. Integrating <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%5Cprime"> (marginalizing) gives <img src="https://latex.codecogs.com/png.latex?%5Cxi(%5Ctheta)">- physically, this means that the earth moved from point x needs to be equal to the amount starting there. The opposite marginalization gives <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Ctheta%5E%5Cprime)"> as you’d expect. Hopefully the physical intuition of the constraints that come with this being a valid joint probability distribution make sense here; visually: <img src="https://andytimm.github.io/posts/Variational MRP Pt6/1024px-Transport-plan.png" class="img-fluid">{By Lambdabadger - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=64872543}↩︎</p></li>
<li id="fn4"><p>This is imprecise, but hopefully this conveys the intuition well here: we want a distance measure that takes into account the distance between the points and how they need to be arranged to form one another, not just the values themselves.↩︎</p></li>
<li id="fn5"><p>See post 3 in the series if you want more of an introduction to this point, but the CUBO bound requires Monte Carlo (not MCMC) to calculate, which isn’t a huge computational cost when the approximation is good, but can quickly become unworkable or at least computationally tedious with bad or middling choice of variational families.↩︎</p></li>
<li id="fn6"><p>Bayesians love a good workflow.↩︎</p></li>
<li id="fn7"><p>More specifically, a common pattern is the variance estimates collapse, and collapse quite hard. Let’s say I fit a simpler model to our mrp survey example first; a lot of reasonable (even non-Bayesian) models will probably have a topline margin of errors of a couple percentage points. If the bounds on that was something like .0005% or so, that provides realistically useful news that our approximation is collapsing to weird point estimates like we saw in some poorly fit examples earlier in the series.↩︎</p></li>
<li id="fn8"><p>Fun fact: a key algorithmic detail here is any software implementation of TADDAA is that it must print “taddaa!” upon completing. Without this, users will lack a sense of magic or wonder that the algorithm otherwise instills.↩︎</p></li>
<li id="fn9"><p>They provide an implementation and replication materials here: https://github.com/TARPS-group/TADDAA. But from my early looks at this, this is more what’s needed to make the paper reproducible than a robust suite of tools for using TADDAA fully. It’s worth pointing out this is a major downside, since the other metrics I discuss all have pretty easy to use tools which implement them for general models at this point.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2023,
  author = {Andy Timm},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2023-06-17},
  url = {https://andytimm.github.io/variational_mrp_pt6.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2023" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2023. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> June 17, 2023. <a href="https://andytimm.github.io/variational_mrp_pt6.html">https://andytimm.github.io/variational_mrp_pt6.html</a>.
</div></div></section></div> ]]></description>
  <category>MRP</category>
  <category>Variational Inference</category>
  <category>Diagnostics</category>
  <guid>https://andytimm.github.io/posts/Variational MRP Pt6/variational_mrp_pt6.html</guid>
  <pubDate>Sat, 17 Jun 2023 04:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Variational MRP Pt6/funnel_example.png" medium="image" type="image/png" height="104" width="144"/>
</item>
<item>
  <title>Variational Inference for MRP with Reliable Posterior Distributions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Variational MRP Pt5/variational_mrp_pt5.html</link>
  <description><![CDATA[ 




<p>This is section 5 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.</p>
<p>The general structure for this post and the posts around it will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in these four posts will go a long way towards more robust variational inference. I’ll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.</p>
<p>In the <a href="https://andytimm.github.io/posts/Variational%20MRP%20Pt4/variational_mrp_4.html">last post</a> we saw a variety of different ways importance sampling can be used to improve VI and make it more robust, from defining a tighter bound to optimize in the importance weighted ELBO, to weighting <img src="https://latex.codecogs.com/png.latex?q(x)"> samples together efficiently to look more like <img src="https://latex.codecogs.com/png.latex?p(x)">, to combining entirely different variational approximations together to cover different parts of the posterior with multiple importance sampling.</p>
<p>In this post, we’ll tackle the problem of how to define a deeply flexible variational family <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BQ%7D"> that can adapt to each problem while still being easy to sample from. To do this, we’ll draw on normalizing flows, a technique for defining a composition of invertible transformations on top of a simple base distribution like a normal distribution. We’ll build our way up to using increasingly complex neural networks to define those transformations, allowing for for truly complex variational families that are problem adaptive, training as we train our variational model.</p>
<p>The rough plan for the series is as follows:</p>
<ol type="1">
<li>Introducing the Problem- Why is VI useful, why VI can produce spherical cows</li>
<li>How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li>Problem 1: KL-D prefers exclusive solutions; are there alternatives?</li>
<li>Problem 2: Not all VI samples are of equal utility; can we weight them cleverly?</li>
<li><strong>(This post)</strong> Problem 3: How can we get deeply flexible variational approximations; are Normalizing Flows the answer?</li>
<li>Problem 4: How can we know when VI is wrong? Are there useful error bounds?</li>
<li>Better grounded diagnostics and workflow</li>
</ol>
<section id="a-problem-adaptive-variational-family-with-less-tinkering" class="level1">
<h1>A problem adaptive variational family with less tinkering?</h1>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt5/images/flows_stairs_meme.png" class="img-fluid" alt="Something about NNs makes me meme more"></p>
<p>Jumping from mean-field or full-rank Gaussians and similar distributions to neural networks feels a little… dramatic<sup>1</sup>, so I want to spend some time justifying why this is a good idea.</p>
<p>For VI to work well, we need something that’s still simple to sample from, but capable of, in aggregate, representing a posterior that is probably pretty complex. Certainly, some problems are amenable to the simple variational families <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BQ%7D"> we’ve tried so far, but it’s worth re-emphasizing that we’re probably trying to represent something complex, and even moderate success at that using a composition of normals should be a little surprising, not the expected outcome.</p>
<p>If we need <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BQ%7D"> to be more complex, aren’t there choices between what we’ve seen and a neural network? There’s a whole literature of them- from using mixture distributions as variational distributions to inducing some additional structure into a mean-field type solution if you have some specific knowledge about your target posterior you can use. By and large though, this type of class of solutions has been surpassed by normalizing flows in much of modern use for more complex posteriors.</p>
<p>Why? A first reason is described in the paper that started the normalizing flows for VI literature, Rezende and Mohamed’s <a href="https://arxiv.org/pdf/1505.05770.pdf"><strong>Variational Inference with Normalizing Flows </strong></a>: making our base variational distribution more complex adds a variety of different computational costs, which add up quickly. This isn’t the most face-valid argument when I’m claiming a neural network is a good alternative, but it gets more plausible when you think through how poorly it’d scale to keep making your mixture distribution more and more complex as your posteriors get harder to handle. So this is a <em>scalability</em> argument- it might sound extreme to bring in a neural net, but as problems get bigger, scaling matters.</p>
<p>The other point I’d raise is that all these other tools aren’t very black box at all- if we can make things work with a problem-adapted version of mean-field with some structure based on the knowledge of a specific problem we have, that sounds like it gets time consuming fast. If I’m going to have to find a particular, problem-specific solution each time I want to use variational inference, that feels fragile and fiddly.</p>
<p>The novel idea with normalizing flows is that we’ll start with a simple base density like a normal distribution that is easy to sample from, but instead of only optimizing the parameters of that normal distribution, we’ll also use the training on our ELBO or other objective to learn a transformation that reshapes that normal distribution to look like our posterior. By having that transforming component be partially composed of a neural network, we give ourselves access to an incredibly expressive, problem adaptive, and heavily scalable variant of variational inference that is quite widely used.</p>
<p>And if the approximation isn’t expressive enough? Deep learning researchers have an unfussy, general purpose innovation for that: MORE LAYERS!<sup>2</sup></p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt5/images/more_layers.png" class="img-fluid" alt="Wow such estimator, very deep"></p>
</section>
<section id="what-is-a-normalizing-flow" class="level1">
<h1>What is a normalizing flow?</h1>
<p>A normalizing flow transforms a simple base density into a complex one through a sequence of invertible transformations. By stacking more and more of these invertible transformations (having the density “flow” through them), we can create arbitrarily complex distributions that remain valid probability distributions. Since it isn’t universal in the flows literature, let me be explicit that I’ll consider “forward” to be the direction flowing from the base density to the posterior, and the “backward” or “normalizing” direction as towards the base density.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt5/images/normalizing-flow.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Image Credit to <a href="https://siboehm.com/articles/19/normalizing-flow-network">Simon Boehm</a> here</figcaption><p></p>
</figure>
</div>
<p>If we have a random variable <img src="https://latex.codecogs.com/png.latex?x">, with distribution <img src="https://latex.codecogs.com/png.latex?q(x)">, some function <img src="https://latex.codecogs.com/png.latex?f"> with an inverse <img src="https://latex.codecogs.com/png.latex?f%5E%7B-1%7D%20=%20g,%20g%20%5Ccirc%20f(x)%20=%20x">, then the distribution of the result of one iteration of x through, <img src="https://latex.codecogs.com/png.latex?q%5E%5Cprime(x)"> is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aq%5Cprime(x)%20=%20q(x)%20%5Clvert%20det%20%5Cfrac%7B%5Cpartial%20f%5E%7B-1%7D%7D%7B%5Cpartial%20x%5E%5Cprime%7D%20%5Crvert%20=%20q(x)%20%5Clvert%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20%5Crvert%5E%7B-1%7D%0A"> I won’t derive this identity<sup>3</sup>, but it follows from the chain rule and the properties of Jacobians of invertible functions.</p>
<p>The real power comes in here when we see that these transformations stack. If we’ve got a chain of transformations (e.g.&nbsp;<img src="https://latex.codecogs.com/png.latex?f_K(...(f_2(f_1(x))))">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax_K%20=%20f(x)%20%5Ccirc%20...%20%5Ccirc%20f_2%20%5Ccirc%20f_1(x_0)%0A"></p>
<p>then the resulting density <img src="https://latex.codecogs.com/png.latex?q_K(x)"> looks like:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aln%20q_K%20(x_K)%20=%20lnq_0(x_0)%20-%20%5Csum%20%5Climits_%7BK%20=%201%7D%5Climits%5E%7BK%7D%20ln%20%20%5Clvert%20%5Cfrac%7B%5Cpartial%20f_k%7D%7B%5Cpartial%20x_%7Bk-1%7D%7D%20%5Crvert%5E%7B-1%7D%0A"></p>
<p>Neat, and surprisingly simple! If the terms above are all easy to calculate, we can very efficiently stack a bunch of these transformations and make an expressive model.</p>
<section id="normalizing-flows-for-variational-inference-versus-other-applications" class="level2">
<h2 class="anchored" data-anchor-id="normalizing-flows-for-variational-inference-versus-other-applications">Normalizing flows for variational inference versus other applications</h2>
<p>One source of confusion when I was learning about normalizing flows for variational inference was that variational inference makes up a fairly small proportion of use cases, and thus the academic literature and online discussion. More common applications include density estimation, image generation, representation learning, and reinforcement learning. In addition to making specifically applicable discussions harder to find, often resources will make strong claims about properties of a given flow structure, that really only holding in some subset of the above applications<sup>4</sup>.</p>
<p>By taking a second to explain this crisply and compare different application’s needs, hopefully I can save you some confusion and make engaging with the broader literature easier.</p>
<p>To start, consider the relevant operations we’ve introduced so far:</p>
<ol type="1">
<li>computing <img src="https://latex.codecogs.com/png.latex?f">, that is pushing a sample through the transformations</li>
<li>computing <img src="https://latex.codecogs.com/png.latex?g">, <img src="https://latex.codecogs.com/png.latex?f">’s inverse which undoes the manipulations</li>
<li>computing the (log) determinant of the Jacobian</li>
</ol>
<p>1 and 3 definitely need to be efficient for our use case, since we need to be able to sample and push through using the formula above efficiently to calculate an ELBO and train our model. 2 is where things get more subtle: we definitely need <img src="https://latex.codecogs.com/png.latex?f"> to be invertible, since our formulas above are dependent on a property of Jacobians of invertible functions. But we don’t actually really need to explicitly compute <img src="https://latex.codecogs.com/png.latex?g"> for variational inference. Even knowing the inverse exists but not having a formula might be fine for us!</p>
<p>Contrast this with density estimation, where the goal would not to sample from the distribution, but instead to estimate the density. In this case, most of the time would be spent going in the opposite direction, so that they can evaluate the log-likliehood of the data, and maximize it to improve the model<sup>5</sup>. The need for an expressive transformation of densities unite these two cases, but the goal is quite different!</p>
<p>This level of goal disagreement also shows it face in what direction papers choose to call forward: Most papers outside of variational inference applications consider forward to be the opposite of what I do here. For them, “forward” is the direction towards the base density, the normalizing direction.</p>
<p>For our use, hopefully this short digression has clarified which operations we need to be fast versus just exist. If you dive deeper into further work on normalizing flows, hopefully recognizing there are two different ways to consider forward helps you more quickly orient yourself to how other work describes flows.</p>
</section>
</section>
<section id="how-to-train-your-neural-net" class="level1">
<h1>How to train your neural net</h1>
<p>Now, let’s turn to how we actually fit a normalizing flow. Since this would be a bit hard to grok a code presentation if I took advantage of the full flexibility and abstraction that something like <a href="https://github.com/abhiagwl/vistan/tree/master"><code>vistan</code></a> provides, before heading into general purpose tools I’ll talk through a bit more explicit implementation of a simpler flow called a planar flow in <code>PyTorch</code> for illustration. Rather than reinventing the wheel, I’ll leverage Edvard Hulten’s great implementation <a href="https://github.com/e-hulten/planar-flows">here</a>.</p>
<p>In this section, I’ll define conceptually how we’re fitting the model, and build out a fun target distribution and loss- since I expect many people reading this may moderately new to PyTorch, I’ll explain in detail than normal what each operation is doing and why we need it.</p>
<p>Let’s first make a fun target posterior distribution from an image to model. I think it’d be a fun preview gif for the post to watch this be fit from a normal, so let’s use this ring shaped density.</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt5/images/ring_true_density.png" class="img-fluid" alt="Wow such estimator, very deep"></p>
<p>This is a solid starting example in that this’d be quite hard to fit with a mean-field normal variational family, but it’s pretty easy to define in PyTorch as well:</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># https://github.com/e-hulten/planar-flows/blob/master/target_distribution.py</span></span>
<span id="cb1-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> ring_density(z):</span>
<span id="cb1-3">                exp1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.exp(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> ((z[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-4">                exp2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.exp(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> ((z[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-5">                u <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> ((torch.norm(z, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb1-6">                u <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> u <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> torch.log(exp1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> exp2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-6</span>)</span>
<span id="cb1-7">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> u</span></code></pre></div>
</div>
<p>Now let’s define our loss for training, which will just be a slight reformulation of our ELBO:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5Blogp(z,x)%5D%20-%20%5Cmathbb%7BE%7D%5Blogq(z)%5D%0A"></p>
<p>To do this, we’ll define a class for the loss.</p>
<p>First, we pick a simple base distribution to push through our flow, here a 2-D Normal distribution called <code>base_distr</code>. We’ll also include the interesting target we just made above, <code>distr</code>.</p>
<p>Next, the forward pass structure. The <code>forward</code> method is the is the core of the computational graph structure in PyTorch. It defines operations that are applied to the input tensors to compute the output, and gives PyTorch the needed information for automatic differentiation, which allows smooth calculation and backpropagation of loss through the model to train it. This <code>VariationalLoss</code> module will run at the end of the forward pass to calculate the loss and allow us to pass it back through the graph for training.</p>
<p>Keeping with the structure above of numbering successive stages of the flow, <code>z0</code> here is our base distribution, and <code>z</code> will be the learned approximation to the target. In addition to the terms you’d expect in the ELBO, we’re also tracking and making use of the sum of the log determinant of the Jacobians to a handle on the distortion of the base density the flows apply.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># https://github.com/e-hulten/planar-flows/blob/master/loss.py</span></span>
<span id="cb2-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> VariationalLoss(nn.Module):</span>
<span id="cb2-3">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,distribution):</span>
<span id="cb2-4">      <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb2-5">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.distr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> distribution</span>
<span id="cb2-6">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.base_distr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> MultivariateNormal(torch.zeros(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), torch.eye(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))</span>
<span id="cb2-7"></span>
<span id="cb2-8">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, z0: Tensor, z: Tensor, sum_log_det_J: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>:</span>
<span id="cb2-9">      base_log_prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.base_distr.log_prob(z0)</span>
<span id="cb2-10">      target_density_log_prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.distr(z)</span>
<span id="cb2-11">      <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (base_log_prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> target_density_log_prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> sum_log_det_J).mean()</span></code></pre></div>
</div>
</section>
<section id="a-basic-flow" class="level1">
<h1>A basic flow</h1>
<p>Next, let’s define the structure of the actual flow. To do this, we’ll first describe a single layer of the flow, then we’ll show structure to stack the flow in layers.</p>
<p>Our first flow we look at will be the <strong>planar flow</strong> from the original Normalizing Flows for Variational Inference paper mentioned above. The name comes from how the function defines a (hyper)plane, and compresses or expand the density around it:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af(x)%20=%20x%20+%20u*tanh(w%5ETx%20+%20b),%20w,%20u%20%5Cin%20%20%20%5Cmathbb%7BR%7D%5Ed,%20b%20%5Cin%20%5Cmathbb%7BR%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?w"> and <img src="https://latex.codecogs.com/png.latex?b"> define the hyperplane and u specifies the direction and strength of the expansion. I’ll show a visualization of just one layer of that below.</p>
<p>If you’re more used to working with neural nets, you might wonder why we choose the non-linearity <img src="https://latex.codecogs.com/png.latex?tanh"> here, which generally isn’t as popular as something like <img src="https://latex.codecogs.com/png.latex?ReLU"> or its variants in more recent years due to it’s more unstable gradient flows. As the authors show in appendix <img src="https://latex.codecogs.com/png.latex?A.1">, functions like the above aren’t actually always invertible, and choosing <img src="https://latex.codecogs.com/png.latex?tanh"> allows them to impose some constraints that make things reliably invertible. See the appendix for more details about how that works, or take a careful look at Edvard’s implementation of the single function below. Realize this sort of constitutes a workaround; ideally we wouldn’t have to force constraints like this for our flow to work.</p>
<p>There isn’t that much that’s new conceptually in this PyTorch code; we’re defining the layer as a stackable module, which provides torch what it needs to calculate both the forward and backward pass of an arbitrary number of layers.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># From https://github.com/e-hulten/planar-flows/blob/master/planar_transform.py</span></span>
<span id="cb3-2"></span>
<span id="cb3-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> PlanarTransform(nn.Module):</span>
<span id="cb3-4">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Implementation of the invertible transformation used in planar flow:</span></span>
<span id="cb3-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">      f(z) = z + u * h(dot(w.T, z) + b)</span></span>
<span id="cb3-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">  See Section 4.1 in https://arxiv.org/pdf/1505.05770.pdf. </span></span>
<span id="cb3-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">  """</span></span>
<span id="cb3-8"></span>
<span id="cb3-9">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, dim: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>):</span>
<span id="cb3-10">      <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Initialise weights and bias.</span></span>
<span id="cb3-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">      </span></span>
<span id="cb3-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">      Args:</span></span>
<span id="cb3-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">          dim: Dimensionality of the distribution to be estimated.</span></span>
<span id="cb3-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">      """</span></span>
<span id="cb3-15">      <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb3-16">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.w <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Parameter(torch.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, dim).normal_(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>))</span>
<span id="cb3-17">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Parameter(torch.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).normal_(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>))</span>
<span id="cb3-18">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.u <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Parameter(torch.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, dim).normal_(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>))</span>
<span id="cb3-19"></span>
<span id="cb3-20">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, z: Tensor) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tensor:</span>
<span id="cb3-21">      <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> torch.mm(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.u, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.w.T) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:</span>
<span id="cb3-22">          <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.get_u_hat()</span>
<span id="cb3-23"></span>
<span id="cb3-24">      <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.u <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> nn.Tanh()(torch.mm(z, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.w.T) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.b)</span>
<span id="cb3-25"></span>
<span id="cb3-26">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> log_det_J(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, z: Tensor) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tensor:</span>
<span id="cb3-27">      <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> torch.mm(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.u, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.w.T) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:</span>
<span id="cb3-28">          <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.get_u_hat()</span>
<span id="cb3-29">      a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.mm(z, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.w.T) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.b</span>
<span id="cb3-30">      psi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> nn.Tanh()(a) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.w</span>
<span id="cb3-31">      abs_det <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> torch.mm(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.u, psi.T)).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>()</span>
<span id="cb3-32">      log_det <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.log(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-4</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> abs_det)</span>
<span id="cb3-33"></span>
<span id="cb3-34">      <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> log_det</span>
<span id="cb3-35"></span>
<span id="cb3-36">  <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_u_hat(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb3-37">      <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Enforce w^T u &gt;= -1. When using h(.) = tanh(.), this is a sufficient condition </span></span>
<span id="cb3-38"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">      for invertibility of the transformation f(z). See Appendix A.1.</span></span>
<span id="cb3-39"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">      """</span></span>
<span id="cb3-40">      wtu <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.mm(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.u, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.w.T)</span>
<span id="cb3-41">      m_wtu <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> torch.log(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> torch.exp(wtu))</span>
<span id="cb3-42">      <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.u.data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb3-43">          <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.u <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (m_wtu <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> wtu) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.w <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> torch.norm(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.w, p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb3-44">      )</span></code></pre></div>
</div>
<p>Where things will start to get exciting is multiple layers of the flow; here’s how we can make an abstraction that allows us to stack up <img src="https://latex.codecogs.com/png.latex?K"> layers of the flow to control the flexibility of our approximation.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> PlanarFlow(nn.Module):</span>
<span id="cb4-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, dim: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, K: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>):</span>
<span id="cb4-3">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Make a planar flow by stacking planar transformations in sequence.</span></span>
<span id="cb4-4"></span>
<span id="cb4-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Args:</span></span>
<span id="cb4-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            dim: Dimensionality of the distribution to be estimated.</span></span>
<span id="cb4-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            K: Number of transformations in the flow. </span></span>
<span id="cb4-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        """</span></span>
<span id="cb4-9">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb4-10">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.layers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [PlanarTransform(dim) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(K)]</span>
<span id="cb4-11">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Sequential(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.layers)</span>
<span id="cb4-12"></span>
<span id="cb4-13">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, z: Tensor) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Tuple[Tensor, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>]:</span>
<span id="cb4-14">        log_det_J <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb4-15"></span>
<span id="cb4-16">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> layer <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.layers:</span>
<span id="cb4-17">            log_det_J <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> layer.log_det_J(z)</span>
<span id="cb4-18">            z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> layer(z)</span>
<span id="cb4-19"></span>
<span id="cb4-20">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> z, log_det_J</span></code></pre></div>
</div>
<p>Let’s run this for a single layer to introduce the training loop, and build some intuition on the planar flow.</p>
<p>The hyperparameter names here should be fairly intuitive, but it’s worth pointing out that the batch size, learning rate (<code>lr</code>), and choice of Adam as an optimizer are all pretty basic reasonable first tries, but something you’d want to consider tuning in a more complicated context- we inherit that level of fiddly-ness when we choose to approach VI using normalizing flows. Also, note that I’m hiding setting up the plot code since it doesn’t add anything to the intuition here.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#From https://github.com/e-hulten/planar-flows/blob/master/train.py</span></span>
<span id="cb5-2">target_distr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ring"</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># U_1, U_2, U_3, U_4, ring</span></span>
<span id="cb5-3">flow_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb5-4">dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb5-5">num_batches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20000</span></span>
<span id="cb5-6">batch_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span></span>
<span id="cb5-7">lr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">6e-4</span></span>
<span id="cb5-8">axlim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xlim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ylim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 5 for U_1 to U_4, 7 for ring</span></span>
<span id="cb5-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ------------------------------------</span></span>
<span id="cb5-10"></span>
<span id="cb5-11">density <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TargetDistribution(target_distr)</span>
<span id="cb5-12">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PlanarFlow(dim, K<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>flow_length)</span>
<span id="cb5-13">bound <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> VariationalLoss(density)</span>
<span id="cb5-14">optimiser <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.optim.Adam(model.parameters(), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>lr)</span>
<span id="cb5-15"></span>
<span id="cb5-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Train model.</span></span>
<span id="cb5-17"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> batch_num <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, num_batches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb5-18">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get batch from N(0,I).</span></span>
<span id="cb5-19">    batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.zeros(size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(batch_size, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)).normal_(mean<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, std<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb5-20">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Pass batch through flow.</span></span>
<span id="cb5-21">    zk, log_jacobians <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(batch)</span>
<span id="cb5-22">    </span>
<span id="cb5-23">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute loss under target distribution.</span></span>
<span id="cb5-24">    loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bound(batch, zk, log_jacobians)</span>
<span id="cb5-25"></span>
<span id="cb5-26">    optimiser.zero_grad()</span>
<span id="cb5-27">    loss.backward()</span>
<span id="cb5-28">    optimiser.step()</span>
<span id="cb5-29"></span>
<span id="cb5-30">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> batch_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb5-31">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"(batch_num </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>batch_num<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:05d}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>num_batches<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">) loss: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>loss<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb5-32">        </span>
<span id="cb5-33"></span>
<span id="cb5-34">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> batch_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">or</span> batch_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb5-35">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Save plots during training. Plots are saved to the 'train_plots' folder.</span></span>
<span id="cb5-36">        plot_training(model, flow_length, batch_num, lr, axlim)</span></code></pre></div>
</div>
<p>At each iteration, we pass the normal draws thorugh the flow, calculate the loss, and propagate that loss backward through the flow to train it using gradient descent.</p>
<p>Here’s a gif<sup>6</sup> of what that looks like over the course of training. With just a single layer of planar flow of course, this isn’t expressive enough to capture the full density, but we can see why this approach has some promise- it’s learning to cover the target density, rather than us having to get creative in specifying a base density that does this.</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt5/images/1_layer.gif" class="img-fluid" alt="Maximally engineered two biomodal"></p>
<p>Let’s try a more serious attempt, with a depth of 32:</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#From https://github.com/e-hulten/planar-flows/blob/master/train.py</span></span>
<span id="cb6-2">target_distr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ring"</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># U_1, U_2, U_3, U_4, ring</span></span>
<span id="cb6-3">flow_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span></span>
<span id="cb6-4">dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb6-5">num_batches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20000</span></span>
<span id="cb6-6">batch_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span></span>
<span id="cb6-7">lr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">6e-4</span></span>
<span id="cb6-8">axlim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xlim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ylim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 5 for U_1 to U_4, 7 for ring</span></span>
<span id="cb6-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ------------------------------------</span></span>
<span id="cb6-10"></span>
<span id="cb6-11">density <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TargetDistribution(target_distr)</span>
<span id="cb6-12">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PlanarFlow(dim, K<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>flow_length)</span>
<span id="cb6-13">bound <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> VariationalLoss(density)</span>
<span id="cb6-14">optimiser <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.optim.Adam(model.parameters(), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>lr)</span>
<span id="cb6-15"></span>
<span id="cb6-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Train model.</span></span>
<span id="cb6-17"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> batch_num <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, num_batches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb6-18">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get batch from N(0,I).</span></span>
<span id="cb6-19">    batch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.zeros(size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(batch_size, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)).normal_(mean<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, std<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb6-20">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Pass batch through flow.</span></span>
<span id="cb6-21">    zk, log_jacobians <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(batch)</span>
<span id="cb6-22">    </span>
<span id="cb6-23">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute loss under target distribution.</span></span>
<span id="cb6-24">    loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bound(batch, zk, log_jacobians)</span>
<span id="cb6-25"></span>
<span id="cb6-26">    optimiser.zero_grad()</span>
<span id="cb6-27">    loss.backward()</span>
<span id="cb6-28">    optimiser.step()</span>
<span id="cb6-29"></span>
<span id="cb6-30">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> batch_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb6-31">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"(batch_num </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>batch_num<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:05d}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>num_batches<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">) loss: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>loss<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb6-32">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#print(log_jacobians)</span></span>
<span id="cb6-33"></span>
<span id="cb6-34">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> batch_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">or</span> batch_num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb6-35">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Save plots during training. Plots are saved to the 'train_plots' folder.</span></span>
<span id="cb6-36">        plot_training(model, flow_length, batch_num, lr, axlim)</span></code></pre></div>
</div>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt5/images/32_layer.gif" class="img-fluid" alt="An actually useful normalizing flow"></p>
<p>Now we’ve got it! With this planar flow, we’ve transformed our base normal into a pretty complex (for 2-D) distribution, cool!</p>
<p>This took about 20 minutes to train, so this is adding some considerable time to our VI workflow, but on the other hand, we’re not spending the human time needed to figure out what weird base density could be fitted to look like this, which is a win. Also worth pointing out here is that we started with a simple example for illustration purposes, so if we did have something much more complicated or high dimensional to fit, we’d start to see the scalability of normalizing flows start to shine more.</p>
<p>Planar flows are a great learning tool, but in reality they aren’t a great choice once we get outside relatively low-dimensional examples. See <a href="https://arxiv.org/abs/2006.00392">Kong and Chadhuri, (2020)</a> if you want more mathematical rigor, but intuitively, expansion or compression around a hyperplane doesn’t scale to high dimensions well given the operation is pretty simple. We can get around this partially by making the flow deeper, but that introduces its own problems. Very deep planar flows can struggle to fit given the somewhat artificial constraints imposed in computing the log determinant of the Jacobian (implemented in <code>get_u_hat</code> above) to ensure invertability. Finally, there are flows developed since the original normalizing flows paper that both are more expressive and have cheaper to compute transformations and log determinants of the Jacobian- let’s turn to those now.</p>
</section>
<section id="what-more-complicated-flows-look-like" class="level1">
<h1>What more complicated Flows look like</h1>
<p>More complex flows are an active area of research, and I won’t attempt to talk through the whole zoo- I’d recommend either Lilian Weng’s <a href="https://lilianweng.github.io/posts/2018-10-13-flow-models/">blog post</a>, or <a href="https://arxiv.org/abs/1908.09257">Kobyzev et al.&nbsp;(2020)</a> as good starting points for seeing the full range of available flows.</p>
<p>Instead, I’ll introduce just a single more complex flow, RealNVP, introduced in <a href="https://arxiv.org/abs/1605.08803">Dinh et al.&nbsp;(2017)</a>. This is a good example both because the flow is shown to be robustly good for high dimensional variational inference tasks in review papers like <a href="https://arxiv.org/abs/2103.01085">Dhaka et al.&nbsp;(2021)</a> and <a href="https://arxiv.org/abs/2006.10343">Agrawal et al.&nbsp;(2020)</a>, and because it illustrates some generalizable ideas about flow design.</p>
<p>Dinh et al.&nbsp;start the RealNVP paper by noting some goals: they want a Jacobian that is <em>triangular</em>, because this makes computing the determinant incredibly cheap (it’s just the product of the diagonal terms). Second, they want a transformation that’s simple to invert, but complex via inducing interdependencies between different parts of the data.</p>
<p>To do both of these at once, the key insight the authors come to is the idea of a <em>coupling layer</em>, where if the layer is <img src="https://latex.codecogs.com/png.latex?D">-dimensional, the first half of the dimensions <img src="https://latex.codecogs.com/png.latex?1:d"> remain unchanged, and <img src="https://latex.codecogs.com/png.latex?d+1:D"> are transformed as complex function of the first half:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0Ay_%7B1:d%7D%20&amp;=%20x_%7B1:d%7D%5C%5C%0Ay_%7Bd+1:D%7D%20&amp;=%20x_%7B1:d%7D%20%5Codot%20exp(s(x_%7B1:d%7D))%20+%20t(x_%7B1:d%7D)%0A%5Cend%7Balign%7D%0A"></p>
<p>Where s and t are scaling and transformation functions from <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Ed%20%5Crightarrow%20%5Cmathbb%7BR%7D%5E%7BD-d%7D">, and <img src="https://latex.codecogs.com/png.latex?%5Codot"> is the Hadamard (element-wise) product. Visually, at each layer:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt5/images/real_nvp_illustration.png" class="img-fluid figure-img" alt="transforming half of the dimensions as a function of the other half"></p>
<p></p><figcaption class="figure-caption">Figure credit due to <a href="https://blog.evjang.com/2018/01/nf2.html">Eric Jang</a>; he uses the notation <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Cmu"> instead of <img src="https://latex.codecogs.com/png.latex?s"> and <img src="https://latex.codecogs.com/png.latex?t"></figcaption><p></p>
</figure>
</div>
<p>This has a lot of really appealing properties. First, this has a triangular Jacobian:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x%5ET%7D=%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7D%5Cmathbb%7BI%7D_d%20&amp;%200%20%5C%5C%20%5Cfrac%7B%5Cpartial%20y_%7Bd+1:%20D%7D%7D%7B%5Cpartial%20x_%7B1:%20d%7D%5ET%7D%20&amp;%20%5Coperatorname%7Bdiag%7D%5Cleft(%5Cexp%20%5Cleft%5Bs%5Cleft(x_%7B1:%20d%7D%5Cright)%5Cright%5D%5Cright)%5Cend%7Barray%7D%5Cright%5D%0A"></p>
<p>which means that we can really efficiently compute the determinant as <img src="https://latex.codecogs.com/png.latex?%5Cexp%20%5Cleft%5B%5Csum_j%20s%5Cleft(x_%7B1:%20d%7D%5Cright)_j%5Cright%5D">. For a sense of scale, with no specific structure to exploit, calculating the determinant is roughly <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(n%5E3)"> or a little better<sup>7</sup>, but for triangular matrices the same operation takes just <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(n)">; that’s a massive speedup!</p>
<p>Another nice characteristic here is that we don’t need to compute the Jacobian of <img src="https://latex.codecogs.com/png.latex?s"> or <img src="https://latex.codecogs.com/png.latex?t"> in computing the determinant of the above Jacobian, so <img src="https://latex.codecogs.com/png.latex?s"> and <img src="https://latex.codecogs.com/png.latex?t"> are much easier to make quite complex. Contrast this with the planar flow, where we needed to use a specific (tanh) non-linearity, and impose somewhat arbitrary constraints to ensure invertability at all, let alone easy, fast invertability. With a realNVP flow constructed out of many such coupling layers, it’s easy to throw in a lot of improvements that make training large neural networks much more reliable, like batch normalization, weight normalization, and architectures like residual connections.</p>
<p>As a last appealing property here, realize this can be really expressive: by varying at each layer which dimensions <img src="https://latex.codecogs.com/png.latex?d"> are held constant and which are transformed, we can build up quite complex interrelationships between different dimensions over the flow. This can be done simply at random, or perhaps even using structure of the problem to decide how to partition the dimensions. For example, Dinh et al.&nbsp;provide an example on image data where a checkerboard pattern is used to structure the partitions. Kingma and Dhariwal take this further with <a href="https://arxiv.org/abs/1807.03039">Glow (2018)</a>, a flow using 1x1 convolutions. Again, it’s really nice we don’t need the Jacobian of <img src="https://latex.codecogs.com/png.latex?s"> and <img src="https://latex.codecogs.com/png.latex?t">; they can have arbitrarily complex structure and we don’t need pay the computational cost of computing their Jacobians.</p>
<p>It doesn’t add that much intuition to see another flow in code, so I’ll hold off on showing off the implementation of RealNVP for another post or two when I return to fitting our MRP model better using all the tools we’ve built up.</p>
<p>Like I said at the start of this section, there are tons and tons of possible flow structures that get more computationally complex in exchange for expressiveness. RealNVP is a great start though, and for many variational inference problems provides the amount of expressiveness we need. It also illustrates a lot of the core strategy for building flow structures well:</p>
<ol type="1">
<li>Make the log determinant of the Jacobian fast to calculate.</li>
<li>Impose structure such that calculating the log determinant of the Jacobian isn’t entangled with your source of learnable complexity; this allows expressiveness not fitting restrictions to guide what’s implemented.</li>
<li>Leverage tools for scalable, stable neural networks, from batch norm to architecture choices like residual connections to GPU computing speed ups.</li>
</ol>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Let’s take stock of how normalizing flows continue our project of extending vanilla variational inference. Normalizing flows allow us to learn the variational family rather than iterating through a bunch of base densities by hand until one works, and can do so for much more complex posteriors than any of the simple choices like a mean-field or full-rank gaussian we’ve seen so far. This is both a gain of functionality (we can now fit posteriors with VI that we absolutely couldn’t before), and a gain of convenience (the workflow for “make my neural network expressive” is much, much more convenient than the one where the analyst tries to find or make increasingly weird distributions themselves).</p>
<p>Of course, this adds compute time, and a requirement to start understanding neural network implementation choices reasonably well. This isn’t a free lunch- even the simple planar flow on a toy example above took about 20 minutes to fit on my laptop, and having to understand neural nets well to fit a Bayesian model feels kind of silly. Still though, in the telling of review papers like <a href="https://arxiv.org/abs/2103.01085">Dhaka et al.&nbsp;(2021)</a> and <a href="https://arxiv.org/abs/2006.10343">Agrawal et al.&nbsp;(2020)</a>, a basic RealNVP flow is a serious improvement for many complex posterior distributions at fairly palatable run times. This is a pretty good tradeoff for many realistic models, and it’s for that reason that normalizing flows are an increasingly popular part of the variational inference toolbox.</p>
<p>Like with alternative optimization objectives or the various uses of (Pareto smoothed) importance sampling from the last post, normalizing flows give us tools to fit a wider range of models with variational inference, and do so more robustly and conveniently. This can come with it’s own problems, but these trades are often worth it. In the next post, we’ll add a final set of tools to our VI toolbox: robust diagnostics to know if our approximation is good or not.</p>
<p>Thanks for reading. The code for this post can be found <a href="https://github.com/andytimm/andytimm.github.io/blob/main/posts/Variational%20MRP%20Pt5/variational_mrp_pt5.qmd">here</a>.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>It also almost has a bit of “no brain no pain” ML guy energy, in the sense that we’re really pulling out the biggest algorithm possible. It really is a funny trajectory to me to go from “I’d like to still be Bayesian, but avoid MCMC because it’s slow” to “screw subtle design, let’s throw a NN at it”.↩︎</p></li>
<li id="fn2"><p>This is mostly a joke, but it really is a tremendous convenience that there’s such a straight forward knob to turn for “expressivity” in this context. We’ll get into the ways that isn’t completely true soon, but NNs provide fantastic convenience in terms of workflow for improving model flexibility.↩︎</p></li>
<li id="fn3"><p>You can see it in the original Normalizing Flows paper linked above, or combined with a nice matrix calc review by <a href="https://lilianweng.github.io/posts/2018-10-13-flow-models/">Lilian Weng</a>. As a more general note, since this is a common topic on a few different talented people’s blogs, I’ll try to focus on covering material I think I can provide more intuition for, or that are most relevant for variational inference.↩︎</p></li>
<li id="fn4"><p>A great example of this is Lilian Weng’s <a href="https://lilianweng.github.io/posts/2018-10-13-flow-models/">NF walkthrough</a> which I recommended above- It has a fantastic review of the needed linear algebra and covers a lot of different flow types, but is a bit overly general about what properties are most desirable in a flow, and is therefore initially a bit fuzzy on the value different flows have.↩︎</p></li>
<li id="fn5"><p>Deriving precisely how this works would take us too far afield, but see <a href="https://arxiv.org/abs/1908.09257">Kobyzev et al.&nbsp;(2020)</a> if you’re interested. It’s a great review paper that does a lot of work to recognize there are multiple different possible applications of normalizing flows, and thus different notations and framings that they very successfully bridge.↩︎</p></li>
<li id="fn6"><p>Depending on your browser settings you may need to refresh the page here to watch it run.↩︎</p></li>
<li id="fn7"><p>Ok fine, you probably get that down to <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(n%5E%7B2.8...%7D)"> using <a href="https://en.wikipedia.org/wiki/Strassen_algorithm">Strassen</a> which is implemented essentially everywhere that matters.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2023,
  author = {Andy Timm},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2023-06-11},
  url = {https://andytimm.github.io/variational_mrp_pt5.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2023" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2023. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> June 11, 2023. <a href="https://andytimm.github.io/variational_mrp_pt5.html">https://andytimm.github.io/variational_mrp_pt5.html</a>.
</div></div></section></div> ]]></description>
  <category>MRP</category>
  <category>Variational Inference</category>
  <category>Normalizing Flows</category>
  <guid>https://andytimm.github.io/posts/Variational MRP Pt5/variational_mrp_pt5.html</guid>
  <pubDate>Sun, 11 Jun 2023 04:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Variational MRP Pt5/images/32_layer.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Variational Inference for MRP with Reliable Posterior Distributions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Variational MRP Pt4/variational_mrp_pt4.html</link>
  <description><![CDATA[ 




<p>This is section 4 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.</p>
<p>The general structure for this post and the ones after it will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in these four posts will go a long way towards more robust variational inference. I’ll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.</p>
<p>In the <a href="https://andytimm.github.io/posts/Variational%20MRP%20Pt3/variational_mrp_3.html">last post</a> we took a look at how our ELBO objective requires specific version of KL Divergence (the “Exclusive” formulation of KLD), and saw that it encoded a preference for a certain type of solution to the VI problem. Then we looked at CUBO and CHIVI, an alternative bound and algorithm that avoid this problem, often leading to a more useful posterior distribution by pursuing a more “inclusive” solution.</p>
<p>In this post, we’ll leverage importance sampling to make the most of the samples we do have, emphasizing the parts of our <img src="https://latex.codecogs.com/png.latex?q(x)"> that look like <img src="https://latex.codecogs.com/png.latex?p(x)"> and de-emphasizing the parts that do not.</p>
<p>The rough plan for the series is as follows:</p>
<ol type="1">
<li>Introducing the Problem- Why is VI useful, why VI can produce spherical cows</li>
<li>How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li>Problem 1: KL-D prefers exclusive solutions; are there alternatives?</li>
<li><strong>(This post)</strong> Problem 2: Not all VI samples are of equal utility; can we weight them cleverly?</li>
<li>Problem 3: How can we get deeply flexible variational approximations; are Normalizing Flows the answer?</li>
<li>Problem 4: How can we know when VI is wrong? Are there useful error bounds?</li>
<li>Better grounded diagnostics and workflow</li>
</ol>
<section id="not-all-samples-are-equally-good" class="level1">
<h1>Not all samples are equally good</h1>
<p>So we’ve made an approximation <img src="https://latex.codecogs.com/png.latex?q(x)"> that’s cheap to sample from, and is somewhat close to <img src="https://latex.codecogs.com/png.latex?p(x)">, our true posterior. The way to improve the approximation we’ve focused on so far is to just go back to the start and make <img src="https://latex.codecogs.com/png.latex?q(x)"> better; for example, through changing up the variational family, or to switching to a different optimization objective like the CUBO. That’s one solution that’s often necessary, but can we work with a particular <img src="https://latex.codecogs.com/png.latex?q(x)"> we have and make better use of the parts of it that are the closest to being right?</p>
<p>… Phased this way, this sounds a lot like importance sampling. If you haven’t seen them before, an importance sampling estimator allows us to take draws from a (preferably) easy to sample from distribution<sup>1</sup> and reweight the samples to look more like our true target distribution. The weight <img src="https://latex.codecogs.com/png.latex?w_i"> (or ratio, <img src="https://latex.codecogs.com/png.latex?r_i">) for each sample <img src="https://latex.codecogs.com/png.latex?i"> take form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw_i%20=%20%5Cfrac%7Bp(x_i)%7D%7Bq(x_i)%7D%0A"> Before you get worried that we don’t have <img src="https://latex.codecogs.com/png.latex?p(x_i)"> because of the normalizing constant like every time we talk about having <img src="https://latex.codecogs.com/png.latex?p(x)"> in this series, there’s a clever estimator that “self-normalizes” such that this can be a reasonable strategy. Intuitively, we’re just placing more weight on samples more likely under <img src="https://latex.codecogs.com/png.latex?p(x)">.</p>
<p>This footenote<sup>2</sup> has a selection of some of my favorite resources for learning more or refreshing your memory about importance sampling, but for the main discussion let me pull out some particularly important sub-problems to solve in making a good importance sampling estimator, and good important sampling estimator for VI.</p>
<p>First, our choice of the “proposal” distribution we’re reweighting to be more like <img src="https://latex.codecogs.com/png.latex?p(x)"> matters for making this process practically feasible. We need the proposal distribution to be close enough to <img src="https://latex.codecogs.com/png.latex?p(x)"> that a realistic number of the draws get non-negligible weights. It might be true that we could draw proposals from a big <img src="https://latex.codecogs.com/png.latex?N"> dimensional uniform distribution for every problem, but if we want to be done sampling enough this century we need to at least get fairly close with our initial <img src="https://latex.codecogs.com/png.latex?q(x)">.</p>
<p>A second, but related problem is that it’s quite common for the unmodified importance sampling estimator to have some weights which are orders and orders of magnitude higher than the average weight, blowing up the variance of the estimator. Dan Simpson’s slides I linked above has an instructive example with not too weird <img src="https://latex.codecogs.com/png.latex?p(x)"> and <img src="https://latex.codecogs.com/png.latex?q(x)">’s, but high dimension, that has a max weight ~1.4 million (!) times the average. If that happens, our estimator will essentially ignore most samples without gigantic weights, and it’ll take ages for that estimator to tell us anything remotely reliable.</p>
<p>So with those points we need to address, here are the next topics in this post:</p>
<ol type="1">
<li>Importance Weighted Variational Inference</li>
<li>Robust importance sampling with built in diagnostics via Pareto-Smoothed Importance Sampling</li>
<li>Combining multiple proposal distributions via Multiple Importance Sampling</li>
</ol>
<section id="importance-weighted-variational-inference" class="level2">
<h2 class="anchored" data-anchor-id="importance-weighted-variational-inference">Importance Weighted Variational Inference</h2>
<p>Importance Weighting for VI in it’s simplest form is pretty intuitive (draw samples from an already trained <img src="https://latex.codecogs.com/png.latex?q(x)">, weight them…), so let’s derive the new Importance Weighted Variational Inference (IWVI) estimator first since some nice intuition will come with it.</p>
<p>I want to emphasize something that wasn’t clear to me for a good while- these two ideas are not equivalent. While both are useful tools, the “train time”, objective-modifying IWVI estimator is a distinct approach from the “test time” importance sampling approach that takes draws from a fixed <img src="https://latex.codecogs.com/png.latex?q(x)"> and reweights them as best it can.</p>
<p>We’ll aim to show that we can get a tighter ELBO by using importance sampling. This type of tighter ELBO was first shown by <a href="https://arxiv.org/abs/1509.00519">Burda et Al. (2015)</a> in the context of Variational Autoencoders after which is was fairly clear this could apply to variational inference, but <a href="https://arxiv.org/abs/1808.09034">Domke and Sheldon (2018)</a> fleshed out some details of that extension- I’ll be explaining some of the latter group’s main results first.</p>
<p>To start, imagine a random variable <img src="https://latex.codecogs.com/png.latex?R">, such that <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%7BR%7D%20=%20p(x)">, which we’ll think of as a estimator of <img src="https://latex.codecogs.com/png.latex?p(x)">. Then by Jensen’s Inequality:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Alogp(x)%20=%20%5Cmathbb%7BE%7DlogR%20+%20%5Cmathbb%7BE%7Dlog%5Cfrac%7Bp(x)%7D%7BR%7D%0A"></p>
<p>The first term is the bound, which will be tighter if <img src="https://latex.codecogs.com/png.latex?R"> is highly concentrated.</p>
<p>This is a more general form of the ELBO; we can make it quite familiar looking by having our <img src="https://latex.codecogs.com/png.latex?R"> above be:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AR%20=%20%5Cfrac%7Bp(z,x)%7D%7Bq(z)%7D,%20z%20%5Csim%20q%0A"></p>
<p>The reason for pointing out this fairly simple generalization is helpful is that it frames how to tighten our ELBO on <img src="https://latex.codecogs.com/png.latex?logp(x)"> via alternative estimators <img src="https://latex.codecogs.com/png.latex?R">.</p>
<p>By drawing <img src="https://latex.codecogs.com/png.latex?M"> samples and averaging them as in importance sampling, we get:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AR_M%20=%20%5Cfrac%7B1%7D%7BM%7D%5Csum_%7Bm=1%7D%5E%7BM%7D%5Cfrac%7Bp(z_m,x)%7D%7Bq(z_m)%7D,%20z_m%20%5Csim%20q%0A"> From there, we can derive a tighter bound on <img src="https://latex.codecogs.com/png.latex?logp(x)">, referred to as the IW-ELBO:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AIW-ELBO_M%5Bq(z)%7C%7Cp(z,x)%5D%20:=%20%5Cmathbb%7BE%7D_%7Bq(z_%7B1:M%7D)%7Dlog%5Cfrac%7B1%7D%7BM%7D%20%5Csum_%7Bm=1%7D%5E%7BM%7D%5Cfrac%7Bp(z_m,x)%7D%7Bq(z_m)%7D%0A"> Where we’re using the <img src="https://latex.codecogs.com/png.latex?1:M"> as a shorthand for <img src="https://latex.codecogs.com/png.latex?q(z_%7B1:M%7D)%20=%20q(z_1)...q(z_M)">.</p>
<p>It’s worth noting that the last few lines don’t specify a particular form of importance sampling- we’re getting the tighter theoretical bounding behavior from the averaging of samples from <img src="https://latex.codecogs.com/png.latex?q">. We’ll see a particularly good form of importance sampling with desirable practical properties in a moment.</p>
<section id="how-does-iw-elbo-change-the-vi-problem-conceptually" class="level3">
<h3 class="anchored" data-anchor-id="how-does-iw-elbo-change-the-vi-problem-conceptually">How does IW-ELBO change the VI problem conceptually?</h3>
<p>The tighter bound is nice, but importance sampling also has the side effect (done right, side benefit) of modifying our incentives in choosing a variational family. To see what I mean, we can re-use the example distributions from last post we used to build intuition for KL Divergence, where red was the true distribution, and green were our potential approximations. If we’re not going to draw multiple samples and weight them, it makes sense to choose something like the first plot below. Every draw in the middle of the two target modes is expensive per our ELBO objective, so better to choose a mode.</p>
<div class="cell">

</div>
<div class="cell">

</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">rkl_plot <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> mixture <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-2">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"red"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> mode_seeking_kl), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"green"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggtitle</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Without weighting, we prefer to capture a mode"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>)</span>
<span id="cb1-5"></span>
<span id="cb1-6">fkl_plot <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> mixture <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-7">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"red"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-8">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> mean_seeking_kl), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"green"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggtitle</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"With importance sampling, weights allow us to prefer coverage"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-9">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>)</span>
<span id="cb1-10"></span>
<span id="cb1-11"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">grid.arrange</span>(rkl_plot,fkl_plot)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt4/variational_mrp_pt4_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>If we can use importance sampling though, quite the opposite is be true! Note that we’re still using the ELBO, a reverse-KL based metric- that hasn’t changed. What has changed is our ability to mitigate the objective costs of those samples between the two extremes. Via this “train time” implementation of IS, points outside the two target modes will get lower importance weights, and points within the modes will get higher ones, so as long as we’re covering the modes with some reasonable amount of probability mass, and drawing enough samples we can actually do better with the distribution centered between the modes.</p>
<p>To further drive home the point about how a “train time” and “test time” implementations of IS differ, could “test time” IS do this? Not really- because the ability to better minimize the ELBO via sampling requires the IW-ELBO variant and associated training process. If we hard-coded <img src="https://latex.codecogs.com/png.latex?q(x)"> as the green <img src="https://latex.codecogs.com/png.latex?N(9,4)"> shown above, “test time” IS could weight the right samples up to better approximate <img src="https://latex.codecogs.com/png.latex?p(x)">, but it doesn’t fundamentally alter our optimization problem the way the IWVI objective does.</p>
<p>We can also imagine how varying the number of samples might effect optimization. Between <img src="https://latex.codecogs.com/png.latex?S=1"> and “enough draws to get all the benefits of IS”, we can imagine there’s a slow transition from “just stick with 1 mode” and “go with IS”. So it seems like we should be worried about getting the number of samples right, but fortunately as we’ll see in the next section there are great rules of thumb in some variants of IS. We’ll still need to bear the cost of sampling (which gets higher as <img src="https://latex.codecogs.com/png.latex?q(x)"> becomes “further” from <img src="https://latex.codecogs.com/png.latex?p(x)">, as we’ll need more samples to weight into a good approximation), but the cost of sampling for most VI implementations will often be pretty manageable if our proposal distribution is somewhat close to <img src="https://latex.codecogs.com/png.latex?p(x)">.</p>
<p>Another way to think about how importance sampling changes our task with variational inference is to think about what sorts of distributions make sense to have as our variational family, and even which objective might be better given IS. On choice of a variational family, if we’re aiming for coverage, moving towards thicker-tailed distributions like t distributions makes a lot of sense. While we explored the IW-ELBO above to build intuition, there’s no reason not to apply IW to the CUBO and thus CHIVI- this also naturally produces nicely overdispersed distributions which can be importance sampled closer to the true <img src="https://latex.codecogs.com/png.latex?p(x)">. This idea of aiming for a wide proposal to sample from is referred to in the importance sampling literature (eg <a href="https://artowen.su.domains/mc/">Owen, 2013</a>) as “defensive sampling”, with <a href="https://arxiv.org/abs/1808.09034">Domke and Sheldon (2018)</a> exploring the VI connection more fully. For intuition, by ensuring most of <img src="https://latex.codecogs.com/png.latex?p(x)"> is covered by some reasonable mass makes it easier to efficiently get draws that can be weighted into a final posterior, even if the unweighted posterior might be too wide.</p>
</section>
</section>
<section id="solving-our-is-problems-with-pareto-smoothed-importance-sampling" class="level2">
<h2 class="anchored" data-anchor-id="solving-our-is-problems-with-pareto-smoothed-importance-sampling">Solving our IS problems with Pareto-Smoothed Importance Sampling</h2>
<p>As we’ve been talking about importance sampling, we’ve been leaving some of the messier details aside (how many samples to draw, how to deal with the cases when some of the weights get huge, how to know when our proposal distribution is “close” enough).</p>
<p>While the Importance Sampling Literature is huge and there are a lot of possible solutions here, I’ll next introduce <a href="https://arxiv.org/abs/1507.02646">Vehtari et Al. (2015)</a>’s Pareto-Smoothed Importance Sampling. I’m a huge fan of this paper- it’s a really elegant and powerful tool, derived from taking Bayesian principles seriously.</p>
<p>Above, I described a common failure mode for IS estimators, where some weights are orders of magnitude larger than others, with this long right tail of ratios dominating the weighted average and blowing up the variance of the estimator. Pareto-Smoothed Importance Sampling proposes to model those tail values as coming from a Generalized Pareto Distribution, a distribution for describing extreme values, and replace the most extreme weights with modeled (and more stable) values.</p>
<p>For concreteness, let’s introduce a simple 1-D example. We’ll aim to use importance sampling to approximate distributions <span style="color:red;"><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BT%7D(%5Cmu%20=%200,%5Csigma%20=%201,t%20=5">)</span> and <span style="color:blue;"><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D(x_0=%200,%5Cgamma%20=%2010)"></span> with a <span style="color:green;"><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BN%7D(%5Cmu%20=%200,%5Csigma%20=%201">)</span> distribution. If that sounds like the opposite of preferring wide tails on <img src="https://latex.codecogs.com/png.latex?q(x)">’s I described above, you’re right, but using a poor choice here will illustrate some useful properties.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">simulated_data <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tibble</span>(</span>
<span id="cb2-2"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">q_x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>),</span>
<span id="cb2-3"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">manageable_p_x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rt</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>),</span>
<span id="cb2-4"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">unmanageable_p_x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rcauchy</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>),</span>
<span id="cb2-5"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">manageable_ratios =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dt</span>(q_x,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x),</span>
<span id="cb2-6"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">unmanageable_ratios =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dcauchy</span>(q_x,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(q_x)</span>
<span id="cb2-7">)</span>
<span id="cb2-8"></span>
<span id="cb2-9">simulated_data <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb2-10">            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pivot_longer</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(q_x,manageable_p_x,unmanageable_p_x),</span>
<span id="cb2-11">                         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">values_to =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"draws"</span>,</span>
<span id="cb2-12">                         <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">names_to =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"distributions"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb2-13">            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> draws, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> distributions)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-14">            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-15">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># If you wanted to show the full reach of the Cauchy, it'd be</span></span>
<span id="cb2-16">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># hard to see the shape of the T vs N; it's that wide.</span></span>
<span id="cb2-17">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Hence the 6k values removed</span></span>
<span id="cb2-18">            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlim</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-19">            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggtitle</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Visualizing the distributions in question"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-20">            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">legend.position=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"none"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Removed 6245 rows containing non-finite values (stat_density).</code></pre>
</div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt4/variational_mrp_pt4_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The tails on that Cauchy distribution are super, super wide compared to our normal, so the samples far, far out in the tails of the normal will need massive weights to approximate the cauchy. The t-distribution is wider too, so we’ll need some higher weights, but not nearly as many. As a way to visualize this, you can see that just a handful of draws have weights away from ~1, but these weights are as much as 5000x higher than the mean ratio, and will dominate any average we make of them.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">simulated_data <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-2">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">arrange</span>(unmanageable_ratios) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">n =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">seq</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> n,<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> unmanageable_ratios)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-5">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_point</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-6">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggtitle</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"A pretty typical 'unsaveable' set of importance ratios"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt4/variational_mrp_pt4_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The t-distribution ratio plot would look similar, but with a much smaller y-scale. The max weight would still be much larger than the average, but more than an order of magnitude or so less large:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">mean_t <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mean</span>(simulated_data<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>manageable_ratios)</span>
<span id="cb5-2">max_t <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">max</span>(simulated_data<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>manageable_ratios)</span>
<span id="cb5-3"></span>
<span id="cb5-4">mean_c <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mean</span>(simulated_data<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>unmanageable_ratios)</span>
<span id="cb5-5">max_c <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">max</span>(simulated_data<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>unmanageable_ratios)</span>
<span id="cb5-6"></span>
<span id="cb5-7"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">print</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">paste0</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"the mean of the t is: "</span>,mean_t,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" compared to a max of "</span>,max_t,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">";"</span>,</span>
<span id="cb5-8">             <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The cauchy cause is more extreme- the mean of the cauchy is: "</span>,mean_c,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" compared to a max of "</span>,max_c))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "the mean of the t is: 0.995904306317625 compared to a max of 164.448932152079;The cauchy cause is more extreme- the mean of the cauchy is: 0.283655933763642 compared to a max of 1428.22324178729"</code></pre>
</div>
</div>
<p>So let’s bring this back to Pareto smoothing here. We want to model and smooth that long tail of the ratio distribution. It turns out there’s plenty of study of the distribution of extreme events, and there’s some classical limit results showing:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ar(%5Ctheta)%20%7C%20r(%5Ctheta)%20%3E%20%5Ctau%20%5Crightarrow%20GPD(%5Ctau,%5Csigma,k),%20%5Ctau%20%5Crightarrow%20%5Cinfty%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Ctau"> is a lower bound parameter, which in our case defines how many ratios from the tail we’ll actually model. <img src="https://latex.codecogs.com/png.latex?%5Csigma"> is a scale parameter, and <img src="https://latex.codecogs.com/png.latex?k"> is a unconstrained shape parameter. Without getting too far into the weeds, we can implicitly define <img src="https://latex.codecogs.com/png.latex?%5Ctau"> via using a well-supported role of thumb suggesting to use the M largest ratios, <img src="https://latex.codecogs.com/png.latex?M%20=%20min(0.2S,3%5Csqrt%7BS%7D)"><sup>3</sup>. From there, the <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D"> have easy and efficient estimators. The Generalized Pareto Distribution has form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7B%5Csigma%7D%20%5Cleft(1%20+%20k%5Cfrac%7Br%20-%20%5Ctau%7D%7B%5Csigma%7D%20%5Cright)%5E%7B-1/k-1%7D%0A"></p>
<p>and we can replace our M biggest ratios with estimated values calculated via the CDF of the Generalized Pareto Distribution.</p>
<p>One of the best things about PSIS is it comes with a built in diagnostic via <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D">. To see how this works, it’s useful to know that importance sampling depends on how many moments <img src="https://latex.codecogs.com/png.latex?r(%5Ctheta)"> has- for example, if at least two moments exist, the vanilla IS estimator has finite variance (which is obviously required, but no guarantee of performance since it might be finite but massive). The GPD has <img src="https://latex.codecogs.com/png.latex?k%5E%7B-1%7D"> finite fractional moments when <img src="https://latex.codecogs.com/png.latex?k%20%3E%200">.</p>
<p>Vehtari et al.&nbsp;show that the replacement of the largest M ratios above changes PSIS to have finite variance and an error distribution converging to normal when <img src="https://latex.codecogs.com/png.latex?k%20%5Cin%20(.5,1)">. Intuitively, <img src="https://latex.codecogs.com/png.latex?k%20%3E%20.5"> implies the raw ratios have infinite variance, but PSIS trades a little bias to make the variance finite again.</p>
<p>What about actually practical to work with variance? This is the really cool bit- <img src="https://latex.codecogs.com/png.latex?k%20%3C%20.7"> turns out to be a remarkably robust indicator of when we can expect PSIS to work in a ton of different simulation studies and practical examples.</p>
<p>Why is this true? 1.4 fractional moments seems awful arbitrary, right? Let’s ask an alternative question, and kill two birds with one stone: what sample size do we need for PSIS to work? <a href="https://arxiv.org/abs/1511.01437">Chaterjee and Draconis (2018)</a> showed that for a given accuracy, how big <img src="https://latex.codecogs.com/png.latex?S"> needs to be for importance sampling more broadly depends on how close <img src="https://latex.codecogs.com/png.latex?q(x)"> is to <img src="https://latex.codecogs.com/png.latex?p(x)"> in KL distance- we need to satisfy <img src="https://latex.codecogs.com/png.latex?log(S)%20%5Cgeq%20%5Cmathbb%7BE%7D_%7B%5Ctheta%20%5Csim%20q(x)%7D%5Br(%5Ctheta)log(r(%5Ctheta))%5D"> to get accuracy.</p>
<p>Well, we don’t know the distribution of <img src="https://latex.codecogs.com/png.latex?r">, we should have some pretty good intuition that the important part (read: that explosive, variance ruining tail) is Pareto. If we take <img src="https://latex.codecogs.com/png.latex?r"> as exactly Pareto, you can trace out <img src="https://latex.codecogs.com/png.latex?S"> for different <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"><sup>4</sup>, and to give a few example points-</p>
<table class="table">
<caption>for given <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D">, roughly what <img src="https://latex.codecogs.com/png.latex?S"> is needed if <img src="https://latex.codecogs.com/png.latex?r"> is exactly Pareto</caption>
<colgroup>
<col style="width: 75%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"></th>
<th><img src="https://latex.codecogs.com/png.latex?S"> needed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>.5</td>
<td>~1,000</td>
</tr>
<tr class="even">
<td>.7</td>
<td>~140,000</td>
</tr>
<tr class="odd">
<td>.8</td>
<td>1,000,000,000,000</td>
</tr>
<tr class="even">
<td>.9</td>
<td>please stop you’re making your compute sad.</td>
</tr>
</tbody>
</table>
<p>While we of course know <img src="https://latex.codecogs.com/png.latex?r"> isn’t Pareto exactly exactly, hopefully this helps with intuition around <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> telling us when we’re getting into “sampling forever to have any chance at all to control the variance” land.</p>
<p>Neat! So what does that look like for our Cauchy and T distribution example?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1">manageable_psis <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">psis</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(simulated_data<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>manageable_ratios),</span>
<span id="cb7-2">                       <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">r_eff =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span>)</span>
<span id="cb7-3"></span>
<span id="cb7-4">unmanageable_psis <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">psis</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(simulated_data<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>unmanageable_ratios),</span>
<span id="cb7-5">                          <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">r_eff =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span>)</span>
<span id="cb7-6"></span>
<span id="cb7-7">manageable_psis<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>diagnostics</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$pareto_k
[1] 0.5963495

$n_eff
[1] 62963.93</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1">unmanageable_psis<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>diagnostics</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$pareto_k
[1] 0.8533127

$n_eff
[1] 249.2052</code></pre>
</div>
</div>
<p>As we expected, the the Normal proposal distribution isn’t ideal for the T distribution, but it’s manageable. On the other hand, we’d need somewhere between <strong>a trillion and “oh god no :(”</strong> samples to make the normal proposal work out for the Cauchy.</p>
<p>Bringing the discussion back to variational inference, PSIS is super helpful- importance sampling more generally broadens the class of <img src="https://latex.codecogs.com/png.latex?q(x)">es that are close enough to <img src="https://latex.codecogs.com/png.latex?p(x)"> for variational inference to work, and PSIS considerably widens that basin of feasibility. The extensive theoretical and simulation framework around the method also give us a solid way to realize when importance sampling isn’t feasible via the <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> diagnostic, and tells us how roughly samples we need to draw. Super, super cool.</p>
<p>One more great thing PSIS does for variational inference- <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> serves as a powerful diagnostic for variational inference itself! I’ll save most of this discussion for the post on diagnostics, but to sketch out the logic- <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> tells us when <img src="https://latex.codecogs.com/png.latex?q(x)"> is too far from <img src="https://latex.codecogs.com/png.latex?p(x)"> for importance sampling to work, which is a function of KL Divergence from <img src="https://latex.codecogs.com/png.latex?q(x)"> to <img src="https://latex.codecogs.com/png.latex?p(x)">- if that distance is too great for importance sampling to allow us to bridge, that implies we aren’t close enough to trust our base variational approximation either!</p>
</section>
<section id="multiple-proposal-distributions-with-multiple-importance-sampling" class="level2">
<h2 class="anchored" data-anchor-id="multiple-proposal-distributions-with-multiple-importance-sampling">Multiple Proposal Distributions with Multiple Importance Sampling</h2>
<p>Why stop at just one proposal distribution? This is basically the jumping off point for Multiple Importance sampling, or MIS. If we have several different <img src="https://latex.codecogs.com/png.latex?q(x)">, and each does a somewhat better job of handling a certain region of the target posterior, then we can efficiently combine them using MIS into an overall better final estimate, and this will work out to be pretty obviously more optimal than just fitting a bunch of VI approximations and averaging them.</p>
<p>If we can suddenly have multiple different <img src="https://latex.codecogs.com/png.latex?q(x)"> working together, this naturally explodes the search space for a good VI strategy. I’d refer the more interested reader to <a href="https://projecteuclid.org/journals/statistical-science/volume-34/issue-1/Generalized-Multiple-Importance-Sampling/10.1214/18-STS668.full">Elvira et al.&nbsp;(2019)</a> which lays out a framework for thinking about all the decision space of MIS more comprehensively, but for the purposes of improving VI specifically, I’ll cover:</p>
<ol type="1">
<li>How do we weight the proposals together?</li>
<li>Which proposals make sense to include in a MIS framework?</li>
<li>How practical is fitting multiple proposals?</li>
</ol>
<section id="how-do-mis-weights-work" class="level3">
<h3 class="anchored" data-anchor-id="how-do-mis-weights-work">How do MIS weights work?</h3>
<p>How do we generalize a notion of importance weights like the one introduced above:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw_i%20=%20%5Cfrac%7Bp(x_i)%7D%7Bq(x_i)%7D%0A"></p>
<p>to multiple proposals? While there are some obviously not good properties we want to avoid (it’d be pretty silly to give up our unbiasedness), there are a ton of apparent degrees of freedom in MIS weighting. we’ll relax this assumption in a bit, but let’s start by assuming we don’t have any prior information about which proposals might be better, and that we’ll draw the same number of samples from each proposal.</p>
<p>While I won’t work through as extensive of an example as in the last section, let’s fix an example where we’ll have <img src="https://latex.codecogs.com/png.latex?J%20=%203"> different proposals, <span style="color:cyan;"><img src="https://latex.codecogs.com/png.latex?q_1(x)">)</span>, <span style="color:purple;"><img src="https://latex.codecogs.com/png.latex?q_2(x)"></span>, and <span style="color:pink;"><img src="https://latex.codecogs.com/png.latex?q_3(x)">)</span>.</p>
<p>A first question is how to choose the denominator in the weight. One simple and efficient option is to simply use the density of a sample from <img src="https://latex.codecogs.com/png.latex?j"> to make a weight, for example weighting a draw from <span style="color:pink;"><img src="https://latex.codecogs.com/png.latex?q_3(x)">)</span> as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw_%7Bi%7D%20=%20%5Cfrac%7Bp(x_i)%7D%7B%5Ctextcolor%7Bpink%7D%7Bq_3(x_i)%7D%7D%0A"> This works, and is pretty common in MIS applications, but we’re not really using all the information we have from having several proposals. We can get a provably lower variance estimator by defining the mixture of the densities <img src="https://latex.codecogs.com/png.latex?%5Cpsi(x)"> as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpsi(x)%20=%20%5Cfrac%7B1%7D%7BJ%7D%20%5Csum%5Climits_%7BJ%20=%201%7D%5Climits%5E%7BJ%7D%20q_j(x)%0A"></p>
<p>and using that as the denominator. So for the example above, this’d be:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw_%7Bi%7D%20=%20%5Cfrac%7Bp(x_i)%7D%7B%5Cfrac%7B1%7D%7B3%7D(%5Ctextcolor%7Bcyan%7D%7Bq_1(x)%7D%20+%20%5Ctextcolor%7Bpurple%7D%7Bq_2(x_i)%7D%20+%20%5Ctextcolor%7Bpink%7D%7Bq_3(x_i))%7D%7D%0A"> By defining this mixture and and incorporating it into our weighting, we intuitively should have more efficient exchange of information between the different <img src="https://latex.codecogs.com/png.latex?q(x)">. By this, I mean that we no longer just are weighting each sample from a proposal using information from that one proposal; we’re now using everything at hand.</p>
<p>This feels like it should be pretty solidly better than just using a single proposal density, and indeed Elvira et al.&nbsp;have a result showing that the variance of the mixture based weighting scheme is under pretty general conditions lesser than or equal to that of the single proposal density one<sup>5</sup>.</p>
<p><a href="https://dl.acm.org/doi/10.1145/218380.218498">Veach and Guibas (1995)</a>, the paper to introduce MIS, called this weighting scheme the <em>balance heuristic</em>, since the weighting scheme is unique in that each sample value at particular <img src="https://latex.codecogs.com/png.latex?x"> is the same regardless of which distribution produced it. They also prove a bound on the variance of this estimator, showing that there isn’t a lot of room to improve on it, even in the most ideal circumstances. Without getting into the weeds, their result suggests that there isn’t a massively better general-case weighting scheme, which is a helpful guide to practical use.</p>
<p>When can we do (a bit) better than the weighting scheme above? The answer is essentially in cases where we know some of our <img src="https://latex.codecogs.com/png.latex?J"> proposals are much better than others. In these situations, the variance can often be lowered by pushing weights towards the extremes, making low weights closer to zero, and high weights closer to 1. Their <em>cutoff heuristic</em> suggests an estimator where you pick some bound <img src="https://latex.codecogs.com/png.latex?%5Calpha">, below which low weights are reassigned to zero (and the rest of the distribution is adjusted back to sum correctly). Their also propose the <em>power heuristic</em>-</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw_i%20=%20%5Cfrac%7Bp_i%5E%5Cbeta%7D%7B%5Csum%5Climits_%7Bj%7Dp_j%5E%5Cbeta%7D%0A"> which raises the weights to a power <img src="https://latex.codecogs.com/png.latex?%5Cbeta">, and normalizes. For intuition, notice that if <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20=%201">, then this is the <em>balance heuristic</em> again, and as <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20%5Crightarrow%20%5Cinfty">, this moves towards only selecting the best proposal at each point.</p>
<p>As a final note, we can also Pareto Smooth any of these types of weights once we have them, and this sparks joy, as we can add begin to envision model setups with glorious abbreviations like IW-ELBO/IW-CUBO-PSIS-MIS-VI.</p>
<p>So stepping back, we have some provably efficient, provably hard to beat ways to use MIS to combine variational approximations together. Again, there’s a whole literature on MIS which the Elvira paper above reviews, but fairly intuitive weighting schemes exist that work well in most cases, and there are reasonable things to try in more atypical cases to reduce the variance of the MIS estimator as well.</p>
</section>
<section id="what-proposals-combine-best" class="level3">
<h3 class="anchored" data-anchor-id="what-proposals-combine-best">What proposals combine best?</h3>
<p>A next natural question is what different proposals should we use? There’s a little less work in this area than I expected, but there are a couple of papers; my favorite is <a href="https://arxiv.org/abs/2002.07217">Lopez et al.&nbsp;(2020)</a><sup>6</sup>.</p>
<p>They find that using VI approximations based on different objectives is quite performant- for example, having all of a vanilla ELBO, IW-ELBO and <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> divergence based VI approximation works particularly well, and as you’d expect, better than any individual model, just like we’d expect with regular ensembling techniques. They also look at taking some samples directly from our priors, which is moderately surprising to me given how broad weakly-informative priors usually are. Overall though, a core nugget of logic from ensembling more generally applies here too: we want to find proposals that are both good and sufficiently different from one another that combining them adds value.</p>
<p>It seems to me there’s a lot of room to explore this search space still; there are a lot of generic ML ensembling tricks that feel like they could work. For example, could we save state several times throughout optimizing a variational approximation, and MIS combine samples from each of those, similar to how people cheaply ensemble for neural networks? Or are there ways to optimize the proposals for use together in this way?</p>
</section>
<section id="how-practical-is-mis-for-vi" class="level3">
<h3 class="anchored" data-anchor-id="how-practical-is-mis-for-vi">How practical is MIS for VI?</h3>
<p>A last obvious question is whether fitting many variational approximations and combining them is computationally practical. While MIS for VIS certainly trades back some computational cost and time for potential accuracy, the good news is everything feels cheap compared to MCMC.</p>
<p>Fitting <img src="https://latex.codecogs.com/png.latex?J"> VI approximations instead of 1 roughly scales your compute need for fitting the models by a factor of ~<img src="https://latex.codecogs.com/png.latex?J">, and then there’s a small additional cost in the MIS combination stage to evaluate all the models to make each importance sampling weight denominator. Unlike with MCMC, these computational needs are parallelizable.</p>
<p>Lopez et al.&nbsp;(2020) find that using 3 proposals slightly more than triples their compute cost given all the objective based models take around the same time to fit, and in practice slightly more than triples their compute time as well since they didn’t do the work to parallelize their models. On the problems they were working on, this is a pretty small (~30s more) time cost in exchange for a meaningful accuracy improvement in the real world biology application they apply this to.</p>
<p>Depending on what you’re working on, the answer may well be yes, this can be computationally feasible and well worth it.</p>
</section>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>Importance Sampling is a workhorse of modern computational statistics, and it should be no surprise it brings a lot to variational inference.</p>
<p>Like with the last post, my overall impression is of decreasing fragility for variational inference and a broader set of tools for increasing performance. With IW-ELBO and similar objectives, we can get a tighter bound than the vanilla ELBO, and introduce some new incentives in training our approximation as well. With importance sampling in general and PSIS especially, we can weight an approximation that is close to the target but not perfect into a much, much better approximation of our posterior, and do some in a principled and theoretically grounded way with built-in diagnostics. With MIS, we can make the most of several approximations at once, if we’re willing to pay that computational cost. Collectively, we’re building up a set of tools that broaden the class of problems for which VI works, provided you’re willing to spend time searching for a combination of tools that works well for your specific application.</p>
<p>Thanks for reading. In the next post, we’ll look at Normalizing Flows, an incredibly powerful and general tool for making maximally flexible variational distributions. All code for this post can be found <a href="https://github.com/andytimm/andytimm.github.io/blob/main/posts/Variational%20MRP%20Pt4/variational_mrp_pt4.qmd">here</a>.</p>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>we’ll call it q(x) here to make the application super clear, but often I see the “proposal” distribution called g(x) and the the distribution we want to approximate called p(x). Another common notation would be <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)"> for the target and <img src="https://latex.codecogs.com/png.latex?q(x)"> for the proposal. It’s also helpful to know that the computer graphics (as in, image rendering) community is the source of a lot of work especially around Multiple Importance Sampling since they need to solve lots of light transport integrals, and they have yet another set of conventions from most statisticians, but you can usually figure out their choices by squinting a bit.↩︎</p></li>
<li id="fn2"><p>If you’re looking to learn about importance sampling for the first time, a great place to start is Ben Lambert’s video introductions to the basic idea: <a href="https://www.youtube.com/watch?v=V8f8ueBc9sY">video 1</a>, and <a href="https://www.youtube.com/watch?v=F5PdIQxMA28">video 2</a>. For building more intuition about why we need all these variance reducing modifications to general IS, Dan Simpson has some great <a href="https://dpsimpson.github.io/pages/talks/Importance_sampling_unsw_2019.pdf">slides</a> which have a side benefit of being hilarious. Those slides will mention a lot of the books/papers I find most instructive, but it’s worth calling out especially Vehtari et Al’s Pareto Smoothed Importance Sampling <a href="https://arxiv.org/abs/1507.02646">paper</a> as particularly well written and paradigm shaping. Finally, Elvira et Al’s (2019) Multiple Importance Sampling <a href="https://projecteuclid.org/journals/statistical-science/volume-34/issue-1/Generalized-Multiple-Importance-Sampling/10.1214/18-STS668.full">paper</a> is the most thorough I know, but isn’t particularly approachable. Instead, for MIS I’d recommend starting with the first few minutes of <a href="https://www.youtube.com/watch?v=dxFSwplfdpk">this talk</a> (although the main topic of their talk is less relevant, the visualizations are super helpful), and the first ~8 pages of <a href="https://arxiv.org/pdf/2102.05407.pdf">this paper</a>, also by Elvira et Al. (2021) (I especially like that it spends a bit more time on notation; since multiple importance sampling comes from/comes up in computer graphics, the notational choices sometimes feel a bit annoying to me). Finally, the <a href="https://dl.acm.org/doi/10.1145/218380.218498">original MIS paper itself</a>, Veach &amp; Guibas (1995) is quite readable, but requires a bit of reading around or reading into computer graphics to grok their examples and notational choices.↩︎</p></li>
<li id="fn3"><p>Slight more weeds here- it turns out that this idea to use Vehtari et al.’s rule of thumb for selecting M and getting <img src="https://latex.codecogs.com/png.latex?%5Ctau"> from there is fairly important. The GPD Approximation is pretty sensitive to getting <img src="https://latex.codecogs.com/png.latex?%5Ctau"> right- it’ll be poor if <img src="https://latex.codecogs.com/png.latex?%5Ctau"> is too low. Having a deterministic rule of thumb that preforms better than alternative more complicated schemes for estimating <img src="https://latex.codecogs.com/png.latex?%5Ctau"> is great, and they work through showing it works well in most reasonable cases.↩︎</p></li>
<li id="fn4"><p>I’ll be lazy here and not derive or plot this- you can see the plot in Dan Simpson’s slides mentioned above.↩︎</p></li>
<li id="fn5"><p>One fascinating caveat here is that they proved this only for the case where we know the normalizing constant, not the self-normalized case we pretty much always have to live with, but they have some numerical results and some pretty common sense arguments that the result should extend in most reasonable cases to SNIS as well.↩︎</p></li>
<li id="fn6"><p>Worth noting that one of the authors here is Michael l. Jordan, which is a pretty good heuristic for “this will be a banger of a stats paper”.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2023,
  author = {Andy Timm},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2023-05-27},
  url = {https://andytimm.github.io/variational_mrp_pt4.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2023" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2023. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> May 27, 2023. <a href="https://andytimm.github.io/variational_mrp_pt4.html">https://andytimm.github.io/variational_mrp_pt4.html</a>.
</div></div></section></div> ]]></description>
  <category>MRP</category>
  <category>Variational Inference</category>
  <guid>https://andytimm.github.io/posts/Variational MRP Pt4/variational_mrp_pt4.html</guid>
  <pubDate>Sat, 27 May 2023 04:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Variational MRP Pt4/images/importance-weights-preview.png" medium="image" type="image/png" height="72" width="144"/>
</item>
<item>
  <title>Variational Inference for MRP with Reliable Posterior Distributions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Variational MRP Pt3/variational_mrp_3.html</link>
  <description><![CDATA[ 




<p><strong>Note:</strong> I’ve gotten a lot more pessimistic about how generally useful the alternatives to simple KL-Divergence are on their own since writing this post. I still think these are really useful ideas to think to build intuition about VI, and techniques like CHIVI are useful for some lower dimensional problems or as part of an ensemble of techniques for high dimensional ones. However, <a href="https://arxiv.org/abs/2103.01085">this paper</a> from Dhaka et al.&nbsp;is very convincing that CHIVI and currently available similar algorithms are in practice very hard to optimize for high dimensional posteriors, and that some of the intuitive benefits shown about CHIVI below in low dimensions don’t really generalize the way we’d expect to higher dimensions.</p>
<hr>
<p>This is section 3 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.</p>
<p>In the <a href="https://andytimm.github.io/posts/Variational%20MRP%20Pt2/Variational_MRP_pt2.html">last post</a> we threw caution to the wind, and tried out some simple variational inference implementations, to build up some intuition about what bad VI might look like. Just pulling a simple variational inference implementation off the shelf and whacking run perhaps unsurprisingly produced dubious models, so in this post we’ll bring in long overdue theory to understand why VI is so difficult, and what we can do about it.</p>
<p>The general structure for the next couple of posts will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in the next three posts will go a long way towards more robust variational inference. I’ll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.</p>
<p>The rough plan for the series is as follows:</p>
<ol type="1">
<li>Introducing the Problem- Why is VI useful, why VI can produce spherical cows</li>
<li>How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li><strong>(This post)</strong> Problem 1: KL-D prefers exclusive solutions; are there alternatives?</li>
<li>Problem 2: Not all samples are of equal utility; can we weight them cleverly?</li>
<li>Problem 3: How can we know when VI is wrong? Are there useful error bounds?</li>
<li>Better grounded diagnostics and workflow</li>
<li>Seeing if some more sophisticated techniques like normalizing flows add much</li>
</ol>
<section id="inclusive-versus-exclusive-kl-divergence" class="level1">
<h1>Inclusive versus Exclusive KL-divergence</h1>
<p>Like I mentioned in the first post in the series, the Evidence Lower Bound (ELBO), our optimization objective we’re working with is a tractable approximation of the Kullback-Leibler Divergence between our choice of approximating distribution <img src="https://latex.codecogs.com/png.latex?q(z)"> to our true posterior <img src="https://latex.codecogs.com/png.latex?p(z)">.</p>
<p>The KL divergence is asymmetric: in general, <img src="https://latex.codecogs.com/png.latex?KL(p%7C%7Cq)%20%5Cneq%20KL(q%7C%7Cp)">. Previously, we saw that this asymmetry mattered quite a bit for our ELBO idea:</p>
<p><img src="https://latex.codecogs.com/png.latex?argmin_%7Bq(z)%20%5Cin%20%5Cmathscr%7BQ%7D%7D(q(z)%7C%7C%5Cfrac%7Bp(z,x)%7D%7B%5Cbf%20p(x)%7D)%20=%20%5Cmathbb%7BE%7D%5Blogq(z)%5D%20-%20%5Cmathbb%7BE%7D%5Blogp(z,x)%5D%20+%20%7B%5Cbf%20logp(x)%7D"> We can’t calculate the bolded term <img src="https://latex.codecogs.com/png.latex?logp(x)">; if we could we wouldn’t be finding this inference thing so hard in the first place. The way we sidestepped that with the ELBO is to note that the term is constant with respect to <img src="https://latex.codecogs.com/png.latex?q">; so we can go on our merry way minimizing the above without it.</p>
<p>If we flip the divergence around though, we’ve got an issue. That term would then be a <img src="https://latex.codecogs.com/png.latex?logq(x)"> … which we can’t write off in the same way- it varies as we optimize. So if we’re doing this ELBO minimizing version of variational inference, we’re obligated to use this “reverse” KL divergence, the second option below.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AKL(p%7C%7Cq)%20=%20%5Csum_%7Bx%20%5Cin%20X%7D%7Bp(x)%7Dlog%5B%5Cfrac%7Bp(x)%7D%7Bq(x)%7D%5D%20%20%5C%5C%0AKL(q%7C%7Cp)%20=%20%5Csum_%7Bx%20%5Cin%20X%7D%7Bq(x)%7Dlog%5B%5Cfrac%7Bq(x)%7D%7Bp(x)%7D%5D%0A%5Cend%7Balign%7D%0A"></p>
<p>Unfortunately, this choice to optimize the “reverse” KL divergence bakes in preference for a certain type of solution<sup>1</sup>.</p>
<p>I found I built better intuition for this encoded preference after seeing it presented many different overlapping ways, so here are a few of my favorites.</p>
<p>One way to see the difference is through a variety of labels for each direction. One could call Forward KL (1) vs.&nbsp;Reverse KL (2):</p>
<ol type="1">
<li>Inclusive vs.&nbsp;Exclusive (my favorite, and so what I’m using for the section header)</li>
<li>Mean Seeking vs.&nbsp;Mode Seeking</li>
<li>Zero Avoiding vs.&nbsp;Zero Forcing</li>
</ol>
<p>let’s quickly sketch what this might look like in the case of a simple mixture of normals with a single normal as a variational family:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(ggplot2)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'ggplot2' was built under R version 4.2.3</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(gridExtra)</span>
<span id="cb3-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(tidyverse)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'tidyverse' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'tibble' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'tidyr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'readr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'purrr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'dplyr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'stringr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'forcats' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'lubridate' was built under R version 4.2.3</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1">mixture <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">normals =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)),</span>
<span id="cb13-2">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">mode_seeking_kl =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2000</span>,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>),</span>
<span id="cb13-3">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">mean_seeking_kl =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2000</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>))</span>
<span id="cb13-4"></span>
<span id="cb13-5">rkl_plot <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> mixture <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb13-6">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"red"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb13-7">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> mode_seeking_kl), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"green"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggtitle</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Exclusive KL"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb13-8">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>)</span>
<span id="cb13-9"></span>
<span id="cb13-10">fkl_plot <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> mixture <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb13-11">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> normals), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"red"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb13-12">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_density</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> mean_seeking_kl), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"green"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggtitle</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Inclusive KL"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb13-13">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>)</span>
<span id="cb13-14"></span>
<span id="cb13-15"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">grid.arrange</span>(rkl_plot,fkl_plot)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt3/variational_mrp_3_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>To approximate the same exact red distribution <img src="https://latex.codecogs.com/png.latex?p(x)">, Inclusive KL (1) and Exclusive KL (2) would optimize the green <img src="https://latex.codecogs.com/png.latex?q(p)"> in quite different manner.</p>
<p>To spell out the ways to describe this above: Inclusive KL will try to cover all the probability mass in <img src="https://latex.codecogs.com/png.latex?p(x)">, even if it means a peak at a unfortunate middle ground. Exclusive KL, on the other hand, will try to concentrate it’s mass on the largest mode, even if it means missing much of the mixture of normals. Alternatively, we could describe the top graph as mode seeking, and the bottom as mean seeking. Finally, we could say the top graph shows “Zero Forcing” behavior- it will heavily favor putting zero mass on some parts of the graph to avoid any weight where <img src="https://latex.codecogs.com/png.latex?p(x)"> has no mass, even if it means missing an entire mode. Conversely, Inclusive KL will aim to cover all the mass of <img src="https://latex.codecogs.com/png.latex?p(x)"> in full even if the result is an odd solution, in order to avoid having zero mass where <img src="https://latex.codecogs.com/png.latex?p(x)"> has some.</p>
<p>How does this follow from the form of the divergence?</p>
<p>To start with, notice that for inclusive KL we could think of the <img src="https://latex.codecogs.com/png.latex?log(%5Cfrac%7Bp(x)%7D%7Bq(x)%7D)"> part of the term being weighted by <img src="https://latex.codecogs.com/png.latex?p(x)">- if in some range of <img src="https://latex.codecogs.com/png.latex?x"> <img src="https://latex.codecogs.com/png.latex?p(x)"> is 0, we don’t pay a penalty if <img src="https://latex.codecogs.com/png.latex?q(x)"> puts mass. The reverse is not true however- if our <img src="https://latex.codecogs.com/png.latex?q(x)"> is zero where there should be mass in our true distribution, our Inclusive KL divergence is infinite<sup>2</sup>.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AKL(p%7C%7Cq)%20=%20%5Csum_%7Bx%20%5Cin%20X%7D%7Bp(x)%7Dlog%5B%5Cfrac%7Bp(x)%7D%7Bq(x)%7D%5D%20%20%5C%5C%0AKL(q%7C%7Cp)%20=%20%5Csum_%7Bx%20%5Cin%20X%7D%7Bq(x)%7Dlog%5B%5Cfrac%7Bq(x)%7D%7Bp(x)%7D%5D%0A%5Cend%7Balign%7D%0A"></p>
<p>And if we change the direction of the divergence, the opposite zeros and infinities show up, enforcing strong preferences for a specific type of solution.</p>
<p>When the example is a simple mix of two gaussians approximated with a single gaussian, it’s fairly easy to intuit how the choice of KL divergence will influence the optimization solution. This all gets a bit more opaque on harder problems- like we saw with the example last post, ELBO based VI will tend to underestimate the support of <img src="https://latex.codecogs.com/png.latex?p(x)"> but whether the solution is narrow but overall reasonable, or pretty much degenerate, is hard to predict. However, this exploration of how the form of the divergence influences the results still gives a rough intuition for why our ELBO optimized posteriors might collapse.</p>
<p>If we want to try the opposite direction of KL divergence, it isn’t immediately obvious there’s a global objective we can choose that favors overdispersed solutions. Like I mentioned above, if we try to make an ELBO-esque target but reverse the KL divergence, the <img src="https://latex.codecogs.com/png.latex?logp(x)"> which is constant with respect to the <img src="https://latex.codecogs.com/png.latex?q(x)"> we’re optimizing becomes a <img src="https://latex.codecogs.com/png.latex?logq(x)"> which we can’t so easily work around.</p>
<p>Let’s look first at a solution in the spirit of VI<sup>3</sup> to the above problem which requires us to pick up a new divergence, the <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E%7B2%7D">-divergence, and optimizes a new bound. Let’s take a look at it.</p>
<section id="chi2-variational-inference-chivi-and-the-cubo-bound" class="level2">
<h2 class="anchored" data-anchor-id="chi2-variational-inference-chivi-and-the-cubo-bound"><img src="https://latex.codecogs.com/png.latex?%5Cchi%5E%7B2%7D"> Variational Inference (CHIVI) and the CUBO bound</h2>
<p>The <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E%7B2%7D">-divergence has form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AD_%7B%5Cchi%5E2%7D(p%7C%7Cq)%20=%20%5Cmathbb%7BE%7D_%7Bq(z;%5Clambda)%7D%5B(%5Cfrac%7Bp(z%7Cx)%7D%7Bq(z;%5Clambda)%7D)%5E2%20-1%5D%0A"> For simplicity and comparability, I’m switching here to using <a href="https://arxiv.org/abs/1611.00328">Dieng et Al. (2017)</a>’s notation here- they use <img src="https://latex.codecogs.com/png.latex?q(z;%5Clambda)"> to refer to the variational family we’re using, indexed by parameters <img src="https://latex.codecogs.com/png.latex?%5Clambda">.</p>
<p>This divergence has the properties we wanted when we tried to use Inclusive KL Divergence- it tends to be mean seeking instead of mode seeking.</p>
<p>Like with the ELBO, we need to show that we have a bound here independent of <img src="https://latex.codecogs.com/png.latex?logp(x)">, and that we have a way to estimate that bound efficiently.</p>
<p>Let’s first move around a few pieces of the first term above:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cmathbb%7BE%7D_%7Bq(z;%5Clambda)%7D%5B(%5Cfrac%7Bp(z%7Cx)%7D%7Bq(z;%5Clambda)%7D)%5E2&amp;%20=%201%20+%20D_%7B%5Cchi%5E2%7D(p(z%7Cx)%7Cq(z;%5Clambda))%20%5C%5C%0A&amp;=%20p(x)%5E2%5B1%20+%20D_%7B%5Cchi%5E2%7D(p(z%7Cx)%7Cq(z;%5Clambda))%5D%0A%5Cend%7Balign%7D%0A"> Then we can take the log of both sides of the equation, which gives us:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7B2%7Dlog(1%20+%20D_%7B%5Cchi%5E2%7D(p(z%7Cx)%7Cq(z;%5Clambda)))%20=%20-logp(x)%20+%20%5Cfrac%7B1%7D%7B2%7Dlog%5Cmathbb%7BE%7D_%7Bq(z;%5Clambda)%7D%5B(%5Cfrac%7Bp(z%7Cx)%7D%7Bq(z;%5Clambda)%7D)%5E2%5D%20%20%0A"> …and this is starting to feel a lot like the ELBO derivation. Log is monotonic, and the <img src="https://latex.codecogs.com/png.latex?-logp(x)"> term is constant as we optimize <img src="https://latex.codecogs.com/png.latex?q">, so we’ve found something that we’re close to able to minimize:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ACUBO_%7B2%7D(%5Clambda)%20=%20%5Cfrac%7B1%7D%7B2%7Dlog%5Cmathbb%7BE%7D_%7Bq(z;%5Clambda)%7D%5B(%5Cfrac%7Bp(z%7Cx)%7D%7Bq(z;%5Clambda)%7D)%5E2%5D%0A"> Since this new divergence is non-negative as well, this is a upper bound of the model evidence. This is thus named <img src="https://latex.codecogs.com/png.latex?%5Cchi"> upper bound (CUBO)<sup>4</sup>.</p>
</section>
<section id="but-can-we-estimate-it" class="level2">
<h2 class="anchored" data-anchor-id="but-can-we-estimate-it">… But can we estimate it?</h2>
<p>One other issue here: how do we estimate this? The CUBO objective got rid of the <img src="https://latex.codecogs.com/png.latex?logp(x)"> we were worried about, but it seems like that expectation is going to be difficult to estimate in general.</p>
<p>Your first idea might be to Monte Carlo (not MCMC) estimate it roughly like this:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ACUBO_2(%5Clambda)%20=%20%5Cfrac%7B1%7D%7B2%7Dlog%5Cfrac%7B1%7D%7BS%7D%5Csum_%7Bs=1%7D%5E%7BS%7D%5B(%5Cfrac%7Bp(x,z%5E%7Bs%7D)%7D%7Bq(z%5E%7Bs%7D;%5Clambda)%7D)%5E2%5D%0A"> Unfortunately, the <img src="https://latex.codecogs.com/png.latex?log"> transform here means our Monte Carlo estimator will be biased: we can see this by applying Jensen’s inequality to the above. To make this stably act as an upper bound, we can apply a clever transformation:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbf%7BL%7D%20=%20exp(n*%20CUBO_2(%5Clambda))%0A"></p>
<p>Since exp is monotonic, this has the same objective as the CUBO, but we can Monte Carlo estimate it unbiasedly. Is that the last problem to solve?</p>
</section>
<section id="but-can-we-calculate-gradients-efficiently" class="level2">
<h2 class="anchored" data-anchor-id="but-can-we-calculate-gradients-efficiently">… But can we calculate gradients efficiently?</h2>
<p>Wait, wait no. Sorry to keep saying there’s one more step here, but there’s a lot that goes into making a full, convenient, general use algorithm here. The last step (for real this time) is that we need to figure out how to get gradients for the estimate of <img src="https://latex.codecogs.com/png.latex?%5Cbf%7BL%7D"> above, <img src="https://latex.codecogs.com/png.latex?%5Cbf%7B%5Chat%7BL%7D%7D">. The issue is that we don’t have any guarantee that a unbiased Monte Carlo estimator of <img src="https://latex.codecogs.com/png.latex?%5Cbf%7B%5Chat%7BL%7D%7D"> gets us a Monte Carlo way to estimate <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%5Clambda%5Cbf%7B%5Chat%7BL%7D%7D">- we can’t guarantee that the gradient of the expectation is equal to the expectation of the gradient.</p>
<p>For this, we need to pull out a trick from the variational autoencoder literature. This is usually referred to as the “Reparameterization Trick”<sup>5</sup>, but the original CHIVI paper refers to them as “reparmeterization gradients”. We will assume we can rewrite the generative process of our model as <img src="https://latex.codecogs.com/png.latex?z%20=%20g(%5Clambda,%5Cepsilon)">, where <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%20%5Csim%20p(%5Cepsilon)"> and g being a deterministic function. Then we have a new estimator for both <img src="https://latex.codecogs.com/png.latex?%5Cbf%7B%5Chat%7BL%7D%7D"> and it’s gradient:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cbf%7B%5Chat%7BL%7D%7D%20&amp;=%20%5Cfrac%7B1%7D%7BB%7D%5Csum_%7Bb=1%7D%5EB(%5Cfrac%7Bp(x,g(%5Clambda,%5Cepsilon%5E%7B(b)%7D))%7D%7Bq(g(%5Clambda,%5Cepsilon%5E%7B(b)%7D;%5Clambda))%7D)%5E%7B2%7D%20%5C%5C%0A%5Cnabla_%5Clambda%5Cbf%7B%5Chat%7BL%7D%7D%20%20&amp;=%20%5Cfrac%7B2%7D%7BB%7D%5Csum_%7Bb=1%7D%5EB(%5Cfrac%7Bp(x,g(%5Clambda,%5Cepsilon%5E%7B(b)%7D))%7D%7Bq(g(%5Clambda,%5Cepsilon%5E%7B(b)%7D;%5Clambda))%7D)%5E2%20%5Cnabla_%5Clambda%20log(%5Cfrac%7Bp(x,g(%5Clambda,%5Cepsilon%5E%7B(b)%7D))%7D%7Bq(g(%5Clambda,%5Cepsilon%5E%7B(b)%7D;%5Clambda))%7D)%0A%5Cend%7Balign%7D%0A"> There are one or two more neat computational tricks in the paper I won’t explain here (essentially: how do we extend this to work in minibatch fashion, and how do we avoid numerical underflow issues), but this is now essentially functional. The whole algorithm, which they dubbed CHIVI is below:</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt3/images/CHIVI_algorithm.png" class="img-fluid"></p>
</section>
</section>
<section id="the-bigger-picture-again" class="level1">
<h1>The Bigger Picture Again</h1>
<p>Stepping back, let’s talk about some practical properties of the algorithm we’ve been stepping through.</p>
<p>First, and probably most exciting given what we saw in the last post, CHIVI’s objective has the property that it is inclusive, unlike the ELBO we were using earlier. This won’t be the right choice for all variational inference problems, but given our prior issues with very narrow posteriors this will be exciting to test<sup>6</sup>. And as we’ll see in the next section, this overdispersion tendency in the posterior will often have a beneficial interaction with importance sampling which can improve our estimates further.</p>
<p>Another nice thing here is that if we were to estimate both the ELBO and CUBO for a given problem, we’d get both a upper and lower bound on the model evidence. This is theoretically convenient in that we now have a sandwich estimator, which actually obtains reasonably tight bounds. We’ll even be able use the fact we have both later to get some bounds on practical bounds on quantities we tend to report in practice like means and covariances!</p>
<p>A final neat benefit here is that to the extent we are willing to consider ensembling models (again, more on that soon), this CHIVI framework will produce estimates that succeed (and fail) in less similar ways that the the ELBO based estimators we looked at last post. Expanding our available set of tools is always good, but it’s even better when we’re ensembling because we can lean more heavily on each model for the tasks it succeeds on.</p>
<p>One potential downside here is that we introduced a solution that partially relies on a Monte Carlo estimator. That said, this is pretty cheap in practice; if we’re using VI as a drop in for MCMC, this is still going to be much much faster than MCMC for any big problem. We’ll need to think about a reasonable number of samples in a given case, but realistically this isn’t going to be a driving factor in determining compute time.</p>
<p>Another final problem is that the estimator we built out for the CUBO that we could actually estimate tends to end up having pretty high variance. Exponentiating the objective isn’t free in that sense; but this problem of variance reduction in estimators is something that feels like a tractable problem to iterate on.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>In truth, both KL divergences encode structural preferences for the type of optimization solution they admit- neither will be the right choice for every problem and variational family combination. But as we’ll see, being able to choose will give us more options to fit models we believe.↩︎</p></li>
<li id="fn2"><p>This is the footnote for those of you that are annoyed because you tried to write out how this would happen, and got something like <img src="https://latex.codecogs.com/png.latex?p(x)%20log%20%5Cfrac%7Bp(x)%7D%7B0%7D">, which should be undefined if we’re following normal math rules. But this is information theory, and in this strange land we say <img src="https://latex.codecogs.com/png.latex?p(x)%20log%20%5Cfrac%7Bp(x)%7D%7B0%7D%20=%20%5Cinfty">. I don’t have a strong intuition for why this is the best solution, but a information encoding perspective makes it make more sense at least: if we know the distribution of <img src="https://latex.codecogs.com/png.latex?p">, we can construct a code for it with average description length <img src="https://latex.codecogs.com/png.latex?H(x)">. One way to understand the KL divergence is as what happens when we try to use the code for a distribution <img src="https://latex.codecogs.com/png.latex?q"> to describe <img src="https://latex.codecogs.com/png.latex?p">, we’d need <img src="https://latex.codecogs.com/png.latex?H(p)%20+%20KL(p%7C%7Cq)"> bits on average to describe <img src="https://latex.codecogs.com/png.latex?p">. In the code for <img src="https://latex.codecogs.com/png.latex?q"> has no way to represent some element of <img src="https://latex.codecogs.com/png.latex?p">, then requiring… infinite bits feels like the right way to describe the breakdown of meaning? All this to say this condition is something our optimizer will try hard to avoid.↩︎</p></li>
<li id="fn3"><p>I’ll mention an alternative approach, Expectation Propagation, that takes a different (not global objective based) approach further down.↩︎</p></li>
<li id="fn4"><p>This approach actually defines a family of <img src="https://latex.codecogs.com/png.latex?n"> new divergences, where you replace the <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D"> with <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7Bn%7D"> and similarly replace the square an exponent with n.&nbsp;Fully stepping through why this is neat wasn’t worth how far afield it’d take us, but the original <img src="https://latex.codecogs.com/png.latex?CHIVI"> paper has some cool derivations based on this, one of which I’ll discuss on the next section on importance sampling.↩︎</p></li>
<li id="fn5"><p>I thought about a section to explain the reparameterization trick, but there are enough good explanations online of the trick. If you’re interested in learning more about why this is important for optimization through stocastic models, I’d recommend starting with Gregory Gundersen’s explanation <a href="https://gregorygundersen.com/blog/2018/04/29/reparameterization/">here</a> and then move on to the original Kingma &amp; Welling, 2013 paper. As general advice on understanding it better though, I’ll echo Greg’s point that some of the online explanations I’ve seen are a bit loose- the key is that we want to express a gradient of an expectation (can’t MC estimate for sure) as an expectation of a gradient (which we can MC estimate provided our convenient deterministic function <img src="https://latex.codecogs.com/png.latex?g"> is differentiable).↩︎</p></li>
<li id="fn6"><p>In a few posts, we’re theoryposting for a bit.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div id="quarto-reuse" class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2023,
  author = {Timm, Andy},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2023-05-02},
  url = {https://andytimm.github.io/posts/Variational MRP Pt3/variational_mrp_3.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2023" class="csl-entry quarto-appendix-citeas">
Timm, Andy. 2023. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> May 2, 2023. <a href="https://andytimm.github.io/posts/Variational MRP Pt3/variational_mrp_3.html">https://andytimm.github.io/posts/Variational
MRP Pt3/variational_mrp_3.html</a>.
</div></div></section></div> ]]></description>
  <category>MRP</category>
  <category>Variational Inference</category>
  <guid>https://andytimm.github.io/posts/Variational MRP Pt3/variational_mrp_3.html</guid>
  <pubDate>Tue, 02 May 2023 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Variational Inference for MRP with Reliable Posterior Distributions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Variational MRP Pt2/Variational_MRP_pt2.html</link>
  <description><![CDATA[ 




<p>This is the second post in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.</p>
<p>In the last post, I laid out why such reformulating the Bayesian inference problem as optimization might be desirable, but previewed why this might be quite hard to find high quality approximations amenable to optimization. I then introduced our running example (predicting national/sub-national opinion on an abortion question from the CCES using MRP), and gave an initial introduction to a version of Variational Inference where we maximize the Evidence Lower Bound (ELBO) as an objective, and do so using a mean-field Gaussian approximation. We saw that with 60k examples, this took about 8 hours to fit with MCMC, but 144 seconds (!) with VI.</p>
<p>In this post, we’ll explore the shortcomings of this initial approximation, and take a first pass at trying to better with a more complex (full rank) variational approximation. The goal is to get a better feel for what failing models could look like, at least in this relatively simple case.</p>
<p>The rough plan for the series is as follows:</p>
<ol type="1">
<li><a href="https://andytimm.github.io/posts/Variational%20MRP%20Pt1/variational_mrp_pt1.html">Introducing the Problem- Why is VI useful, why VI can produce spherical cows</a></li>
<li><strong>(This post)</strong> How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li>Some theory on why posterior approximation with VI can be so poor</li>
<li>Seeing if some more sophisticated techniques like normalizing flows help</li>
</ol>
<div class="cell">

</div>
<section id="the-disclaimer" class="level1">
<h1>The disclaimer</h1>
<p>One sort of obvious objections to how I’ve set up this series is “Why not talk about theory on why VI approximations can be poor before trying stuff?”. While in practice I did read a lot of the papers for the next post before writing this one, I think there’s a lot of value is looking at failed solutions to a problem to build up intuition about what our failure mode looks like, and what it might require to get it right.</p>
</section>
<section id="toplines" class="level1">
<h1>Toplines</h1>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">meanfield_60k <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">readRDS</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"fit_60k_meanfield.rds"</span>)</span>
<span id="cb1-2">mcmc_60k <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">readRDS</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"fit_60k_mcmc.rds"</span>)</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Meanfield </span></span>
<span id="cb1-5">epred_mat_mf <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">posterior_epred</span>(meanfield_60k, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">newdata =</span> poststrat_df_60k, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">draws =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb1-6">mrp_estimates_vector_mf <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> epred_mat_mf <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> poststrat_df_60k<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>n <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> </span>
<span id="cb1-7">                                              <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(poststrat_df_60k<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>n)</span>
<span id="cb1-8">mrp_estimate_mf <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">mean =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mean</span>(mrp_estimates_vector_mf),</span>
<span id="cb1-9">                     <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">sd =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sd</span>(mrp_estimates_vector_mf))</span>
<span id="cb1-10"></span>
<span id="cb1-11"></span>
<span id="cb1-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># MCMC </span></span>
<span id="cb1-13">epred_mat_mcmc <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">posterior_epred</span>(mcmc_60k, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">newdata =</span> poststrat_df_60k, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">draws =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb1-14">mrp_estimates_vector_mcmc <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> epred_mat_mcmc <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> poststrat_df_60k<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>n <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span></span>
<span id="cb1-15">                                                  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(poststrat_df_60k<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>n)</span>
<span id="cb1-16">mrp_estimate_mcmc <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">mean =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mean</span>(mrp_estimates_vector_mcmc),</span>
<span id="cb1-17">                       <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">sd =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sd</span>(mrp_estimates_vector_mcmc))</span>
<span id="cb1-18"></span>
<span id="cb1-19"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">cat</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Meanfield MRP estimate mean, sd: "</span>, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">round</span>(mrp_estimate_mf, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span>
<span id="cb1-20"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">cat</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MCMC MRP estimate mean, sd: "</span>, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">round</span>(mrp_estimate_mcmc, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span></code></pre></div>
</div>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>Mean</th>
<th>SD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MCMC</td>
<td>43.9%</td>
<td>.2%</td>
</tr>
<tr class="even">
<td>mean-field VI</td>
<td>43.7%</td>
<td>.2%</td>
</tr>
</tbody>
</table>
<p>Starting with basics, the toplines are pretty much identical, which is a good start. The minor difference here could easily reverse on a different seed- from a few quick re-runs these often end up having matching means to 3 decimals.</p>
</section>
<section id="state-level-estimates" class="level1">
<h1>State Level Estimates</h1>
<p>What happens if we produce state level estimates, similar to the plot last post comparing MRP to a simple weighted estimate? Note that I’ll steer away from the MRP Case Study example here in a few ways. I’ll use <code>tidybayes</code> for working with the draws (more elegant than their loop based approach), and I’ll use more draws (helps with simulation error in smaller states).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">mcmc_state_level <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> poststrat_df_60k <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">add_epred_draws</span>(mcmc_60k, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ndraws =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb2-2">mfvi_state_level <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> poststrat_df_60k <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">add_epred_draws</span>(meanfield_60k, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ndraws =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb2-3"></span>
<span id="cb2-4">mcmc_state_level <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">glimpse</span>()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 12,000,000
Columns: 13
Groups: state, eth, male, age, educ, n, repvote, region, .row [12,000]
$ state      &lt;chr&gt; "AL", "AL", "AL", "AL", "AL", "AL", "AL", "AL", "AL", "AL",…
$ eth        &lt;chr&gt; "White", "White", "White", "White", "White", "White", "Whit…
$ male       &lt;dbl&gt; -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,…
$ age        &lt;chr&gt; "18-29", "18-29", "18-29", "18-29", "18-29", "18-29", "18-2…
$ educ       &lt;chr&gt; "No HS", "No HS", "No HS", "No HS", "No HS", "No HS", "No H…
$ n          &lt;dbl&gt; 23948, 23948, 23948, 23948, 23948, 23948, 23948, 23948, 239…
$ repvote    &lt;dbl&gt; 0.6437414, 0.6437414, 0.6437414, 0.6437414, 0.6437414, 0.64…
$ region     &lt;chr&gt; "South", "South", "South", "South", "South", "South", "Sout…
$ .row       &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
$ .chain     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
$ .iteration &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
$ .draw      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …
$ .epred     &lt;dbl&gt; 0.5771322, 0.5189677, 0.5483006, 0.5421404, 0.5417602, 0.55…</code></pre>
</div>
</div>
<p>If you haven’t worked with <code>tidybayes</code> before, the glimpse above should help give some intuition about the new shape of the data- we’ve take the 12,000 row <code>poststrat_df_60k</code>, and added a row per observation per draw, with the prediction (.epred) and related metadata. This gives 12,000 x 1,000 = 12 million rows. This really isn’t the most space efficient storage, but it allows for very elegant <code>dplyr</code> style manipulation of results and quick exploration.</p>
<p>Let’s now plot and compare the 50 and 95% credible intervals by state between the two models.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">mcmc_state_summary <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> mcmc_state_level <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb4-2">                        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># multiply each draw by it's cell's proportion of state N</span></span>
<span id="cb4-3">                        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># this is the P in MRP</span></span>
<span id="cb4-4">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(state,.draw) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-5">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">postrat_draw =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(.epred<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(n<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(n)))) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-6">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(state) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-7">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">median_qi</span>(postrat_draw, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">.width =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">95</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-8">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MCMC"</span>)</span>
<span id="cb4-9"></span>
<span id="cb4-10">mfvi_state_summary <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> mfvi_state_level <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb4-11">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(state,.draw) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-12">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">postrat_draw =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(.epred<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(n<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(n)))) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-13">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(state) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-14">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">median_qi</span>(postrat_draw, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">.width =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">95</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-15">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MF-VI"</span>)</span>
<span id="cb4-16"></span>
<span id="cb4-17">combined_summary <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bind_rows</span>(mcmc_state_summary,mfvi_state_summary)</span>
<span id="cb4-18"></span>
<span id="cb4-19">combined_summary <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-20">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ordered_state =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">fct_reorder</span>(combined_summary<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>state,</span>
<span id="cb4-21">                                     combined_summary<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>postrat_draw)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb4-22">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> ordered_state,</span>
<span id="cb4-23">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> postrat_draw,</span>
<span id="cb4-24">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">xmin =</span> .lower,</span>
<span id="cb4-25">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">xmax =</span> .upper,</span>
<span id="cb4-26">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> model)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-27">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_pointinterval</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">position =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">position_dodge</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-28">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlim</span>(.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">75</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-29">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">legend.position=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"top"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-30">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Should employers be allowed to deny their employees abortion care?"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb4-31">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ylab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"State"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt2/Variational_MRP_pt2_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>… That looks concerning.</p>
<p>What might you get wrong if you used the VI approximation for inference here? If you only cared about the median estimate primarily, you might be ok with this effort. If you care about uncertainty though, here’s a non-exhaustive list of concerns here:</p>
<ol type="1">
<li>Probably unimodal, smooth posterior distributions from MCMC have gone off-course to the point where the Median/50/95% presentation no longer seems up to expressing the posterior shape (more on this in a second).</li>
<li>The MF-VI posteriors are often narrower in 50% or 95% CI- we’d on average underestimate various types of uncertainty here.</li>
<li>Worse<sup>1</sup>, the MF-VI posterior’s CIs aren’t <strong>consistently</strong> narrower, either in the sense they are always narrower, or that they tend to consistently distort the same way. Sometimes both the 50% and 95% are just a small amount narrower than MCMC- the Michigan posterior attempt looks passable. Sometimes things are worse, with 50% MFVI CIs almost as wide as the MCMC 95% interval- Wyoming shows such a distortion. Sometimes the probability mass between 50% and 95% is confined to such a minuscule range it looks like I forgot to plot it.</li>
</ol>
<p>That last point is particularly important because it suggests there’s no easy rule of thumb for mechanically correcting these intervals, or deciding which could be plausible approximations without the MCMC plot alongside to guide that process. We can’t use VI to save a ton of time, infer the intervals consistently need to x% be wider, and call it a day- we need to reckon more precisely with why they’re distorted.</p>
<p>Let’s return now to the point about how the shape has gone wrong. Below is a dot plot (<a href="https://dl.acm.org/doi/10.1145/2858036.2858558">Kay et al., 2016</a>)- each point here represents about 1% of the probability mass. I enjoy this approach to posterior visualization when things are getting weird, as this clarifies a lot about the full shape of the posterior distribution, making fewer smoothing assumptions like a density or eye plot might.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">mcmc_state_points <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> mcmc_state_level <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb5-2">                        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># multiply each draw by it's cell's proportion of state N</span></span>
<span id="cb5-3">                        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># this is the P in MRP</span></span>
<span id="cb5-4">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(state,.draw) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb5-5">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarize</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">postrat_draw =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(.epred<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(n<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(n)))) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb5-6">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MCMC"</span>)</span>
<span id="cb5-7">mfvi_state_points <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> mfvi_state_level <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb5-8">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(state,.draw) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb5-9">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarize</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">postrat_draw =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(.epred<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(n<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(n)))) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb5-10">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MF-VI"</span>)</span>
<span id="cb5-11"></span>
<span id="cb5-12">combined_points <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> mcmc_state_points <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb5-13">                      <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bind_rows</span>(mfvi_state_points) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb5-14">                      <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ungroup</span>()</span>
<span id="cb5-15"></span>
<span id="cb5-16">combined_points <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb5-17">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ordered_state =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">fct_reorder</span>(combined_points<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>state,</span>
<span id="cb5-18">                                     combined_points<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>postrat_draw)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb5-19">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> ordered_state,</span>
<span id="cb5-20">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> postrat_draw,</span>
<span id="cb5-21">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> model)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb5-22">     <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stat_dots</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">quantiles =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb5-23">     <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">facet_wrap</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>model) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb5-24">     <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">legend.position=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"none"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb5-25">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Should employers be allowed to deny their employees abortion care?"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb5-26">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ylab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"State"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt2/Variational_MRP_pt2_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>Eek. The closer to the individual draws we get, the less these two models seem to be producing comparable estimates. This isn’t me expressing an aesthetic preference for smooth, unimodal distributions- the MFVI plots in this view imply beliefs like “support for this policy in Wyoming is overwhelmingly likely to fall in 1 of 3 narrow ranges, all other values are unlikely”<sup>2</sup>. Other similar humorous claims are easy to find.</p>
<p>Stepping back for a second, if our use-case for this model takes pretty much any form of interest in quantifying uncertainty accurately, this is not an acceptable approximation. I could poke more holes, but I can more profitably do that after I’ve explored some theory of why VI models struggle, and brought in some more sophisticated diagnostic tools than looking with our eyeballs<sup>3</sup>; so let’s hold off on that.</p>
</section>
<section id="do-more-basic-fixes-solve-anything" class="level1">
<h1>Do more basic fixes solve anything?</h1>
<p>So I’ve been billing this simple mean-field model as a first pass- I fit it on more or less default <code>rstanarm</code> parameters. I think it’s worth taking a moment to show that getting this approximation problem right isn’t going to be solved with low hanging fruit ideas, since that will motivate our need for better diagnostics and more expressive approximations.</p>
<section id="lowering-the-tolerance" class="level2">
<h2 class="anchored" data-anchor-id="lowering-the-tolerance">Lowering the tolerance</h2>
<p>So we managed to structure our Bayesian inference problem as an optimization problem. Can’t we just optimize better? Maybe with more training the result will be less bad?</p>
<p>the <code>tol_rel_obj</code> parameter control’s the convergence tolerance on the relative norm of the objective. In other words, it controls what (change in the) Evidence Lower Bound value we consider accurate enough to stop at. The default is 0.01, which feels a bit opaque, but let’s try setting it way down to 1e-8 (1Mx lower). Then we can plot it alongside the MCMC estimates and original MF-VI attempt.</p>
<div class="cell" data-warnings="false">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tic</span>()</span>
<span id="cb6-2">fit_60k_1e8 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stan_glmer</span>(abortion <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> state) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb6-3">                                      male <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> male<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>age) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb6-4">                                      (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> repvote <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">factor</span>(region),</span>
<span id="cb6-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">binomial</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">link =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"logit"</span>),</span>
<span id="cb6-6">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> cces_all_df,</span>
<span id="cb6-7">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">autoscale =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>),</span>
<span id="cb6-8">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior_covariance =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">decov</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">scale =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.50</span>),</span>
<span id="cb6-9">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">adapt_delta =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.99</span>,</span>
<span id="cb6-10">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Printing the ELBO every 1k draws</span></span>
<span id="cb6-11">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">refresh =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>,</span>
<span id="cb6-12">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">tol_rel_obj =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-8</span>,</span>
<span id="cb6-13">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">algorithm =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"meanfield"</span>,</span>
<span id="cb6-14">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">seed =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">605</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Chain 1: ------------------------------------------------------------
Chain 1: EXPERIMENTAL ALGORITHM:
Chain 1:   This procedure has not been thoroughly tested and may be unstable
Chain 1:   or buggy. The interface is subject to change.
Chain 1: ------------------------------------------------------------
Chain 1: 
Chain 1: 
Chain 1: 
Chain 1: Gradient evaluation took 0.032 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 320 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Begin eta adaptation.
Chain 1: Iteration:   1 / 250 [  0%]  (Adaptation)
Chain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)
Chain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)
Chain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)
Chain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)
Chain 1: Success! Found best value [eta = 1] earlier than expected.
Chain 1: 
Chain 1: Begin stochastic gradient ascent.
Chain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes 
Chain 1:    100       -40291.889             1.000            1.000
Chain 1:    200       -39947.669             0.504            1.000
Chain 1:    300       -39802.182             0.337            0.009
Chain 1:    400       -39776.283             0.253            0.009
Chain 1:    500       -39733.863             0.203            0.004
Chain 1:    600       -39733.198             0.169            0.004
Chain 1:    700       -39728.255             0.145            0.001
Chain 1:    800       -39784.557             0.127            0.001
Chain 1:    900       -39724.366             0.113            0.001
Chain 1:   1000       -39732.042             0.102            0.001
Chain 1:   1100       -39731.525             0.002            0.001
Chain 1:   1200       -39732.049             0.001            0.001
Chain 1:   1300       -39728.119             0.001            0.000
Chain 1:   1400       -39740.928             0.000            0.000
Chain 1:   1500       -39726.114             0.000            0.000
Chain 1:   1600       -39734.740             0.000            0.000
Chain 1:   1700       -39734.129             0.000            0.000
Chain 1:   1800       -39740.719             0.000            0.000
Chain 1:   1900       -39743.591             0.000            0.000
Chain 1:   2000       -39737.155             0.000            0.000
Chain 1:   2100       -39720.432             0.000            0.000
Chain 1:   2200       -39738.138             0.000            0.000
Chain 1:   2300       -39731.045             0.000            0.000
Chain 1:   2400       -39716.393             0.000            0.000
Chain 1:   2500       -39729.189             0.000            0.000
Chain 1:   2600       -39722.239             0.000            0.000
Chain 1:   2700       -39719.508             0.000            0.000
Chain 1:   2800       -39718.709             0.000            0.000
Chain 1:   2900       -39735.110             0.000            0.000
Chain 1:   3000       -39725.900             0.000            0.000
Chain 1:   3100       -39726.123             0.000            0.000
Chain 1:   3200       -39718.736             0.000            0.000
Chain 1:   3300       -39718.141             0.000            0.000
Chain 1:   3400       -39717.147             0.000            0.000
Chain 1:   3500       -39725.738             0.000            0.000
Chain 1:   3600       -39732.190             0.000            0.000
Chain 1:   3700       -39723.666             0.000            0.000
Chain 1:   3800       -39725.470             0.000            0.000
Chain 1:   3900       -39741.504             0.000            0.000
Chain 1:   4000       -39722.951             0.000            0.000
Chain 1:   4100       -39721.852             0.000            0.000
Chain 1:   4200       -39717.894             0.000            0.000
Chain 1:   4300       -39717.474             0.000            0.000
Chain 1:   4400       -39716.244             0.000            0.000
Chain 1:   4500       -39727.542             0.000            0.000
Chain 1:   4600       -39716.670             0.000            0.000
Chain 1:   4700       -39723.714             0.000            0.000
Chain 1:   4800       -39727.123             0.000            0.000
Chain 1:   4900       -39722.517             0.000            0.000
Chain 1:   5000       -39722.485             0.000            0.000
Chain 1:   5100       -39719.107             0.000            0.000
Chain 1:   5200       -39722.873             0.000            0.000
Chain 1:   5300       -39720.153             0.000            0.000
Chain 1:   5400       -39718.807             0.000            0.000
Chain 1:   5500       -39719.687             0.000            0.000
Chain 1:   5600       -39730.850             0.000            0.000
Chain 1:   5700       -39719.315             0.000            0.000
Chain 1:   5800       -39717.985             0.000            0.000
Chain 1:   5900       -39715.943             0.000            0.000
Chain 1:   6000       -39721.574             0.000            0.000
Chain 1:   6100       -39716.072             0.000            0.000
Chain 1:   6200       -39715.947             0.000            0.000
Chain 1:   6300       -39716.325             0.000            0.000
Chain 1:   6400       -39716.206             0.000            0.000
Chain 1:   6500       -39720.508             0.000            0.000
Chain 1:   6600       -39717.566             0.000            0.000
Chain 1:   6700       -39718.903             0.000            0.000
Chain 1:   6800       -39716.766             0.000            0.000
Chain 1:   6900       -39724.482             0.000            0.000
Chain 1:   7000       -39717.376             0.000            0.000
Chain 1:   7100       -39721.566             0.000            0.000
Chain 1:   7200       -39725.641             0.000            0.000
Chain 1:   7300       -39717.909             0.000            0.000
Chain 1:   7400       -39720.096             0.000            0.000
Chain 1:   7500       -39716.243             0.000            0.000
Chain 1:   7600       -39738.451             0.000            0.000
Chain 1:   7700       -39715.841             0.000            0.000
Chain 1:   7800       -39716.561             0.000            0.000
Chain 1:   7900       -39716.865             0.000            0.000
Chain 1:   8000       -39721.972             0.000            0.000
Chain 1:   8100       -39723.864             0.000            0.000
Chain 1:   8200       -39716.157             0.000            0.000
Chain 1:   8300       -39720.235             0.000            0.000
Chain 1:   8400       -39718.693             0.000            0.000
Chain 1:   8500       -39727.325             0.000            0.000
Chain 1:   8600       -39716.809             0.000            0.000
Chain 1:   8700       -39716.760             0.000            0.000
Chain 1:   8800       -39721.577             0.000            0.000
Chain 1:   8900       -39716.910             0.000            0.000
Chain 1:   9000       -39721.631             0.000            0.000
Chain 1:   9100       -39721.102             0.000            0.000
Chain 1:   9200       -39718.303             0.000            0.000
Chain 1:   9300       -39715.759             0.000            0.000
Chain 1:   9400       -39719.769             0.000            0.000
Chain 1:   9500       -39719.046             0.000            0.000
Chain 1:   9600       -39720.854             0.000            0.000
Chain 1:   9700       -39717.968             0.000            0.000
Chain 1:   9800       -39721.396             0.000            0.000
Chain 1:   9900       -39728.139             0.000            0.000
Chain 1:   10000       -39715.367             0.000            0.000
Chain 1: Informational Message: The maximum number of iterations is reached! The algorithm may not have converged.
Chain 1: This variational approximation is not guaranteed to be meaningful.
Chain 1: 
Chain 1: Drawing a sample of size 1000 from the approximate posterior... 
Chain 1: COMPLETED.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Pareto k diagnostic value is 2.05. Resampling is disabled. Decreasing
tol_rel_obj may help if variational algorithm has terminated prematurely.
Otherwise consider using sampling instead.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Setting 'QR' to TRUE can often be helpful when using one of the variational inference algorithms. See the documentation for the 'QR' argument.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">toc</span>()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>298.73 sec elapsed</code></pre>
</div>
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1">lower_tol_draws <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> poststrat_df_60k <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">add_epred_draws</span>(fit_60k_1e8, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ndraws =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb12-2"></span>
<span id="cb12-3">mfvi_lower_tol_points <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> lower_tol_draws <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb12-4">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(state,.draw) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb12-5">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarize</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">postrat_draw =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(.epred<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(n<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(n)))) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb12-6">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MF-VI 1e-8"</span>)</span>
<span id="cb12-7"></span>
<span id="cb12-8">combined_points_w_lower_tol <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> combined_points <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb12-9">                      <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bind_rows</span>(mfvi_lower_tol_points) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb12-10">                      <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ungroup</span>()</span>
<span id="cb12-11"></span>
<span id="cb12-12">combined_points_w_lower_tol <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb12-13">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ordered_state =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">fct_reorder</span>(combined_points_w_lower_tol<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>state,</span>
<span id="cb12-14">                                     combined_points_w_lower_tol<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>postrat_draw)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb12-15">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> ordered_state,</span>
<span id="cb12-16">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> postrat_draw,</span>
<span id="cb12-17">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> model)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb12-18">     <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stat_dots</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">quantiles =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb12-19">     <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">facet_wrap</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>model) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb12-20">     <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">legend.position=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"none"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb12-21">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Should employers be allowed to deny their employees abortion care?"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb12-22">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ylab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"State"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt2/Variational_MRP_pt2_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>… That certainly looks different, but I don’t really think I’d say it looks meaningfully better<sup>4</sup>.</p>
<p>Looking at the printed out ELBO, it’s pretty clear that there was no traction after the first ~1000 samples. A variational family this simple isn’t going to get much better, no matter how much time you give it.</p>
</section>
<section id="full-rank-approximation" class="level2">
<h2 class="anchored" data-anchor-id="full-rank-approximation">Full-Rank Approximation</h2>
<p>So if extend training time, but improvements don’t result, maybe the next option is ask whether we need something more sophisticated than a mean-field approximation. Instead of</p>
<p><img src="https://latex.codecogs.com/png.latex?q(z)%20=%20%5Cprod_%7Bj=1%7D%5E%7Bm%7D%20q_j(z_j)"></p>
<p>let’s now try the full-rank approximation. Gather than each <img src="https://latex.codecogs.com/png.latex?z_j"> getting it’s own independent Gaussian, this uses a single multivariate normal distribution- so we can now (roughly) learn correlation structure, fancy.</p>
<p><img src="https://latex.codecogs.com/png.latex?q(z)%20=%20%5Cmathcal%7BN%7D(z%7C%5Cmu,%5CSigma)"></p>
<div class="cell" data-warnings="false">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tic</span>()</span>
<span id="cb13-2">fit_60k_fullrank <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stan_glmer</span>(abortion <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> state) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb13-3">                                      male <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> male<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>age) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb13-4">                                      (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> repvote <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">factor</span>(region),</span>
<span id="cb13-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">binomial</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">link =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"logit"</span>),</span>
<span id="cb13-6">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> cces_all_df,</span>
<span id="cb13-7">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">autoscale =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>),</span>
<span id="cb13-8">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior_covariance =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">decov</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">scale =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.50</span>),</span>
<span id="cb13-9">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">adapt_delta =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.99</span>,</span>
<span id="cb13-10">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">tol_rel_obj =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-8</span>,</span>
<span id="cb13-11">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Printing the ELBO every 1k draws</span></span>
<span id="cb13-12">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">refresh =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>,</span>
<span id="cb13-13">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">algorithm =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"fullrank"</span>,</span>
<span id="cb13-14">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">QR =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>,</span>
<span id="cb13-15">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">seed =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">605</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Chain 1: ------------------------------------------------------------
Chain 1: EXPERIMENTAL ALGORITHM:
Chain 1:   This procedure has not been thoroughly tested and may be unstable
Chain 1:   or buggy. The interface is subject to change.
Chain 1: ------------------------------------------------------------
Chain 1: 
Chain 1: 
Chain 1: 
Chain 1: Gradient evaluation took 0.025 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 250 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Begin eta adaptation.
Chain 1: Iteration:   1 / 250 [  0%]  (Adaptation)
Chain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)
Chain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)
Chain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)
Chain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)
Chain 1: Iteration: 250 / 250 [100%]  (Adaptation)
Chain 1: Success! Found best value [eta = 0.1].
Chain 1: 
Chain 1: Begin stochastic gradient ascent.
Chain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes 
Chain 1:    100      -248586.032             1.000            1.000
Chain 1:    200      -180460.369             0.689            1.000
Chain 1:    300      -121675.221             0.620            0.483
Chain 1:    400       -87431.017             0.563            0.483
Chain 1:    500      -120999.829             0.506            0.392
Chain 1:    600       -96768.296             0.463            0.392
Chain 1:    700       -93851.607             0.402            0.378
Chain 1:    800       -92494.273             0.353            0.378
Chain 1:    900       -74378.556             0.341            0.277
Chain 1:   1000       -77681.560             0.311            0.277
Chain 1:   1100       -77465.866             0.211            0.250
Chain 1:   1200       -68692.287             0.186            0.244
Chain 1:   1300       -75140.633             0.147            0.128
Chain 1:   1400       -49430.772             0.160            0.128
Chain 1:   1500       -59011.994             0.148            0.128
Chain 1:   1600       -57033.572             0.127            0.086
Chain 1:   1700       -56133.855             0.125            0.086
Chain 1:   1800       -46605.149             0.144            0.128
Chain 1:   1900       -47895.964             0.122            0.086
Chain 1:   2000       -44745.890             0.125            0.086
Chain 1:   2100       -43472.467             0.128            0.086
Chain 1:   2200       -43454.384             0.115            0.070
Chain 1:   2300       -41781.249             0.110            0.040
Chain 1:   2400       -42045.221             0.059            0.035
Chain 1:   2500       -41381.652             0.044            0.029
Chain 1:   2600       -40754.440             0.043            0.027
Chain 1:   2700       -41108.136             0.042            0.027
Chain 1:   2800       -40450.439             0.023            0.016
Chain 1:   2900       -40423.015             0.020            0.016
Chain 1:   3000       -40375.121             0.013            0.015
Chain 1:   3100       -40227.022             0.011            0.009
Chain 1:   3200       -40302.411             0.011            0.009
Chain 1:   3300       -40352.339             0.007            0.006
Chain 1:   3400       -40174.196             0.007            0.004
Chain 1:   3500       -40089.973             0.006            0.004
Chain 1:   3600       -40143.009             0.004            0.002
Chain 1:   3700       -40123.486             0.003            0.002
Chain 1:   3800       -40044.004             0.002            0.002
Chain 1:   3900       -39955.515             0.002            0.002
Chain 1:   4000       -40003.851             0.002            0.002
Chain 1:   4100       -39948.544             0.002            0.002
Chain 1:   4200       -40028.027             0.002            0.002
Chain 1:   4300       -39907.006             0.002            0.002
Chain 1:   4400       -39868.266             0.002            0.002
Chain 1:   4500       -39938.386             0.002            0.002
Chain 1:   4600       -39837.339             0.002            0.002
Chain 1:   4700       -39852.349             0.002            0.002
Chain 1:   4800       -39823.670             0.002            0.002
Chain 1:   4900       -39809.797             0.001            0.001
Chain 1:   5000       -39807.261             0.001            0.001
Chain 1:   5100       -39806.402             0.001            0.001
Chain 1:   5200       -39818.805             0.001            0.001
Chain 1:   5300       -39797.428             0.001            0.001
Chain 1:   5400       -39790.469             0.001            0.000
Chain 1:   5500       -39785.797             0.001            0.000
Chain 1:   5600       -39779.121             0.000            0.000
Chain 1:   5700       -39780.314             0.000            0.000
Chain 1:   5800       -39771.363             0.000            0.000
Chain 1:   5900       -39770.673             0.000            0.000
Chain 1:   6000       -39764.096             0.000            0.000
Chain 1:   6100       -39764.173             0.000            0.000
Chain 1:   6200       -39765.651             0.000            0.000
Chain 1:   6300       -39756.809             0.000            0.000
Chain 1:   6400       -39753.724             0.000            0.000
Chain 1:   6500       -39754.753             0.000            0.000
Chain 1:   6600       -39750.392             0.000            0.000
Chain 1:   6700       -39753.067             0.000            0.000
Chain 1:   6800       -39750.341             0.000            0.000
Chain 1:   6900       -39745.696             0.000            0.000
Chain 1:   7000       -39743.521             0.000            0.000
Chain 1:   7100       -39739.157             0.000            0.000
Chain 1:   7200       -39736.689             0.000            0.000
Chain 1:   7300       -39743.472             0.000            0.000
Chain 1:   7400       -39738.431             0.000            0.000
Chain 1:   7500       -39740.789             0.000            0.000
Chain 1:   7600       -39735.842             0.000            0.000
Chain 1:   7700       -39733.493             0.000            0.000
Chain 1:   7800       -39735.015             0.000            0.000
Chain 1:   7900       -39736.429             0.000            0.000
Chain 1:   8000       -39733.548             0.000            0.000
Chain 1:   8100       -39732.722             0.000            0.000
Chain 1:   8200       -39734.720             0.000            0.000
Chain 1:   8300       -39732.932             0.000            0.000
Chain 1:   8400       -39727.658             0.000            0.000
Chain 1:   8500       -39734.522             0.000            0.000
Chain 1:   8600       -39728.602             0.000            0.000
Chain 1:   8700       -39724.690             0.000            0.000
Chain 1:   8800       -39725.374             0.000            0.000
Chain 1:   8900       -39731.450             0.000            0.000
Chain 1:   9000       -39725.866             0.000            0.000
Chain 1:   9100       -39728.639             0.000            0.000
Chain 1:   9200       -39730.156             0.000            0.000
Chain 1:   9300       -39729.036             0.000            0.000
Chain 1:   9400       -39725.536             0.000            0.000
Chain 1:   9500       -39727.031             0.000            0.000
Chain 1:   9600       -39725.389             0.000            0.000
Chain 1:   9700       -39727.947             0.000            0.000
Chain 1:   9800       -39723.932             0.000            0.000
Chain 1:   9900       -39723.173             0.000            0.000
Chain 1:   10000       -39723.944             0.000            0.000
Chain 1: Informational Message: The maximum number of iterations is reached! The algorithm may not have converged.
Chain 1: This variational approximation is not guaranteed to be meaningful.
Chain 1: 
Chain 1: Drawing a sample of size 1000 from the approximate posterior... 
Chain 1: COMPLETED.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Pareto k diagnostic value is 2.95. Resampling is disabled. Decreasing
tol_rel_obj may help if variational algorithm has terminated prematurely.
Otherwise consider using sampling instead.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">toc</span>()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>350.16 sec elapsed</code></pre>
</div>
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1">full_rank_draws <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> poststrat_df_60k <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">add_epred_draws</span>(fit_60k_fullrank,</span>
<span id="cb18-2">                                                        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ndraws =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb18-3"></span>
<span id="cb18-4">frvi_points <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> full_rank_draws <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb18-5">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(state,.draw) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb18-6">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarize</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">postrat_draw =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(.epred<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(n<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(n)))) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb18-7">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">model =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"FR-VI"</span>)</span>
<span id="cb18-8"></span>
<span id="cb18-9">combined_points_w_frvi <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> combined_points_w_lower_tol <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb18-10">                      <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bind_rows</span>(frvi_points) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb18-11">                      <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ungroup</span>()</span>
<span id="cb18-12"></span>
<span id="cb18-13">combined_points_w_frvi <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb18-14">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ordered_state =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">fct_reorder</span>(combined_points_w_frvi<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>state,</span>
<span id="cb18-15">                                     combined_points_w_frvi<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>postrat_draw)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb18-16">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> ordered_state,</span>
<span id="cb18-17">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> postrat_draw,</span>
<span id="cb18-18">             <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> model)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb18-19">     <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stat_dots</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">quantiles =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb18-20">     <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">facet_wrap</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>model) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb18-21">     <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">legend.position=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"none"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb18-22">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Should employers be allowed to deny their employees abortion care?"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb18-23">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ylab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"State"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt2/Variational_MRP_pt2_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>The first thing to note here is that unlike the mean-field approximation, fitting this model required some tinkering to get it to fit. I ended up needing to set <code>QR = TRUE</code> (ie, use a QR decomposition) to get this to fit at all (unless I set the initialization to 0, at which point the posterior collapsed to nearly a single point).</p>
<p>Unfortunately, this version has a similar spiky posterior distribution. In terms of uncertainty, it’s clearly worse than the mean-field implementation. The ELBO starts from higher, spends some time actually improving, but also quickly reaches a plateau. It doesn’t seem like this is a way out either.</p>
</section>
</section>
<section id="where-to-from-here-why-is-it-like-this" class="level1">
<h1>Where to from here? (Why is it like this?)</h1>
<p>We’ve seen that simple variational families like the mean-field and full-rank can approximately mirror the central tendencies of MCMC, but things fall apart as we attempt to consider uncertainty, either through simple credible intervals, or especially once we start to visualize the unrealistic, lumpy VI posterior distributions in their entirety.</p>
<p>This isn’t something we can solve with more training time: each of these algorithms had reached the lowest ELBO they could well before we produced final draws. If I had to guess, I think we need a fundamentally more expressive class of variational family to make progress.</p>
<p>While trying to fit models without digging too much into the theory of why VI approximations can be poor has been fun, it’s time to bring in some theory. In the next post, I’ll explore the literature on why the uncertainty behavior of VI can be so dubious. In the following one, I’ll illustrate some better diagnostics as well.</p>
<p>The code for this post can be found <a href="https://github.com/andytimm/andytimm.github.io/blob/main/posts/Variational%20MRP%20Pt2/Variational_MRP_pt2.qmd">here</a>. Thanks for reading.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Really, the worst type of wrong, completely unpredictable wrong. If you spend time staring to try to infer a causal pattern of which states we can’t estimate well, you’re likely just going to end up confused.↩︎</p></li>
<li id="fn2"><p>Some of these MFVI distributions are bad enough that you might reasonably wonder if some of the badness is just plotting weirdness. That was my intuition at first. Of course though, this is sufficient granularity to make the MCMC results look reasonable. But even if you zoom in on 1 or two states and add way more points, the improbably sharp spikes remain.↩︎</p></li>
<li id="fn3"><p>Phrase due to Richard McElreath. The magic of good visualizations like Kay et al.’s is that makes it trivial to let pattern recognition go to work, and be able to go “oh, that looks wrong”.↩︎</p></li>
<li id="fn4"><p>Also, apologies for showing every 100 iterations; the rstanarm parameter to set this, <code>refresh</code> doesn’t appear to work properly with non-MCMC models, so I can either not show the ELBO or blow up the post with this.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2022,
  author = {Andy Timm},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2022-11-20},
  url = {https://andytimm.github.io/Variational_MRP_pt2.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2022" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2022. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> November 20, 2022. <a href="https://andytimm.github.io/Variational_MRP_pt2.html">https://andytimm.github.io/Variational_MRP_pt2.html</a>.
</div></div></section></div> ]]></description>
  <category>MRP</category>
  <category>Variational Inference</category>
  <guid>https://andytimm.github.io/posts/Variational MRP Pt2/Variational_MRP_pt2.html</guid>
  <pubDate>Sun, 20 Nov 2022 05:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Variational MRP Pt2/images/cover_photo.png" medium="image" type="image/png" height="180" width="144"/>
</item>
<item>
  <title>Variational Inference for MRP with Reliable Posterior Distributions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Variational MRP Pt1/variational_mrp_pt1.html</link>
  <description><![CDATA[ 




<p>This post introduces a series I intend to write, exploring using <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">Variational Inference</a> to massively speed up running complex survey estimation models like variants of <a href="https://en.wikipedia.org/wiki/Multilevel_regression_with_poststratification">Multilevel Regression and Poststratification</a> while aiming to keep approximation error from completely ruining the model.</p>
<p>The rough plan for the series is as follows:</p>
<ol type="1">
<li><strong>(This post)</strong> Introducing the Problem- Why is VI useful, why VI can produce spherical cows</li>
<li>How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li>Some theory on why posterior approximation with VI can be so poor</li>
<li>Seeing if some more sophisticated techniques like normalizing flows help</li>
</ol>
<section id="motivation-for-series" class="level1">
<h1>Motivation for series</h1>
<p>I learn well by explaining things to others, and I’ve been particularly excited to learn about variational inference and ways to improve it over the past few months. There are lots of Bayesian models I would like to fit, especially in my political work, that I would categorize as being incredibly useful, but on the edge of practically acceptable run times. For example, the somewhat but not particularly complex model I’ll use as a running example for the series <strong>takes ~8 hours to fit on 60k observations</strong>.</p>
<p>Having a model run overnight or for a full work day can be fine sometimes, but what if there is a more urgent need for the results? What if we need to iterate to find the “right” model? What if the predictions from this model need to feed into a later one? How constrained do we feel about adding just a little bit more complexity to the model, or increasing our N size just a bit more?</p>
<p>If we can get VI to fit well, we can make complex Bayesian models a lot more practical to use in a wider variety of scenarios, and maybe even extend the complexity of what we can build given time and resource constraints.</p>
</section>
<section id="spherical-cow-sadness" class="level1">
<h1>Spherical Cow Sadness</h1>
<section id="ive-got-that" class="level5">
<h5 class="anchored" data-anchor-id="ive-got-that">I’ve got that…</h5>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 67.6%;justify-content: center;">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt1/rstanarm_disclaimer.png" class="img-fluid"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 5.4%;justify-content: center;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 27.0%;justify-content: center;">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt1/blei_vi_spherical.png" class="img-fluid"></p>
</div>
</div>
</div>
<p>If VI can make Bayesian inference much faster, what’s the catch? The above two images encapsulate the problem pretty well. First, as the left screenshot from <a href="https://mc-stan.org/rstanarm/reference/rstanarm-package.html#estimation-algorithms">rstanarm’s documentation</a> shows, variational inference requires a (bold text warning requiring) set of approximating distribution choices in order to be tractable to optimize. On the right, in their survey paper on VI, <a href="https://arxiv.org/pdf/1601.00670.pdf">Blei et al.&nbsp;(2018)</a> are showing one of the potential posterior distorting consequences of our choice to approximate.</p>
<p>So stepping back for a second, we’ve taken a problem for which there’s usually no closed form solution (Bayesian inference), where even the best approximation algorithm we can usually use (MCMC) isn’t always enough for valid inference without very careful validation and tinkering. Then we decided our approximation could do with being more approximate.</p>
<p>That was perhaps an overly bleak description, but it should give some intuition why this is a hard problem. We want to choose some method of approximating our posterior such that it is amenable to optimization-based solving instead of requiring sampling, but not trade away our ability to correctly understand the full complexity of the posterior distribution<sup>1</sup>.</p>
</section>
</section>
<section id="introducing-mrp-and-our-running-example" class="level1">
<h1>Introducing MRP and our running example</h1>
<section id="introducing-mrp" class="level2">
<h2 class="anchored" data-anchor-id="introducing-mrp">Introducing MRP</h2>
<p>While I’m mostly focused on the way we choose to actually fit a given model with this series, here’s a super quick review of the intuition in building a MRP model. If you want a more complete introduction, Kastellec’s <a href="https://scholar.princeton.edu/jkastellec/publications">MRP Primer</a> is a great starting point, as are the case studies I link a bit later.</p>
<p>MRP casts estimation of a population quantity of interest <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> as a prediction problem. That is, instead of the more traditional approach of building <a href="https://www.pewresearch.org/methods/2018/01/26/how-different-weighting-methods-work/#raking">simple raked weights</a> and using weighted estimators, MRP leans more heavily on modeling and then poststratification to make the estimates representative.</p>
<p>To sketch out the steps-</p>
<ol type="1">
<li>Either gather or run a survey or collection of surveys that collect both information on the outcome of interest, <img src="https://latex.codecogs.com/png.latex?y">, and a set of demographic and geographic predictors, <img src="https://latex.codecogs.com/png.latex?%5Cleft(X_%7B1%7D,%20X_%7B2%7D,%20X_%7B3%7D,%20%5Cldots,%20X_%7Bm%7D%5Cright)">.</li>
<li>Build a poststratification table, with population counts or estimated population counts <img src="https://latex.codecogs.com/png.latex?N_%7Bj%7D"> for each possible combination of the features gathered above. Each possible combination <img src="https://latex.codecogs.com/png.latex?j"> is called a cell, one of <img src="https://latex.codecogs.com/png.latex?J"> possible cells. For example, if we poststratified only on state, there would be <img src="https://latex.codecogs.com/png.latex?J=51"> (with DC) total cells; in practice, <img src="https://latex.codecogs.com/png.latex?J"> is often several thousand.</li>
<li>Build a model, usually a Bayesian multilevel regression, to predict <img src="https://latex.codecogs.com/png.latex?y"> using the demographic characteristic from the survey or set of surveys, estimating model parameters along the way.</li>
<li>Estimate <img src="https://latex.codecogs.com/png.latex?y"> for each cell in the poststratification table, using the model built on the sample.</li>
<li>Aggregate the cells to the population of interest, weighting by the <img src="https://latex.codecogs.com/png.latex?N_%7Bj%7D">’s to obtain population level estimates: <img src="https://latex.codecogs.com/png.latex?%5Ctheta_%7B%5Cmathrm%7BPOP%7D%7D=%5Cfrac%7B%5Csum_%7Bj%20%5Cin%20J%7D%20N_%7Bj%7D%20%5Ctheta_%7Bj%7D%7D%7B%5Csum_%7Bj%20%5Cin%20J%7D%20N_%7BJ%7D%7D"></li>
</ol>
<p>Why would we want to do this over building more typical survey weights? To the extent your new model has desirable properties like the ability to incorporate priors, can partially pool to manage rare subpopulations where you don’t have a lot of sample, and so on, you can get the benefits of that more efficient model through MRP. Raking in its simplest form is really just a linear model; we have plenty of methods that can do better. Outside of bayesian multilevel models which are the most common, there’s an increasing literature on using a wide variety of machine learning algorithms like BART<sup>2</sup> to do the estimation stage; Andrew Gelman calls this <a href="https://statmodeling.stat.columbia.edu/2018/05/19/regularized-prediction-poststratification-generalization-mister-p/">RRP</a>.</p>
</section>
<section id="introducing-the-running-example" class="level2">
<h2 class="anchored" data-anchor-id="introducing-the-running-example">Introducing the Running Example</h2>
<p>Rather than reinvent the wheel, I’ll follow the lead of the excellent <a href="https://bookdown.org/jl5522/MRP-case-studies/">Multilevel Regression and Poststratification Case Studies</a> by Lopez-Martin, Philips, and Gelman, and model survey binary responses from the <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ZSBZ7K">2018 CCES</a> for the following question:</p>
<blockquote class="blockquote">
<p>Allow employers to decline coverage of abortions in insurance plans (Support / Oppose)</p>
</blockquote>
<p>From the CCES, we get information on each participant’s state, age, gender, ethnicity, and education level. Supplementing this individual level data, we also include region flags for each state, and Republican vote share in the 2016 election- these state level predictors have been shown to be critical for getting strong MRP estimates by <a href="http://www.columbia.edu/~jhp2121/publications/HowShouldWeEstimateOpinion.pdf">Lax and Philips (2009)</a> and others. and If you’d like deeper detail on the dataset itself, I’d refer you to <a href="https://bookdown.org/jl5522/MRP-case-studies/introduction-to-mister-p.html#ref-2018CCES">this part</a> MRP case study.</p>
<p>Using these, we setup the model for <img src="https://latex.codecogs.com/png.latex?Pr(y_i%20=%201)"> the probability of supporting allowing employers to decline coverage of abortions in insurance plans as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0APr(y_i%20=%201)%20=&amp;%20logit%5E%7B-1%7D(%0A%5Cgamma%5E0%0A+%20%5Calpha_%7B%5Crm%20s%5Bi%5D%7D%5E%7B%5Crm%20state%7D%0A+%20%5Calpha_%7B%5Crm%20a%5Bi%5D%7D%5E%7B%5Crm%20age%7D%0A+%20%5Calpha_%7B%5Crm%20r%5Bi%5D%7D%5E%7B%5Crm%20eth%7D%0A+%20%5Calpha_%7B%5Crm%20e%5Bi%5D%7D%5E%7B%5Crm%20educ%7D%0A+%20%5Cbeta%5E%7B%5Crm%20male%7D%20%5Ccdot%20%7B%5Crm%20Male%7D_%7B%5Crm%20i%7D%20%5C%5C%0A&amp;+%20%5Calpha_%7B%5Crm%20g%5Bi%5D,%20r%5Bi%5D%7D%5E%7B%5Crm%20male.eth%7D%0A+%20%5Calpha_%7B%5Crm%20e%5Bi%5D,%20a%5Bi%5D%7D%5E%7B%5Crm%20educ.age%7D%0A+%20%5Calpha_%7B%5Crm%20e%5Bi%5D,%20r%5Bi%5D%7D%5E%7B%5Crm%20educ.eth%7D%0A+%20%5Cgamma%5E%7B%5Crm%20south%7D%20%5Ccdot%20%7B%5Crm%20South%7D_%7B%5Crm%20s%7D%20%5C%5C%0A&amp;+%20%5Cgamma%5E%7B%5Crm%20northcentral%7D%20%5Ccdot%20%7B%5Crm%20NorthCentral%7D_%7B%5Crm%20s%7D%0A+%20%5Cgamma%5E%7B%5Crm%20west%7D%20%5Ccdot%20%7B%5Crm%20West%7D_%7B%5Crm%20s%7D%0A+%20%5Cgamma%5E%7B%5Crm%20repvote%7D%20%5Ccdot%20%7B%5Crm%20RepVote%7D_%7B%5Crm%20s%7D)%0A%5Cend%7Baligned%7D%0A"></p>
<p>Where we incorporate pretty much all of our predictors as varying intercepts to allow for pooling across demographic and geographic characteristics:</p>
<ul>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20a%7D%5E%7B%5Crm%20age%7D">: The effect of subject <img src="https://latex.codecogs.com/png.latex?i">’s age on the probability of supporting the statement.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20r%7D%5E%7B%5Crm%20eth%7D">: The effect of subject <img src="https://latex.codecogs.com/png.latex?i">’s ethnicity on the probability of supporting the statement.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20e%7D%5E%7B%5Crm%20educ%7D">: The effect of subject <img src="https://latex.codecogs.com/png.latex?i">’s education on the probability of supporting the statement.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20s%7D%5E%7B%5Crm%20state%7D">: The effect of subject <img src="https://latex.codecogs.com/png.latex?i">’s state on the probability of supporting the statement.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Cbeta%5E%7B%5Crm%20male%7D">: The average effect of being male on the probability of supporting abortion. Note that it doesn’t really make much sense to model a two category<sup>3</sup> factor as a varying intercept.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20e,r%7D%5E%7B%5Crm%20male.eth%7D">, <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20e,r%7D%5E%7B%5Crm%20educ.age%7D">, <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20e,r%7D%5E%7B%5Crm%20educ.eth%7D">: Are several reasonable guesses at important interactions for this question. We could add many more two way, or even some three way interactions here, but this is enough for my testing here.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Cgamma%5E%7B%5Crm%20south%7D,%20%5Cgamma%5E%7B%5Crm%20northcentral%7D,%20%5Cgamma%5E%7B%5Crm%20west%7D,%5Cgamma%5E%7B%5Crm%20repvote%7D">: are the state level predictors which are not represented as varying intercepts. Following the case study, I use <img src="https://latex.codecogs.com/png.latex?%5Cgamma">’s for the state level coefficients, keeping <img src="https://latex.codecogs.com/png.latex?%5Cbeta">’s for individual coefficients. Note that Northeast is the base region of the region factor here, so it doesn’t get it’s own coefficient.</p></li>
</ul>
<p>Stepping back for a second, let’s describe the complexity of this model in more general terms. This certainly isn’t state of the art for MRP, and you could definitely add in things like a lot more interactions, some varying slopes, non-univariate prior and/or structured priors, or other elements to make this a more interesting model. That said, this is already clearly enough of a model to improve on simple raking in many cases, and it produces a nuanced enough posterior that we can feasibly imagine a bad approximation going all spherical cow shaped on us.</p>
<p>Why this dataset and this model for this series? The question we model itself isn’t super important- as long as we can expect some significant regional and demographic variation in the outcome we’ll be able to explore if VI smoothes away some posterior complexity that MCMC can capture. Drawing an example from the CCES is quite useful, as the 60k total sample is much larger than typical publicly available surveys, and so we can check behavior under larger N sizes. Practically, fitting this with <code>rstanarm</code> allows us to switch easily from a great MCMC implementation to a decent VI optimizer quickly for some early tests. Finally, the complexity and runtime of the model is a nice balance of being something that we can fit with MCMC in a not terrible amount of time for comparison’s sake, and something challenging enough that it should teach us something about VI’s ability to handle non-toy models of the world.</p>
<p>Fitting this<sup>4</sup> with MCMC in <code>rstanarm</code> is as simple as:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Fit in stan_glmer</span></span>
<span id="cb1-2">fit <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stan_glmer</span>(abortion <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> state) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> male <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-3">                    (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> male<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>age) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-4">                    repvote <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">factor</span>(region),</span>
<span id="cb1-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">binomial</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">link =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"logit"</span>),</span>
<span id="cb1-6">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> cces_df,</span>
<span id="cb1-7">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">autoscale =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>),</span>
<span id="cb1-8">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior_covariance =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">decov</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">scale =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.50</span>),</span>
<span id="cb1-9">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">adapt_delta =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.99</span>,</span>
<span id="cb1-10">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">refresh =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,</span>
<span id="cb1-11">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">seed =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">605</span>)</span></code></pre></div>
</div>
<p>Since it isn’t relevant for the rest of my discussion here, I’ll summarize the model diagnostics here and say that this seems to be a pretty reasonable fit- no issues with divergences, and no issues with poor <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D">’s. Worth quickly pointing out that we did have to tune <code>adapt_delta</code> a bit to get no divergences though- even before getting to fitting this with VI, a model like this requires some adjustments to fit correctly.</p>
<p>With a model like this on just a 5k sample, we can produce pretty solid state level predictions that have clearly benefited from being fit with a Bayesian multilevel model:</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt1/5k_sample_full_results.png" class="img-fluid"></p>
<p>With a 5k sample, MRP lands much closer to the complete weighted survey than a 5k unweighted sample: neat. That’s certainly not a fully fair comparison, but it gives some intution around the promise of this approach.</p>
<p>Somewhat less neat is that even a 5k sample here takes about 13 minutes to fit. How does this change as we fit on more and more of the data?</p>
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 75%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Sample Size</th>
<th style="text-align: left;">Runtime</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">5,000</td>
<td style="text-align: left;">13 minutes</td>
</tr>
<tr class="even">
<td style="text-align: left;">10,000</td>
<td style="text-align: left;">44 minutes</td>
</tr>
<tr class="odd">
<td style="text-align: left;">60,000</td>
<td style="text-align: left;">526 minutes (~8 hours!)</td>
</tr>
</tbody>
</table>
<p>As the table above should illustrate, if you’re fitting a decently complex Bayesian model on even somewhat large N sizes, you’re pretty quickly going to cap out what you can reasonably fit in a acceptable amount of time. If you’re scaling N past the above example, or deepening the modeling complexity, you’ll pretty quickly feel effectively locked out of using these models in fast-paced environments.</p>
<p>Hopefully fitting my running example has helped for building intuition here. Even a reasonably complex Bayesian model can have some pretty desirable estimation properties. To make iterating on modelling choices faster, to scale our N or model complexity higher, or just to use a model like this day to day when time matters, we’d really like to scale these fitting times back. Can Variational Inference help?</p>
</section>
</section>
<section id="introducing-variational-inference" class="level1">
<h1>Introducing Variational Inference</h1>
<p>I’ve gotten relatively far in this post without clearly explaining what Variational Inference is, and why it might provide a more efficient and scalable way to fix large Bayesian models. Let’s fully flesh that out here to ground the rest of the series.</p>
<p>In the bigger picture, pretty much all of our efforts in Bayesian inference are a form of approximate inference. Almost no models we care about for real world applications have closed form solutions- conjugate prior type situations are a math problem for stats classes, not a general tool for inference.</p>
<p>Following <a href="https://arxiv.org/abs/1601.00670">Blei et al.&nbsp;(2018)</a>’s notation, let’s setup the general problem first, describe (briefly) how MCMC solves it, and then more slowly demonstrate how VI does. Let’s say we have some observations <img src="https://latex.codecogs.com/png.latex?x_%7B1:N%7D">, and and some latent variables that define the model <img src="https://latex.codecogs.com/png.latex?z_%7B1:M%7D">. Note for concreteness these latent variables represent our quantities of interest: key parameters and so on- we’re calling them latent in the sense that we can’t go out and directly measure a <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> or <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> from the model above, we have to gather data that allows us to estimate them. We call <img src="https://latex.codecogs.com/png.latex?p(z)"> priors, and they define our model prior to contact with the data. The goal of Bayesian inference then is conditioning on our data in order to get the posterior:</p>
<p><img src="https://latex.codecogs.com/png.latex?p(z%7Cx)%20=%20%5Cfrac%7Bp(z,x)%7D%7Bp(x)%7D"></p>
<p>If you’re reading this post series, it’s likely you recognize that the denominator on the right here (often called the “evidence”) is the sticking point; the integral <img src="https://latex.codecogs.com/png.latex?p(x)%20=%20%5Cint%7Bp(z,x)dz%7D"> won’t have a closed form solution.</p>
<p>When we use Markov Chain Monte Carlo as we did above to estimate the model, we’re defining a Markov Chain on <img src="https://latex.codecogs.com/png.latex?z">, whose stationary distribution if we’ve done everything right is <img src="https://latex.codecogs.com/png.latex?p(z%7Cx)">. There are better and worse ways to do this certainly- the development of the <a href="https://mc-stan.org/">Stan</a> language, with associated <a href="https://mc-stan.org/docs/2_19/reference-manual/hamiltonian-monte-carlo.html">Hamiltonian Monte Carlo</a> with <a href="https://arxiv.org/abs/1111.4246">NUTS</a> sampler has massively expanded what was possible to fit in recent years. However, while actively improving the speed and scalability of sampling is an active area of research (for example, by using <a href="https://mc-stan.org/cmdstanr/articles/opencl.html">GPU compute</a> where possible), some of the speed challenges just seem a bit baked into the approach. For example, the sequential nature of markov chains makes parallelization within chains seem out of reach absent some as-yet unknown clever tricks.</p>
<p>Instead of sampling, variational inference asks what we’d need to figure out to treat the Bayesian inference problem as an <strong>optimization problem</strong>, where we could bring to bear all the tools for efficient, scalable, and parallelizable optimization we have developed.</p>
<p>Let’s start with the idea of a family of approximate densities <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BQ%7D"> over our latent variables<sup>5</sup>.</p>
<p>Within that <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BQ%7D">, we want to try the best <img src="https://latex.codecogs.com/png.latex?q(z)">, call it <img src="https://latex.codecogs.com/png.latex?q%5E*(z)">, that minimizes the Kullback-Leibler divergence to the true posterior:</p>
<p><img src="https://latex.codecogs.com/png.latex?q%5E*(z)%20=%20argmin_%7Bq(z)%20%5Cin%20%5Cmathscr%7BQ%7D%7D(q(z)%7C%7Cp(z%7Cx))"></p>
<p>If we choose a good <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BQ%7D">, managing the complexity so that it includes a density close to <img src="https://latex.codecogs.com/png.latex?p(z%7Cx)">, without becoming too slow or impossible to optimize, this approach may provide a significant speed boost.</p>
<p>To start working with this approach though, there’s one major remaining problem. Do you see it in the equation above?</p>
<section id="the-elbo" class="level2">
<h2 class="anchored" data-anchor-id="the-elbo">The ELBO</h2>
<p>If you haven’t seen it yet, this quick substitution should clarify a potential issue with VI as I’ve described it so far:</p>
<p><img src="https://latex.codecogs.com/png.latex?q%5E*(z)%20=%20argmin_%7Bq(z)%20%5Cin%20%5Cmathscr%7BQ%7D%7D(q(z)%7C%7C%5Cfrac%7Bp(z,x)%7D%7B%5Cbf%20p(x)%7D)%20=%20%5Cmathbb%7BE%7D%5Blogq(z)%5D%20-%20%5Cmathbb%7BE%7D%5Blogp(z,x)%5D%20+%20%7B%5Cbf%20logp(x)%7D"> Without some new trick, all I’ve said so far is to approximate a thing I can’t analytically calculate (the posterior, specially the issue evidence piece of it), I’m going to calculate the distance between my approximation and… the thing I said has a component can’t calculate?</p>
<p>Fortunately, a clever solution exists here that makes this strategy possible. Instead of trying to minimize the above KL divergence, we can optimize the alternative objective:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5Blogp(z,x)%5D%20-%20%5Cmathbb%7BE%7D%5Blogq(z)%5D"></p>
<p>This is just the negative of the first two terms above, leaving aside the <img src="https://latex.codecogs.com/png.latex?logp(x)">. Why can we treat maximizing this as minimizing the KL divergence? The <img src="https://latex.codecogs.com/png.latex?logp(x)"> term is just a constant (with respect to q), so regardless of how we vary q, this will still be a valid alternative objective. We call this the Evidence Lower Bound (ELBO)<sup>6</sup>.</p>
<p>If it’s helpful for intuition, play around with this great interactive ELBO optimizer by Felix Köhler:</p>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt1/elboplot.png" class="img-fluid"></p>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt1/elboeqs.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Link to demonstration <a href="https://englishprobabilistic-machine-learningelbo-interactive--or5u7m.streamlitapp.com/">here</a>; check out Felix’s Youtube explanation of the ELBO <a href="https://www.youtube.com/watch?v=HxQ94L8n0vU">also</a>!</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>By twiddling the knobs on <img src="https://latex.codecogs.com/png.latex?%5Cmu"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma"> for our approximating normal, we can get our surrogate distribution pretty close to the True Posterior (which we know for purposes of demonstration, so we can calculate the true KL, not just it’s ELBO component). No matter how we twiddle though, the evidence remains constant.</p>
<p>For further intuition- notice that we can only do this trick in one direction. The KL divergence isn’t symmetrical, and if we wanted to calculate the “reverse” KL, we couldn’t use this strategy as <img src="https://latex.codecogs.com/png.latex?logq(x)"> would not be a constant. Even if we thought that optimizing other direction of KL might have desirable properties like emphasizing <a href="https://agustinus.kristia.de/techblog/2016/12/21/forward-reverse-kl/">mass-seeking over mode-seeking behavior</a>, that simply isn’t an option.</p>
</section>
</section>
<section id="a-first-try-at-vi-on-this-dataset" class="level1">
<h1>A first try at VI on this dataset</h1>
<p>Ok, so we have an objective to optimize that should actually work. What’s a good <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BQ%7D">? The choice has been shown to matter a lot, but for purposes of a first swing here, let’s try one of the simpler ideas people have explored, the mean-field family. These latent variables will be assumed mutually independent<sup>7</sup> and each get it’s own distinct factor in the variational density. A member of this would look something like:</p>
<p><img src="https://latex.codecogs.com/png.latex?q(z)%20=%20%5Cprod_%7Bj=1%7D%5E%7Bm%7D%20q_j(z_j)"></p>
<p>Each latent <img src="https://latex.codecogs.com/png.latex?z_j"> get it’s own variational factor with density <img src="https://latex.codecogs.com/png.latex?q_j(z_j)">, whose knobs we play with to maximize the ELBO. In the particular implementation below normal distributions are used, plenty of other options like t distributions are common too.</p>
<p>Probably not the best we can do, but let’s give it a roll. Since we’ve been told this will scale really well too supposedly, let’s use all 60k of the observations just to get a sense how it’ll compare to our 8+ hours in that case.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">tic</span>()</span>
<span id="cb2-2">fit_60k <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stan_glmer</span>(abortion <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> state) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> male <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-3">                    (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> male<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>age) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> educ<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>eth) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-4">                    repvote <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">factor</span>(region),</span>
<span id="cb2-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">binomial</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">link =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"logit"</span>),</span>
<span id="cb2-6">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> cces_all_df,</span>
<span id="cb2-7">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">autoscale =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>),</span>
<span id="cb2-8">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior_covariance =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">decov</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">scale =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.50</span>),</span>
<span id="cb2-9">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">adapt_delta =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.99</span>,</span>
<span id="cb2-10">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">refresh =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,</span>
<span id="cb2-11">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">algorithm =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"meanfield"</span>,</span>
<span id="cb2-12">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">seed =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">605</span>)</span>
<span id="cb2-13"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">toc</span>()</span></code></pre></div>
</div>
<p>This finishes in a blazing <strong>144.03 seconds</strong>. Is this a good fit, or have we created a ridiculous spherical cow?</p>
<p>You’ll have to find out in the next post. Thanks for reading!</p>
<p><em>Typically, I’ll include links to code at the end of these posts, but since the only thing going on in this notebook is mentioning some runtimes of the models displayed inline at various sample sizes, I’m skipping that for now.</em></p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>If I were that type of Bayesian, this is where I’d complain that if we screw this up badly enough, we might as well be frequentists or worse, machine learning folk.↩︎</p></li>
<li id="fn2"><p>In grad school, using BART as the estimator (also combining it with some portions of the model being estimated as multilevel models) was the focus of my <a href="https://andytimm.github.io/posts/BART%20VI/2020-03-06-BART-vi.html">masters thesis</a>. This pairs the best parts of relatively black box machine learning sensibility with the advantages of still having a truly Bayesian model. With comparatively minimal iteration you can get a pretty decent set of MRP models that will be better than many basic versions of multilevel models fit early in the MRP literature. Of course, if you’re willing to spend a bunch of time iterating on the absolute best models for a given problem, and incorporate lots of problem specific knowledge into model forms you can and should do better than <a href="https://github.com/jbisbee1/BARP">BARP</a>. Also, a lot of pretty cool things you can do like jointly model multiple question responses at the same time aren’t going to be easily to implement unless you get way in the weeds of your own BART implementation.↩︎</p></li>
<li id="fn3"><p>Insert snark about CCES folks doing a poor job at gender inclusivity despite 80+ researchers working on it here.↩︎</p></li>
<li id="fn4"><p>Again, see the MRP case studies linked above if you want see all the data prep and draw manipulation here; I’ll be leaving out most such details that aren’t relevant for comparisons to fitting this model with VI from now on.↩︎</p></li>
<li id="fn5"><p>In grad school, I had a friend who insisted on calling this “spicy Q”. For a while we had a latex package that made <code>\spicy{}</code> equivalent to <code>\mathscr{}</code>. Apologies for the footnote for the dumb LaTeX joke, but now I’m pretty sure you won’t have a sudden moment of “what is that symbol again” discussing VI ever.↩︎</p></li>
<li id="fn6"><p>Why is this a lower bound? Notice that we could write the evidence from above equations as <img src="https://latex.codecogs.com/png.latex?logp(x)%20=%20KL(q(z)%7C%7Cp(z%7Cx))%20+%20ELBO(q)">. Since the KL divergence is non-negative (it’s zero when distributions <img src="https://latex.codecogs.com/png.latex?p"> and <img src="https://latex.codecogs.com/png.latex?q"> are identical), the ELBO is a lower bound of the evidence.↩︎</p></li>
<li id="fn7"><p>If this seems like it could go fully spherical cow, both literally in the sense that if we use a bunch of independent normals we make a sphere, and in the sense that this may not represent the full complexity of public opinion, you’re correct. Assuming independence here could very easily cause problems, and part of why this VI strategy is so challenging is the subset of things we can easily optimize doesn’t have the best overlap with fully realistic distributional assumptions over our latent variables.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2022,
  author = {Andy Timm},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2022-10-10},
  url = {https://andytimm.github.io/variational_mrp_pt1.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2022" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2022. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> October 10, 2022. <a href="https://andytimm.github.io/variational_mrp_pt1.html">https://andytimm.github.io/variational_mrp_pt1.html</a>.
</div></div></section></div> ]]></description>
  <category>MRP</category>
  <category>BART</category>
  <category>Variational Inference</category>
  <guid>https://andytimm.github.io/posts/Variational MRP Pt1/variational_mrp_pt1.html</guid>
  <pubDate>Mon, 10 Oct 2022 04:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Variational MRP Pt1/elboplot.png" medium="image" type="image/png" height="43" width="144"/>
</item>
<item>
  <title>BART with varying intercepts in the MRP framework</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/BART VI/2020-03-06-BART-vi.html</link>
  <description><![CDATA[ 




<p>This is the first of a few posts about some of the substantive and modeling findings from my master’s thesis, where I use Bayesian Additive Regression Trees (BART) and Poststratification to model support for a border wall from 2015-2019. In this post I explore some of the properties of using BART with varying intercepts (BART-vi) within the MRP framework.</p>
<!--more-->
<section id="choosing-a-prediction-model-for-mrp" class="level2">
<h2 class="anchored" data-anchor-id="choosing-a-prediction-model-for-mrp">Choosing a prediction model for MRP</h2>
<p>Multilevel Regression and Poststratification has seen huge success as a tool for obtaining accurate estimates of public opinion in small areas using national surveys. As the name would suggest, the most common tool used for the modeling step of these models are multilevel regressions, often Bayesian multilevel ones. The key intuition here is that pooling data across levels of predictors like state makes much more efficient use of the underlying data, which leads to more accurate estimates. However, there is no strict requirement that the models used here be multilevel regressions, it’s simply an efficient and stable way to get regularized predictions using quite a large set of predictors. Recently, academic work has begun to explore using a wide class of machine learning algorithms as the predictive component in this framework. Andrew Gelman calls this RRP: <a href="https://statmodeling.stat.columbia.edu/2018/05/19/regularized-prediction-poststratification-generalization-mister-p/">Regularized Regression and Poststratification</a>.</p>
<p>One particularly promising alternative prediction algorithm is <a href="https://arxiv.org/abs/0806.3286">BART</a>, which puts the high accuracy of tree-based algorithms like random forests or gradient boosting into a Bayesian framework. This has a number of appealing advantages compared to other machine learning options, especially for RRP. First, unlike other algorithms which might not have clear measures of their uncertainty or confidence intervals around their predictions, BART approximates a full posterior distribution. Second, BART has a number of prior and hyperparameter choices that have been shown to be highly effective in a wide variety of settings, somewhat reducing the need for parameter search. Finally, BART runs fast, especially when compared to the Bayesian multilevel models commonly used for MRP.</p>
<p>Of course, BART models are not without their disadvantages. First and foremost, there is currently only a small amount of recent work on BART models for categorical (as opposed to binary) response <a href="https://arxiv.org/abs/1701.01503">(Murray, 2019)</a>, and no public implementation of that model that I am aware of. In my case, this means modeling the border wall question as binary “support vs.&nbsp;oppose”, as opposed to the three categories “support, oppose, don’t know”. Given the salience of the issue, only 3.1% of people responded “Don’t Know”, so this is a relatively minor loss. However, for questions like the formerly crowded 2020 democratic primary, or a general election where third parties play a major role, this could be a much more serious loss.</p>
<p>While only a small amount of work has compared the two so far, estimates using BART appear to slightly outperform those using multilevel models. For example, <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12361">Montgomery &amp; Olivella (2018)</a> compared using BART to <a href="http://www.stat.columbia.edu/~gelman/research/published/misterp.pdf">Gelman &amp; Ghitza’s (2013)</a>’s fairly complex multilevel model, finding the predictions were incredibly similar, being as much as .97 correlated. As they note however, their BART model produced these results both without large amounts of iteration on model form (which has been a constant challenge for MRP), and producing such estimates orders of magnitude faster. Similarly, <a href="https://www.cambridge.org/core/journals/american-political-science-review/article/barp-improving-mister-p-using-bayesian-additive-regression-trees/630866EB47F9366EDB3C22CFD951BB6F">Bisbee (2019)</a> finds across 89 different datasets that BART and MRP produced very similar estimates, but BART’s were of slightly higher quality, both by Mean Absolute Error (MAE) and Interstate Correlation (a measure of how well the state level predictions from a model using national surveys line up with state level polls). Ending his article, Bisbee writes “One avenue of future research might focus on variants of Bayesian additive regression trees that embed a multilevel component, likely providing further improvements as the best of both worlds.”</p>
<p>This is exactly what I do in my thesis, using BART with varying intercepts by state to model support for a border wall by state. To present my findings around BART-vi, I’ll start by providing a brief overview of the MRP framework. Next, I’ll explain BART, and how BART-vi extends this model. Finally, I’ll build one model of each BART type, and compare them.</p>
</section>
<section id="a-quick-review-of-multilevel-regression-and-poststratification" class="level2">
<h2 class="anchored" data-anchor-id="a-quick-review-of-multilevel-regression-and-poststratification">A Quick Review of Multilevel Regression and Poststratification</h2>
<p>While I’m mostly focused on the modeling step with this post, here’s a quick review of the overall process in building a MRP/RRP model. If you want a more complete introduction, Kastellec’s <a href="https://scholar.princeton.edu/jkastellec/publications">MRP Primer</a> is a great starting point.</p>
<p>MRP or RRP cast estimation of a population quantity of interest <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> as a prediction problem. That is, instead of the more traditional approach of attempting to design the initial survey to be representative of the population, MRP leans more heavily on modeling and poststratification to make the estimates representative.</p>
<p>To sketch out the steps-</p>
<ol type="1">
<li>Either gather or run a survey or collection of surveys that collect both information on the outcome of interest, <img src="https://latex.codecogs.com/png.latex?y">, and a set of demographic and geographic predictors, <img src="https://latex.codecogs.com/png.latex?%5Cleft(X_%7B1%7D,%20X_%7B2%7D,%20X_%7B3%7D,%20%5Cldots,%20X_%7Bm%7D%5Cright)">.</li>
<li>Build a poststratification table, with population counts or estimated population counts <img src="https://latex.codecogs.com/png.latex?N_%7Bj%7D"> for each possible combination of the features gathered above. Each possible combination <img src="https://latex.codecogs.com/png.latex?j"> is called a cell, one of <img src="https://latex.codecogs.com/png.latex?J"> possible cells. For example, if we poststratified only on state, there would be <img src="https://latex.codecogs.com/png.latex?J=51"> (with DC) total cells; in practice, <img src="https://latex.codecogs.com/png.latex?J"> is often several thousand.</li>
<li>Build a model, usually a Bayesian multilevel regression, to predict <img src="https://latex.codecogs.com/png.latex?y"> using the demographic characteristic from the survey or set of surveys, estimating model parameters along the way.</li>
<li>Estimate <img src="https://latex.codecogs.com/png.latex?y"> for each cell in the poststratification table, using the model built on the sample.</li>
<li>Aggregate the cells to the population of interest, weighting by the <img src="https://latex.codecogs.com/png.latex?N_%7Bj%7D">’s to obtain population level estimates: <img src="https://latex.codecogs.com/png.latex?%5Ctheta_%7B%5Cmathrm%7BPOP%7D%7D=%5Cfrac%7B%5Csum_%7Bj%20%5Cin%20J%7D%20N_%7Bj%7D%20%5Ctheta_%7Bj%7D%7D%7B%5Csum_%7Bj%20%5Cin%20J%7D%20N_%7BJ%7D%7D"></li>
</ol>
</section>
<section id="bart" class="level2">
<h2 class="anchored" data-anchor-id="bart">BART</h2>
<p>In this section, I review the general BART model, and discuss the hyperparameter choices I use. Proposed by <a href="https://arxiv.org/abs/0806.3286">Chipman et al, (2008)</a>, BART is a Bayesian machine learning algorithm that has seen widespread success in a wide variety of both predictive and causal inference applications. Like most machine learning models, it treats the prediction task as modeling the outcome <img src="https://latex.codecogs.com/png.latex?y"> as an unknown function <img src="https://latex.codecogs.com/png.latex?f"> of the <img src="https://latex.codecogs.com/png.latex?k"> predictors <img src="https://latex.codecogs.com/png.latex?y%20=%20f(X_%7Bk%7D)">.</p>
<p>BART does with this a sum of decision trees:</p>
<p><img src="https://latex.codecogs.com/png.latex?Y_%7Bk%7D=%5Csum_%7Bj=1%7D%5E%7Bm%7D%20g%5Cleft(%5Cmathbf%7BX%7D_%7Bk%7D,%20T_%7Bj%7D,%20%5Cmathbf%7BM%7D_%7Bj%7D%5Cright)+%5Cepsilon_%7Bk%7D%20%5Cquad%20%5Cepsilon_%7Bk%7D%20%5Cstackrel%7Bi%20.%20i%20.%20d%7D%7B%5Csim%7D%20N%5Cleft(0,%20%5Csigma%5E%7B2%7D%5Cright)"></p>
<p>(To start with the continuous case, before generalizing to the binary case in a moment)</p>
<p>Each tree <img src="https://latex.codecogs.com/png.latex?T_j"> splits the data along a variety of predictors, seeking to improve the purity of outcomes in each group. For instance, in seeking to partition respondents into purer groups of support or opposition for a border wall, one natural split is that of white vs.&nbsp;nonwhite respondents, after which a further split by education might further partition the white node. At the end of fitting such a tree, there are <img src="https://latex.codecogs.com/png.latex?b_%7Bj%7D"> terminal nodes (nodes at the bottom of the tree), which contain groups where the average outcome <img src="https://latex.codecogs.com/png.latex?%5Cmu_%7Bj%7D"> should be purer due to iterative splitting. This iterative splitting is equivalent to the modeling of interaction effects, and combining many such trees allows for flexible and highly non-linear functions of the predictors to be calculated. Each data point <img src="https://latex.codecogs.com/png.latex?x"> is thought of as assigned to one such terminal node for each tree, which captures <img src="https://latex.codecogs.com/png.latex?E(y%20%5Cvert%20x)">, with the collection of <img src="https://latex.codecogs.com/png.latex?u_%7Bj%7D">’s referred to collectively as <img src="https://latex.codecogs.com/png.latex?M">. Together, <img src="https://latex.codecogs.com/png.latex?m"> such trees are fit to residual errors from an initial baseline prediction iteratively, ensuring that the trees are grown in varying structures that predict well for different parts of the covariate space, not just split on the same features producing identical predictions.</p>
<p>To fit such trees to the data and not overfit, BART utilizes a Bayesian framework, placing priors on tree structure, terminal node parameters, and variance, <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">. The prior for the <img src="https://latex.codecogs.com/png.latex?u_%7Bj%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2"> are:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%20%5Cmu_%7Bj%7D%20%7C%20T_%7Bj%7D%20&amp;%20%5Csim%20N%5Cleft(%5Cmu,%20%5Csigma_%7B%5Cmu%7D%5E%7B2%7D%5Cright)%20%5C%5C%20%5Csigma%5E%7B2%7D%20&amp;%20%5Csim%20I%20G%5Cleft(%5Cfrac%7B%5Cnu%7D%7B2%7D,%20%5Cfrac%7B%5Cnu%20%5Clambda%7D%7B2%7D%5Cright)%20%5Cend%7Baligned%7D"></p>
<p>Where <img src="https://latex.codecogs.com/png.latex?I%20G(%5Calpha,%20%5Cbeta)"> is the inverse gamma distribution with shape parameter <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and rate <img src="https://latex.codecogs.com/png.latex?%5Cbeta">. The priors on the tree structure can be thought of as having 3 components. First, there is a prior on the probability that a tree of depth <img src="https://latex.codecogs.com/png.latex?d%20=%200,1,2..."> is not terminal, which is <img src="https://latex.codecogs.com/png.latex?%5Calpha(1+d)%5E%7B-%5Cbeta%7D">, with <img src="https://latex.codecogs.com/png.latex?%5Calpha%20%5Cin(0,1)%20%5Ctext%20%7B%20and%20%7D%20%5Cbeta%20%5Cin%5B0,%20%5Cinfty)">. This <img src="https://latex.codecogs.com/png.latex?%5Calpha"> controls how likely a terminal node is to be split, with smaller values indicating a lower likelihood of split, and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> controls the number of terminal nodes, larger <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> implying more nodes. The second prior on the trees is on the distribution used to choose which covariate is split on. The final prior on the trees is on the value of the chosen splitting covariate at which to split. For both these later parameters, a common choice (and the one dbarts makes) is a simple discrete uniform distribution.</p>
<p>This set of priors also requires choosing the <img src="https://latex.codecogs.com/png.latex?m,%20%5Calpha,%20%5Cbeta,%20%5Cmu_%7B%5Cmu%7D,%20%5Csigma,%20%5Cnu"> and <img src="https://latex.codecogs.com/png.latex?%5Clambda"> hyperparameters, which can be chosen via cross-validation or simply set to defaults. In general, the past literature on BART finds that the defaults developed by Mculloch work quite well in a surprisingly large number of contexts <a href="https://deepblue.lib.umich.edu/handle/2027.42/147594">(Tan, 2018)</a>. More specifically in the MRP context, both <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12361">Montgomery &amp; Olivella (2018)</a> and <a href="https://www.cambridge.org/core/journals/american-political-science-review/article/barp-improving-mister-p-using-bayesian-additive-regression-trees/630866EB47F9366EDB3C22CFD951BB6F">Bisbee (2019)</a> found little reason to utilize non-default hyperparameter choices after reasonable search. For completeness, however, I ran a small number of hyperparameter searches on my complete records data, as recommended by the author of the <a href="https://cran.r-project.org/web/packages/dbarts/index.html">dbarts</a> package. Similar to prior work, I found little reason to diverge from the defaults suggested by Chipman et al, and implemented in dbarts, although I did ultimately go with <img src="https://latex.codecogs.com/png.latex?m%20=%20200"> trees as Chipman et al.&nbsp;suggest, not the <img src="https://latex.codecogs.com/png.latex?m%20=%2075"> default in dbarts. For a full derivation of these choices and their resultant properties, see <a href="https://arxiv.org/abs/0806.3286">Chipman et al.&nbsp;(2008)</a> or <a href="https://deepblue.lib.umich.edu/handle/2027.42/147594">(Tan, 2018)</a>.</p>
<p>A final modification of this formulation of BART is needed for binary outcomes. For binary outcomes, BART uses the probit link function to model the relationship between <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?y">:</p>
<p><img src="https://latex.codecogs.com/png.latex?P%5Cleft(Y_%7Bk%7D=1%20%7C%20%5Cmathbf%7BX%7D_%7Bk%7D%5Cright)=%5CPhi%5Cleft%5BG%5Cleft(%5Cmathbf%7BX%7D_%7Bk%7D%5Cright)%5Cright%5D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5CPhi%5B.%5D"> is the cumulative distribution function of a standard normal distribution, and <img src="https://latex.codecogs.com/png.latex?G"> is the full BART model we saw earlier. This slightly modifies steps for drawing from the posterior distribution, as discussed further in <a href="https://arxiv.org/abs/0806.3286">Chipman et al.&nbsp;(2008)</a>.</p>
</section>
<section id="bart-vi" class="level2">
<h2 class="anchored" data-anchor-id="bart-vi">BART-vi</h2>
<p>To the best of my knowledge, no prior work has utilized BART with varying intercepts (BART-vi) in the MRP framework. Given the huge amount of prior work on MRP that leverages a large set of varying intercepts, this seems like a natural extension. This modifies the BART predictor for binary outcomes to</p>
<p><img src="https://latex.codecogs.com/png.latex?P%5Cleft(Y_%7Bk%7D=1%20%7C%20%5Cmathbf%7BX%7D_%7Bk%7D%5Cright)=%5CPhi%5Cleft%5BG%5Cleft(%5Cmathbf%7BX%7D_%7Bk%7D%5Cright)%20+%20%5Calpha_%7Bk%7D%5Cright%5D"></p>
<p>with <img src="https://latex.codecogs.com/png.latex?a_%7Bk%7D%20%5Csim%20N%5Cleft(0,%20%5Ctau%5E%7B2%7D%5Cright)">. Critically, this also removes the varying intercept variable from the choice of possible features to split on, modeling it purely as a varying intercept. Given that the <a href="https://cran.r-project.org/web/packages/dbarts/index.html">dbarts</a> package which I use currently only supports 1 varying intercept, the natural choice is the state variable, as it both has the most categories and is one of the original motivations for varying intercepts in MRP work. All the old priors and hyperparameters remain the same, and dbarts places an additional cauchy prior on <img src="https://latex.codecogs.com/png.latex?%5Ctau%5E%7B2%7D">. While this cauchy prior is much less informative than the half-t, half-normal, or other priors typically used for MRP, at this time it is not possible to modify the prior choice except to a gamma distribution which is also not ideal. Future work could consider fitting this type of model with the more informative priors favored by the MRP literature for random effects, although such an improvement would require a time investment in learning to modify the c++ codebase of dbarts.</p>
</section>
<section id="comparing-the-predictions-of-the-two" class="level2">
<h2 class="anchored" data-anchor-id="comparing-the-predictions-of-the-two">Comparing the predictions of the two</h2>
<p>I provide two forms of evaluation for BART-vi vs regular BART, a quantitative assessment based on 10-fold cross validation, and a graphical/qualitative comparison of state level estimates resulting from the two.</p>
<p>To test the performance of this modification, I fit BART with and without varying intercept on state to the same <img src="https://latex.codecogs.com/png.latex?m%20=%2050"> imputed<sup>1</sup> datasets, using 10-fold cross validation within each dataset. Overall, while both models are extremely accurate, the BART-vi model slightly outperforms the regular BART model without varying intercepts in terms of RMSE, MSE, and AUC on average. Of course, given that this is a test of predictive accuracy before the final poststratification, this isn’t a full validation of BART-vi’s predictive superiority in the MRP context. However, this is consistent with <a href="https://deepblue.lib.umich.edu/handle/2027.42/147594">(Tan, 2018)</a>’s result in a more extensive set of simulation studies that there are small gains in accuracy to be had with BART-vi when random effects are used with an appropriate grouping variable. To make such a comparison completely rigorously, one would need to fit both types of models on a dataset with a ground truth such as vote share, poststratify, and then contrast their properties relative to that ground truth, not simply compare predictive accuracy on the initial set of surveys. However, as this is not possible for the border wall question, I take this as a rough suggestion that BART-vi may preform better in my context, and possibly in others.</p>
<p>Plotting a comparison of the state-level median prediction from the two models after poststratification shows a familiar pattern of pooling. The BART-vi estimates are pulled somewhat towards the grand mean, whereas the ones without varying intercepts are a bit more spread out. Note, however, that we don’t see the sort of <a href="https://twitter.com/rlmcelreath/status/878268413952634880/photo/1">idealized pooling</a> trend often shown in textbook examples of multilevel models, with non-multilevel predictions that are uniformly higher above the grand mean and uniformly lower below it compared to the multilevel predictions. This is due to the simple BART model modeling much more complex interactions based on the state variables than a single level regression.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/BART VI/bart-compare.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">BART models with and without pooling</figcaption><p></p>
</figure>
</div>
<p>One particularly interesting qualitative example to illustrate the differences between the models is that of DC, which is both a small state, and an incredibly liberal one. This presents a dilemma from the perspective of varying intercepts pooling: on the one hand, with only 94 observations in the full data, we should want some pooling on DC’s estimate. On the other, DC genuinely is exceptionally liberal, which suggests that pooling it too much could hurt predictive performance. While both already somewhat regularize the 13% raw approval for the wall in our aggregated polls, BART-vi does so much more. Thus, while the average predictions are of higher quality with BART-vi, the DC and other extreme state predictions are superior without random effects. Most prior MRP work has been happy to make this sort of trade off, and based on the rough accuracy comparisons I’ve made, this appears to work well for my data and my BART model as well.</p>
<p>Comparing the full posterior distributions of the two models below, we can also see BART-vi has noticably wider 50 and 90% intervals as well (the dot indicates the median, the thick bar is the 50% interval, and the thinnest bar is the 90% one). Like with my CV testing, a complete sense of which level of uncertainty provided here is appropriate will have to wait for future MRP work that leverages data with a ground truth. However, in many cases, the fixed effects intervals border on what I’d call concerningly small- I wouldn’t be suprised if the coverage properties of the BART-vi intervals are better.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/BART VI/full-post.jpeg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Full Posterior of the Two Models</figcaption><p></p>
</figure>
</div>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next steps</h2>
<p>Given that my work shows BART-vi having some desirable properties for RRP, what might be some extensions to explore next?</p>
<p>A first obvious step might be explore this type of model on data where we do have a ground truth like voter turnout, or vote share. For a more extensive comparison, one could leverage Bisbee (2019)’s replication data, which would hopefully provide a more complete answer to whether this strategy works well in general.</p>
<p>Probably the most theoretically interesting question would be how to handle the possibility of multiple random intercepts, if dbarts or another package eventually implements them. This represents a tradeoff between the benefits of flexible Bayesian non-parametrics in BART, and the pooling behavior of varying intercepts. Initially, I thought it was entirely feasible that the BART-vi I fit would have worse predictive accuracy, given the potential benefits of splitting on state. However, given that I utilize both 2012 vote share and region as predictors, it seems that the model still had ample state level information. However, as we pooled across more variables, this would increasingly weaken the non-parametric component of the BART-vi model. In this scenario, would pooling across demographic predictors that have many fewer categories make sense? While future work will have to tell, my guess is that the answer might be that only state or other geographic variables benefit from pooling.</p>
<hr>
<p><a name="imputationnote">1</a>: Given my data had a relatively large proportion of respondents who refused to answer at least 1 demographic question (10.54%), I also explored imputing the missing characteristics using a variety of different approaches. The full details of that are coming in another post, but I ran 10-fold CV on the imputations so that the evaluation would more fully mirror my final modeling scenario.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2019,
  author = {Andy Timm},
  title = {BART with Varying Intercepts in the {MRP} Framework},
  date = {2019-07-03},
  url = {https://andytimm.github.io/2020-03-06-BART-vi.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2019" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2019. <span>“BART with Varying Intercepts in the MRP
Framework.”</span> July 3, 2019. <a href="https://andytimm.github.io/2020-03-06-BART-vi.html">https://andytimm.github.io/2020-03-06-BART-vi.html</a>.
</div></div></section></div> ]]></description>
  <category>From Old Website</category>
  <category>MRP</category>
  <category>BART</category>
  <guid>https://andytimm.github.io/posts/BART VI/2020-03-06-BART-vi.html</guid>
  <pubDate>Wed, 03 Jul 2019 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Convention Prediction with a Bayesian Hierarchical Multinomial Model</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Convention Model/2019-07-13-convention-model.html</link>
  <description><![CDATA[ 




<p>Here, I use a Bayesian hierarchical multinomial model to predict the first ballot results at the 2018 DFL (Democratic) State Convention, with data aggregated to the Party Unit level (ex: State Senate district) to guarantee anonymity. While using aggregated data obviously isn’t ideal, this sort of strategy shows a lot of promise, especially if individual level predictors could be harnessed as another level of the hierarchical model. As it stands, this is mostly a proof of concept for Bayesian hierarchical models in this context. To use something like this in practice, one could use prior predictive simulation to game out the convention under various assumptions, or condition on the first ballot data and use it to analyze trends in support and predict subsequent ballots as your floor team collects further data.</p>
<p>At this convention, I was working for Erin Murphy (who ultimately won) on her data team, and so I have access to their database on the Delegates heading into the convention. A winner is declared if any candidate reaches 60% of the Delegate pool of 1307.5 Delegates, so 785 votes. For simplicity, I focus in this project solely on predicting the first ballot results.</p>
<!--more-->
<p>Briefly, my goals with this project are:</p>
<ol type="1">
<li>Represent my pre-convention uncertainty about outcomes given the data we have intelligently.</li>
<li>See how much we can improve predictions by exploiting the hierarchical nature of districts (Party Units within Congressional Districts).</li>
<li>See if this makes sense as a modeling strategy to keep developing by adding the individual level data.</li>
</ol>
<section id="the-dataset" class="level1">
<h1>The Dataset</h1>
<p>For each of the non-empty 121 Party Units in Minnesota, my response is the number of Delegate that each candidate (Erin Murphy, Rebecca Otto, and Tim Walz) received, along with a count for “No Endorsement” voters. Thus, summing across the 121 PUs would give the full first ballot results by candidate. I thus model these using a <strong>multinomial logit model</strong> in brms.</p>
<p>In terms of predictors, we have the Congressional district each party unit is in, which should explain some variation, as Otto/Walz are generally perceived to be more appealing to rural voters, while Murphy was from the Twin Cities. I also have the estimated proportion of Delegates the Murphy campaign believed they had the support of in each Party Unit, based on their field campaign, and on the subcaucuses the Delegates were elected out of. These proportions turned out to be quite accurate, and so my assumption is they’ll be strong predictors. Using the delegate level data (including issue and candidate IDs, subcaucuses, and voter file information like gender and age) would no doubt significantly improve things.</p>
<p>I exclude the data cleaning here, but it’s available in the .Rmd on my <a href="https://github.com/andytimm/ConventionPrediction/blob/master/Convention_Prediction_Final.Rmd">github</a>. One non-obvious transformation I make is to double all counts before modeling, but halve them before analysis, as some rural, low population areas are awarded “half delegates”, and I need integer outcomes to work with a multinomial model.</p>
</section>
<section id="prior-predictive-simulation-intercept-only" class="level1">
<h1>Prior Predictive Simulation: Intercept Only</h1>
<p>I start with an intercept only model, and plot realizations of the first ballot across draws. This helps explain my level of uncertainty before we include predictors and condition on the data. To keep this post short, I only include the final result of iterating to find suitably cautious priors at this early stage, not as I add further predictors to the model. With a multinomial model like this, even just enforcing the count constraint (vote counts in each party unit have to add to their total allocation of delegates) already produces a suprisingly reasonable model.</p>
<p>For context on these priors, there was a relatively large amount of uncertainty for our campaign and all the campaigns heading into first ballot for a variety of reasons. First, we had only ID’d about 2/3 of the delegate body by first ballot. Second, it was becoming increasingly clear that Rebecca Otto didn’t have the delegates to win the convention, so it was possible we’d see a decent portion of her delegates switch sides even before first ballot. Finally, a major statewide c4, ISAIAH, had been telling their delegates to hold off on committing, but the rumor was that they were going to endorse Erin Murphy’s or Otto’s campaign (whichever progressive was more viable), so a large number of Delegates had a preference they didn’t openly state.</p>
<p>All that said, while it was highly unlikely that anyone was going to reach a winning 60% of the delegate pool (785 Delegates) on the first ballot, I did want at least a bit of probability on those outcomes. From our ID data, it looked something like 40%-20%-40% was the most likely outcome for the first ballot, which is how I set the means.</p>
<p>As a final note, voting “No Endorsement” on an early ballot is extremely rare, which is correctly reflected.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Base class is No-Endorsement</span></span>
<span id="cb1-2">initial_prior <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Intercept"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dpar =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mu2"</span>),</span>
<span id="cb1-3">              <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Intercept"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dpar =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mu3"</span>),</span>
<span id="cb1-4">              <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Intercept"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dpar =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mu4"</span>)</span>
<span id="cb1-5">              )</span>
<span id="cb1-6"></span>
<span id="cb1-7">int_only <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">brm</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bf</span>(y <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">trials</span>(Total) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family=</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">multinomial</span>(),<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data=</span>erin_data, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior =</span> initial_prior, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">sample_prior=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"only"</span>)</span>
<span id="cb1-8"></span>
<span id="cb1-9">linpred <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">posterior_linpred</span>(int_only, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">transform =</span> T)</span>
<span id="cb1-10"></span>
<span id="cb1-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Divide by 2 after summing the totals to put back on original delegate count scale</span></span>
<span id="cb1-12"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">boxplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">apply</span>(linpred, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">FUN =</span> sumdiv2), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">pch =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"."</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">las =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb1-13">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">names =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"No Endorse"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Erin Murphy"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Rebeca Otto"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Tim Walz"</span>))</span></code></pre></div>
<p><img src="https://andytimm.github.io/posts/Convention Model/unnamed-chunk-2-1.png" class="img-fluid" alt="first test"><!-- --></p>
</section>
<section id="initial-posterior" class="level1">
<h1>Initial Posterior</h1>
<p>Now let’s add in our non-CD predictors and condition on our data. I add a <img src="https://latex.codecogs.com/png.latex?N(0,3)"> prior over all the <img src="https://latex.codecogs.com/png.latex?%5Cbeta">’s as a weakly informative prior to help the model fit. The model fits well; there are no divergences, all <img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D"> were 1, and I got a good number of effective samples for each parameter.</p>
<p>As we’d expect with so little data, the standard errors are very large compared to the coefficients, but generally point the right ways- the Strong/Lean Erin predictions have a positive influence on Erin’s support (mu2), for example, and similar with Walz (mu4), and Otto (mu2). Later, when I look at marginal plots, I’ll talk more about some of the more interesting coefficients, namely the ISAIAH and Unknown ones.</p>
<pre><code>##  Family: multinomial
##   Links: mu2 = logit; mu3 = logit; mu4 = logit
## Formula: y | trials(Total) ~ ISAIAH + Lean.Erin + Lean.Walz + Lean.Otto + Strong.Erin + Strong.Otto + Strong.Walz + Undecided + Unknown
##    Data: erin_data (Number of observations: 121)
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
##
## Population-Level Effects:
##                 Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## mu2_Intercept       4.15      1.07     2.07     6.27       4931 1.00
## mu3_Intercept       3.40      1.06     1.40     5.51       5520 1.00
## mu4_Intercept       4.19      1.03     2.14     6.23       5352 1.00
## mu2_ISAIAH          1.59      1.68    -1.71     4.96       5557 1.00
## mu2_Lean.Erin       1.00      1.95    -2.92     4.80       5198 1.00
## mu2_Lean.Walz       0.24      1.99    -3.72     4.10       6179 1.00
## mu2_Lean.Otto      -1.74      2.08    -5.67     2.44       5975 1.00
## mu2_Strong.Erin     2.18      1.69    -1.17     5.52       5448 1.00
## mu2_Strong.Otto    -1.91      1.71    -5.23     1.54       6238 1.00
## mu2_Strong.Walz    -1.70      1.65    -5.03     1.63       5958 1.00
## mu2_Undecided      -0.53      1.78    -4.15     3.03       5513 1.00
## mu2_Unknown         0.11      1.55    -2.94     3.17       5267 1.00
## mu3_ISAIAH         -0.64      1.74    -3.97     2.82       5673 1.00
## mu3_Lean.Erin       0.73      2.03    -3.34     4.69       4694 1.00
## mu3_Lean.Walz      -1.27      2.10    -5.35     2.89       6218 1.00
## mu3_Lean.Otto       3.26      2.14    -0.99     7.40       5679 1.00
## mu3_Strong.Erin    -1.22      1.68    -4.51     2.08       5648 1.00
## mu3_Strong.Otto     4.28      1.70     0.94     7.74       5737 1.00
## mu3_Strong.Walz    -2.46      1.66    -5.60     0.85       6205 1.00
## mu3_Undecided       0.63      1.74    -2.81     4.06       5264 1.00
## mu3_Unknown         0.42      1.56    -2.56     3.45       6235 1.00
## mu4_ISAIAH         -1.86      1.68    -5.10     1.57       5066 1.00
## mu4_Lean.Erin      -0.39      1.97    -4.21     3.44       5386 1.00
## mu4_Lean.Walz       1.51      1.98    -2.32     5.48       6163 1.00
## mu4_Lean.Otto      -1.28      2.09    -5.41     2.80       6302 1.00
## mu4_Strong.Erin    -1.48      1.68    -4.67     1.90       5654 1.00
## mu4_Strong.Otto    -2.00      1.70    -5.33     1.47       6029 1.00
## mu4_Strong.Walz     1.46      1.60    -1.65     4.61       5859 1.00
## mu4_Undecided       0.19      1.72    -3.22     3.59       5522 1.00
## mu4_Unknown         1.08      1.51    -1.94     4.07       6072 1.00
##
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample
## is a crude measure of effective sample size, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</section>
<section id="nesting-the-party-units-in-cd" class="level1">
<h1>Nesting the Party Units in CD</h1>
<p>A next logical step for the model would be to incorporate the hierarchical structure present in the data- Party Units nested within Congressional Districts. Given that the CD’s reflect both the progressive/moderate and rural/urban divides that defined the election, expecting some significant between group variation is reasonable.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">final_prior <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Intercept"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dpar =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mu2"</span>),</span>
<span id="cb3-2">              <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Intercept"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dpar =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mu3"</span>),</span>
<span id="cb3-3">              <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Intercept"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dpar =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mu4"</span>),</span>
<span id="cb3-4">              <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>),<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"b"</span>),</span>
<span id="cb3-5">              <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sd"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">group =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"CD"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dpar =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mu2"</span>),</span>
<span id="cb3-6">              <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sd"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">group =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"CD"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dpar =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mu3"</span>),</span>
<span id="cb3-7">              <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">prior</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">normal</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sd"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">group =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"CD"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dpar =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mu4"</span>)</span>
<span id="cb3-8">              )</span>
<span id="cb3-9"></span>
<span id="cb3-10">full_model <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">brm</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bf</span>(y <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">trials</span>(Total) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> ISAIAH <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Lean.Erin <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Lean.Walz <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Lean.Otto <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Strong.Erin <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Strong.Otto <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb3-11">        Strong.Walz <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Undecided <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Unknown <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span>CD)), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family=</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">multinomial</span>(),<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior =</span> final_prior, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data=</span>erin_data)</span></code></pre></div>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summary</span>(full_model)</span></code></pre></div>
<pre><code>##  Family: multinomial
##   Links: mu2 = logit; mu3 = logit; mu4 = logit
## Formula: y | trials(Total) ~ ISAIAH + Lean.Erin + Lean.Walz + Lean.Otto + Strong.Erin + Strong.Otto + Strong.Walz + Undecided + Unknown + (1 | CD)
##    Data: erin_data (Number of observations: 121)
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
##
## Group-Level Effects:
## ~CD (Number of levels: 9)
##                   Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sd(mu2_Intercept)     0.08      0.06     0.00     0.23       2406 1.00
## sd(mu3_Intercept)     0.14      0.09     0.01     0.35       1760 1.00
## sd(mu4_Intercept)     0.09      0.07     0.00     0.24       2311 1.00
##
## Population-Level Effects:
##                 Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## mu2_Intercept       4.14      1.08     2.03     6.29       2895 1.00
## mu3_Intercept       3.33      1.09     1.24     5.46       2566 1.00
## mu4_Intercept       4.27      1.07     2.21     6.36       2919 1.00
## mu2_ISAIAH          1.69      1.66    -1.42     5.00       3289 1.00
## mu2_Lean.Erin       1.07      1.95    -2.71     4.96       3678 1.00
## mu2_Lean.Walz       0.17      2.06    -3.81     4.18       3957 1.00
## mu2_Lean.Otto      -1.38      2.10    -5.57     2.70       3972 1.00
## mu2_Strong.Erin     2.17      1.65    -1.03     5.52       3294 1.00
## mu2_Strong.Otto    -1.84      1.73    -5.14     1.55       3077 1.00
## mu2_Strong.Walz    -1.72      1.62    -4.81     1.40       3741 1.00
## mu2_Undecided      -0.52      1.80    -4.01     3.13       2834 1.00
## mu2_Unknown         0.11      1.53    -2.93     3.17       3079 1.00
## mu3_ISAIAH         -0.70      1.73    -4.01     2.66       2931 1.00
## mu3_Lean.Erin       0.68      2.05    -3.29     4.62       4101 1.00
## mu3_Lean.Walz      -1.01      2.11    -5.12     3.11       4077 1.00
## mu3_Lean.Otto       3.16      2.17    -1.11     7.43       4086 1.00
## mu3_Strong.Erin    -1.15      1.69    -4.40     2.23       3109 1.00
## mu3_Strong.Otto     4.15      1.76     0.80     7.64       2990 1.00
## mu3_Strong.Walz    -2.19      1.62    -5.38     0.99       3156 1.00
## mu3_Undecided       0.62      1.83    -2.95     4.16       3098 1.00
## mu3_Unknown         0.47      1.55    -2.56     3.48       2478 1.00
## mu4_ISAIAH         -1.91      1.73    -5.23     1.47       3164 1.00
## mu4_Lean.Erin      -0.42      1.98    -4.21     3.45       3632 1.00
## mu4_Lean.Walz       1.34      2.02    -2.61     5.35       3723 1.00
## mu4_Lean.Otto      -1.47      2.09    -5.46     2.66       3452 1.00
## mu4_Strong.Erin    -1.57      1.63    -4.74     1.57       3367 1.00
## mu4_Strong.Otto    -1.91      1.76    -5.34     1.64       2914 1.00
## mu4_Strong.Walz     1.38      1.58    -1.84     4.50       3574 1.00
## mu4_Undecided      -0.01      1.80    -3.44     3.57       3232 1.00
## mu4_Unknown         0.99      1.54    -2.00     3.99       3137 1.00
##
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample
## is a crude measure of effective sample size, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</section>
<section id="model-comparison" class="level1">
<h1>Model Comparison</h1>
<p>Intuitively, a model with pooling across CD’s should outperform a single level model using them, but let’s make sure. Below, the multilevel level model far outperforms the single level one, with it’s ELPD more than 2 standard errors better.</p>
<p>One limitation of the analysis below is that I wasn’t able refit without the final observation (the super delegates) to calculate ELPDs for the super delegates directly. This is because the “Super” level only exists in 1 example, and the loo package doesn’t allow new factor levels- holding it out would thus throw an error.</p>
<pre><code>##                  elpd_diff se_diff
## full_model         0.0       0.0
## not_pooled_model -10.7       4.9

## Method: stacking
## ------
##        weight
## model1 1.000
## model2 0.000</code></pre>
</section>
<section id="interesting-marginal-plots" class="level1">
<h1>Interesting Marginal Plots</h1>
<p>For the most part, the marginal plots were what I’d expect- for instance, PUs with a greater estimated strong support for Erin (“Strong Erin”), had increasingly greater support for Erin, and vice versa for Walz, and so I don’t show most of these.</p>
<p>Even though the predictions are obviously very noisy however, they did pick up on two important, more subtle trends. First, in the ISAIAH plot below, it’s beginning to appear that Murphy (2) does well in places with many ISAIAH delegates, whereas Walz (4) does progressively worse.</p>
<p><img src="https://andytimm.github.io/posts/Convention Model/unnamed-chunk-6-1.png" class="img-fluid"><!-- --></p>
<p>The other interesting trend the model picked up on was that it tends to be harder to ID your opponent’s supporters than your own- as your supporters want to contact and work with the campaign, but the opponents’ have no reason to do so, and help their candidate by not giving you much information. This is correctly reflected in the negative slope in Unknown for Erin (2), but positive one for Walz (4), given the predictor data comes from the Murphy campaign.</p>
<p>While these plots suggest a understandably high level of uncertainty, the fact that they’re correctly reflecting many of the relationships I believe to be true offers some level of face validity of the model.</p>
<p><img src="https://andytimm.github.io/posts/Convention Model/unnamed-chunk-7-1.png" class="img-fluid" alt="Marginal_Plot"><!-- --></p>
</section>
<section id="posterior-predictive-check" class="level1">
<h1>Posterior Predictive Check</h1>
<p>Plotting the predictions for Erin Murphy (blue) and Tim Walz (red) against the actual results, we can see the predictions track reasonably closely considering the limited data. While many point predictions fall outside the 25-75 quantile range, the vast majority stay within the boxplot’s whiskers. Again, stressing the limitations of the small dataset we have, this is a fairly reasonable range of outcomes to predict, and there’s no systematic pattern I can see to which Party Units the model struggles with.</p>
<p><img src="https://andytimm.github.io/posts/Convention Model/unnamed-chunk-8-1.png" class="img-fluid" alt="Murphy"><!-- --><img src="https://andytimm.github.io/posts/Convention Model/unnamed-chunk-8-2.png" class="img-fluid" alt="Walz"><!-- --></p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>To actually use a model like this on a convention floor, I’d definitely want to fully incorporate the individual level data that underlie what’s shown here, as attempting to model with 1 obs/district is challenging. However, this initial model is fairly promising- nesting PUs within CDs seems like a strong overall strategy, and even with limited data, the model already can pick up on relationships like that between estimated ISAIAH and Unknown support and Delegate returns. Some further details can be found in the .rmd <a href="https://github.com/andytimm/ConventionPrediction/blob/master/Convention_Prediction_Final.Rmd">here</a>.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2019,
  author = {Andy Timm},
  title = {Convention {Prediction} with a {Bayesian} {Hierarchical}
    {Multinomial} {Model}},
  date = {2019-07-03},
  url = {https://andytimm.github.io/2019-07-13-convention-model.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2019" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2019. <span>“Convention Prediction with a Bayesian
Hierarchical Multinomial Model.”</span> July 3, 2019. <a href="https://andytimm.github.io/2019-07-13-convention-model.html">https://andytimm.github.io/2019-07-13-convention-model.html</a>.
</div></div></section></div> ]]></description>
  <category>From Old Website</category>
  <category>Stan</category>
  <guid>https://andytimm.github.io/posts/Convention Model/2019-07-13-convention-model.html</guid>
  <pubDate>Wed, 03 Jul 2019 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Is Voting Habit Forming? Replication, and additional robustness checks</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html</link>
  <description><![CDATA[ 




<p>This post walks through my replication of the fuzzy regression discontinuity portion of Coppock and Green’s 2016 paper <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12210">Is Voting Habit Forming? New Evidence from Experiments and Regression Discontinuities</a>, and details some additional robustness checks I conducted. While I was able to reproduce all of their estimates fairly easily due to great <a href="http://dx.doi.org/10.7910/DVN/ALZVAW">replication materials</a>, my additional robustness checks suggest that their results are more sensitive to bandwith choices than their testing suggests. Additionally, Coppock and Green argue the effects they find are likely due to habit alone, whereas I’m unconvinced that’s the sole mechanism involved. This is my work from Jennifer Hill and Joe Robinson-Cimpian’s Causal Inference class at NYU, and I’m grateful for both their feedback on the project.</p>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>When we first vote in an election, is that experience habit forming? If so, how strong is the effect of habit? This is a critical question of voting behavior. For example, it has implications for attempting to make the electorate more representative of the general population. If people continue to vote at a reasonable rate after their first ballot cast, then the burden of organizations doing GOTV work gets comparatively lighter; further, their work has dividends for elections to come. On the extreme other end, if almost no habit forming effect existed, then turnout work would be a project of individual races. Fortunately, practical experience from campaigns suggests there’s likely at least some habit effect. After all, the best predictor of voting in the current election is voting in the prior.</p>
<p>If we believe that there may be a habit effect on voting in an initial in future elections, how can we estimate a causal effect? Of course, we cannot use a randomized experiment: we cannot ethically randomize some citizens to vote, or more concerningly, randomize some citizens to not do so. Thus, the main observational strategies used to estimate such causal effects involve instrumental variable approaches, utilizing randomized experiments in the “upstream” election which attempt to mobilize voters in the treatment group as an instrument to estimate the Complier Average Causal Effect (CACE) in subsequent “downstream” elections. For example, <a href="https://isps.yale.edu/research/publications/isps12-024">Bedolla and Michelson (2012)</a>, utilizing a series of experiments that aimed to improve turnout in minority voters in California, find overall that voting in the upstream election results in a 23-percentage point increase in probability to vote in subsequent elections for compliers. However, these designs are frequently limited by somewhat weak instruments and low overall sample sizes, as increasing turnout through campaign intervention is extremely difficult and expensive per voter reached, especially in non-white and younger populations <a href="https://www.brookings.edu/book/get-out-the-vote-2/">(Gerber &amp; Green, 2012)</a>. The weakness of these instruments has prompted recent research using fuzzy regression discontinuity (FRD) designs, leveraging the fact that being just barely 18 or just too young to vote on the upstream election day should set voters on very different voting trajectories if such a habit effect exists <a href="https://www.sas.upenn.edu/~marcmere/workingpapers/PersistenceParticipation.pdf">(Meredith 2009</a>; <a href="https://www.tandfonline.com/doi/full/10.1080/17457289.2012.718280">Dinas (2012)</a>.</p>
<p>The latest and most comprehensive such fuzzy regression discontinuity paper is Coppock and Green’s 2016 paper <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12210">Is Voting Habit Forming? New Evidence from Experiments and Regression Discontinuities</a>, which considerably expands both the amount of data used and sophistication of modeling design used. Beyond just using the FRD design, they also incorporate results from several instrumental variable approaches in their very impressive paper.</p>
</section>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">Data</h2>
<p>States are required by the Help America Vote Act to make available to the public individual level data on every registered voter in their state, although states vary considerably in how much information about each voter they provide. For example, some states provide complete histories of which election a voter has participated in, whereas others provide only the most recent 8 elections. Similarly, some states provide birthdate, whereas others provide only age, or age buckets.</p>
<p>Given that the analysis they propose requires precise voting histories and a specific birthdate, Coppock and Green gathered the full voter file in 2013 from the 15 states where all three of</p>
<ol type="1">
<li>a complete history of which elections the individual had voted in,</li>
<li>information on that voter’s eligibility to vote (to rule out the otherwise ineligible such as felons), and</li>
<li>birthdate were available.</li>
</ol>
<p>In the replication file, these 15 voter files have been grouped by birthdate cohort and state, so that the unit of analysis is a group such as registered voters born on 11/6/1998, in Arkansas. This grouping sidesteps the potential problems arising from the fact the voter file only includes people registered to vote. While there is not a complete list of just eligible and just ineligible 17 and 18-year olds, through this cohort grouping, we can work with the 2008/2012 votes cast by cohorts above and below the eligibility threshold instead. Unfortunately, this also adds some additional complexity to interpreting results, as most potential violations of the fuzzy regression discontinuity design assumptions would occur at the individual level. Further, demographic profiles of the cohorts aren’t available, limiting our set of possible confounders to work with. The full dataset has 172,616 state and birthdate state cohorts, but for the purpose of my analysis, I work with the 11,680 birthdate state cohorts whose birthday fall within 365 days of eligibility to vote in the 2008 presidential election.</p>
<p>My outcome of interest is the number of votes cast in the downstream election, the 2012 presidential general election. The “treatment” is having voted in the upstream 2008 presidential election. The instrument is eligibility to participate in the 2008 election, which is determined by being 18 on election day, not 18 by the registration deadline as is sometimes commonly believed. The eligibility criteria being uniform across states greatly simplifies generating the remaining variables used. Due to this uniformity, the forcing variable is simply the cohort’s number of days above or below turning 18 on the upstream election day. Finally, to account for seasonal and day of the week birth trends which subsequently influence total votes cast by each cohort, Coppock and Green include a lagged downstream vote total for the birthdate cohort one year older. As an illustration of these quantities, here is an example row:</p>
<table class="table">
<colgroup>
<col style="width: 18%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 19%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Cohort</th>
<th>2008 Vote Total</th>
<th>2012 Vote Total</th>
<th>2008 Eligible?</th>
<th>Days to Eligible</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1988-11-06-AR</td>
<td>20</td>
<td>27</td>
<td>Yes</td>
<td>-364</td>
</tr>
</tbody>
</table>
<p>To show variation between states, I present below the total number of votes cast in each state for 2008 and 2012. As we’d expect with half the group ineligible in 2008, the vote totals rise considerably for 2012. The variation in state population carries through to the number of votes cast in each state in the sample, which will later influence the standard errors we are able to achieve. Overall, however, this is quite a large sample, both in the number of state-birthdate cohorts (11,680), and in total 2008/2012 votes cast (532,459 and 961,894 respectively).</p>
<table class="table">
<thead>
<tr class="header">
<th><strong>State</strong></th>
<th><strong>Sum 2008</strong></th>
<th><strong>Sum 2012</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AR</td>
<td>11,796</td>
<td>20,915</td>
</tr>
<tr class="even">
<td>CT</td>
<td>22,475</td>
<td>33,040</td>
</tr>
<tr class="odd">
<td>FL</td>
<td>88,704</td>
<td>173,999</td>
</tr>
<tr class="even">
<td>IA</td>
<td>5,336</td>
<td>10,600</td>
</tr>
<tr class="odd">
<td>IL</td>
<td>59,149</td>
<td>108,721</td>
</tr>
<tr class="even">
<td>KY</td>
<td>22,017</td>
<td>40,946</td>
</tr>
<tr class="odd">
<td>MO</td>
<td>35,603</td>
<td>63,381</td>
</tr>
<tr class="even">
<td>MT</td>
<td>6,782</td>
<td>11,030</td>
</tr>
<tr class="odd">
<td>NJ</td>
<td>52,125</td>
<td>87,395</td>
</tr>
<tr class="even">
<td>NV</td>
<td>12,494</td>
<td>24,502</td>
</tr>
<tr class="odd">
<td>NY</td>
<td>80,180</td>
<td>150,489</td>
</tr>
<tr class="even">
<td>OK</td>
<td>15,608</td>
<td>23,565</td>
</tr>
<tr class="odd">
<td>OR</td>
<td>17,900</td>
<td>36,843</td>
</tr>
<tr class="even">
<td>PA</td>
<td>95,412</td>
<td>165,622</td>
</tr>
<tr class="odd">
<td>RI</td>
<td>6,878</td>
<td>10,846</td>
</tr>
</tbody>
</table>
</section>
<section id="estimand" class="level2">
<h2 class="anchored" data-anchor-id="estimand">Estimand</h2>
<p>Given this data, we can estimate a Complier Average Causal Effect (CACE). Define <img src="https://latex.codecogs.com/png.latex?D"> to be the number of votes cast in the 2008 election, and <img src="https://latex.codecogs.com/png.latex?Y"> to be votes in the 20212 election. As a reminder, one cannot assign <img src="https://latex.codecogs.com/png.latex?D">, you can only leverage the encouragement to do so created by being just eligible, which I label <img src="https://latex.codecogs.com/png.latex?Z%20%5Cin%20%5B0,1%5D">, with 0 being too young, and 1 being old enough to vote in 2008 respectively. The forcing variable, days above or below being old enough to vote in 2008, I label <img src="https://latex.codecogs.com/png.latex?T">. <em>Lagged</em> refers to lagged downstream vote total for the birthdate cohort one year older, used to eliminate other temporal trends. For individual birthdate cohorts, I use subscript <img src="https://latex.codecogs.com/png.latex?i">’s.</p>
<p>The estimand is thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Clim%20_%7BT%20%5Cdownarrow%200%7D%20%5Csum_%7B1%7D%5E%7BN%7D%5Cleft%5BY_%7Bi%7D%20%7C%20D_%7B1%20i%7D=1%5Cright%5D-%20%5Clim%20_%7BT%20%5Cuparrow%200%7D%20%5Csum_%7B1%7D%5E%7BN%7D%5Cleft%5BY_%7Bi%7D%20%7C%20D_%7B1%20i%7D=0%5Cright%5D"></p>
<p>This is the additional number of votes in 2012 we would expect across cohorts if all voters did vote in 2008, beyond the number if each cohort’s subjects had not voted in 2008, among only the compliers, subjects who vote if and only if they receive the “encouragement” of being eligible in 2008. Following Coppock and Green, I present these as expected proportions of increase, ie .06 or 6% increase in 2012 turnout, as this makes it easier to compare amongst states with different vote totals. This must be restricted to compliers only because they are the only group in our analysis whose behavior changes just above or below being 18 on election day. For example, the encouragement provided by becoming eligible does not influence the behavior of “never takers”, those who would never vote. Our encouragement cannot create a difference in votes cast for this group or another other that is not the compliers: being just above or below the eligibility threshold only potentially alters the observed outcome of the compliers.</p>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">Methods</h2>
<p>To build up to the full fuzzy regression discontinuity method for estimating the CACE which comprises both an instrumental variable approach and a regression discontinuity, it is helpful to start by first introducing the instrumental variable approach to estimating the CACE in a simpler scenario, after which I will describe how utilizing the regression discontinuity modifies the problem.</p>
<p>For the moment, imagine if instead of a regression discontinuity, the encouragement to vote in 2008, <img src="https://latex.codecogs.com/png.latex?Z">, was a nonpartisan mailer encouraging voting. Further, voters were randomized into a control and treatment group, with some voters receiving the piece, while the other received either nothing or a placebo mailer. This is a classical randomized experiment, and if assumptions of SUTVA and ignorability hold, this allows us to estimate an average treatment effect. To allow us to estimate the effect of voting in 2008 (<img src="https://latex.codecogs.com/png.latex?D">) on voting in 2012 (<img src="https://latex.codecogs.com/png.latex?Y">) using this simple random experiment in the first election as an instrument, however, we need 3 additional assumptions. First, we must assume that any effect of the experimental encouragement on <img src="https://latex.codecogs.com/png.latex?Y"> operates only through <img src="https://latex.codecogs.com/png.latex?D">, referred to as excludability. This would be violated if the nonpartisan mailer in 2008 had such a profound effect on a voter that it still influenced the voter 4 years later, in 2012. However, as most campaign interventions exhibit relatively quick decay in effectiveness (<a href="https://www.brookings.edu/book/get-out-the-vote-2/">Gerber &amp; Green, 2012</a>), it is reasonable to make this assumption.</p>
<p>Second, we need to assume that the treatment is effective, and convinces some subjects who would have not have voted otherwise to vote. In other words, the experiment needs to generate at least some compliers, and more would improve our ability to estimate the CACE. Multiple studies have shown that such mail pieces are effective (<a href="https://www.brookings.edu/book/get-out-the-vote-2/">Gerber &amp; Green, 2012</a>), although the effect sizes are relatively weak.</p>
<p>Third, we need to make an assumption that our experimental encouragement creates no defiers, called the monotonicity assumption. To define defiers, partition the subjects into 4 groups, based on their potential outcomes arising from receiving the mail piece or not. “Always Takers” vote whether they receive the mailer or not: both potential outcomes have them voting. “Never Takers” are the reverse: they never vote regardless of the mailer. Compliers, as already discussed, are those who vote if they receive the encouragement, but don’t otherwise. Defiers invert the mailer’s intended influence, and vote only if not encouraged, refusing to vote if they are sent the mail piece. While one can easily imagine a registered voter frustrated with inundation of political mail, it seems unlikely that a single mail piece would entirely invert a subject’s attitudes towards voting.</p>
<p>Given this strong set of assumptions, it is finally possible to estimate the effect of <img src="https://latex.codecogs.com/png.latex?D"> on <img src="https://latex.codecogs.com/png.latex?Y">, through two stage least squares. First, regressing <img src="https://latex.codecogs.com/png.latex?D"> on <img src="https://latex.codecogs.com/png.latex?Z"> gives <img src="https://latex.codecogs.com/png.latex?E%5BD%20%5Cvert%20Z%5D=%5Calpha_%7B0%7D+%5Calpha_%7B1%7D%20Z+%5Cvarepsilon">. Then, using the predictions from the first stage, regressing <img src="https://latex.codecogs.com/png.latex?Y"> on <img src="https://latex.codecogs.com/png.latex?D"> gives <img src="https://latex.codecogs.com/png.latex?E%5BY%20%5Cvert%20D%5D=%5Cbeta_%7B0%7D+%5Ctau%20D+%5Cvarepsilon">, with tau being the instrumental variable estimate of voting in 2008’s effect on voting in 2012 for the compliers. Intuitively, we are utilizing the variation in 2008 turnout induced by the random experiment to isolate downstream variation in turnout for the compliers. This can be extended to include further covariates, however we will wait until the full fuzzy RD design to illustrate this.</p>
<p>In the full fuzzy regression discontinuity design, instead of the treatment being a randomly assigned treatment, Coppock and Green leverage the variability created by the fact that around election day, some subjects were just slightly too young or just old enough to vote. Informally, we replace the random assignment of an experiment in the upstream year with the as-if-random assignment of young voters near the age threshold to either ineligibility or eligibility to vote in 2008.</p>
<p>More formally, instead of the type of ignorability assumption of the random experiment, we require ignorability conditional on covariates within some bandwith of the eligibility cutoff, <img src="https://latex.codecogs.com/png.latex?Y(1),%20Y(0)%20%5Cperp%20Z%20%5Cvert%20x,%20x%20%5Cin(C-a,%20C+a)">. Also, we require that the eligibility cutoff and birthdates be determined independently of one another.</p>
<p>Finally, we must be able to model the two outcomes <img src="https://latex.codecogs.com/png.latex?Y(1)%20%5Cvert%20x,%20Y(0)%5Cvert%20x"> accurately in this region. One major challenge in modeling these are questions of how much of the data to use for estimation. While we can only estimate a causal effect right at the threshold (the “jump” created by being too young or just old enough), estimating the effects at that point can use different widths of data around the disconintuity.</p>
<p>Leaving further discussion of whether these assumptions are reasonable with Coppock and Green’s data to the next section, we can use a similar set of 2SLS regressions to estimate:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A&amp;D=%5Calpha_%7B0%7D+%5Calpha_%7B1%7D%20Z+%5Calpha_%7B2%7D%20T+%5Calpha_%7B3%7D%20T%20*%20Z+%5Calpha_%7B4%7D%20%5Ctext%20%7BLagged%7D+%5Cvarepsilon%5C%5C%0A&amp;Y=%5Cbeta_%7B0%7D+%5Cbeta_%7B1%7D%20D+%5Cbeta_%7B2%7D%20T+%5Cbeta_%7B3%7D%20T%20*%20D+%5Cbeta_%7B4%7D%20%5Ctext%20%7BLagged%7D+%5Cvarepsilon%0A%5Cend%7Baligned%7D"></p>
<p>With our fuzzy regression discontinuity estimate of the CACE as <img src="https://latex.codecogs.com/png.latex?%5Cbeta_%7B1%7D">, which is calculated separately for each state. <img src="https://latex.codecogs.com/png.latex?T"> is centered at 0, removing the need to complicate this definition by including the interaction term.</p>
<p>What properties does this CACE have? Given that is calculated using only the compliers not the full sample, its standard errors are much larger than the ITT ones we might expect from a simple random experiment, if we could run one in this scenario. The CACE also encompasses the full causal process that is set in motion by the upstream election – even effects resulting from voting in intermediate elections, or possible additional campaign outreach due to voting in 2008, although we cannot tease such effects apart. Despite these problems, the CACE from this fuzzy regression discontinuity has the potential to have much smaller standard errors than leveraging an upstream randomized experiment, given that the quasi-treatment is applied across the entire voter file, not just who an experiment would target.</p>
<p>As an extension to the methods used by Coppock and Green, I also consider bandwith selection algorithms for the regression discontinuity, the results of which I will discuss with the diagnostics. These tools attempt to find a bandwith that is optimal to some criterion, seeking to reduce the researcher degrees of freedom available in setting many possible bandwiths. For example, one such metric would be finding the bandwith that minimizes an approximation to the mean squared error (MSE) in estimating tau <a href="https://www.nber.org/papers/w14726">(Imbens and Kalyanaraman, 2009)</a>.</p>
</section>
<section id="assumptions" class="level2">
<h2 class="anchored" data-anchor-id="assumptions">Assumptions</h2>
<p>Having described the extensive assumptions needed above in order to explain the fuzzy regression discontinuity model, I now evaluate the plausibility of these assumptions for Coppock and Green's case.</p>
<p><strong>Exclusion:</strong> Is there any path other than through <img src="https://latex.codecogs.com/png.latex?D"> through which the "treatment" of being just eligible in 2008 could affect propensity to vote in 2012? Absent a history of actually participating, it is unlikely that someone who becomes eligible slightly earlier would be more likely to be targeted by a campaign to turn out. We can, however, imagine a subject who knew they would be eligible paying more attention in 2008 than someone who knew they would not be, which could spark more interest by the 2012 election. Similarly, a subject who missed voting in their first presidential election but was eligible might be more motivated in 2012 for a fear of missing out again ("I missed voting for Obama, but won't do so again"). With all such paths that imagine a more engaged citizen for their first presidential election, however, one has to imagine that most such citizens would want to put that energy towards voting in 2008. Thus, while we have no way to rule out such backdoor paths from being just eligible in 2008 to voting in 2012, we have to imagine such cases would be relatively rare.</p>
<p><strong>Effectiveness of the Instrument:</strong> While just being eligible alone likely has a relatively weak effect on 2008 turnout, given that we are working at the scale of full state voter files, we can be confident we have generated a reasonable number of compliers to work with out of over five hundred thousand ballots cast by the birthdate cohorts studied.</p>
<p><strong>Monotonicity:</strong> A defier in Coppock's and Green's design would be someone who votes despite not quite being old enough, or vice versa. Given that this constitutes a felony, and a hard to commit one with little benefit, we can be extremely confident in this assumption. Alternatively, someone could choose to not vote only if just eligible, which again implies an extremely unlikely stance towards election law.</p>
<p><strong>Ignorability:</strong> Here, we need to be clear that we mean ignorability within some cutoff of being just eligible, given our covariates: <img src="https://latex.codecogs.com/png.latex?Y(1),%20Y(0)%20%5Cperp%20Z%20%5Cvert%20x,%20x%20%5Cin(C-a,%20C+a)">. It seems plausible that being a month above or below 18 on election day shouldn't create any other major changes in a 17 or 18 year old's potential outcomes. However, as we get farther from the eligibility threshold it becomes more plausible that the groups could diverge through, for example, differences in maturity or differences in education due to birthdate.</p>
<p><strong>Cutoff and forcing variable determined independently:</strong> It seems unlikely that either Election Day or someone's recorded birthday could be moved to help a subject vote earlier. Federal Election Day has been fixed in the United States since 1945. Further, it seems exceedingly unlikely that a parent or hospital administrator would attempt to modify a birth certificate simply to allow their child to vote 1 year earlier.</p>
<p><strong>Estimation:</strong> As we will see in plots below, there seems to be almost no non-linear trend in Y above or below the cutoff, simplifying modeling <img src="https://latex.codecogs.com/png.latex?E%5BY%5Cvert%20X%5D">. Also, while there might be some concern with day or the week, seasonal, or other temporal trends in the number of dates on a given birthdate and thus the number of expected votes there, including the year lagged variable should arguably be sufficient to model them, provided any trends aren't particularly unique to 1990 births. While we'd ideally like more covariates than are available in the flattened data presented here, what we have seems sufficient to make at least a reasonable estimate.</p>
<p><strong>SUTVA:</strong> Lastly, there is little reason to believe that the encouragement to vote provided to one subject by turning 18 close to election day could influence another subject. While we can imagine students influencing each other's politics, it seems implausible that a friend turning 18 close to election day alone could change a student's propensity to vote meaningfully.</p>
</section>
<section id="diagnostics" class="level2">
<h2 class="anchored" data-anchor-id="diagnostics">Diagnostics</h2>
<p>Given that my greatest concern with Coppock and Green’s design is their large choice of bandwith at 365 days around 2008’s election day, I first briefly describe results of other diagnostics I replicated before focusing attention on a variety of ways to evaluate the bandwidth’s effect on the CACE. This emphasizes my work beyond the replication, most of which isn’t shown here- after the following two paragraphs and first table, diagnostics in this section are my extensions of Coppock and Green’s work.</p>
<p>First, being just eligible appears to have an effect on voting 2008, as checked through running just the first stage of a 2 stage least squares model, so our instrument generates compliers. There were no discontinuities in the year lagged vote counts around the 2008 eligibility cutoff, our covariate.</p>
<p>I was able to successfully reproduce Coppock and Green’s robustness check across different order polynomials, shown below. Note, however, that these estimates are from meta-analyses across states; I develop a similar table by state and bandwith later. Both 1st and 2nd order polynomials returned similar results, while a 3rd order polynomial estimates became extremely sensitive to choice of bandwith. As <a href="https://www.nber.org/papers/w20405">Gelman and Imbens (2014)</a> discuss however, cubic polynomials are problematic more generally in regression discontinuity designs, so this result was expected. Robust SE's are in parentheses below their corresponding estimate. <strong>The primary estimates presented by CG in the main paper are bolded.</strong></p>
<table class="table">
<colgroup>
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>90</th>
<th>180</th>
<th>270</th>
<th>365</th>
<th>455</th>
<th>545</th>
<th>635</th>
<th>730</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Difference-in-Means</td>
<td>0.108</td>
<td>0.118</td>
<td>0.124</td>
<td>0.119</td>
<td>0.118</td>
<td>0.117</td>
<td>0.11</td>
<td>0.103</td>
</tr>
<tr class="even">
<td></td>
<td>(0.01)</td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
</tr>
<tr class="odd">
<td>First-order Polynomial</td>
<td>0.058</td>
<td>0.089</td>
<td>0.101</td>
<td><strong>0.117</strong></td>
<td>0.118</td>
<td>0.121</td>
<td>0.13</td>
<td>0.136</td>
</tr>
<tr class="even">
<td></td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td><strong>(0.01)</strong></td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
</tr>
<tr class="odd">
<td>Second-order Polynomial</td>
<td>0.016</td>
<td>0.055</td>
<td>0.074</td>
<td>0.083</td>
<td>0.102</td>
<td>0.104</td>
<td>0.099</td>
<td>0.103</td>
</tr>
<tr class="even">
<td></td>
<td>(0.02)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
</tr>
<tr class="odd">
<td>Third-order Polynomial</td>
<td>0.012</td>
<td>0.035</td>
<td>0.05</td>
<td>0.056</td>
<td>0.06</td>
<td>0.086</td>
<td>0.095</td>
<td>0.097</td>
</tr>
<tr class="even">
<td></td>
<td>(0.02)</td>
<td>(0.02)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
</tr>
</tbody>
</table>
<p>Next, somewhat surprisingly, Coppock and Green never actually plot the discontinuity they work with either in the main paper or appendix. The plot of the discontinuity in Figure 1 below establishes several informative points. First, there is a noticeable dip in total votes cast in 2012 by cohort upon being slightly too young to vote in 2008, a good first validation of the design. Second, the trend in the data both to the left and right of the discontinuity appears to be extremely linear, which may influence my later bandwith selection algorithm findings. Finally, even in the binned plot the wide range of votes cast by birthdate state cohorts is obvious. Upon further inspection much of this variation is driven by state, which motivates my next analysis, seeing how each state's CACE changes as the cutoff varies.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Is Voting Habit Forming/discontinuity.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Discontinuity Plot</figcaption><p></p>
</figure>
</div>
<p>Note that the full 10,000+ birthdate cohorts are binned with their average plotted to make the graph legible. Lines plotted are first order polynomials.</p>
<p>While Coppock and Green present results at their final chosen bandwith by state, they conduct robustness checks only at the level of meta-analysis pooling across states. Given that some of the variation in vote total also seems to be driven by state, below are CACE estimates by state, by a variety of bandwiths. These are considerably more sensitive to the bandwith choice than the above table that pools across states, as we’d expect given the disaggregation. For example, the average across the estimates using 30-day bandwith (.01) is closer to 0 than the .1 resulting from 365 bandwith concreated on in the main paper. This somewhat lowers my confidence in the by-state estimates as Coppock and Green present them. Their claim that their results are insensitive to bandwith variation only really appear to hold when estimates are aggregated through meta-analysis, as in the earlier table.</p>
<table class="table">
<colgroup>
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>365</th>
<th></th>
<th>180</th>
<th></th>
<th>90</th>
<th></th>
<th>60</th>
<th></th>
<th>30</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AR</td>
<td>0.20</td>
<td>(0.03)</td>
<td>0.15</td>
<td>(0.04)</td>
<td>0.14</td>
<td>(0.07)</td>
<td>0.01</td>
<td>(0.10)</td>
<td>0.10</td>
<td>(0.16)</td>
</tr>
<tr class="even">
<td>CT</td>
<td>0.16</td>
<td>(0.02)</td>
<td>0.14</td>
<td>(0.02)</td>
<td>0.14</td>
<td>(0.03)</td>
<td>0.11</td>
<td>(0.03)</td>
<td>0.11</td>
<td>(0.05)</td>
</tr>
<tr class="odd">
<td>IA</td>
<td>0.08</td>
<td>(0.04)</td>
<td>0.03</td>
<td>(0.05)</td>
<td>0.03</td>
<td>(0.08)</td>
<td>-0.05</td>
<td>(0.11)</td>
<td>-0.03</td>
<td>(0.17)</td>
</tr>
<tr class="even">
<td>IL</td>
<td>0.08</td>
<td>(0.01)</td>
<td>0.05</td>
<td>(0.02)</td>
<td>0.02</td>
<td>(0.02)</td>
<td>0.01</td>
<td>(0.03)</td>
<td>0.04</td>
<td>(0.04)</td>
</tr>
<tr class="odd">
<td>FL</td>
<td>0.10</td>
<td>(0.01)</td>
<td>0.09</td>
<td>(0.02)</td>
<td>0.06</td>
<td>(0.03)</td>
<td>0.03</td>
<td>(0.04)</td>
<td>-0.01</td>
<td>(0.05)</td>
</tr>
<tr class="even">
<td>KY</td>
<td>0.08</td>
<td>(0.02)</td>
<td>0.08</td>
<td>(0.03)</td>
<td>0.05</td>
<td>(0.04)</td>
<td>0.05</td>
<td>(0.05)</td>
<td>0.08</td>
<td>(0.07)</td>
</tr>
<tr class="odd">
<td>MO</td>
<td>0.16</td>
<td>(0.02)</td>
<td>0.15</td>
<td>(0.03)</td>
<td>0.14</td>
<td>(0.04)</td>
<td>0.13</td>
<td>(0.06)</td>
<td>0.11</td>
<td>(0.08)</td>
</tr>
<tr class="even">
<td>MT</td>
<td>0.11</td>
<td>(0.03)</td>
<td>0.08</td>
<td>(0.04)</td>
<td>0.03</td>
<td>(0.07)</td>
<td>-0.01</td>
<td>(0.09)</td>
<td>-0.08</td>
<td>(0.12)</td>
</tr>
<tr class="odd">
<td>NJ</td>
<td>0.15</td>
<td>(0.01)</td>
<td>0.12</td>
<td>(0.02)</td>
<td>0.09</td>
<td>(0.03)</td>
<td>0.03</td>
<td>(0.03)</td>
<td>0.01</td>
<td>(0.05)</td>
</tr>
<tr class="even">
<td>NV</td>
<td>0.17</td>
<td>(0.03)</td>
<td>0.14</td>
<td>(0.04)</td>
<td>0.09</td>
<td>(0.06)</td>
<td>0.00</td>
<td>(0.08)</td>
<td>-0.01</td>
<td>(0.11)</td>
</tr>
<tr class="odd">
<td>NY</td>
<td>0.07</td>
<td>(0.01)</td>
<td>0.02</td>
<td>(0.02)</td>
<td>-0.03</td>
<td>(0.03)</td>
<td>-0.06</td>
<td>(0.03)</td>
<td>-0.03</td>
<td>(0.05)</td>
</tr>
<tr class="even">
<td>OK</td>
<td>0.14</td>
<td>(0.02)</td>
<td>0.11</td>
<td>(0.03)</td>
<td>0.09</td>
<td>(0.06)</td>
<td>0.01</td>
<td>(0.08)</td>
<td>0.04</td>
<td>(0.15)</td>
</tr>
<tr class="odd">
<td>OR</td>
<td>0.11</td>
<td>(0.02)</td>
<td>0.07</td>
<td>(0.04)</td>
<td>0.06</td>
<td>(0.06)</td>
<td>0.01</td>
<td>(0.08)</td>
<td>-0.07</td>
<td>(0.12)</td>
</tr>
<tr class="even">
<td>PA</td>
<td>0.12</td>
<td>(0.02)</td>
<td>0.08</td>
<td>(0.03)</td>
<td>0.01</td>
<td>(0.04)</td>
<td>-0.05</td>
<td>(0.05)</td>
<td>-0.04</td>
<td>(0.07)</td>
</tr>
<tr class="odd">
<td>RI</td>
<td>0.11</td>
<td>(0.03)</td>
<td>0.14</td>
<td>(0.05)</td>
<td>0.05</td>
<td>(0.08)</td>
<td>0.03</td>
<td>(0.10)</td>
<td>-0.15</td>
<td>(0.16)</td>
</tr>
</tbody>
</table>
<p>Bandwith Sensitivity for State estimates. Robust Standard errors are to the right of their respective estimate in parentheses.</p>
<p>As a final extension of the robustness to different bandwith checks Coppock and Green consider in their appendix, I also explored a variety of algorithms for bandwith selection. Across different order polynomials, kernels, and metrics to optimize for, all results suggested using the full 365-day bandwith. In retrospect, looking at the quite linear trend in the data, this makes some sense: given little difficult variation to model, choosing to include all the data could easily be the most effective strategy to reduce metrics such as the MSE in estimating <img src="https://latex.codecogs.com/png.latex?%5Ctau_%7BD%7D">.</p>
</section>
<section id="results-and-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="results-and-interpretation">Results and Interpretation</h2>
<p>While I was able to reproduce Coppock and Green’s results, I differ slightly in how I choose to interpret them. Their identification strategy overall makes sense. At worst, it seems possible the exclusion assumption could be subject to minor violations, but the frequency of such events would be low. Unlike Coppock and Green, however, I don’t think its fair to say that the results are completely robust to bandwith selection or result from habit alone.</p>
<p>While their meta-analysis level results are insensitive to variations in the order of polynomial used or bandwith both in their appendix and my replication, allowing the bandwith to vary by state revealed significant additional variability. Given that by state estimates are a core part of their results, I would argue it is best to present something closer to my bandwith by state table than the singular estimates resulting from a bandwith of 365. If one accepts their bandwith, either on its own or as a result of bandwith selection algorithms, it appears that the overall CACE across states is around .1, establishing that there does appear to be some causal effect on 2012 participation from 2008 participation for compliers. If a reader prefers a much narrower bandwith, this overall shrinks to roughly .06 for a 90 day bandwith, or close to 0 for a 30 day one.</p>
<p>A second concern is that Coppock and Green argue that these effects result mostly from habit, not campaigns in 2012 choosing to target those who voted in 2008. This would not represent a violation of exclusion, as campaign effects represent a valid path from <img src="https://latex.codecogs.com/png.latex?D"> to <img src="https://latex.codecogs.com/png.latex?Y">, not <img src="https://latex.codecogs.com/png.latex?Z"> to <img src="https://latex.codecogs.com/png.latex?Y">. Thus, this is a disagreement around causes of effects. To argue that habit, not campaigns, cause the CACEs they find, Coppock and Green note that battleground states do not have significantly higher estimates than non-battleground states. That is, if the cause was campaign activity, we would expect higher CACEs in battleground states in presidential years like 2012. While this would be somewhat convincing as an argument, the ability to tease out the pattern they describe is dependent on a large bandwith to generate small enough standard errors to tell states apart. Beyond this, during campaign years, even in non-battleground states, most people with some voting history can expect campaign outreach. Further, in non-battleground states, this contact sometimes prioritizes young voters and others whose turnout is more ambiguous. Overall, there does appear to be some evidence that the effects Coppock and Green find aren’t mainly due to campaign effects, however this isn’t sufficient to fully support their claim that most of what they find is habit alone.</p>
<p>To provide an interpretation of the individual CACEs, consider the Florida estimate at a bandwith of 90 days on either side of the 2008 election. If our assumptions hold, the CACE of .06 suggests that for compliers, participation in the 2008 election caused a 6% increase in total 2012 votes cast in Florida, above the total votes cast for those compliers who did not participate in 2008.</p>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<p>Overall, reproducing this paper convinced me that Coppock and Green have a strong method for estimating the causal effect of voting in the 2008 election on voting in the 2012 election for compliers. However, these estimates are sensitive to bandwith variation at the state level, with effects shrinking towards zero as the bandwith becomes tighter. Also, it is unclear if their results are due to habit alone, although the estimates do support this somewhat. Thus, I think a narrower interpretation of the 2008-2012 election pair analysis is that there is some suggestion of an effect of voting in 2008 on 2012 votes cast for compliers, provided you accept a large bandwith, and the effect is most likely not due to campaign effects.</p>
<p>Of course, replicating such a colossal paper and set of analyses has many limitations. First, I restricted my analysis and robustness checks to the 2008-2012 election pair, in order to replicate the pair the original authors used in their robustness checks. I also didn’t study or replicate their instrumental variables CACE estimates resulting from using older randomized experiments as instruments. Finally, given the flattened form of the data provided, I was unable to provide fully satisfying descriptive profiles of the birthdate-state cohorts, for example about their demographic composition.</p>
<p>Future work could consider replicating the rest of Coppock and Green’s study with additional robustness checks, or conducting the same analysis using recent advances in voter file linking. For example, given that midterm elections have much lower turnout, and thus fewer compliers in a fuzzy regression discontinuity design, it would be interesting to see if an election pair like 2006-2010 exhibits even higher sensitivity to variation in bandwith. In the past few years, companies such as Catalist and Movement Cooperative have also improved the quality of data available in voter files. These provide a number of advantages that could be helpful for a future study like Coppock and Green’s, including better cleaning of the underlying data, and richer information on the individual voters, either through integration of additional data sources, or through modeling. Coppock and Green’s analysis takes an incredible step towards identifying whether voting is habit forming, but further analysis is needed to show that their results are deeply robust to electoral context and modeling choices. Some of these steps will likely form my project for my Advanced Causal Inference course this semester, although I’m still deciding which points are the most interesting to look at.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2019,
  author = {Andy Timm},
  title = {Is {Voting} {Habit} {Forming?} {Replication,} and Additional
    Robustness Checks},
  date = {2019-07-03},
  url = {https://andytimm.github.io/2020-03-09-Voting-Habit-FRD.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2019" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2019. <span>“Is Voting Habit Forming? Replication, and
Additional Robustness Checks.”</span> July 3, 2019. <a href="https://andytimm.github.io/2020-03-09-Voting-Habit-FRD.html">https://andytimm.github.io/2020-03-09-Voting-Habit-FRD.html</a>.
</div></div></section></div> ]]></description>
  <category>From Old Website</category>
  <category>causal inference</category>
  <guid>https://andytimm.github.io/posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html</guid>
  <pubDate>Wed, 03 Jul 2019 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Type S/M errors in R with retrodesign()</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html</link>
  <description><![CDATA[ 




<p>This is a online version of the vignette for my r package <strong>retrodesign</strong>. It can be installed with:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">install.packages</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"retrodesign"</span>)</span></code></pre></div>
<p>In light of the replication and reproducibility crisis, researchers across many fields have been reexamining their relationship with the Null Hypothesis Significance Testing (NHST) framework, and developing tools to more fully understand the implications of their research designs for replicable and reproducible science. One group of papers in this vein are works by Andrew Gelman, John Carlin, Francis Tuerlinckx, and others which develop two new metrics for better understanding hypothesis testing in noisy samples. For example, Gelman and Carlin’s <a href="http://www.stat.columbia.edu/~gelman/research/published/retropower20.pdf">Assessing Type S and Type M Errors</a> (2014) argues that looking at power, type I errors, and type II errors are insufficient to fully capture the risks of NHST analyses, in that such analysis focuses excessively on statistical significance. Instead, they argue for consideration of the probability you’ll get the sign on your effect wrong, or <strong>type S error</strong>, and the factor by which your effect size might be exaggerated, or <strong>type M</strong> error. Together, these additional statistics more fully explain the dangers of working in the NHST framework, especially in noisy, small sample environments.</p>
<p><strong>retrodesign</strong> is a package designed to help researchers better understand type S and M errors and their implications for their research. In this vignette, I introduce both the need for the type S/M error metrics, and the tools retrodesign provides for examining them. I assume only a basic familiarity with hypothesis testing, and provide definitional reminders along the way.</p>
<!--more-->
<section id="an-initial-example" class="level2">
<h2 class="anchored" data-anchor-id="an-initial-example">An Initial Example</h2>
<p>To nail down the assumptions we’re working with, we’ll start with an abstract example from <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/bmsp.12132">Lu et al.</a> (2018); the second will draw on a real world scenario and follow Gelman and Carlin’s suggested workflow for design analysis more closely.</p>
<p>Let’s say we’re testing whether a true effect size is zero or not, in a two tailed test.</p>
<p><img src="https://latex.codecogs.com/png.latex?H%20_%20%7B%200%20%7D%20:%20%5Cmu%20=%200%20%5Cquad%20%5Ctext%20%7B%20vs.%20%7D%20%5Cquad%20H%20_%20%7B%201%20%7D%20:%20%5Cmu%20%5Cneq%200"></p>
<p>We’re assuming that the test statistic here is normally distributed. As <a href="http://www.stat.columbia.edu/~gelman/research/published/francis8.pdf">Gelman and Tuerlinckx</a> (2009) note, this is a pretty widely applicable assumption; through the Central Limit Theorem, it applies to common settings like differences between test and control groups in a randomized experiment, averages, or linear regression coefficients. If it helps, imagine the following in the context of one of those from your field.</p>
<p>Traditionally, we’d focus on Type I/II errors. <strong>Type I error</strong> is rejecting the null hypothesis, when it is true. <strong>Type II error</strong> is failing to reject the null hypothesis, when it is not true. The <strong>power</strong>, then, is just is the probability that the test correctly rejects the null hypothesis when a specific alternative hypothesis is true.</p>
<p>Beyond those, we’ll also consider:</p>
<ol type="1">
<li><strong>Type S (sign)</strong>: the test statistic is in the opposite direction of the true effect size, given that the statistic is statistically significant;</li>
<li><strong>Type M (magnitude or exaggeration ratio)</strong>: the test statistic in magnitude exaggerates the true effect size, given that the statistic is statistically significant.</li>
</ol>
<p>Notice that both of these are conditional on the test statistic being statistically significant; we’ll come back to this fact several times.</p>
<p>To visualize these, we’ll draw 5000 samples from a normal distribution with mean .5, and standard deviation 1. We’ll then analyze these in a NHST setting where we have a standard error of 1. We can use <code>sim_plot()</code> to do so, with the first parameter being our postulated effect size .5, and the second being our hypothetical standard error of 1. If you prefer to not use ggplot graphics like I do here, set the <code>gg</code> argument to FALSE.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sim_plot</span>(.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
<p><img src="https://andytimm.github.io/posts/Intro to Retrodesign/unnamed-chunk-2-1.png" class="img-fluid" alt="Visual of what type S and M errors are"><!-- --></p>
<p>Here, the dotted line is the true effect size, and the full lines are where the statistic becomes statistically significantly different from 0, given our standard error of 1. The greyed out points aren’t statistically significant, the squares are type M errors, and the triangles are type S errors.</p>
<p>Even though the full distribution is faithfully centered around the true effect, once we filter using statistical significance, we will both exaggerate the effect and get its sign wrong often.</p>
<p>Of course, trying to find a true effect size of .5 with a standard error of 1 is extremely underpowered. More precisely, the power, Type S and type M error here are:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">retro_design</span>(.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; $power</span></span>
<span id="cb3-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; [1] 0.07909753</span></span>
<span id="cb3-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt;</span></span>
<span id="cb3-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; $typeS</span></span>
<span id="cb3-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; [1] 0.0878352</span></span>
<span id="cb3-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt;</span></span>
<span id="cb3-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; $typeM</span></span>
<span id="cb3-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; [1] 4.788594</span></span></code></pre></div>
<p>The function arguments here the same as those for <code>sim_plot()</code> above. If you want to work with a t-distribution instead, you’ll need to use <code>retrodesign()</code> instead, and provide the degrees of freedom for the <code>df</code> argument. <code>retrodesign()</code> is the original function provided by Gelman &amp; Carlin (2014), which uses simulation rather than an exact solution to calculate the errors. It’s thus slower, but can work with t-distributions as well (the closed form solution only applies in the normal case).</p>
<p>A power of .07 isn’t considered good research in most fields, so most cases won’t be this severe. However, it does illustrate two important points. In a underpowered example like this, we will hugely overestimate our effect size (by a factor of 4.7x! on average), or even get its sign wrong if we’re unlucky (around 8% of the time). Further, these are practical measures to focus on; getting the sign wrong could mean recommending the wrong drug treatment, exaggerating the treatment effect could mean undertreating someone once the drug goes to market.</p>
</section>
<section id="a-severe-example" class="level2">
<h2 class="anchored" data-anchor-id="a-severe-example">A Severe Example</h2>
<p>Now that you hopefully have a sense of what type S/M error add to our understanding of NHST in the context of noisy, small sample studies, we’ll move onto a real world example, where we’ll focus on following Gelman and Carlin’s suggested design analysis through one of their examples.</p>
<p>We’ll be working with <a href="https://www.ncbi.nlm.nih.gov/pubmed/16949101">Kanazawa</a> (2007), which claims to have found that the most attractive people are more likely to have girls. To be more specific, each of their ~3,000 people surveyed had been assigned a “attractiveness” score from 1-5, and they then compared the first born children of the most attractive to other groups; 56% were girls compared to 48% in the other groups. They thus obtained a difference estimate of 8%, with a p-value of .015, so significant at the traditional <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%20.05">.</p>
<p>Stepping back for a second, this is fishy in numerous ways. First, comparing the first borne children is an oddly specific comparison to have run- at worst, this might be a case of p-hacking. Or maybe they saw a strong comparison, and decided to test it, and in doing so fell into a <a href="http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf">Garden of Forking Paths</a> problem. Second, if attractive people had many more girls, it seems unlikely that gender balance would be as even as it is. So again, this example has a likely spurious result, is likely to have low power, and a high chance of S/M errors.</p>
<p>To do a design analysis of this, Gelman and Carlin’s first step is to review the literature for a posited true effect size. There is plenty of prior research on variation in sex ratio of human births. A huge variety of factors have been studied such as race, parental age, season of birth, and so on, only finding effects from .3% to 2%. In the most extreme cases (conditions like extreme poverty or famine), these effects only rise to 3%. (If you’re interested, the causal reasoning seems to be that male fetuses are more likely than female ones to die under adverse conditions.)</p>
<p>Like traditional design analyses, we’ll posit a wide range of effects here, and see how our power, type S error, and type M error rates change correspondingly. Gelman and Carlin end up looking at .1-1%, reasoning that sex ratios vary very little in general, and that the subject attractiveness rating is quite noisy as well. Even if we compare their effect size to the effect sizes found in the most extreme scenarios in prior literature, it doesn’t look good.</p>
<p>We can infer their standard error of the difference to be 3.3% from their reported 8% estimate and p-value of .015; we now have everything we need.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The posited effects Gelman and Carlin consider</span></span>
<span id="cb4-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">retro_design</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>),<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.3</span>)</span>
<span id="cb4-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt;   effect_size      power    type_s   type_m</span></span>
<span id="cb4-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; 1         0.1  0.0501052 0.4646377 77.15667</span></span>
<span id="cb4-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; 2         0.3 0.05094724 0.3953041 25.74305</span></span>
<span id="cb4-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; 3           1 0.06058446 0.1950669 7.795564</span></span>
<span id="cb4-7"></span>
<span id="cb4-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># A particularly charitable set of posited effects</span></span>
<span id="cb4-9"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">retro_design</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>),<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.3</span>)</span>
<span id="cb4-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt;   effect_size      power     type_s   type_m</span></span>
<span id="cb4-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; 1         0.1  0.0501052  0.4646377 77.15667</span></span>
<span id="cb4-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; 2         0.3 0.05094724  0.3953041 25.74305</span></span>
<span id="cb4-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; 3           1 0.06058446  0.1950669 7.795564</span></span>
<span id="cb4-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; 4           2 0.09302718 0.05529112 3.982141</span></span>
<span id="cb4-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#&gt; 5           3  0.1487169 0.01384174 2.719244</span></span></code></pre></div>
<p>By providing a list for our first argument <code>A</code>, we get a dataframe with a posited effect size, and corresponding power, type S, and type M errors in each row. If you just want the lists of power/type S/type M, you can just feed in a vector, ie <code>c(.1,.3,1)</code>.</p>
<p>So if we go with a high but fairly reasonable posited effect of 1%, there’s a nearly 1 in 5 chance that such an experiment would get the direction of the effect wrong. Even if we assume that being the daughter of a highly attractive person has equivalent effect to being born in a famine (effect size 3%), this experiment would exaggerate the true effect size by a factor of 2.7x on average.</p>
<p>This analysis has given us added information beyond what we’d get from the point estimate, the confidence interval, and the p-value. Under reasonable assumptions about the true effect size, this study simply seems too small to be informative: Under most assumptions, we’re quite likely to get the sign wrong, and even with a quite generous assumption of true effect size we’ll greatly exaggerate the effect size.</p>
</section>
<section id="assessing-type-sm-errors-when-you-dont-have-prior-information" class="level2">
<h2 class="anchored" data-anchor-id="assessing-type-sm-errors-when-you-dont-have-prior-information">Assessing Type S/M errors when you don’t have prior information</h2>
<p>One obvious objection you might have to this framework is that you may not have a clear sense of what your effect size will be. As extreme as it sounds, you may not even have a sense of the right order of magnitude. In these cases, it makes even more sense to calculate type s/m errors across a variety of posited effect sizes and see how they influence your research.</p>
</section>
<section id="so-how-worried-should-we-be-in-more-reasonable-studies" class="level2">
<h2 class="anchored" data-anchor-id="so-how-worried-should-we-be-in-more-reasonable-studies">So how worried should we be in more reasonable studies?</h2>
<p>Another concern might be that my first two examples were severely underpowered. However, even with powers that are publishable in many fields, we should still be worried about type M errors, but not type S errors.</p>
<p>To sketch out the relationship between possible effect sizes and these errors, we adopt the standard error from the prior example, but greatly expand the posited effect sizes, to max out at 10, where the power would be a perfectly reasonable .85. We’ll use <code>type_s</code> and <code>type_m</code>, both of which take a single or list of possible <code>A</code>’s, and a standard error, similar to <code>retrodesign</code>, but only return the respective error.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">possible_effects <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">seq</span>(.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">by =</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb5-2">effect_s_pairs <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(possible_effects,<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">type_s</span>(possible_effects,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.3</span>))</span>
<span id="cb5-3">effect_m_pairs <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(possible_effects,<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">type_m</span>(possible_effects,<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.3</span>))</span>
<span id="cb5-4"></span>
<span id="cb5-5">s_plot<span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(effect_s_pairs, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(possible_effects, type_s)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_point</span>()</span>
<span id="cb5-6">m_plot <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(effect_m_pairs, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(possible_effects, type_m)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_point</span>()</span>
<span id="cb5-7"></span>
<span id="cb5-8"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">grid.arrange</span>(s_plot, m_plot, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ncol=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span></code></pre></div>
<p><img src="https://andytimm.github.io/posts/Intro to Retrodesign/unnamed-chunk-5-1.png" class="img-fluid" alt="Comparison of Type S/M error across effect sizes"><!-- --></p>
<p>As <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/bmsp.12132">Lu et al.</a> (2018) note, the type S and M error shrink at very different rates as power rises.</p>
<p>They find the probability of type S error decreases rapidly. To ensure that $s $ and <img src="https://latex.codecogs.com/png.latex?s%5Cleq%200.01">, we only need <img src="https://latex.codecogs.com/png.latex?power%20=%200.08"> and <img src="https://latex.codecogs.com/png.latex?power%20=%200.17">, respectively. Thus, unless your study is severely underpowered, you shouldn’t need to worry about type s errors very often.</p>
<p>On the other hand, The type m error decreases relatively slowly. To ensure that <img src="https://latex.codecogs.com/png.latex?m%20%5Cleq%201.5"> and <img src="https://latex.codecogs.com/png.latex?m%20%5Cleq%201.1">, we need <img src="https://latex.codecogs.com/png.latex?power%20=%200.52"> and <img src="https://latex.codecogs.com/png.latex?power%20=%200.85">, respectively. Whereas even moderately powered studies make type s errors relatively improbable, only very high powered studies keep exaggeration of effect sizes down. If your field requires a power of .80, you should thus be cognizant that effect sizes are likely somewhat inflated.</p>
</section>
<section id="solutions-outside-nhst" class="level2">
<h2 class="anchored" data-anchor-id="solutions-outside-nhst">Solutions outside NHST</h2>
<p>For the majority of this vignette, I’ve avoided questioning whether we should be working in the NHST framework. However, <code>retrodesign</code> is a package to help address some limitations of NHST work as it’s traditionally practiced, so it makes sense to address these solutions.</p>
<p>Going back to the plot we started with, remember that the underlying gaussian does actually faithfully center around it’s mean of .5.</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sim_plot</span>(.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
<p><img src="https://andytimm.github.io/posts/Intro to Retrodesign/unnamed-chunk-6-1.png" class="img-fluid" alt="Return to showing first plot"><!-- --></p>
<p>Type S and M errors are an artifact of the hard thresholding implicit in a NHST environment, where an arbitrary p-value (usually .05) decides what is and isn’t noteworthy.</p>
<p>If you have to work in the NHST framework because it’s what your discipline publishes, you can better understand some problems it might cause by exploring type S and M errors. If you’re able to choose how you analyze your experiments though, you can avoid these errors (and many others) by abandoning statistical significance as a hard filter entirely. If learning more about problems with NHST beyond type S and M errors, and suggestions for alternative strategies is interesting to you, check out the <strong>Abandon Statistical Significance</strong> paper linked in the further reading below.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<ol type="1">
<li><strong>Gelman and Tuerlinckx’s <a href="http://www.stat.columbia.edu/~gelman/research/published/francis8.pdf">Type S error rates for classical and Bayesian single and multiple comparisons procedures</a> (2000)</strong>: A comparison of the properties of Type S errors of frequentist and Bayesian confidence statements. Useful for how this all plays out in a Bayesian context. Bayesian confidence statements have the desirable property of being more conservative than frequentist ones.</li>
<li><strong>Gelman and Carlin’s <a href="http://www.stat.columbia.edu/~gelman/research/published/retropower20.pdf">Assessing Type S and Type M Errors</a> (2014)</strong>: Gelman and Carlin compare their suggested design analysis, as we’ve written about above, to more traditional design analysis, through several examples, and discuss the desirable properties it has in more depth than I do here. It is also the source of the original retrodesign() function, which I re-use in the package with permission.</li>
<li><strong>Lu et al’s <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/bmsp.12132">A note on Type S/M errors in hypothesis testing</a> (2018)</strong>: Lu and coauthors go further into the mathematical properties of Type S/M errors, and prove the closed form solutions implimented in <code>retrodesign</code>.</li>
<li><strong>McShane et al’s <a href="https://arxiv.org/abs/1709.07588">Abandon Statistical Significance</a> (2017)</strong>: If you want a starting point on the challenges with NHST that have led many statisticians to argue for abandoning NHST all together, and starting points for alternative ways of doing science.</li>
</ol>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2018,
  author = {Andy Timm},
  title = {Type {S/M} Errors in {R} with Retrodesign()},
  date = {2018-05-11},
  url = {https://andytimm.github.io/2019-02-05-Intro_To_retrodesign.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2018" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2018. <span>“Type S/M Errors in R with Retrodesign().”</span>
May 11, 2018. <a href="https://andytimm.github.io/2019-02-05-Intro_To_retrodesign.html">https://andytimm.github.io/2019-02-05-Intro_To_retrodesign.html</a>.
</div></div></section></div> ]]></description>
  <category>From Old Website</category>
  <guid>https://andytimm.github.io/posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html</guid>
  <pubDate>Fri, 11 May 2018 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Why the normal distribution?</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Maximum Entropy Normal/2018-05-11-maxentropynormal.html</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Maximum Entropy Normal/boring_normal.jpg" class="img-fluid figure-img"></p>
</figure>
</div>
<p>When I was first studying probability theory as an undergrad, I had a bit of a conceptual hang-up with the Central Limit Theorem. <a href="http://www.rpubs.com/christopher_castle/137490">Simulating it in R</a> gave a nice visual of how each additional random variable smoothed out some of the original distribution’s individuality, and asymptotically we were left with a more generic shape. The proofs were relatively straightforward. One part, however, didn’t really make sense to me. My problem was this: <strong>Of all the many possible distributions, why is the normal distribution in particular that our i.i.d random variables converge to in distribution?</strong></p>
<!--more-->
<p>The normal distribution has lots of interesting properties that I looked at to gain some intuition, but many of the them seem to follow from the CLT, rather than explaining it. In fact, it wasn’t until a later course in machine learning that integrated some <a href="https://en.wikipedia.org/wiki/Information_theory">information theory</a> that I found a satisfactory answer to my question.</p>
<p>In this post, I’ll explain an insight from the principle of maximum entropy that conceptually justifies the normal distribution’s role in the CLT. To do so, we’ll first build up a basic introduction to entropy, how probability and entropy interact, and then explain the entropy property of the normal distribution that helped me understand the question above. For this post, all you’ll really need is a rough idea of what a random variable is, and some familiarity with common probability distributions; we’ll build up the rest from scratch.</p>
<section id="entropy" class="level2">
<h2 class="anchored" data-anchor-id="entropy">Entropy</h2>
<p>Imagine two situations in which you’ve lost your keys at home. In the first scenario, you’re well and truly confused as to where they are. As far as you’re concerned, any location within your home is equally likely. In the second scenario, you remember having them by your kitchen counter, so you’re pretty sure they’re there, or at least somewhere near there. Intuitively, the knowledge that they’re probably in your kitchen is much, much more informative that the idea that they’re equally likely to be anywhere in your house. In fact, once you accept the constraint that they’re within the bounds of your house, it’s pretty hard to imagine a more useless, uninformative statement than “they’re equally likely to be anywhere”.</p>
<p>Information theory helps makes rigorous many of our informal ideas about how much information or uncertainty is contained in situations like the above. Let’s start then, by defining <strong>entropy</strong>, a measure of the uncertainty of a random variable:</p>
<p><img src="https://latex.codecogs.com/png.latex?H(X)%20=%20-%20%5Cint_%7BI%7D%20p(x)%20%5Clog%7Bp(x)%7D%20dx"></p>
<p>Where p(x) is our continuous probability distribution over some interval I. It can be analogously defined for the discrete case by replacing the integral with a summation. To stress the intuition here, the higher the entropy, the less we know about the value of the random variable. As an example, a uniform distribution over the bounds <img src="https://latex.codecogs.com/png.latex?%5Ba,b%5D"> is analogous to our “keys could be anywhere” scenario- the entropy of our variable is high. In fact, once you accept the constraint that the distribution is supported on [a,b], one can prove that the uniform distribution is the maximum entropy distribution among all continuous distributions on that interval. We won’t include the proof here, but showing a similar maximum entropy property about the normal distribution is where we’re headed.</p>
<p>Before we get back to the normal distribution and CLT though, let’s think a little bit more about the concept of maximum entropy. Why would we want to use something specifically because it’s minimally informative? Let’s think about how we’d express or model our belief that our keys are equally likely to be anywhere. If we’re truly of the belief that they’re equally likely to be anywhere in our house, then some sort of model based on a uniform distribution over the space likely makes sense. If they’re equally likely to be anywhere, placing high probability in a specific room would be wrong, and making that additional assumption would probably slow down our search. This is Jaynes’ principle of maximum entropy- we should use the distribution with the highest entropy, subject to any prior constraints we already have.</p>
<p>This principle, then, is about epistemic modesty. We can want to choose the distribution that meets our constraints, and assumes as little additional information as possible.</p>
</section>
<section id="the-normal-distribution" class="level2">
<h2 class="anchored" data-anchor-id="the-normal-distribution">The Normal Distribution</h2>
<p>Once you specify a variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E%7B2%7D">, and that the distribution be supported on the reals from <img src="https://latex.codecogs.com/png.latex?(-%20%5Cinfty,%20%5Cinfty)">, the normal distribution is the maximum entropy distribution! <a href="http://notstatschat.tumblr.com/post/146886495511/how-do-we-prove-the-central-limit-theorem">Thomas Lumley</a> calls this a “a precise characterization of the normal’s boringness”. In our mental image of the CLT at work then, we’re approaching the normal because of it’s generic-ness, it’s lack of information. If we were mixing colors, the normal would be a nondescript grey.</p>
<p>In one sense, this result may be counter intuitive. As statisticians, when we find out that a distribution we’re working with is roughly normal, we tend to feel like we have a lot to work with. Many of our favorite tools like maximum likelihood will work well, and we have straightforward ways to estimate most quantities of interest. However, this result illustrates a subtle point: ease of use and information content aren’t the same thing.</p>
<p>If you found this use of information theory improved your intuition for probability and want more, here are some suggestions for further reading-</p>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources:</h2>
<ul>
<li>If you’re wondering how we actually prove that the normal is the maximum entropy distribution for a specified variance, there’s a self-contained proof in section <a href="http://www.deeplearningbook.org/contents/inference.html">19.4.2 of Deep Learning</a>. I excluded it in this post because it required a lot of extra math, and didn’t add much to the intuitive point I was trying to show.</li>
<li><a href="http://colah.github.io/posts/2015-09-Visual-Information/">Chris Olah’s Visual Information Theory</a> is a great introduction to the information compression and distribution comparison parts of information theory.</li>
</ul>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2018,
  author = {Andy Timm},
  title = {Why the Normal Distribution?},
  date = {2018-05-11},
  url = {https://andytimm.github.io/2018-05-11-maxentropynormal.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2018" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2018. <span>“Why the Normal Distribution?”</span> May 11,
2018. <a href="https://andytimm.github.io/2018-05-11-maxentropynormal.html">https://andytimm.github.io/2018-05-11-maxentropynormal.html</a>.
</div></div></section></div> ]]></description>
  <category>From Old Website</category>
  <guid>https://andytimm.github.io/posts/Maximum Entropy Normal/2018-05-11-maxentropynormal.html</guid>
  <pubDate>Fri, 11 May 2018 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Predicting race part 1- Bayes’ rule method and extensions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Race Models Pt 1/race_models_part1.html</link>
  <description><![CDATA[ 




<p>Race is a defining part of political identity in the United States, and so it should be no surprise that accurately modeling race can be beneficial for many political campaign activities. For instance, many organizations work to improve turnout in specific communities of color, or want to target persuasion on a given issue to certain racial group. Alternatively, race and ethnicity might be desired as an input to a larger voting or support likelihood model, given that race is generally predictive of both <a href="http://www.electproject.org/home/voter-turnout/demographics">voting likelihood</a> and <a href="https://www.nytimes.com/interactive/2016/11/08/us/politics/election-exit-polls.html">candidate support</a>.</p>
<p>Unfortunately, complete self-reported race data is only available in 7 states, where it is required by law: Alabama, Florida, Georgia, Louisiana, Mississippi, North Carolina, and South Carolina. Pennsylvania and Tennessee also have an optional field on voter registration forms. Outside of these states, race and ethnicity need to be collected individually, or modeled. These models commonly take advantage of the name (especially surname) of the individual voter, and other information in the voter file to do so.</p>
<p>In this post, I’ll explore a Bayes’ rule based method for modeling racial identity, and how it can be extended with additional information from state voter files where available. Obviously, it won’t be as predictive as something like Catalist or Civis Analytics’ ones (which have the benefit of huges swathes of additional survey and other data, not to mention more sophisticated modeling), but it can help illustrate the benfits and limitations of such tools. In the next posts, I’ll explain how natural language processing models can achieve still higher accuracy by extracting more information from names themselves. Since the Florida voter file has self-reported race data and is <a href="http://flvoters.com/downloads.html">easy to access</a>, we’ll test our models’ effectiveness against that ground truth.</p>
<section id="a-simple-first-method" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-first-method">A simple first method</h2>
<p>The <a href="https://www.census.gov/data/developers/data-sets/surnames.html">census surname files</a> are a an incredible source of information on how race correlates with names, and are the starting point of our model. In these files, the census provides race percentage breakdowns for any surname with more than 100 occurrences in the United States, except where redacted for privacy reasons. The data is available only aggregated at the national level. In practice, this means coverage of about 90% of the population.</p>
<p>For many voters, surname is the only information we need to make an accurate classification: names such as Carlson, Meyer, and Hanson are in excess of 90% white, with similar surnames existing for other races. Unfortunately, many names aren’t as clear cut, and some names like Chavis are less than 40% likely to be any race. This is a good starting point, but we can do better.</p>
<p>As our first improvement, <a href="https://link.springer.com/article/10.1007/s10742-009-0047-1">Elliot et al.&nbsp;(2009)</a> incorporates census geolocation (county, tract, or block) data, using Bayes’ theorem.</p>
<p>Bayes’ theorem is a natural way to integrate the new census evidence we have, updating our initial beliefs with more data. <img src="https://latex.codecogs.com/png.latex?P(A)"> and <img src="https://latex.codecogs.com/png.latex?P(B)"> are the probabilities of events A and B occurring independently of each other, and <img src="https://latex.codecogs.com/png.latex?P(A%20%5Cvert%20B)"> and <img src="https://latex.codecogs.com/png.latex?P(B%20%5Cvert%20A)"> are conditional probabilities, such as the probability a given voter is white given their surname, or <img src="https://latex.codecogs.com/png.latex?P(white%20%5Cvert%20surname)">. Bayes’ theorem appears in it’s simplest form below, giving us an updated probability utilizing the new information:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20P(A%7CB)%20=%20%5Cfrac%7BP(B%7CA)%20P(A)%7D%7BP(B)%7D%20"></p>
<p>Of course, we’re interested in integrating multiple pieces of evidence (surname/geolocation for now, with more coming later), and applying it to multiple voters. The equations will be much more involved, but remember that we’re essentially just extending the above equation to work with multiple inputs, over multiple voters. Note that I’ll be using the notation from <a href="https://imai.princeton.edu/research/race.html">Imai and Khanna (2016)</a>, which is somewhat clearer than the notion from Elliot et al.</p>
<p>We want <img src="https://latex.codecogs.com/png.latex?P(R_i%20=%20r%20%5Cvert%20S_i%20=%20s,%20G_i%20=%20g)">, the conditional probability that the voter <img src="https://latex.codecogs.com/png.latex?i"> belongs to the racial group <img src="https://latex.codecogs.com/png.latex?r"> given their surname <img src="https://latex.codecogs.com/png.latex?s"> and geolocation <img src="https://latex.codecogs.com/png.latex?g">. <img src="https://latex.codecogs.com/png.latex?R">, <img src="https://latex.codecogs.com/png.latex?S">, and <img src="https://latex.codecogs.com/png.latex?G"> are the sets of all racial groups, all surnames, and all geolocations respectively. Thus, as a final expression we’ll get:</p>
<p><img src="https://latex.codecogs.com/png.latex?P(R_i%20=%20r%20%5Cvert%20S_i%20=%20s,%20G_i%20=%20g)%20=%20%5Cfrac%7BP(G_i%20=%20g%20%5Cvert%20R_i%20=%20r)%20P(R_i%20=%20r%7CS_i%20=%20s)%7D%7B%5Csum_%7Br'%5Cin%20R%7D%20P(G_i%20=%20g%7CR_i%20=%20r)%20P(R_i%20=%20r%7CS_i%20=%20s)%20%7D"></p>
<p>We already have almost all these probabilities between the census surname list and census demographic data. <img src="https://latex.codecogs.com/png.latex?P(R_i%20=%20r%20%5Cvert%20S_i%20=%20s)"> is the racial composition of surnames from the surname list. But what about <img src="https://latex.codecogs.com/png.latex?P(G_i%20=%20g%20%5Cvert%20R_i%20=%20r)">? As an intermediate step, we first need to calculate <img src="https://latex.codecogs.com/png.latex?P(G_i%20=%20g%20%5Cvert%20R_i%20=%20r)"> , which we can calculate using Bayes’ rule again. <img src="https://latex.codecogs.com/png.latex?P(G_i%20=%20g%20%5Cvert%20R_i%20=%20r)"> is then <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BP(R_i%20=%20r%20%5Cvert%20G_i%20=%20g)%20P(G_i%20=%20g)%7D%20%7B%5Csum_%7Br'%5Cin%20R%7D%20P(R_i%20=%20r%20%5Cvert%20G_i%20=%20g')%20P(G_i%20=%20g)%7D">, completing everything we need to produce our second model.</p>
<p>This model, like the surname list, produces probabilistic predictions of race, for instance, a given voter is 94.3% likely to be white, given their name and geolocation. The probabilities sum to 1 across the races.</p>
<p>We’ll get to how to use and evaluate such models soon, but first: given that we’ve started to include information from the voter file to improve our predictions, why don’t we use other fields we have such as age, party registration, and gender? They all are likely to contain some conditional information about a voter’s race, and while party registration isn’t reported in every state, it is in Florida.</p>
<p>That’s exactly the proposal of <a href="https://imai.princeton.edu/research/race.html">Imai and Khanna (2016)</a>, and it works well. The equations get slightly more complicated, but extending Bayes’ rule to include more and more variables doesn’t change all that much. We have to calculate more intermediate probabilities, but the essential process and reasoning of incorporating new information to update our belief is the same. If you want to see the full model written out, with space for an arbitrary number of new variables <img src="https://latex.codecogs.com/png.latex?X_i">, you can find it in the linked paper.</p>
</section>
<section id="use-and-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="use-and-evaluation">Use and Evaluation</h2>
<p>Now that we have a probability distribution over the racial groups, how do we utilize them?</p>
<p>We might take the highest probability race and use that as our prediction. Alternatively, as an input in a later model, we might choose to simply incorporate all 5 probabilities, allowing our following model as much information as possible about the racial identity of a voter. Catalist, the democratic data vendor, turns probabilities into simple categories such as “likely white” or “possibly black”, which simplify working with the results on a campaign.</p>
<p>As a final idea, we might have an application in mind where you want to only predict a certain race when you’re very confident in your prediction. For example, you might be hoping to target a turnout mailer written in Spanish to only Hispanics, and as few non-Hispanics as possible. To do this, we’d utilize only high probability predictions- say, above 85% likely to be Hispanic. By changing that threshold, you could optimize the size of your mail universe versus the specificity and efficiency of it, finding the best balance for your campaign.</p>
<p>As our last example showed, when utilizing such probabilistic predictions, there is naturally an accuracy tradeoff involved in selecting what type of threshold to use. We could misleadingly say our model is extremely accurate if we only use a high threshold, but a fairer, more systematic evaluation would require looking at how it preforms over multiple such thresholds. That’s what we’ll develop next: Area Under the Curve (AUC), a systematic method for evaluating classifiers.</p>
<p>There are 4 possible outcomes to making a classification: a True Positive (TP), False Positive (FP), True negative (TN), and False Negative (FN). As an example then, a true positive is when we predicted positive, and the ground truth label was actually positive.</p>
<p>Rather than building a table of these 4 outcomes, called a confusion matrix, we’ll be working with summary statistics of these outcomes. The True Positive Rate (or sensitivity or recall) is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BTP%7D%7BTP+FN%7D">. In other words, out of all the points that are (ground truth) positive, how many did we correctly classify? Similarly, the False Positive Rate is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BFP%7D%7BFP+TN%7D">. High True Positive Rate is good: it means we’ll miss relatively few positive examples. On the other end of things, low False Positive Rate is what we want: it means fewer negative points will be misclassified.</p>
<p>By calculating these two statistics at a variety of thresholds, then plotting a curve with the FPR on the x-axis and TPR on the y-axis, we can get a deeper understanding of the tradeoff. This is called an Receiver Operating Characteristic. Given what I’ve said about the meaning of the TPR and FPR, can you figure out quality of the 3 classifiers that are graphed below?</p>
<p><img src="https://andytimm.github.io/posts/Race Models Pt 1/example_curves.jpg" class="img-fluid" alt="Example ROCs"> The first is a perfect classifier: at all tradeoff points, it has a TPR of 1, and FPR of 0. The second is pretty good: at most tradeoff points it does well. The third, a straight line from (0, 0) to (1,1) is a random classifier: it’s equivalent to guessing.</p>
<p>As an overall summary then, the area under this curve (AUC) is of our one number summary of these graphs. The shown classifiers have AUC 1, .8, and .5 respectively.</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>Now that we’ve built up a relatively complex model, and learned how to use and evaluate it, let’s plot some ROC curves, and look at AUCs for our models.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Race Models Pt 1/ROC_for_wru.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">ROC graphs for 4 models</figcaption><p></p>
</figure>
</div>
<p>And here’s the AUC table: bold is the highest overall for each race.</p>
<table class="table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>White</strong></th>
<th><strong>Black</strong></th>
<th><strong>Hispanic</strong></th>
<th><strong>Asian</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Surname</strong></td>
<td>0.8414369</td>
<td>0.8562679</td>
<td>0.9325489</td>
<td>0.8606716</td>
</tr>
<tr class="even">
<td><strong>Geo/Surname</strong></td>
<td>0.8792853</td>
<td>0.8918873</td>
<td><strong>0.9492531</strong></td>
<td><strong>0.8717517</strong></td>
</tr>
<tr class="odd">
<td><strong>Kitchen Sink</strong></td>
<td><strong>0.8903922</strong></td>
<td><strong>0.8979003</strong></td>
<td>0.9362794</td>
<td>0.7648270</td>
</tr>
</tbody>
</table>
<p>Looking at these, there are a lot of interesting patterns!</p>
<p><strong>White:</strong> The White models’ results are perhaps what we most expected. With more data, each subsequent model does better, and the overall result is strong.</p>
<p><strong>Black:</strong> Interestingly, the kitchen sink model still does the best overall with black voters, but by a much smaller margin than with whites. Also, once the true positive rate gets to around .90, curves cross! This is a good illustration of the importance of plotting ROC curves when doing classification- depending on your campaign, you might correctly choose either the Geo/Surname model or the Kitchen Sink one, depending on what type of cutoff you need.</p>
<p><strong>Hispanic:</strong> The Hispanic models are all very close together: with surname information being so effective in classifying Hispanics (more effective than any model for the other races), there isn’t much room for census or voter file data to improve things.</p>
<p><strong>Asian:</strong> These models are significantly weaker than all the others, but still reasonable. The unexpected trend though, is that the kitchen sink model preforms much worse than the other two. Given how few Asian voters there are overall in Florida, this downturn is probably explained by the relatively low density of any Asians across the other inputs, like age, sex, or party. Thus, while geographic information might slightly improve things, Floridians are so unlikely to be Asian overall that all of those extra variables don’t carry any useful information about who might be Asian.</p>
<p>Overall, these Bayes’ Theorem models have a lot of attractive features. While the predictiveness of names, and what variables you have from the voter file might vary state to state, the models can used anywhere in the US. They’re also quite a strong baseline for accuracy as well- far, far better than random, even for Asian voters. Unlike models we’ll discuss in the next post, these models require no training data. Finally, they’re transparent- if you want to check how surname, geolocation, and party weighed in to a particular decision, it’s only a few calculations with Bayes’ rule away.</p>
<p>Of course, we’d love higher accuracy, and we can get there with natural language processing. What about those ~10% of voters whose name aren’t in the census surname file? If a name starts with “Mc”, but isn’t in the census surname list, I personally would guess they’re of Irish descent, and therefore white. And what about using first names, middle names, and name suffixes? It’s linguistic patterns like these that we’ll hope to exploit with NLP, in the next post.</p>
<p>You can find the code used to write this post <a href="https://github.com/andytimm/CampaignBlog/tree/master/Race_Prediction/part1_wru">here</a>.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2018,
  author = {Andy Timm},
  title = {Predicting Race Part 1- {Bayes’} Rule Method and Extensions},
  date = {2018-04-10},
  url = {https://andytimm.github.io/race_models_part1.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2018" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2018. <span>“Predicting Race Part 1- Bayes’ Rule Method and
Extensions.”</span> April 10, 2018. <a href="https://andytimm.github.io/race_models_part1.html">https://andytimm.github.io/race_models_part1.html</a>.
</div></div></section></div> ]]></description>
  <category>From Old Website</category>
  <guid>https://andytimm.github.io/posts/Race Models Pt 1/race_models_part1.html</guid>
  <pubDate>Tue, 10 Apr 2018 04:00:00 GMT</pubDate>
</item>
</channel>
</rss>
