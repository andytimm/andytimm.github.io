<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Andy Timm</title>
<link>https://andytimm.github.io/blog.html</link>
<atom:link href="https://andytimm.github.io/blog.xml" rel="self" type="application/rss+xml"/>
<description>Personal website of Andy Timm</description>
<generator>quarto-1.2.269</generator>
<lastBuildDate>Sat, 27 May 2023 04:00:00 GMT</lastBuildDate>
<item>
  <title>Variational Inference for MRP with Reliable Posterior Distributions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Variational MRP Pt4/variational_mrp_pt4.html</link>
  <description><![CDATA[ 




<p>This is section 4 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.</p>
<p>The general structure for this post and the ones after it will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in these four posts will go a long way towards more robust variational inference. I’ll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.</p>
<p>In the <a href="https://andytimm.github.io/posts/Variational%20MRP%20Pt3/variational_mrp_3.html">last post</a> we took a look at how our ELBO objective requires specific version of KL Divergence (the “Exclusive” formulation of KLD), and saw that it encoded a preference for a certain type of solution to the VI problem. Then we looked at CUBO and CHIVI, an alternative bound and algorithm that avoid this problem, often leading to a more useful posterior distribution by pursuing a more “inclusive” solution.</p>
<p>In this post, we’ll leverage importance sampling to make the most of the samples we do have, emphasizing the parts of our <img src="https://latex.codecogs.com/png.latex?q(x)"> that look like <img src="https://latex.codecogs.com/png.latex?p(x)"> and de-emphasizing the parts that do not.</p>
<p>The rough plan for the series is as follows:</p>
<ol type="1">
<li>Introducing the Problem- Why is VI useful, why VI can produce spherical cows</li>
<li>How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li>Problem 1: KL-D prefers exclusive solutions; are there alternatives?</li>
<li><strong>(This post)</strong> Problem 2: Not all VI samples are of equal utility; can we weight them cleverly?</li>
<li>Problem 3: How can we get deeply flexible variational approximations; are Normalizing Flows the answer?</li>
<li>Problem 4: How can we know when VI is wrong? Are there useful error bounds?</li>
<li>Better grounded diagnostics and workflow</li>
</ol>
<section id="not-all-samples-are-equally-good" class="level1">
<h1>Not all samples are equally good</h1>
<p>So we’ve made an approximation <img src="https://latex.codecogs.com/png.latex?q(x)"> that’s cheap to sample from, and is somewhat close to <img src="https://latex.codecogs.com/png.latex?p(x)">, our true posterior. The way to improve the approximation we’ve focused on so far is to just go back to the start and make <img src="https://latex.codecogs.com/png.latex?q(x)"> better; for example, through changing up the variational family, or to switching to a different optimization objective like the CUBO. That’s one solution that’s often necessary, but can we work with a particular <img src="https://latex.codecogs.com/png.latex?q(x)"> we have and make better use of the parts of it that are the closest to being right?</p>
<p>… Phased this way, this sounds a lot like importance sampling. If you haven’t seen them before, an importance sampling estimator allows us to take draws from a (preferably) easy to sample from distribution<sup>1</sup> and reweight the samples to look more like our true target distribution. The weight <img src="https://latex.codecogs.com/png.latex?w_i"> (or ratio, <img src="https://latex.codecogs.com/png.latex?r_i">) for each sample <img src="https://latex.codecogs.com/png.latex?i"> take form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw_i%20=%20%5Cfrac%7Bp(x_i)%7D%7Bq(x_i)%7D%0A"> Before you get worried that we don’t have <img src="https://latex.codecogs.com/png.latex?p(x_i)"> because of the normalizing constant like every time we talk about having <img src="https://latex.codecogs.com/png.latex?p(x)"> in this series, there’s a clever estimator that “self-normalizes” such that this can be a reasonable strategy. Intuitively, we’re just placing more weight on samples more likely under <img src="https://latex.codecogs.com/png.latex?p(x)">.</p>
<p>This footenote<sup>2</sup> has a selection of some of my favorite resources for learning more or refreshing your memory about importance sampling, but for the main discussion let me pull out some particularly important sub-problems to solve in making a good importance sampling estimator, and good important sampling estimator for VI.</p>
<p>First, our choice of the “proposal” distribution we’re reweighting to be more like <img src="https://latex.codecogs.com/png.latex?p(x)"> matters for making this process practically feasible. We need the proposal distribution to be close enough to <img src="https://latex.codecogs.com/png.latex?p(x)"> that a realistic number of the draws get non-negligible weights. It might be true that we could draw proposals from a big <img src="https://latex.codecogs.com/png.latex?N"> dimensional uniform distribution for every problem, but if we want to be done sampling enough this century we need to at least get fairly close with our initial <img src="https://latex.codecogs.com/png.latex?q(x)">.</p>
<p>A second, but related problem is that it’s quite common for the unmodified importance sampling estimator to have some weights which are orders and orders of magnitude higher than the average weight, blowing up the variance of the estimator. Dan Simpson’s slides I linked above has an instructive example with not too weird <img src="https://latex.codecogs.com/png.latex?p(x)"> and <img src="https://latex.codecogs.com/png.latex?q(x)">’s, but high dimension, that has a max weight ~1.4 million (!) times the average. If that happens, our estimator will essentially ignore most samples without gigantic weights, and it’ll take ages for that estimator to tell us anything remotely reliable.</p>
<p>So with those points we need to address, here are the next topics in this post:</p>
<ol type="1">
<li>Importance Weighted Variational Inference</li>
<li>Robust importance sampling with built in diagnostics via Pareto-Smoothed Importance Sampling</li>
<li>Combining multiple proposal distributions via Multiple Importance Sampling</li>
</ol>
<section id="importance-weighted-variational-inference" class="level2">
<h2 class="anchored" data-anchor-id="importance-weighted-variational-inference">Importance Weighted Variational Inference</h2>
<p>Importance Weighting for VI in it’s simplest form is pretty intuitive (draw samples from an already trained <img src="https://latex.codecogs.com/png.latex?q(x)">, weight them…), so let’s derive the new Importance Weighted Variational Inference (IWVI) estimator first since some nice intuition will come with it.</p>
<p>I want to emphasize something that wasn’t clear to me for a good while- these two ideas are not equivalent. While both are useful tools, the “train time”, objective-modifying IWVI estimator is a distinct approach from the “test time” importance sampling approach that takes draws from a fixed <img src="https://latex.codecogs.com/png.latex?q(x)"> and reweights them as best it can.</p>
<p>We’ll aim to show that we can get a tighter ELBO by using importance sampling. This type of tighter ELBO was first shown by <a href="https://arxiv.org/abs/1509.00519">Burda et Al. (2015)</a> in the context of Variational Autoencoders after which is was fairly clear this could apply to variational inference, but <a href="https://arxiv.org/abs/1808.09034">Domke and Sheldon (2018)</a> fleshed out some details of that extension- I’ll be explaining some of the latter group’s main results first.</p>
<p>To start, imagine a random variable <img src="https://latex.codecogs.com/png.latex?R">, such that <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%7BR%7D%20=%20p(x)">, which we’ll think of as a estimator of <img src="https://latex.codecogs.com/png.latex?p(x)">. Then by Jensen’s Inequality:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Alogp(x)%20=%20%5Cmathbb%7BE%7DlogR%20+%20%5Cmathbb%7BE%7Dlog%5Cfrac%7Bp(x)%7D%7BR%7D%0A"></p>
<p>The first term is the bound, which will be tighter if <img src="https://latex.codecogs.com/png.latex?R"> is highly concentrated.</p>
<p>This is a more general form of the ELBO; we can make it quite familiar looking by having our <img src="https://latex.codecogs.com/png.latex?R"> above be:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AR%20=%20%5Cfrac%7Bp(z,x)%7D%7Bq(z)%7D,%20z%20%5Csim%20q%0A"></p>
<p>The reason for pointing out this fairly simple generalization is helpful is that it frames how to tighten our ELBO on <img src="https://latex.codecogs.com/png.latex?logp(x)"> via alternative estimators <img src="https://latex.codecogs.com/png.latex?R">.</p>
<p>By drawing <img src="https://latex.codecogs.com/png.latex?M"> samples and averaging them as in importance sampling, we get:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AR_M%20=%20%5Cfrac%7B1%7D%7BM%7D%5Csum_%7Bm=1%7D%5E%7BM%7D%5Cfrac%7Bp(z_m,x)%7D%7Bq(z_m)%7D,%20z_m%20%5Csim%20q%0A"> From there, we can derive a tighter bound on <img src="https://latex.codecogs.com/png.latex?logp(x)">, referred to as the IW-ELBO:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AIW-ELBO_M%5Bq(z)%7C%7Cp(z,x)%5D%20:=%20%5Cmathbb%7BE%7D_%7Bq(z_%7B1:M%7D)%7Dlog%5Cfrac%7B1%7D%7BM%7D%20%5Csum_%7Bm=1%7D%5E%7BM%7D%5Cfrac%7Bp(z_m,x)%7D%7Bq(z_m)%7D%0A"> Where we’re using the <img src="https://latex.codecogs.com/png.latex?1:M"> as a shorthand for <img src="https://latex.codecogs.com/png.latex?q(z_%7B1:M%7D)%20=%20q(z_1)...q(z_M)">.</p>
<p>It’s worth noting that the last few lines don’t specify a particular form of importance sampling- we’re getting the tighter theoretical bounding behavior from the averaging of samples from <img src="https://latex.codecogs.com/png.latex?q">. We’ll see a particularly good form of importance sampling with desirable practical properties in a moment.</p>
<section id="how-does-iw-elbo-change-the-vi-problem-conceptually" class="level3">
<h3 class="anchored" data-anchor-id="how-does-iw-elbo-change-the-vi-problem-conceptually">How does IW-ELBO change the VI problem conceptually?</h3>
<p>The tighter bound is nice, but importance sampling also has the side effect (done right, side benefit) of modifying our incentives in choosing a variational family. To see what I mean, we can re-use the example distributions from last post we used to build intuition for KL Divergence, where red was the true distribution, and green were our potential approximations. If we’re not going to draw multiple samples and weight them, it makes sense to choose something like the first plot below. Every draw in the middle of the two target modes is expensive per our ELBO objective, so better to choose a mode.</p>
<div class="cell">

</div>
<div class="cell">

</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">rkl_plot <span class="ot" style="color: #003B4F;">&lt;-</span> mixture <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> normals)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-2">  <span class="fu" style="color: #4758AB;">geom_density</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> normals), <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"red"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-3">  <span class="fu" style="color: #4758AB;">geom_density</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> mode_seeking_kl), <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"green"</span>) <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">ggtitle</span>(<span class="st" style="color: #20794D;">"Without weighting, we prefer to capture a mode"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-4">  <span class="fu" style="color: #4758AB;">xlab</span>(<span class="st" style="color: #20794D;">""</span>)</span>
<span id="cb1-5"></span>
<span id="cb1-6">fkl_plot <span class="ot" style="color: #003B4F;">&lt;-</span> mixture <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> normals)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-7">  <span class="fu" style="color: #4758AB;">geom_density</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> normals), <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"red"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-8">  <span class="fu" style="color: #4758AB;">geom_density</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> mean_seeking_kl), <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"green"</span>) <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">ggtitle</span>(<span class="st" style="color: #20794D;">"With importance sampling, weights allow us to prefer coverage"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-9">  <span class="fu" style="color: #4758AB;">xlab</span>(<span class="st" style="color: #20794D;">""</span>)</span>
<span id="cb1-10"></span>
<span id="cb1-11"><span class="fu" style="color: #4758AB;">grid.arrange</span>(rkl_plot,fkl_plot)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt4/variational_mrp_pt4_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>If we can use importance sampling though, quite the opposite is be true! Note that we’re still using the ELBO, a reverse-KL based metric- that hasn’t changed. What has changed is our ability to mitigate the objective costs of those samples between the two extremes. Via this “train time” implementation of IS, points outside the two target modes will get lower importance weights, and points within the modes will get higher ones, so as long as we’re covering the modes with some reasonable amount of probability mass, and drawing enough samples we can actually do better with the distribution centered between the modes.</p>
<p>To further drive home the point about how a “train time” and “test time” implementations of IS differ, could “test time” IS do this? Not really- because the ability to better minimize the ELBO via sampling requires the IW-ELBO variant and associated training process. If we hard-coded <img src="https://latex.codecogs.com/png.latex?q(x)"> as the green <img src="https://latex.codecogs.com/png.latex?N(9,4)"> shown above, “test time” IS could weight the right samples up to better approximate <img src="https://latex.codecogs.com/png.latex?p(x)">, but it doesn’t fundamentally alter our optimization problem the way the IWVI objective does.</p>
<p>We can also imagine how varying the number of samples might effect optimization. Between <img src="https://latex.codecogs.com/png.latex?S=1"> and “enough draws to get all the benefits of IS”, we can imagine there’s a slow transition from “just stick with 1 mode” and “go with IS”. So it seems like we should be worried about getting the number of samples right, but fortunately as we’ll see in the next section there are great rules of thumb in some variants of IS. We’ll still need to bear the cost of sampling (which gets higher as <img src="https://latex.codecogs.com/png.latex?q(x)"> becomes “further” from <img src="https://latex.codecogs.com/png.latex?p(x)">, as we’ll need more samples to weight into a good approximation), but the cost of sampling for most VI implementations will often be pretty manageable if our proposal distribution is somewhat close to <img src="https://latex.codecogs.com/png.latex?p(x)">.</p>
<p>Another way to think about how importance sampling changes our task with variational inference is to think about what sorts of distributions make sense to have as our variational family, and even which objective might be better given IS. On choice of a variational family, if we’re aiming for coverage, moving towards thicker-tailed distributions like t distributions makes a lot of sense. While we explored the IW-ELBO above to build intuition, there’s no reason not to apply IW to the CUBO and thus CHIVI- this also naturally produces nicely overdispersed distributions which can be importance sampled closer to the true <img src="https://latex.codecogs.com/png.latex?p(x)">. This idea of aiming for a wide proposal to sample from is referred to in the importance sampling literature (eg <a href="https://artowen.su.domains/mc/">Owen, 2013</a>) as “defensive sampling”, with <a href="https://arxiv.org/abs/1808.09034">Domke and Sheldon (2018)</a> exploring the VI connection more fully. For intuition, by ensuring most of <img src="https://latex.codecogs.com/png.latex?p(x)"> is covered by some reasonable mass makes it easier to efficiently get draws that can be weighted into a final posterior, even if the unweighted posterior might be too wide.</p>
</section>
</section>
<section id="solving-our-is-problems-with-pareto-smoothed-importance-sampling" class="level2">
<h2 class="anchored" data-anchor-id="solving-our-is-problems-with-pareto-smoothed-importance-sampling">Solving our IS problems with Pareto-Smoothed Importance Sampling</h2>
<p>As we’ve been talking about importance sampling, we’ve been leaving some of the messier details aside (how many samples to draw, how to deal with the cases when some of the weights get huge, how to know when our proposal distribution is “close” enough).</p>
<p>While the Importance Sampling Literature is huge and there are a lot of possible solutions here, I’ll next introduce <a href="https://arxiv.org/abs/1507.02646">Vehtari et Al. (2015)</a>’s Pareto-Smoothed Importance Sampling. I’m a huge fan of this paper- it’s a really elegant and powerful tool, derived from taking Bayesian principles seriously.</p>
<p>Above, I described a common failure mode for IS estimators, where some weights are orders of magnitude larger than others, with this long right tail of ratios dominating the weighted average and blowing up the variance of the estimator. Pareto-Smoothed Importance Sampling proposes to model those tail values as coming from a Generalized Pareto Distribution, a distribution for describing extreme values, and replace the most extreme weights with modeled (and more stable) values.</p>
<p>For concreteness, let’s introduce a simple 1-D example. We’ll aim to use importance sampling to approximate distributions <span style="color:red;"><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BT%7D(%5Cmu%20=%200,%5Csigma%20=%201,t%20=5">)</span> and <span style="color:blue;"><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D(x_0=%200,%5Cgamma%20=%2010)"></span> with a <span style="color:green;"><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BN%7D(%5Cmu%20=%200,%5Csigma%20=%201">)</span> distribution. If that sounds like the opposite of preferring wide tails on <img src="https://latex.codecogs.com/png.latex?q(x)">’s I described above, you’re right, but using a poor choice here will illustrate some useful properties.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">simulated_data <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">tibble</span>(</span>
<span id="cb2-2"><span class="at" style="color: #657422;">q_x =</span> <span class="fu" style="color: #4758AB;">rnorm</span>(<span class="dv" style="color: #AD0000;">100000</span>),</span>
<span id="cb2-3"><span class="at" style="color: #657422;">manageable_p_x =</span> <span class="fu" style="color: #4758AB;">rt</span>(<span class="dv" style="color: #AD0000;">100000</span>,<span class="dv" style="color: #AD0000;">5</span>),</span>
<span id="cb2-4"><span class="at" style="color: #657422;">unmanageable_p_x =</span> <span class="fu" style="color: #4758AB;">rcauchy</span>(<span class="dv" style="color: #AD0000;">100000</span>),</span>
<span id="cb2-5"><span class="at" style="color: #657422;">manageable_ratios =</span> <span class="fu" style="color: #4758AB;">dt</span>(q_x,<span class="dv" style="color: #AD0000;">5</span>)<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">dnorm</span>(q_x),</span>
<span id="cb2-6"><span class="at" style="color: #657422;">unmanageable_ratios =</span> <span class="fu" style="color: #4758AB;">dcauchy</span>(q_x,<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">10</span>)<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">dnorm</span>(q_x)</span>
<span id="cb2-7">)</span>
<span id="cb2-8"></span>
<span id="cb2-9">simulated_data <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb2-10">            <span class="fu" style="color: #4758AB;">pivot_longer</span>(<span class="fu" style="color: #4758AB;">c</span>(q_x,manageable_p_x,unmanageable_p_x),</span>
<span id="cb2-11">                         <span class="at" style="color: #657422;">values_to =</span> <span class="st" style="color: #20794D;">"draws"</span>,</span>
<span id="cb2-12">                         <span class="at" style="color: #657422;">names_to =</span> <span class="st" style="color: #20794D;">"distributions"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb2-13">            <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> draws, <span class="at" style="color: #657422;">color =</span> distributions)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-14">            <span class="fu" style="color: #4758AB;">geom_density</span>() <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-15">            <span class="co" style="color: #5E5E5E;"># If you wanted to show the full reach of the Cauchy, it'd be</span></span>
<span id="cb2-16">            <span class="co" style="color: #5E5E5E;"># hard to see the shape of the T vs N; it's that wide.</span></span>
<span id="cb2-17">            <span class="co" style="color: #5E5E5E;"># Hence the 6k values removed</span></span>
<span id="cb2-18">            <span class="fu" style="color: #4758AB;">xlim</span>(<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">10</span>,<span class="dv" style="color: #AD0000;">10</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-19">            <span class="fu" style="color: #4758AB;">ggtitle</span>(<span class="st" style="color: #20794D;">"Visualizing the distributions in question"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-20">            <span class="fu" style="color: #4758AB;">theme</span>(<span class="at" style="color: #657422;">legend.position=</span><span class="st" style="color: #20794D;">"none"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Removed 6245 rows containing non-finite values (stat_density).</code></pre>
</div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt4/variational_mrp_pt4_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The tails on that Cauchy distribution are super, super wide compared to our normal, so the samples far, far out in the tails of the normal will need massive weights to approximate the cauchy. The t-distribution is wider too, so we’ll need some higher weights, but not nearly as many. As a way to visualize this, you can see that just a handful of draws have weights away from ~1, but these weights are as much as 5000x higher than the mean ratio, and will dominate any average we make of them.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">simulated_data <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb4-2">  <span class="fu" style="color: #4758AB;">arrange</span>(unmanageable_ratios) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb4-3">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">n =</span> <span class="fu" style="color: #4758AB;">seq</span>(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">100000</span>)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb4-4">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> n,<span class="at" style="color: #657422;">y =</span> unmanageable_ratios)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-5">  <span class="fu" style="color: #4758AB;">geom_point</span>() <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-6">  <span class="fu" style="color: #4758AB;">ggtitle</span>(<span class="st" style="color: #20794D;">"A pretty typical 'unsaveable' set of importance ratios"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt4/variational_mrp_pt4_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The t-distribution ratio plot would look similar, but with a much smaller y-scale. The max weight would still be much larger than the average, but more than an order of magnitude or so less large:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">mean_t <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">mean</span>(simulated_data<span class="sc" style="color: #5E5E5E;">$</span>manageable_ratios)</span>
<span id="cb5-2">max_t <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">max</span>(simulated_data<span class="sc" style="color: #5E5E5E;">$</span>manageable_ratios)</span>
<span id="cb5-3"></span>
<span id="cb5-4">mean_c <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">mean</span>(simulated_data<span class="sc" style="color: #5E5E5E;">$</span>unmanageable_ratios)</span>
<span id="cb5-5">max_c <span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">max</span>(simulated_data<span class="sc" style="color: #5E5E5E;">$</span>unmanageable_ratios)</span>
<span id="cb5-6"></span>
<span id="cb5-7"><span class="fu" style="color: #4758AB;">print</span>(<span class="fu" style="color: #4758AB;">paste0</span>(<span class="st" style="color: #20794D;">"the mean of the t is: "</span>,mean_t,<span class="st" style="color: #20794D;">" compared to a max of "</span>,max_t,<span class="st" style="color: #20794D;">";"</span>,</span>
<span id="cb5-8">             <span class="st" style="color: #20794D;">"The cauchy cause is more extreme- the mean of the cauchy is: "</span>,mean_c,<span class="st" style="color: #20794D;">" compared to a max of "</span>,max_c))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "the mean of the t is: 0.995904306317625 compared to a max of 164.448932152079;The cauchy cause is more extreme- the mean of the cauchy is: 0.283655933763642 compared to a max of 1428.22324178729"</code></pre>
</div>
</div>
<p>So let’s bring this back to Pareto smoothing here. We want to model and smooth that long tail of the ratio distribution. It turns out there’s plenty of study of the distribution of extreme events, and there’s some classical limit results showing:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ar(%5Ctheta)%20%7C%20r(%5Ctheta)%20%3E%20%5Ctau%20%5Crightarrow%20GPD(%5Ctau,%5Csigma,k),%20%5Ctau%20%5Crightarrow%20%5Cinfty%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Ctau"> is a lower bound parameter, which in our case defines how many ratios from the tail we’ll actually model. <img src="https://latex.codecogs.com/png.latex?%5Csigma"> is a scale parameter, and <img src="https://latex.codecogs.com/png.latex?k"> is a unconstrained shape parameter. Without getting too far into the weeds, we can implicitly define <img src="https://latex.codecogs.com/png.latex?%5Ctau"> via using a well-supported role of thumb suggesting to use the M largest ratios, <img src="https://latex.codecogs.com/png.latex?M%20=%20min(0.2S,3%5Csqrt%7BS%7D)"><sup>3</sup>. From there, the <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D"> have easy and efficient estimators. The Generalized Pareto Distribution has form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7B%5Csigma%7D%20%5Cleft(1%20+%20k%5Cfrac%7Br%20-%20%5Ctau%7D%7B%5Csigma%7D%20%5Cright)%5E%7B-1/k-1%7D%0A"></p>
<p>and we can replace our M biggest ratios with estimated values calculated via the CDF of the Generalized Pareto Distribution.</p>
<p>One of the best things about PSIS is it comes with a built in diagnostic via <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D">. To see how this works, it’s useful to know that importance sampling depends on how many moments <img src="https://latex.codecogs.com/png.latex?r(%5Ctheta)"> has- for example, if at least two moments exist, the vanilla IS estimator has finite variance (which is obviously required, but no guarantee of performance since it might be finite but massive). The GPD has <img src="https://latex.codecogs.com/png.latex?k%5E%7B-1%7D"> finite fractional moments when <img src="https://latex.codecogs.com/png.latex?k%20%3E%200">.</p>
<p>Vehtari et al.&nbsp;show that the replacement of the largest M ratios above changes PSIS to have finite variance and an error distribution converging to normal when <img src="https://latex.codecogs.com/png.latex?k%20%5Cin%20(.5,1)">. Intuitively, <img src="https://latex.codecogs.com/png.latex?k%20%3E%20.5"> implies the raw ratios have infinite variance, but PSIS trades a little bias to make the variance finite again.</p>
<p>What about actually practical to work with variance? This is the really cool bit- <img src="https://latex.codecogs.com/png.latex?k%20%3C%20.7"> turns out to be a remarkably robust indicator of when we can expect PSIS to work in a ton of different simulation studies and practical examples.</p>
<p>Why is this true? 1.4 fractional moments seems awful arbitrary, right? Let’s ask an alternative question, and kill two birds with one stone: what sample size do we need for PSIS to work? <a href="https://arxiv.org/abs/1511.01437">Chaterjee and Draconis (2018)</a> showed that for a given accuracy, how big <img src="https://latex.codecogs.com/png.latex?S"> needs to be for importance sampling more broadly depends on how close <img src="https://latex.codecogs.com/png.latex?q(x)"> is to <img src="https://latex.codecogs.com/png.latex?p(x)"> in KL distance- we need to satisfy <img src="https://latex.codecogs.com/png.latex?log(S)%20%5Cgeq%20%5Cmathbb%7BE%7D_%7B%5Ctheta%20%5Csim%20q(x)%7D%5Br(%5Ctheta)log(r(%5Ctheta))%5D"> to get accuracy.</p>
<p>Well, we don’t know the distribution of <img src="https://latex.codecogs.com/png.latex?r">, we should have some pretty good intuition that the important part (read: that explosive, variance ruining tail) is Pareto. If we take <img src="https://latex.codecogs.com/png.latex?r"> as exactly Pareto, you can trace out <img src="https://latex.codecogs.com/png.latex?S"> for different <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"><sup>4</sup>, and to give a few example points-</p>
<table class="table">
<caption>for given <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D">, roughly what <img src="https://latex.codecogs.com/png.latex?S"> is needed if <img src="https://latex.codecogs.com/png.latex?r"> is exactly Pareto</caption>
<colgroup>
<col style="width: 75%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"></th>
<th><img src="https://latex.codecogs.com/png.latex?S"> needed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>.5</td>
<td>~1,000</td>
</tr>
<tr class="even">
<td>.7</td>
<td>~140,000</td>
</tr>
<tr class="odd">
<td>.8</td>
<td>1,000,000,000,000</td>
</tr>
<tr class="even">
<td>.9</td>
<td>please stop you’re making your compute sad.</td>
</tr>
</tbody>
</table>
<p>While we of course know <img src="https://latex.codecogs.com/png.latex?r"> isn’t Pareto exactly exactly, hopefully this helps with intuition around <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> telling us when we’re getting into “sampling forever to have any chance at all to control the variance” land.</p>
<p>Neat! So what does that look like for our Cauchy and T distribution example?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1">manageable_psis <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">psis</span>(<span class="fu" style="color: #4758AB;">log</span>(simulated_data<span class="sc" style="color: #5E5E5E;">$</span>manageable_ratios),</span>
<span id="cb7-2">                       <span class="at" style="color: #657422;">r_eff =</span> <span class="cn" style="color: #8f5902;">NA</span>)</span>
<span id="cb7-3"></span>
<span id="cb7-4">unmanageable_psis <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">psis</span>(<span class="fu" style="color: #4758AB;">log</span>(simulated_data<span class="sc" style="color: #5E5E5E;">$</span>unmanageable_ratios),</span>
<span id="cb7-5">                          <span class="at" style="color: #657422;">r_eff =</span> <span class="cn" style="color: #8f5902;">NA</span>)</span>
<span id="cb7-6"></span>
<span id="cb7-7">manageable_psis<span class="sc" style="color: #5E5E5E;">$</span>diagnostics</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$pareto_k
[1] 0.5963495

$n_eff
[1] 62963.93</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1">unmanageable_psis<span class="sc" style="color: #5E5E5E;">$</span>diagnostics</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$pareto_k
[1] 0.8533127

$n_eff
[1] 249.2052</code></pre>
</div>
</div>
<p>As we expected, the the Normal proposal distribution isn’t ideal for the T distribution, but it’s manageable. On the other hand, we’d need somewhere between <strong>a trillion and “oh god no :(”</strong> samples to make the normal proposal work out for the Cauchy.</p>
<p>Bringing the discussion back to variational inference, PSIS is super helpful- importance sampling more generally broadens the class of <img src="https://latex.codecogs.com/png.latex?q(x)">es that are close enough to <img src="https://latex.codecogs.com/png.latex?p(x)"> for variational inference to work, and PSIS considerably widens that basin of feasibility. The extensive theoretical and simulation framework around the method also give us a solid way to realize when importance sampling isn’t feasible via the <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> diagnostic, and tells us how roughly samples we need to draw. Super, super cool.</p>
<p>One more great thing PSIS does for variational inference- <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> serves as a powerful diagnostic for variational inference itself! I’ll save most of this discussion for the post on diagnostics, but to sketch out the logic- <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> tells us when <img src="https://latex.codecogs.com/png.latex?q(x)"> is too far from <img src="https://latex.codecogs.com/png.latex?p(x)"> for importance sampling to work, which is a function of KL Divergence from <img src="https://latex.codecogs.com/png.latex?q(x)"> to <img src="https://latex.codecogs.com/png.latex?p(x)">- if that distance is too great for importance sampling to allow us to bridge, that implies we aren’t close enough to trust our base variational approximation either!</p>
</section>
<section id="multiple-proposal-distributions-with-multiple-importance-sampling" class="level2">
<h2 class="anchored" data-anchor-id="multiple-proposal-distributions-with-multiple-importance-sampling">Multiple Proposal Distributions with Multiple Importance Sampling</h2>
<p>Why stop at just one proposal distribution? This is basically the jumping off point for Multiple Importance sampling, or MIS. If we have several different <img src="https://latex.codecogs.com/png.latex?q(x)">, and each does a somewhat better job of handling a certain region of the target posterior, then we can efficiently combine them using MIS into an overall better final estimate, and this will work out to be pretty obviously more optimal than just fitting a bunch of VI approximations and averaging them.</p>
<p>If we can suddenly have multiple different <img src="https://latex.codecogs.com/png.latex?q(x)"> working together, this naturally explodes the search space for a good VI strategy. I’d refer the more interested reader to <a href="https://projecteuclid.org/journals/statistical-science/volume-34/issue-1/Generalized-Multiple-Importance-Sampling/10.1214/18-STS668.full">Elvira et al.&nbsp;(2019)</a> which lays out a framework for thinking about all the decision space of MIS more comprehensively, but for the purposes of improving VI specifically, I’ll cover:</p>
<ol type="1">
<li>How do we weight the proposals together?</li>
<li>Which proposals make sense to include in a MIS framework?</li>
<li>How practical is fitting multiple proposals?</li>
</ol>
<section id="how-do-mis-weights-work" class="level3">
<h3 class="anchored" data-anchor-id="how-do-mis-weights-work">How do MIS weights work?</h3>
<p>How do we generalize a notion of importance weights like the one introduced above:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw_i%20=%20%5Cfrac%7Bp(x_i)%7D%7Bq(x_i)%7D%0A"></p>
<p>to multiple proposals? While there are some obviously not good properties we want to avoid (it’d be pretty silly to give up our unbiasedness), there are a ton of apparent degrees of freedom in MIS weighting. we’ll relax this assumption in a bit, but let’s start by assuming we don’t have any prior information about which proposals might be better, and that we’ll draw the same number of samples from each proposal.</p>
<p>While I won’t work through as extensive of an example as in the last section, let’s fix an example where we’ll have <img src="https://latex.codecogs.com/png.latex?J%20=%203"> different proposals, <span style="color:cyan;"><img src="https://latex.codecogs.com/png.latex?q_1(x)">)</span>, <span style="color:purple;"><img src="https://latex.codecogs.com/png.latex?q_2(x)"></span>, and <span style="color:pink;"><img src="https://latex.codecogs.com/png.latex?q_3(x)">)</span>.</p>
<p>A first question is how to choose the denominator in the weight. One simple and efficient option is to simply use the density of a sample from <img src="https://latex.codecogs.com/png.latex?j"> to make a weight, for example weighting a draw from <span style="color:pink;"><img src="https://latex.codecogs.com/png.latex?q_3(x)">)</span> as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw_%7Bi%7D%20=%20%5Cfrac%7Bp(x_i)%7D%7B%5Ctextcolor%7Bpink%7D%7Bq_3(x_i)%7D%7D%0A"> This works, and is pretty common in MIS applications, but we’re not really using all the information we have from having several proposals. We can get a provably lower variance estimator by defining the mixture of the densities <img src="https://latex.codecogs.com/png.latex?%5Cpsi(x)"> as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpsi(x)%20=%20%5Cfrac%7B1%7D%7BJ%7D%20%5Csum%5Climits_%7BJ%20=%201%7D%5Climits%5E%7BJ%7D%20q_j(x)%0A"></p>
<p>and using that as the denominator. So for the example above, this’d be:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw_%7Bi%7D%20=%20%5Cfrac%7Bp(x_i)%7D%7B%5Cfrac%7B1%7D%7B3%7D(%5Ctextcolor%7Bcyan%7D%7Bq_1(x)%7D%20+%20%5Ctextcolor%7Bpurple%7D%7Bq_2(x_i)%7D%20+%20%5Ctextcolor%7Bpink%7D%7Bq_3(x_i))%7D%7D%0A"> By defining this mixture and and incorporating it into our weighting, we intuitively should have more efficient exchange of information between the different <img src="https://latex.codecogs.com/png.latex?q(x)">. By this, I mean that we no longer just are weighting each sample from a proposal using information from that one proposal; we’re now using everything at hand.</p>
<p>This feels like it should be pretty solidly better than just using a single proposal density, and indeed Elvira et al.&nbsp;have a result showing that the variance of the mixture based weighting scheme is under pretty general conditions lesser than or equal to that of the single proposal density one<sup>5</sup>.</p>
<p><a href="https://dl.acm.org/doi/10.1145/218380.218498">Veach and Guibas (1995)</a>, the paper to introduce MIS, called this weighting scheme the <em>balance heuristic</em>, since the weighting scheme is unique in that each sample value at particular <img src="https://latex.codecogs.com/png.latex?x"> is the same regardless of which distribution produced it. They also prove a bound on the variance of this estimator, showing that there isn’t a lot of room to improve on it, even in the most ideal circumstances. Without getting into the weeds, their result suggests that there isn’t a massively better general-case weighting scheme, which is a helpful guide to practical use.</p>
<p>When can we do (a bit) better than the weighting scheme above? The answer is essentially in cases where we know some of our <img src="https://latex.codecogs.com/png.latex?J"> proposals are much better than others. In these situations, the variance can often be lowered by pushing weights towards the extremes, making low weights closer to zero, and high weights closer to 1. Their <em>cutoff heuristic</em> suggests an estimator where you pick some bound <img src="https://latex.codecogs.com/png.latex?%5Calpha">, below which low weights are reassigned to zero (and the rest of the distribution is adjusted back to sum correctly). Their also propose the <em>power heuristic</em>-</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw_i%20=%20%5Cfrac%7Bp_i%5E%5Cbeta%7D%7B%5Csum%5Climits_%7Bj%7Dp_j%5E%5Cbeta%7D%0A"> which raises the weights to a power <img src="https://latex.codecogs.com/png.latex?%5Cbeta">, and normalizes. For intuition, notice that if <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20=%201">, then this is the <em>balance heuristic</em> again, and as <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20%5Crightarrow%20%5Cinfty">, this moves towards only selecting the best proposal at each point.</p>
<p>As a final note, we can also Pareto Smooth any of these types of weights once we have them, and this sparks joy, as we can add begin to envision model setups with glorious abbreviations like IW-ELBO/IW-CUBO-PSIS-MIS-VI.</p>
<p>So stepping back, we have some provably efficient, provably hard to beat ways to use MIS to combine variational approximations together. Again, there’s a whole literature on MIS which the Elvira paper above reviews, but fairly intuitive weighting schemes exist that work well in most cases, and there are reasonable things to try in more atypical cases to reduce the variance of the MIS estimator as well.</p>
</section>
<section id="what-proposals-combine-best" class="level3">
<h3 class="anchored" data-anchor-id="what-proposals-combine-best">What proposals combine best?</h3>
<p>A next natural question is what different proposals should we use? There’s a little less work in this area than I expected, but there are a couple of papers; my favorite is <a href="https://arxiv.org/abs/2002.07217">Lopez et al.&nbsp;(2020)</a><sup>6</sup>.</p>
<p>They find that using VI approximations based on different objectives is quite performant- for example, having all of a vanilla ELBO, IW-ELBO and <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> divergence based VI approximation works particularly well, and as you’d expect, better than any individual model, just like we’d expect with regular ensembling techniques. They also look at taking some samples directly from our priors, which is moderately surprising to me given how broad weakly-informative priors usually are. Overall though, a core nugget of logic from ensembling more generally applies here too: we want to find proposals that are both good and sufficiently different from one another that combining them adds value.</p>
<p>It seems to me there’s a lot of room to explore this search space still; there are a lot of generic ML ensembling tricks that feel like they could work. For example, could we save state several times throughout optimizing a variational approximation, and MIS combine samples from each of those, similar to how people cheaply ensemble for neural networks? Or are there ways to optimize the proposals for use together in this way?</p>
</section>
<section id="how-practical-is-mis-for-vi" class="level3">
<h3 class="anchored" data-anchor-id="how-practical-is-mis-for-vi">How practical is MIS for VI?</h3>
<p>A last obvious question is whether fitting many variational approximations and combining them is computationally practical. While MIS for VIS certainly trades back some computational cost and time for potential accuracy, the good news is everything feels cheap compared to MCMC.</p>
<p>Fitting <img src="https://latex.codecogs.com/png.latex?J"> VI approximations instead of 1 roughly scales your compute need for fitting the models by a factor of ~<img src="https://latex.codecogs.com/png.latex?J">, and then there’s a small additional cost in the MIS combination stage to evaluate all the models to make each importance sampling weight denominator. Unlike with MCMC, these computational needs are parallelizable.</p>
<p>Lopez et al.&nbsp;(2020) find that using 3 proposals slightly more than triples their compute cost given all the objective based models take around the same time to fit, and in practice slightly more than triples their compute time as well since they didn’t do the work to parallelize their models. On the problems they were working on, this is a pretty small (~30s more) time cost in exchange for a meaningful accuracy improvement in the real world biology application they apply this to.</p>
<p>Depending on what you’re working on, the answer may well be yes, this can be computationally feasible and well worth it.</p>
</section>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>Importance Sampling is a workhorse of modern computational statistics, and it should be no surprise it brings a lot to variational inference.</p>
<p>Like with the last post, my overall impression is of decreasing fragility for variational inference and a broader set of tools for increasing performance. With IW-ELBO and similar objectives, we can get a tighter bound than the vanilla ELBO, and introduce some new incentives in training our approximation as well. With importance sampling in general and PSIS especially, we can weight an approximation that is close to the target but not perfect into a much, much better approximation of our posterior, and do some in a principled and theoretically grounded way with built-in diagnostics. With MIS, we can make the most of several approximations at once, if we’re willing to pay that computational cost. Collectively, we’re building up a set of tools that broaden the class of problems for which VI works, provided you’re willing to spend time searching for a combination of tools that works well for your specific application.</p>
<p>Thanks for reading. In the next post, we’ll look at Normalizing Flows, an incredibly powerful and general tool for making maximally flexible variational distributions. All code for this post can be found <a href="https://github.com/andytimm/andytimm.github.io/blob/main/posts/Variational%20MRP%20Pt4/variational_mrp_pt4.qmd">here</a>.</p>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>we’ll call it q(x) here to make the application super clear, but often I see the “proposal” distribution called g(x) and the the distribution we want to approximate called p(x). Another common notation would be <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)"> for the target and <img src="https://latex.codecogs.com/png.latex?q(x)"> for the proposal. It’s also helpful to know that the computer graphics (as in, image rendering) community is the source of a lot of work especially around Multiple Importance Sampling since they need to solve lots of light transport integrals, and they have yet another set of conventions from most statisticians, but you can usually figure out their choices by squinting a bit.↩︎</p></li>
<li id="fn2"><p>If you’re looking to learn about importance sampling for the first time, a great place to start is Ben Lambert’s video introductions to the basic idea: <a href="https://www.youtube.com/watch?v=V8f8ueBc9sY">video 1</a>, and <a href="https://www.youtube.com/watch?v=F5PdIQxMA28">video 2</a>. For building more intuition about why we need all these variance reducing modifications to general IS, Dan Simpson has some great <a href="https://dpsimpson.github.io/pages/talks/Importance_sampling_unsw_2019.pdf">slides</a> which have a side benefit of being hilarious. Those slides will mention a lot of the books/papers I find most instructive, but it’s worth calling out especially Vehtari et Al’s Pareto Smoothed Importance Sampling <a href="https://arxiv.org/abs/1507.02646">paper</a> as particularly well written and paradigm shaping. Finally, Elvira et Al’s (2019) Multiple Importance Sampling <a href="https://projecteuclid.org/journals/statistical-science/volume-34/issue-1/Generalized-Multiple-Importance-Sampling/10.1214/18-STS668.full">paper</a> is the most thorough I know, but isn’t particularly approachable. Instead, for MIS I’d recommend starting with the first few minutes of <a href="https://www.youtube.com/watch?v=dxFSwplfdpk">this talk</a> (although the main topic of their talk is less relevant, the visualizations are super helpful), and the first ~8 pages of <a href="https://arxiv.org/pdf/2102.05407.pdf">this paper</a>, also by Elvira et Al. (2021) (I especially like that it spends a bit more time on notation; since multiple importance sampling comes from/comes up in computer graphics, the notational choices sometimes feel a bit annoying to me). Finally, the <a href="https://dl.acm.org/doi/10.1145/218380.218498">original MIS paper itself</a>, Veach &amp; Guibas (1995) is quite readable, but requires a bit of reading around or reading into computer graphics to grok their examples and notational choices.↩︎</p></li>
<li id="fn3"><p>Slight more weeds here- it turns out that this idea to use Vehtari et al.’s rule of thumb for selecting M and getting <img src="https://latex.codecogs.com/png.latex?%5Ctau"> from there is fairly important. The GPD Approximation is pretty sensitive to getting <img src="https://latex.codecogs.com/png.latex?%5Ctau"> right- it’ll be poor if <img src="https://latex.codecogs.com/png.latex?%5Ctau"> is too low. Having a deterministic rule of thumb that preforms better than alternative more complicated schemes for estimating <img src="https://latex.codecogs.com/png.latex?%5Ctau"> is great, and they work through showing it works well in most reasonable cases.↩︎</p></li>
<li id="fn4"><p>I’ll be lazy here and not derive or plot this- you can see the plot in Dan Simpson’s slides mentioned above.↩︎</p></li>
<li id="fn5"><p>One fascinating caveat here is that they proved this only for the case where we know the normalizing constant, not the self-normalized case we pretty much always have to live with, but they have some numerical results and some pretty common sense arguments that the result should extend in most reasonable cases to SNIS as well.↩︎</p></li>
<li id="fn6"><p>Worth noting that one of the authors here is Michael l. Jordan, which is a pretty good heuristic for “this will be a banger of a stats paper”.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2023,
  author = {Andy Timm},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2023-05-27},
  url = {https://andytimm.github.io/variational_mrp_pt4.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2023" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2023. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> May 27, 2023. <a href="https://andytimm.github.io/variational_mrp_pt4.html">https://andytimm.github.io/variational_mrp_pt4.html</a>.
</div></div></section></div> ]]></description>
  <category>MRP</category>
  <category>Variational Inference</category>
  <guid>https://andytimm.github.io/posts/Variational MRP Pt4/variational_mrp_pt4.html</guid>
  <pubDate>Sat, 27 May 2023 04:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Variational MRP Pt4/images/importance-weights-preview.png" medium="image" type="image/png" height="72" width="144"/>
</item>
<item>
  <title>Variational Inference for MRP with Reliable Posterior Distributions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Variational MRP Pt3/variational_mrp_3.html</link>
  <description><![CDATA[ 




<p><strong>Note:</strong> I’ve gotten a lot more pessimistic about how generally useful the alternatives to simple KL-Divergence are on their own since writing this post. I still think these are really useful ideas to think to build intuition about VI, and techniques like CHIVI are useful for some lower dimensional problems or as part of an ensemble of techniques for high dimensional ones. However, <a href="https://arxiv.org/abs/2103.01085">this paper</a> from Dhaka et al.&nbsp;is very convincing that CHIVI and currently available similar algorithms are in practice very hard to optimize for high dimensional, and that some of the intuitive benefits shown about CHIVI below in low dimensions don’t really generalize the way we’d expect to higher dimensions.</p>
<hr>
<p>This is section 3 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.</p>
<p>In the <a href="https://andytimm.github.io/posts/Variational%20MRP%20Pt2/Variational_MRP_pt2.html">last post</a> we threw caution to the wind, and tried out some simple variational inference implementations, to build up some intuition about what bad VI might look like. Just pulling a simple variational inference implementation off the shelf and whacking run perhaps unsurprisingly produced dubious models, so in this post we’ll bring in long overdue theory to understand why VI is so difficult, and what we can do about it.</p>
<p>The general structure for the next couple of posts will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in the next three posts will go a long way towards more robust variational inference. I’ll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.</p>
<p>The rough plan for the series is as follows:</p>
<ol type="1">
<li>Introducing the Problem- Why is VI useful, why VI can produce spherical cows</li>
<li>How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li><strong>(This post)</strong> Problem 1: KL-D prefers exclusive solutions; are there alternatives?</li>
<li>Problem 2: Not all samples are of equal utility; can we weight them cleverly?</li>
<li>Problem 3: How can we know when VI is wrong? Are there useful error bounds?</li>
<li>Better grounded diagnostics and workflow</li>
<li>Seeing if some more sophisticated techniques like normalizing flows add much</li>
</ol>
<section id="inclusive-versus-exclusive-kl-divergence" class="level1">
<h1>Inclusive versus Exclusive KL-divergence</h1>
<p>Like I mentioned in the first post in the series, the Evidence Lower Bound (ELBO), our optimization objective we’re working with is a tractable approximation of the Kullback-Leibler Divergence between our choice of approximating distribution <img src="https://latex.codecogs.com/png.latex?q(z)"> to our true posterior <img src="https://latex.codecogs.com/png.latex?p(z)">.</p>
<p>The KL divergence is asymmetric: in general, <img src="https://latex.codecogs.com/png.latex?KL(p%7C%7Cq)%20%5Cneq%20KL(q%7C%7Cp)">. Previously, we saw that this asymmetry mattered quite a bit for our ELBO idea:</p>
<p><img src="https://latex.codecogs.com/png.latex?argmin_%7Bq(z)%20%5Cin%20%5Cmathscr%7BQ%7D%7D(q(z)%7C%7C%5Cfrac%7Bp(z,x)%7D%7B%5Cbf%20p(x)%7D)%20=%20%5Cmathbb%7BE%7D%5Blogq(z)%5D%20-%20%5Cmathbb%7BE%7D%5Blogp(z,x)%5D%20+%20%7B%5Cbf%20logp(x)%7D"> We can’t calculate the bolded term <img src="https://latex.codecogs.com/png.latex?logp(x)">; if we could we wouldn’t be finding this inference thing so hard in the first place. The way we sidestepped that with the ELBO is to note that the term is constant with respect to <img src="https://latex.codecogs.com/png.latex?q">; so we can go on our merry way minimizing the above without it.</p>
<p>If we flip the divergence around though, we’ve got an issue. That term would then be a <img src="https://latex.codecogs.com/png.latex?logq(x)"> … which we can’t write off in the same way- it varies as we optimize. So if we’re doing this ELBO minimizing version of variational inference, we’re obligated to use this “reverse” KL divergence, the second option below.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AKL(p%7C%7Cq)%20=%20%5Csum_%7Bx%20%5Cin%20X%7D%7Bp(x)%7Dlog%5B%5Cfrac%7Bp(x)%7D%7Bq(x)%7D%5D%20%20%5C%5C%0AKL(q%7C%7Cp)%20=%20%5Csum_%7Bx%20%5Cin%20X%7D%7Bq(x)%7Dlog%5B%5Cfrac%7Bq(x)%7D%7Bp(x)%7D%5D%0A%5Cend%7Balign%7D%0A"></p>
<p>Unfortunately, this choice to optimize the “reverse” KL divergence bakes in preference for a certain type of solution<sup>1</sup>.</p>
<p>I found I built better intuition for this encoded preference after seeing it presented many different overlapping ways, so here are a few of my favorites.</p>
<p>One way to see the difference is through a variety of labels for each direction. One could call Forward KL (1) vs.&nbsp;Reverse KL (2):</p>
<ol type="1">
<li>Inclusive vs.&nbsp;Exclusive (my favorite, and so what I’m using for the section header)</li>
<li>Mean Seeking vs.&nbsp;Mode Seeking</li>
<li>Zero Avoiding vs.&nbsp;Zero Forcing</li>
</ol>
<p>let’s quickly sketch what this might look like in the case of a simple mixture of normals with a single normal as a variational family:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;">library</span>(ggplot2)</span>
<span id="cb1-2"><span class="fu" style="color: #4758AB;">library</span>(gridExtra)</span>
<span id="cb1-3"><span class="fu" style="color: #4758AB;">library</span>(tidyverse)</span>
<span id="cb1-4"></span>
<span id="cb1-5">mixture <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">data.frame</span>(<span class="at" style="color: #657422;">normals =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="fu" style="color: #4758AB;">rnorm</span>(<span class="dv" style="color: #AD0000;">1000</span>,<span class="dv" style="color: #AD0000;">3</span>,<span class="dv" style="color: #AD0000;">1</span>),<span class="fu" style="color: #4758AB;">rnorm</span>(<span class="dv" style="color: #AD0000;">1000</span>,<span class="dv" style="color: #AD0000;">15</span>,<span class="dv" style="color: #AD0000;">2</span>)),</span>
<span id="cb1-6">                      <span class="at" style="color: #657422;">mode_seeking_kl =</span> <span class="fu" style="color: #4758AB;">rnorm</span>(<span class="dv" style="color: #AD0000;">2000</span>,<span class="fl" style="color: #AD0000;">3.5</span>,<span class="dv" style="color: #AD0000;">2</span>),</span>
<span id="cb1-7">                      <span class="at" style="color: #657422;">mean_seeking_kl =</span> <span class="fu" style="color: #4758AB;">rnorm</span>(<span class="dv" style="color: #AD0000;">2000</span>,<span class="dv" style="color: #AD0000;">9</span>,<span class="dv" style="color: #AD0000;">4</span>))</span>
<span id="cb1-8"></span>
<span id="cb1-9">rkl_plot <span class="ot" style="color: #003B4F;">&lt;-</span> mixture <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> normals)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-10">  <span class="fu" style="color: #4758AB;">geom_density</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> normals), <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"red"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-11">  <span class="fu" style="color: #4758AB;">geom_density</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> mode_seeking_kl), <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"green"</span>) <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">ggtitle</span>(<span class="st" style="color: #20794D;">"Exclusive KL"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-12">  <span class="fu" style="color: #4758AB;">xlab</span>(<span class="st" style="color: #20794D;">""</span>)</span>
<span id="cb1-13"></span>
<span id="cb1-14">fkl_plot <span class="ot" style="color: #003B4F;">&lt;-</span> mixture <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> normals)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-15">  <span class="fu" style="color: #4758AB;">geom_density</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> normals), <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"red"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-16">  <span class="fu" style="color: #4758AB;">geom_density</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> mean_seeking_kl), <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"green"</span>) <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">ggtitle</span>(<span class="st" style="color: #20794D;">"Inclusive KL"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-17">  <span class="fu" style="color: #4758AB;">xlab</span>(<span class="st" style="color: #20794D;">""</span>)</span>
<span id="cb1-18"></span>
<span id="cb1-19"><span class="fu" style="color: #4758AB;">grid.arrange</span>(rkl_plot,fkl_plot)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt3/variational_mrp_3_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>To approximate the same exact red distribution <img src="https://latex.codecogs.com/png.latex?p(x)">, Inclusive KL (1) and Exclusive KL (2) would optimize the green <img src="https://latex.codecogs.com/png.latex?q(p)"> in quite different manner.</p>
<p>To spell out the ways to describe this above: Inclusive KL will try to cover all the probability mass in <img src="https://latex.codecogs.com/png.latex?p(x)">, even if it means a peak at a unfortunate middle ground. Exclusive KL, on the other hand, will try to concentrate it’s mass on the largest mode, even if it means missing much of the mixture of normals. Alternatively, we could describe the top graph as mode seeking, and the bottom as mean seeking. Finally, we could say the top graph shows “Zero Forcing” behavior- it will heavily favor putting zero mass on some parts of the graph to avoid any weight where <img src="https://latex.codecogs.com/png.latex?p(x)"> has no mass, even if it means missing an entire mode. Conversely, Inclusive KL will aim to cover all the mass of <img src="https://latex.codecogs.com/png.latex?p(x)"> in full even if the result is an odd solution, in order to avoid having zero mass where <img src="https://latex.codecogs.com/png.latex?p(x)"> has some.</p>
<p>How does this follow from the form of the divergence?</p>
<p>To start with, notice that for inclusive KL we could think of the <img src="https://latex.codecogs.com/png.latex?log(%5Cfrac%7Bp(x)%7D%7Bq(x)%7D)"> part of the term being weighted by <img src="https://latex.codecogs.com/png.latex?p(x)">- if in some range of <img src="https://latex.codecogs.com/png.latex?x"> <img src="https://latex.codecogs.com/png.latex?p(x)"> is 0, we don’t pay a penalty if <img src="https://latex.codecogs.com/png.latex?q(x)"> puts mass. The reverse is not true however- if our <img src="https://latex.codecogs.com/png.latex?q(x)"> is zero where there should be mass in our true distribution, our Inclusive KL divergence is infinite<sup>2</sup>.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AKL(p%7C%7Cq)%20=%20%5Csum_%7Bx%20%5Cin%20X%7D%7Bp(x)%7Dlog%5B%5Cfrac%7Bp(x)%7D%7Bq(x)%7D%5D%20%20%5C%5C%0AKL(q%7C%7Cp)%20=%20%5Csum_%7Bx%20%5Cin%20X%7D%7Bq(x)%7Dlog%5B%5Cfrac%7Bq(x)%7D%7Bp(x)%7D%5D%0A%5Cend%7Balign%7D%0A"></p>
<p>And if we change the direction of the divergence, the opposite zeros and infinities show up, enforcing strong preferences for a specific type of solution.</p>
<p>When the example is a simple mix of two gaussians approximated with a single gaussian, it’s fairly easy to intuit how the choice of KL divergence will influence the optimization solution. This all gets a bit more opaque on harder problems- like we saw with the example last post, ELBO based VI will tend to underestimate the support of <img src="https://latex.codecogs.com/png.latex?p(x)"> but whether the solution is narrow but overall reasonable, or pretty much degenerate, is hard to predict. However, this exploration of how the form of the divergence influences the results still gives a rough intuition for why our ELBO optimized posteriors might collapse.</p>
<p>If we want to try the opposite direction of KL divergence, it isn’t immediately obvious there’s a global objective we can choose that favors overdispersed solutions. Like I mentioned above, if we try to make an ELBO-esque target but reverse the KL divergence, the <img src="https://latex.codecogs.com/png.latex?logp(x)"> which is constant with respect to the <img src="https://latex.codecogs.com/png.latex?q(x)"> we’re optimizing becomes a <img src="https://latex.codecogs.com/png.latex?logq(x)"> which we can’t so easily work around.</p>
<p>Let’s look first at a solution in the spirit of VI<sup>3</sup> to the above problem which requires us to pick up a new divergence, the <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E%7B2%7D">-divergence, and optimizes a new bound. Let’s take a look at it.</p>
<section id="chi2-variational-inference-chivi-and-the-cubo-bound" class="level2">
<h2 class="anchored" data-anchor-id="chi2-variational-inference-chivi-and-the-cubo-bound"><img src="https://latex.codecogs.com/png.latex?%5Cchi%5E%7B2%7D"> Variational Inference (CHIVI) and the CUBO bound</h2>
<p>The <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E%7B2%7D">-divergence has form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AD_%7B%5Cchi%5E2%7D(p%7C%7Cq)%20=%20%5Cmathbb%7BE%7D_%7Bq(z;%5Clambda)%7D%5B(%5Cfrac%7Bp(z%7Cx)%7D%7Bq(z;%5Clambda)%7D)%5E2%20-1%5D%0A"> For simplicity and comparability, I’m switching here to using <a href="https://arxiv.org/abs/1611.00328">Dieng et Al. (2017)</a>’s notation here- they use <img src="https://latex.codecogs.com/png.latex?q(z;%5Clambda)"> to refer to the variational family we’re using, indexed by parameters <img src="https://latex.codecogs.com/png.latex?%5Clambda">.</p>
<p>This divergence has the properties we wanted when we tried to use Inclusive KL Divergence- it tends to be mean seeking instead of mode seeking.</p>
<p>Like with the ELBO, we need to show that we have a bound here independent of <img src="https://latex.codecogs.com/png.latex?logp(x)">, and that we have a way to estimate that bound efficiently.</p>
<p>Let’s first move around a few pieces of the first term above:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cmathbb%7BE%7D_%7Bq(z;%5Clambda)%7D%5B(%5Cfrac%7Bp(z%7Cx)%7D%7Bq(z;%5Clambda)%7D)%5E2&amp;%20=%201%20+%20D_%7B%5Cchi%5E2%7D(p(z%7Cx)%7Cq(z;%5Clambda))%20%5C%5C%0A&amp;=%20p(x)%5E2%5B1%20+%20D_%7B%5Cchi%5E2%7D(p(z%7Cx)%7Cq(z;%5Clambda))%5D%0A%5Cend%7Balign%7D%0A"> Then we can take the log of both sides of the equation, which gives us:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7B2%7Dlog(1%20+%20D_%7B%5Cchi%5E2%7D(p(z%7Cx)%7Cq(z;%5Clambda)))%20=%20-logp(x)%20+%20%5Cfrac%7B1%7D%7B2%7Dlog%5Cmathbb%7BE%7D_%7Bq(z;%5Clambda)%7D%5B(%5Cfrac%7Bp(z%7Cx)%7D%7Bq(z;%5Clambda)%7D)%5E2%5D%20%20%0A"> …and this is starting to feel a lot like the ELBO derivation. Log is monotonic, and the <img src="https://latex.codecogs.com/png.latex?-logp(x)"> term is constant as we optimize <img src="https://latex.codecogs.com/png.latex?q">, so we’ve found something that we’re close to able to minimize:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ACUBO_%7B2%7D(%5Clambda)%20=%20%5Cfrac%7B1%7D%7B2%7Dlog%5Cmathbb%7BE%7D_%7Bq(z;%5Clambda)%7D%5B(%5Cfrac%7Bp(z%7Cx)%7D%7Bq(z;%5Clambda)%7D)%5E2%5D%0A"> Since this new divergence is non-negative as well, this is a upper bound of the model evidence. This is thus named <img src="https://latex.codecogs.com/png.latex?%5Cchi"> upper bound (CUBO)<sup>4</sup>.</p>
</section>
<section id="but-can-we-estimate-it" class="level2">
<h2 class="anchored" data-anchor-id="but-can-we-estimate-it">… But can we estimate it?</h2>
<p>One other issue here: how do we estimate this? The CUBO objective got rid of the <img src="https://latex.codecogs.com/png.latex?logp(x)"> we were worried about, but it seems like that expectation is going to be difficult to estimate in general.</p>
<p>Your first idea might be to Monte Carlo (not MCMC) estimate it roughly like this:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ACUBO_2(%5Clambda)%20=%20%5Cfrac%7B1%7D%7B2%7Dlog%5Cfrac%7B1%7D%7BS%7D%5Csum_%7Bs=1%7D%5E%7BS%7D%5B(%5Cfrac%7Bp(x,z%5E%7Bs%7D)%7D%7Bq(z%5E%7Bs%7D;%5Clambda)%7D)%5E2%5D%0A"> Unfortunately, the <img src="https://latex.codecogs.com/png.latex?log"> transform here means our Monte Carlo estimator will be biased: we can see this by applying Jensen’s inequality to the above. To make this stably act as an upper bound, we can apply a clever transformation:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbf%7BL%7D%20=%20exp(n*%20CUBO_2(%5Clambda))%0A"></p>
<p>Since exp is monotonic, this has the same objective as the CUBO, but we can Monte Carlo estimate it unbiasedly. Is that the last problem to solve?</p>
</section>
<section id="but-can-we-calculate-gradients-efficiently" class="level2">
<h2 class="anchored" data-anchor-id="but-can-we-calculate-gradients-efficiently">… But can we calculate gradients efficiently?</h2>
<p>Wait, wait no. Sorry to keep saying there’s one more step here, but there’s a lot that goes into making a full, convenient, general use algorithm here. The last step (for real this time) is that we need to figure out how to get gradients for the estimate of <img src="https://latex.codecogs.com/png.latex?%5Cbf%7BL%7D"> above, <img src="https://latex.codecogs.com/png.latex?%5Cbf%7B%5Chat%7BL%7D%7D">. The issue is that we don’t have any guarantee that a unbiased Monte Carlo estimator of <img src="https://latex.codecogs.com/png.latex?%5Cbf%7B%5Chat%7BL%7D%7D"> gets us a Monte Carlo way to estimate <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%5Clambda%5Cbf%7B%5Chat%7BL%7D%7D">- we can’t guarantee that the gradient of the expectation is equal to the expectation of the gradient.</p>
<p>For this, we need to pull out a trick from the variational autoencoder literature. This is usually referred to as the “Reparameterization Trick”<sup>5</sup>, but the original CHIVI paper refers to them as “reparmeterization gradients”. We will assume we can rewrite the generative process of our model as <img src="https://latex.codecogs.com/png.latex?z%20=%20g(%5Clambda,%5Cepsilon)">, where <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%20%5Csim%20p(%5Cepsilon)"> and g being a deterministic function. Then we have a new estimator for both <img src="https://latex.codecogs.com/png.latex?%5Cbf%7B%5Chat%7BL%7D%7D"> and it’s gradient:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cbf%7B%5Chat%7BL%7D%7D%20&amp;=%20%5Cfrac%7B1%7D%7BB%7D%5Csum_%7Bb=1%7D%5EB(%5Cfrac%7Bp(x,g(%5Clambda,%5Cepsilon%5E%7B(b)%7D))%7D%7Bq(g(%5Clambda,%5Cepsilon%5E%7B(b)%7D;%5Clambda))%7D)%5E%7B2%7D%20%5C%5C%0A%5Cnabla_%5Clambda%5Cbf%7B%5Chat%7BL%7D%7D%20%20&amp;=%20%5Cfrac%7B2%7D%7BB%7D%5Csum_%7Bb=1%7D%5EB(%5Cfrac%7Bp(x,g(%5Clambda,%5Cepsilon%5E%7B(b)%7D))%7D%7Bq(g(%5Clambda,%5Cepsilon%5E%7B(b)%7D;%5Clambda))%7D)%5E2%20%5Cnabla_%5Clambda%20log(%5Cfrac%7Bp(x,g(%5Clambda,%5Cepsilon%5E%7B(b)%7D))%7D%7Bq(g(%5Clambda,%5Cepsilon%5E%7B(b)%7D;%5Clambda))%7D)%0A%5Cend%7Balign%7D%0A"> There are one or two more neat computational tricks in the paper I won’t explain here (essentially: how do we extend this to work in minibatch fashion, and how do we avoid numerical underflow issues), but this is now essentially functional. The whole algorithm, which they dubbed CHIVI is below:</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt3/images/CHIVI_algorithm.png" class="img-fluid"></p>
</section>
</section>
<section id="the-bigger-picture-again" class="level1">
<h1>The Bigger Picture Again</h1>
<p>Stepping back, let’s talk about some practical properties of the algorithm we’ve been stepping through.</p>
<p>First, and probably most exciting given what we saw in the last post, CHIVI’s objective has the property that it is inclusive, unlike the ELBO we were using earlier. This won’t be the right choice for all variational inference problems, but given our prior issues with very narrow posteriors this will be exciting to test<sup>6</sup>. And as we’ll see in the next section, this overdispersion tendency in the posterior will often have a beneficial interaction with importance sampling which can improve our estimates further.</p>
<p>Another nice thing here is that if we were to estimate both the ELBO and CUBO for a given problem, we’d get both a upper and lower bound on the model evidence. This is theoretically convenient in that we now have a sandwich estimator, which actually obtains reasonably tight bounds. We’ll even be able use the fact we have both later to get some bounds on practical bounds on quantities we tend to report in practice like means and covariances!</p>
<p>A final neat benefit here is that to the extent we are willing to consider ensembling models (again, more on that soon), this CHIVI framework will produce estimates that succeed (and fail) in less similar ways that the the ELBO based estimators we looked at last post. Expanding our available set of tools is always good, but it’s even better when we’re ensembling because we can lean more heavily on each model for the tasks it succeeds on.</p>
<p>One potential downside here is that we introduced a solution that partially relies on a Monte Carlo estimator. That said, this is pretty cheap in practice; if we’re using VI as a drop in for MCMC, this is still going to be much much faster than MCMC for any big problem. We’ll need to think about a reasonable number of samples in a given case, but realistically this isn’t going to be a driving factor in determining compute time.</p>
<p>Another final problem is that the estimator we built out for the CUBO that we could actually estimate tends to end up having pretty high variance. Exponentiating the objective isn’t free in that sense; but this problem of variance reduction in estimators is something that feels like a tractable problem to iterate on.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>In truth, both KL divergences encode structural preferences for the type of optimization solution they admit- neither will be the right choice for every problem and variational family combination. But as we’ll see, being able to choose will give us more options to fit models we believe.↩︎</p></li>
<li id="fn2"><p>This is the footnote for those of you that are annoyed because you tried to write out how this would happen, and got something like <img src="https://latex.codecogs.com/png.latex?p(x)%20log%20%5Cfrac%7Bp(x)%7D%7B0%7D">, which should be undefined if we’re following normal math rules. But this is information theory, and in this strange land we say <img src="https://latex.codecogs.com/png.latex?p(x)%20log%20%5Cfrac%7Bp(x)%7D%7B0%7D%20=%20%5Cinfty">. I don’t have a strong intuition for why this is the best solution, but a information encoding perspective makes it make more sense at least: if we know the distribution of <img src="https://latex.codecogs.com/png.latex?p">, we can construct a code for it with average description length <img src="https://latex.codecogs.com/png.latex?H(x)">. One way to understand the KL divergence is as what happens when we try to use the code for a distribution <img src="https://latex.codecogs.com/png.latex?q"> to describe <img src="https://latex.codecogs.com/png.latex?p">, we’d need <img src="https://latex.codecogs.com/png.latex?H(p)%20+%20KL(p%7C%7Cq)"> bits on average to describe <img src="https://latex.codecogs.com/png.latex?p">. In the code for <img src="https://latex.codecogs.com/png.latex?q"> has no way to represent some element of <img src="https://latex.codecogs.com/png.latex?p">, then requiring… infinite bits feels like the right way to describe the breakdown of meaning? All this to say this condition is something our optimizer will try hard to avoid.↩︎</p></li>
<li id="fn3"><p>I’ll mention an alternative approach, Expectation Propagation, that takes a different (not global objective based) approach further down.↩︎</p></li>
<li id="fn4"><p>This approach actually defines a family of <img src="https://latex.codecogs.com/png.latex?n"> new divergences, where you replace the <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D"> with <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7Bn%7D"> and similarly replace the square an exponent with n.&nbsp;Fully stepping through why this is neat wasn’t worth how far afield it’d take us, but the original <img src="https://latex.codecogs.com/png.latex?CHIVI"> paper has some cool derivations based on this, one of which I’ll discuss on the next section on importance sampling.↩︎</p></li>
<li id="fn5"><p>I thought about a section to explain the reparameterization trick, but there are enough good explanations online of the trick. If you’re interested in learning more about why this is important for optimization through stocastic models, I’d recommend starting with Gregory Gundersen’s explanation <a href="https://gregorygundersen.com/blog/2018/04/29/reparameterization/">here</a> and then move on to the original Kingma &amp; Welling, 2013 paper. As general advice on understanding it better though, I’ll echo Greg’s point that some of the online explanations I’ve seen are a bit loose- the key is that we want to express a gradient of an expectation (can’t MC estimate for sure) as an expectation of a gradient (which we can MC estimate provided our convenient deterministic function <img src="https://latex.codecogs.com/png.latex?g"> is differentiable).↩︎</p></li>
<li id="fn6"><p>In a few posts, we’re theoryposting for a bit.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2023,
  author = {Andy Timm},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2023-05-02},
  url = {https://andytimm.github.io/variational_mrp_3.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2023" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2023. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> May 2, 2023. <a href="https://andytimm.github.io/variational_mrp_3.html">https://andytimm.github.io/variational_mrp_3.html</a>.
</div></div></section></div> ]]></description>
  <category>MRP</category>
  <category>Variational Inference</category>
  <guid>https://andytimm.github.io/posts/Variational MRP Pt3/variational_mrp_3.html</guid>
  <pubDate>Tue, 02 May 2023 04:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Variational MRP Pt3/images/CHIVI_algorithm.png" medium="image" type="image/png" height="70" width="144"/>
</item>
<item>
  <title>Variational Inference for MRP with Reliable Posterior Distributions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Variational MRP Pt2/Variational_MRP_pt2.html</link>
  <description><![CDATA[ 




<p>This is the second post in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.</p>
<p>In the last post, I laid out why such reformulating the Bayesian inference problem as optimization might be desirable, but previewed why this might be quite hard to find high quality approximations amenable to optimization. I then introduced our running example (predicting national/sub-national opinion on an abortion question from the CCES using MRP), and gave an initial introduction to a version of Variational Inference where we maximize the Evidence Lower Bound (ELBO) as an objective, and do so using a mean-field Gaussian approximation. We saw that with 60k examples, this took about 8 hours to fit with MCMC, but 144 seconds (!) with VI.</p>
<p>In this post, we’ll explore the shortcomings of this initial approximation, and take a first pass at trying to better with a more complex (full rank) variational approximation. The goal is to get a better feel for what failing models could look like, at least in this relatively simple case.</p>
<p>The rough plan for the series is as follows:</p>
<ol type="1">
<li><a href="https://andytimm.github.io/posts/Variational%20MRP%20Pt1/variational_mrp_pt1.html">Introducing the Problem- Why is VI useful, why VI can produce spherical cows</a></li>
<li><strong>(This post)</strong> How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li>Some theory on why posterior approximation with VI can be so poor</li>
<li>Seeing if some more sophisticated techniques like normalizing flows help</li>
</ol>
<div class="cell">

</div>
<section id="the-disclaimer" class="level1">
<h1>The disclaimer</h1>
<p>One sort of obvious objections to how I’ve set up this series is “Why not talk about theory on why VI approximations can be poor before trying stuff?”. While in practice I did read a lot of the papers for the next post before writing this one, I think there’s a lot of value is looking at failed solutions to a problem to build up intuition about what our failure mode looks like, and what it might require to get it right.</p>
</section>
<section id="toplines" class="level1">
<h1>Toplines</h1>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">meanfield_60k <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">readRDS</span>(<span class="st" style="color: #20794D;">"fit_60k_meanfield.rds"</span>)</span>
<span id="cb1-2">mcmc_60k <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">readRDS</span>(<span class="st" style="color: #20794D;">"fit_60k_mcmc.rds"</span>)</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="co" style="color: #5E5E5E;"># Meanfield </span></span>
<span id="cb1-5">epred_mat_mf <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">posterior_epred</span>(meanfield_60k, <span class="at" style="color: #657422;">newdata =</span> poststrat_df_60k, <span class="at" style="color: #657422;">draws =</span> <span class="dv" style="color: #AD0000;">1000</span>)</span>
<span id="cb1-6">mrp_estimates_vector_mf <span class="ot" style="color: #003B4F;">&lt;-</span> epred_mat_mf <span class="sc" style="color: #5E5E5E;">%*%</span> poststrat_df_60k<span class="sc" style="color: #5E5E5E;">$</span>n <span class="sc" style="color: #5E5E5E;">/</span> </span>
<span id="cb1-7">                                              <span class="fu" style="color: #4758AB;">sum</span>(poststrat_df_60k<span class="sc" style="color: #5E5E5E;">$</span>n)</span>
<span id="cb1-8">mrp_estimate_mf <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="at" style="color: #657422;">mean =</span> <span class="fu" style="color: #4758AB;">mean</span>(mrp_estimates_vector_mf),</span>
<span id="cb1-9">                     <span class="at" style="color: #657422;">sd =</span> <span class="fu" style="color: #4758AB;">sd</span>(mrp_estimates_vector_mf))</span>
<span id="cb1-10"></span>
<span id="cb1-11"></span>
<span id="cb1-12"><span class="co" style="color: #5E5E5E;"># MCMC </span></span>
<span id="cb1-13">epred_mat_mcmc <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">posterior_epred</span>(mcmc_60k, <span class="at" style="color: #657422;">newdata =</span> poststrat_df_60k, <span class="at" style="color: #657422;">draws =</span> <span class="dv" style="color: #AD0000;">1000</span>)</span>
<span id="cb1-14">mrp_estimates_vector_mcmc <span class="ot" style="color: #003B4F;">&lt;-</span> epred_mat_mcmc <span class="sc" style="color: #5E5E5E;">%*%</span> poststrat_df_60k<span class="sc" style="color: #5E5E5E;">$</span>n <span class="sc" style="color: #5E5E5E;">/</span></span>
<span id="cb1-15">                                                  <span class="fu" style="color: #4758AB;">sum</span>(poststrat_df_60k<span class="sc" style="color: #5E5E5E;">$</span>n)</span>
<span id="cb1-16">mrp_estimate_mcmc <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="at" style="color: #657422;">mean =</span> <span class="fu" style="color: #4758AB;">mean</span>(mrp_estimates_vector_mcmc),</span>
<span id="cb1-17">                       <span class="at" style="color: #657422;">sd =</span> <span class="fu" style="color: #4758AB;">sd</span>(mrp_estimates_vector_mcmc))</span>
<span id="cb1-18"></span>
<span id="cb1-19"><span class="fu" style="color: #4758AB;">cat</span>(<span class="st" style="color: #20794D;">"Meanfield MRP estimate mean, sd: "</span>, <span class="fu" style="color: #4758AB;">round</span>(mrp_estimate_mf, <span class="dv" style="color: #AD0000;">3</span>))</span>
<span id="cb1-20"><span class="fu" style="color: #4758AB;">cat</span>(<span class="st" style="color: #20794D;">"MCMC MRP estimate mean, sd: "</span>, <span class="fu" style="color: #4758AB;">round</span>(mrp_estimate_mcmc, <span class="dv" style="color: #AD0000;">3</span>))</span></code></pre></div>
</div>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>Mean</th>
<th>SD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MCMC</td>
<td>43.9%</td>
<td>.2%</td>
</tr>
<tr class="even">
<td>mean-field VI</td>
<td>43.7%</td>
<td>.2%</td>
</tr>
</tbody>
</table>
<p>Starting with basics, the toplines are pretty much identical, which is a good start. The minor difference here could easily reverse on a different seed- from a few quick re-runs these often end up having matching means to 3 decimals.</p>
</section>
<section id="state-level-estimates" class="level1">
<h1>State Level Estimates</h1>
<p>What happens if we produce state level estimates, similar to the plot last post comparing MRP to a simple weighted estimate? Note that I’ll steer away from the MRP Case Study example here in a few ways. I’ll use <code>tidybayes</code> for working with the draws (more elegant than their loop based approach), and I’ll use more draws (helps with simulation error in smaller states).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">mcmc_state_level <span class="ot" style="color: #003B4F;">&lt;-</span> poststrat_df_60k <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">add_epred_draws</span>(mcmc_60k, <span class="at" style="color: #657422;">ndraws =</span> <span class="dv" style="color: #AD0000;">1000</span>)</span>
<span id="cb2-2">mfvi_state_level <span class="ot" style="color: #003B4F;">&lt;-</span> poststrat_df_60k <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">add_epred_draws</span>(meanfield_60k, <span class="at" style="color: #657422;">ndraws =</span> <span class="dv" style="color: #AD0000;">1000</span>)</span>
<span id="cb2-3"></span>
<span id="cb2-4">mcmc_state_level <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">glimpse</span>()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 12,000,000
Columns: 13
Groups: state, eth, male, age, educ, n, repvote, region, .row [12,000]
$ state      &lt;chr&gt; "AL", "AL", "AL", "AL", "AL", "AL", "AL", "AL", "AL", "AL",…
$ eth        &lt;chr&gt; "White", "White", "White", "White", "White", "White", "Whit…
$ male       &lt;dbl&gt; -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,…
$ age        &lt;chr&gt; "18-29", "18-29", "18-29", "18-29", "18-29", "18-29", "18-2…
$ educ       &lt;chr&gt; "No HS", "No HS", "No HS", "No HS", "No HS", "No HS", "No H…
$ n          &lt;dbl&gt; 23948, 23948, 23948, 23948, 23948, 23948, 23948, 23948, 239…
$ repvote    &lt;dbl&gt; 0.6437414, 0.6437414, 0.6437414, 0.6437414, 0.6437414, 0.64…
$ region     &lt;chr&gt; "South", "South", "South", "South", "South", "South", "Sout…
$ .row       &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
$ .chain     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
$ .iteration &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
$ .draw      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …
$ .epred     &lt;dbl&gt; 0.5771322, 0.5189677, 0.5483006, 0.5421404, 0.5417602, 0.55…</code></pre>
</div>
</div>
<p>If you haven’t worked with <code>tidybayes</code> before, the glimpse above should help give some intuition about the new shape of the data- we’ve take the 12,000 row <code>poststrat_df_60k</code>, and added a row per observation per draw, with the prediction (.epred) and related metadata. This gives 12,000 x 1,000 = 12 million rows. This really isn’t the most space efficient storage, but it allows for very elegant <code>dplyr</code> style manipulation of results and quick exploration.</p>
<p>Let’s now plot and compare the 50 and 95% credible intervals by state between the two models.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">mcmc_state_summary <span class="ot" style="color: #003B4F;">&lt;-</span> mcmc_state_level <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb4-2">                        <span class="co" style="color: #5E5E5E;"># multiply each draw by it's cell's proportion of state N</span></span>
<span id="cb4-3">                        <span class="co" style="color: #5E5E5E;"># this is the P in MRP</span></span>
<span id="cb4-4">                        <span class="fu" style="color: #4758AB;">group_by</span>(state,.draw) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb4-5">                        <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">postrat_draw =</span> <span class="fu" style="color: #4758AB;">sum</span>(.epred<span class="sc" style="color: #5E5E5E;">*</span>(n<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sum</span>(n)))) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb4-6">                        <span class="fu" style="color: #4758AB;">group_by</span>(state) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb4-7">                        <span class="fu" style="color: #4758AB;">median_qi</span>(postrat_draw, <span class="at" style="color: #657422;">.width =</span> <span class="fu" style="color: #4758AB;">c</span>(.<span class="dv" style="color: #AD0000;">5</span>,.<span class="dv" style="color: #AD0000;">95</span>)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb4-8">                        <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">model =</span> <span class="st" style="color: #20794D;">"MCMC"</span>)</span>
<span id="cb4-9"></span>
<span id="cb4-10">mfvi_state_summary <span class="ot" style="color: #003B4F;">&lt;-</span> mfvi_state_level <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb4-11">                        <span class="fu" style="color: #4758AB;">group_by</span>(state,.draw) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb4-12">                        <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">postrat_draw =</span> <span class="fu" style="color: #4758AB;">sum</span>(.epred<span class="sc" style="color: #5E5E5E;">*</span>(n<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sum</span>(n)))) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb4-13">                        <span class="fu" style="color: #4758AB;">group_by</span>(state) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb4-14">                        <span class="fu" style="color: #4758AB;">median_qi</span>(postrat_draw, <span class="at" style="color: #657422;">.width =</span> <span class="fu" style="color: #4758AB;">c</span>(.<span class="dv" style="color: #AD0000;">5</span>,.<span class="dv" style="color: #AD0000;">95</span>)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb4-15">                        <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">model =</span> <span class="st" style="color: #20794D;">"MF-VI"</span>)</span>
<span id="cb4-16"></span>
<span id="cb4-17">combined_summary <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">bind_rows</span>(mcmc_state_summary,mfvi_state_summary)</span>
<span id="cb4-18"></span>
<span id="cb4-19">combined_summary <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb4-20">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">ordered_state =</span> <span class="fu" style="color: #4758AB;">fct_reorder</span>(combined_summary<span class="sc" style="color: #5E5E5E;">$</span>state,</span>
<span id="cb4-21">                                     combined_summary<span class="sc" style="color: #5E5E5E;">$</span>postrat_draw)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb4-22">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">y =</span> ordered_state,</span>
<span id="cb4-23">             <span class="at" style="color: #657422;">x =</span> postrat_draw,</span>
<span id="cb4-24">             <span class="at" style="color: #657422;">xmin =</span> .lower,</span>
<span id="cb4-25">             <span class="at" style="color: #657422;">xmax =</span> .upper,</span>
<span id="cb4-26">             <span class="at" style="color: #657422;">color =</span> model)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-27">  <span class="fu" style="color: #4758AB;">geom_pointinterval</span>(<span class="at" style="color: #657422;">position =</span> <span class="fu" style="color: #4758AB;">position_dodge</span>(<span class="dv" style="color: #AD0000;">1</span>)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-28">  <span class="fu" style="color: #4758AB;">xlim</span>(.<span class="dv" style="color: #AD0000;">25</span>,.<span class="dv" style="color: #AD0000;">75</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-29">  <span class="fu" style="color: #4758AB;">theme</span>(<span class="at" style="color: #657422;">legend.position=</span><span class="st" style="color: #20794D;">"top"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-30">  <span class="fu" style="color: #4758AB;">xlab</span>(<span class="st" style="color: #20794D;">"Should employers be allowed to deny their employees abortion care?"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-31">  <span class="fu" style="color: #4758AB;">ylab</span>(<span class="st" style="color: #20794D;">"State"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt2/Variational_MRP_pt2_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>… That looks concerning.</p>
<p>What might you get wrong if you used the VI approximation for inference here? If you only cared about the median estimate primarily, you might be ok with this effort. If you care about uncertainty though, here’s a non-exhaustive list of concerns here:</p>
<ol type="1">
<li>Probably unimodal, smooth posterior distributions from MCMC have gone off-course to the point where the Median/50/95% presentation no longer seems up to expressing the posterior shape (more on this in a second).</li>
<li>The MF-VI posteriors are often narrower in 50% or 95% CI- we’d on average underestimate various types of uncertainty here.</li>
<li>Worse<sup>1</sup>, the MF-VI posterior’s CIs aren’t <strong>consistently</strong> narrower, either in the sense they are always narrower, or that they tend to consistently distort the same way. Sometimes both the 50% and 95% are just a small amount narrower than MCMC- the Michigan posterior attempt looks passable. Sometimes things are worse, with 50% MFVI CIs almost as wide as the MCMC 95% interval- Wyoming shows such a distortion. Sometimes the probability mass between 50% and 95% is confined to such a minuscule range it looks like I forgot to plot it.</li>
</ol>
<p>That last point is particularly important because it suggests there’s no easy rule of thumb for mechanically correcting these intervals, or deciding which could be plausible approximations without the MCMC plot alongside to guide that process. We can’t use VI to save a ton of time, infer the intervals consistently need to x% be wider, and call it a day- we need to reckon more precisely with why they’re distorted.</p>
<p>Let’s return now to the point about how the shape has gone wrong. Below is a dot plot (<a href="https://dl.acm.org/doi/10.1145/2858036.2858558">Kay et al., 2016</a>)- each point here represents about 1% of the probability mass. I enjoy this approach to posterior visualization when things are getting weird, as this clarifies a lot about the full shape of the posterior distribution, making fewer smoothing assumptions like a density or eye plot might.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">mcmc_state_points <span class="ot" style="color: #003B4F;">&lt;-</span> mcmc_state_level <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb5-2">                        <span class="co" style="color: #5E5E5E;"># multiply each draw by it's cell's proportion of state N</span></span>
<span id="cb5-3">                        <span class="co" style="color: #5E5E5E;"># this is the P in MRP</span></span>
<span id="cb5-4">                        <span class="fu" style="color: #4758AB;">group_by</span>(state,.draw) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb5-5">                        <span class="fu" style="color: #4758AB;">summarize</span>(<span class="at" style="color: #657422;">postrat_draw =</span> <span class="fu" style="color: #4758AB;">sum</span>(.epred<span class="sc" style="color: #5E5E5E;">*</span>(n<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sum</span>(n)))) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb5-6">                        <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">model =</span> <span class="st" style="color: #20794D;">"MCMC"</span>)</span>
<span id="cb5-7">mfvi_state_points <span class="ot" style="color: #003B4F;">&lt;-</span> mfvi_state_level <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb5-8">                        <span class="fu" style="color: #4758AB;">group_by</span>(state,.draw) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb5-9">                        <span class="fu" style="color: #4758AB;">summarize</span>(<span class="at" style="color: #657422;">postrat_draw =</span> <span class="fu" style="color: #4758AB;">sum</span>(.epred<span class="sc" style="color: #5E5E5E;">*</span>(n<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sum</span>(n)))) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb5-10">                        <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">model =</span> <span class="st" style="color: #20794D;">"MF-VI"</span>)</span>
<span id="cb5-11"></span>
<span id="cb5-12">combined_points <span class="ot" style="color: #003B4F;">&lt;-</span> mcmc_state_points <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb5-13">                      <span class="fu" style="color: #4758AB;">bind_rows</span>(mfvi_state_points) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb5-14">                      <span class="fu" style="color: #4758AB;">ungroup</span>()</span>
<span id="cb5-15"></span>
<span id="cb5-16">combined_points <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb5-17">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">ordered_state =</span> <span class="fu" style="color: #4758AB;">fct_reorder</span>(combined_points<span class="sc" style="color: #5E5E5E;">$</span>state,</span>
<span id="cb5-18">                                     combined_points<span class="sc" style="color: #5E5E5E;">$</span>postrat_draw)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb5-19">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">y =</span> ordered_state,</span>
<span id="cb5-20">             <span class="at" style="color: #657422;">x =</span> postrat_draw,</span>
<span id="cb5-21">             <span class="at" style="color: #657422;">color =</span> model)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb5-22">     <span class="fu" style="color: #4758AB;">stat_dots</span>(<span class="at" style="color: #657422;">quantiles =</span> <span class="dv" style="color: #AD0000;">100</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb5-23">     <span class="fu" style="color: #4758AB;">facet_wrap</span>(<span class="sc" style="color: #5E5E5E;">~</span>model) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb5-24">     <span class="fu" style="color: #4758AB;">theme</span>(<span class="at" style="color: #657422;">legend.position=</span><span class="st" style="color: #20794D;">"none"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb5-25">  <span class="fu" style="color: #4758AB;">xlab</span>(<span class="st" style="color: #20794D;">"Should employers be allowed to deny their employees abortion care?"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb5-26">  <span class="fu" style="color: #4758AB;">ylab</span>(<span class="st" style="color: #20794D;">"State"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt2/Variational_MRP_pt2_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>Eek. The closer to the individual draws we get, the less these two models seem to be producing comparable estimates. This isn’t me expressing an aesthetic preference for smooth, unimodal distributions- the MFVI plots in this view imply beliefs like “support for this policy in Wyoming is overwhelmingly likely to fall in 1 of 3 narrow ranges, all other values are unlikely”<sup>2</sup>. Other similar humorous claims are easy to find.</p>
<p>Stepping back for a second, if our use-case for this model takes pretty much any form of interest in quantifying uncertainty accurately, this is not an acceptable approximation. I could poke more holes, but I can more profitably do that after I’ve explored some theory of why VI models struggle, and brought in some more sophisticated diagnostic tools than looking with our eyeballs<sup>3</sup>; so let’s hold off on that.</p>
</section>
<section id="do-more-basic-fixes-solve-anything" class="level1">
<h1>Do more basic fixes solve anything?</h1>
<p>So I’ve been billing this simple mean-field model as a first pass- I fit it on more or less default <code>rstanarm</code> parameters. I think it’s worth taking a moment to show that getting this approximation problem right isn’t going to be solved with low hanging fruit ideas, since that will motivate our need for better diagnostics and more expressive approximations.</p>
<section id="lowering-the-tolerance" class="level2">
<h2 class="anchored" data-anchor-id="lowering-the-tolerance">Lowering the tolerance</h2>
<p>So we managed to structure our Bayesian inference problem as an optimization problem. Can’t we just optimize better? Maybe with more training the result will be less bad?</p>
<p>the <code>tol_rel_obj</code> parameter control’s the convergence tolerance on the relative norm of the objective. In other words, it controls what (change in the) Evidence Lower Bound value we consider accurate enough to stop at. The default is 0.01, which feels a bit opaque, but let’s try setting it way down to 1e-8 (1Mx lower). Then we can plot it alongside the MCMC estimates and original MF-VI attempt.</p>
<div class="cell" data-warnings="false">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><span class="fu" style="color: #4758AB;">tic</span>()</span>
<span id="cb6-2">fit_60k_1e8 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">stan_glmer</span>(abortion <span class="sc" style="color: #5E5E5E;">~</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> state) <span class="sc" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> eth) <span class="sc" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> educ) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-3">                                      male <span class="sc" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> male<span class="sc" style="color: #5E5E5E;">:</span>eth) <span class="sc" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> educ<span class="sc" style="color: #5E5E5E;">:</span>age) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-4">                                      (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> educ<span class="sc" style="color: #5E5E5E;">:</span>eth) <span class="sc" style="color: #5E5E5E;">+</span> repvote <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">factor</span>(region),</span>
<span id="cb6-5">  <span class="at" style="color: #657422;">family =</span> <span class="fu" style="color: #4758AB;">binomial</span>(<span class="at" style="color: #657422;">link =</span> <span class="st" style="color: #20794D;">"logit"</span>),</span>
<span id="cb6-6">  <span class="at" style="color: #657422;">data =</span> cces_all_df,</span>
<span id="cb6-7">  <span class="at" style="color: #657422;">prior =</span> <span class="fu" style="color: #4758AB;">normal</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">autoscale =</span> <span class="cn" style="color: #8f5902;">TRUE</span>),</span>
<span id="cb6-8">  <span class="at" style="color: #657422;">prior_covariance =</span> <span class="fu" style="color: #4758AB;">decov</span>(<span class="at" style="color: #657422;">scale =</span> <span class="fl" style="color: #AD0000;">0.50</span>),</span>
<span id="cb6-9">  <span class="at" style="color: #657422;">adapt_delta =</span> <span class="fl" style="color: #AD0000;">0.99</span>,</span>
<span id="cb6-10">  <span class="co" style="color: #5E5E5E;"># Printing the ELBO every 1k draws</span></span>
<span id="cb6-11">  <span class="at" style="color: #657422;">refresh =</span> <span class="dv" style="color: #AD0000;">1000</span>,</span>
<span id="cb6-12">  <span class="at" style="color: #657422;">tol_rel_obj =</span> <span class="fl" style="color: #AD0000;">1e-8</span>,</span>
<span id="cb6-13">  <span class="at" style="color: #657422;">algorithm =</span> <span class="st" style="color: #20794D;">"meanfield"</span>,</span>
<span id="cb6-14">  <span class="at" style="color: #657422;">seed =</span> <span class="dv" style="color: #AD0000;">605</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Chain 1: ------------------------------------------------------------
Chain 1: EXPERIMENTAL ALGORITHM:
Chain 1:   This procedure has not been thoroughly tested and may be unstable
Chain 1:   or buggy. The interface is subject to change.
Chain 1: ------------------------------------------------------------
Chain 1: 
Chain 1: 
Chain 1: 
Chain 1: Gradient evaluation took 0.032 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 320 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Begin eta adaptation.
Chain 1: Iteration:   1 / 250 [  0%]  (Adaptation)
Chain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)
Chain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)
Chain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)
Chain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)
Chain 1: Success! Found best value [eta = 1] earlier than expected.
Chain 1: 
Chain 1: Begin stochastic gradient ascent.
Chain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes 
Chain 1:    100       -40291.889             1.000            1.000
Chain 1:    200       -39947.669             0.504            1.000
Chain 1:    300       -39802.182             0.337            0.009
Chain 1:    400       -39776.283             0.253            0.009
Chain 1:    500       -39733.863             0.203            0.004
Chain 1:    600       -39733.198             0.169            0.004
Chain 1:    700       -39728.255             0.145            0.001
Chain 1:    800       -39784.557             0.127            0.001
Chain 1:    900       -39724.366             0.113            0.001
Chain 1:   1000       -39732.042             0.102            0.001
Chain 1:   1100       -39731.525             0.002            0.001
Chain 1:   1200       -39732.049             0.001            0.001
Chain 1:   1300       -39728.119             0.001            0.000
Chain 1:   1400       -39740.928             0.000            0.000
Chain 1:   1500       -39726.114             0.000            0.000
Chain 1:   1600       -39734.740             0.000            0.000
Chain 1:   1700       -39734.129             0.000            0.000
Chain 1:   1800       -39740.719             0.000            0.000
Chain 1:   1900       -39743.591             0.000            0.000
Chain 1:   2000       -39737.155             0.000            0.000
Chain 1:   2100       -39720.432             0.000            0.000
Chain 1:   2200       -39738.138             0.000            0.000
Chain 1:   2300       -39731.045             0.000            0.000
Chain 1:   2400       -39716.393             0.000            0.000
Chain 1:   2500       -39729.189             0.000            0.000
Chain 1:   2600       -39722.239             0.000            0.000
Chain 1:   2700       -39719.508             0.000            0.000
Chain 1:   2800       -39718.709             0.000            0.000
Chain 1:   2900       -39735.110             0.000            0.000
Chain 1:   3000       -39725.900             0.000            0.000
Chain 1:   3100       -39726.123             0.000            0.000
Chain 1:   3200       -39718.736             0.000            0.000
Chain 1:   3300       -39718.141             0.000            0.000
Chain 1:   3400       -39717.147             0.000            0.000
Chain 1:   3500       -39725.738             0.000            0.000
Chain 1:   3600       -39732.190             0.000            0.000
Chain 1:   3700       -39723.666             0.000            0.000
Chain 1:   3800       -39725.470             0.000            0.000
Chain 1:   3900       -39741.504             0.000            0.000
Chain 1:   4000       -39722.951             0.000            0.000
Chain 1:   4100       -39721.852             0.000            0.000
Chain 1:   4200       -39717.894             0.000            0.000
Chain 1:   4300       -39717.474             0.000            0.000
Chain 1:   4400       -39716.244             0.000            0.000
Chain 1:   4500       -39727.542             0.000            0.000
Chain 1:   4600       -39716.670             0.000            0.000
Chain 1:   4700       -39723.714             0.000            0.000
Chain 1:   4800       -39727.123             0.000            0.000
Chain 1:   4900       -39722.517             0.000            0.000
Chain 1:   5000       -39722.485             0.000            0.000
Chain 1:   5100       -39719.107             0.000            0.000
Chain 1:   5200       -39722.873             0.000            0.000
Chain 1:   5300       -39720.153             0.000            0.000
Chain 1:   5400       -39718.807             0.000            0.000
Chain 1:   5500       -39719.687             0.000            0.000
Chain 1:   5600       -39730.850             0.000            0.000
Chain 1:   5700       -39719.315             0.000            0.000
Chain 1:   5800       -39717.985             0.000            0.000
Chain 1:   5900       -39715.943             0.000            0.000
Chain 1:   6000       -39721.574             0.000            0.000
Chain 1:   6100       -39716.072             0.000            0.000
Chain 1:   6200       -39715.947             0.000            0.000
Chain 1:   6300       -39716.325             0.000            0.000
Chain 1:   6400       -39716.206             0.000            0.000
Chain 1:   6500       -39720.508             0.000            0.000
Chain 1:   6600       -39717.566             0.000            0.000
Chain 1:   6700       -39718.903             0.000            0.000
Chain 1:   6800       -39716.766             0.000            0.000
Chain 1:   6900       -39724.482             0.000            0.000
Chain 1:   7000       -39717.376             0.000            0.000
Chain 1:   7100       -39721.566             0.000            0.000
Chain 1:   7200       -39725.641             0.000            0.000
Chain 1:   7300       -39717.909             0.000            0.000
Chain 1:   7400       -39720.096             0.000            0.000
Chain 1:   7500       -39716.243             0.000            0.000
Chain 1:   7600       -39738.451             0.000            0.000
Chain 1:   7700       -39715.841             0.000            0.000
Chain 1:   7800       -39716.561             0.000            0.000
Chain 1:   7900       -39716.865             0.000            0.000
Chain 1:   8000       -39721.972             0.000            0.000
Chain 1:   8100       -39723.864             0.000            0.000
Chain 1:   8200       -39716.157             0.000            0.000
Chain 1:   8300       -39720.235             0.000            0.000
Chain 1:   8400       -39718.693             0.000            0.000
Chain 1:   8500       -39727.325             0.000            0.000
Chain 1:   8600       -39716.809             0.000            0.000
Chain 1:   8700       -39716.760             0.000            0.000
Chain 1:   8800       -39721.577             0.000            0.000
Chain 1:   8900       -39716.910             0.000            0.000
Chain 1:   9000       -39721.631             0.000            0.000
Chain 1:   9100       -39721.102             0.000            0.000
Chain 1:   9200       -39718.303             0.000            0.000
Chain 1:   9300       -39715.759             0.000            0.000
Chain 1:   9400       -39719.769             0.000            0.000
Chain 1:   9500       -39719.046             0.000            0.000
Chain 1:   9600       -39720.854             0.000            0.000
Chain 1:   9700       -39717.968             0.000            0.000
Chain 1:   9800       -39721.396             0.000            0.000
Chain 1:   9900       -39728.139             0.000            0.000
Chain 1:   10000       -39715.367             0.000            0.000
Chain 1: Informational Message: The maximum number of iterations is reached! The algorithm may not have converged.
Chain 1: This variational approximation is not guaranteed to be meaningful.
Chain 1: 
Chain 1: Drawing a sample of size 1000 from the approximate posterior... 
Chain 1: COMPLETED.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Pareto k diagnostic value is 2.05. Resampling is disabled. Decreasing
tol_rel_obj may help if variational algorithm has terminated prematurely.
Otherwise consider using sampling instead.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Setting 'QR' to TRUE can often be helpful when using one of the variational inference algorithms. See the documentation for the 'QR' argument.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><span class="fu" style="color: #4758AB;">toc</span>()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>298.73 sec elapsed</code></pre>
</div>
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1">lower_tol_draws <span class="ot" style="color: #003B4F;">&lt;-</span> poststrat_df_60k <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">add_epred_draws</span>(fit_60k_1e8, <span class="at" style="color: #657422;">ndraws =</span> <span class="dv" style="color: #AD0000;">1000</span>)</span>
<span id="cb12-2"></span>
<span id="cb12-3">mfvi_lower_tol_points <span class="ot" style="color: #003B4F;">&lt;-</span> lower_tol_draws <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb12-4">                        <span class="fu" style="color: #4758AB;">group_by</span>(state,.draw) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb12-5">                        <span class="fu" style="color: #4758AB;">summarize</span>(<span class="at" style="color: #657422;">postrat_draw =</span> <span class="fu" style="color: #4758AB;">sum</span>(.epred<span class="sc" style="color: #5E5E5E;">*</span>(n<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sum</span>(n)))) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb12-6">                        <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">model =</span> <span class="st" style="color: #20794D;">"MF-VI 1e-8"</span>)</span>
<span id="cb12-7"></span>
<span id="cb12-8">combined_points_w_lower_tol <span class="ot" style="color: #003B4F;">&lt;-</span> combined_points <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb12-9">                      <span class="fu" style="color: #4758AB;">bind_rows</span>(mfvi_lower_tol_points) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb12-10">                      <span class="fu" style="color: #4758AB;">ungroup</span>()</span>
<span id="cb12-11"></span>
<span id="cb12-12">combined_points_w_lower_tol <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb12-13">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">ordered_state =</span> <span class="fu" style="color: #4758AB;">fct_reorder</span>(combined_points_w_lower_tol<span class="sc" style="color: #5E5E5E;">$</span>state,</span>
<span id="cb12-14">                                     combined_points_w_lower_tol<span class="sc" style="color: #5E5E5E;">$</span>postrat_draw)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb12-15">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">y =</span> ordered_state,</span>
<span id="cb12-16">             <span class="at" style="color: #657422;">x =</span> postrat_draw,</span>
<span id="cb12-17">             <span class="at" style="color: #657422;">color =</span> model)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb12-18">     <span class="fu" style="color: #4758AB;">stat_dots</span>(<span class="at" style="color: #657422;">quantiles =</span> <span class="dv" style="color: #AD0000;">100</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb12-19">     <span class="fu" style="color: #4758AB;">facet_wrap</span>(<span class="sc" style="color: #5E5E5E;">~</span>model) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb12-20">     <span class="fu" style="color: #4758AB;">theme</span>(<span class="at" style="color: #657422;">legend.position=</span><span class="st" style="color: #20794D;">"none"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb12-21">  <span class="fu" style="color: #4758AB;">xlab</span>(<span class="st" style="color: #20794D;">"Should employers be allowed to deny their employees abortion care?"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb12-22">  <span class="fu" style="color: #4758AB;">ylab</span>(<span class="st" style="color: #20794D;">"State"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt2/Variational_MRP_pt2_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>… That certainly looks different, but I don’t really think I’d say it looks meaningfully better<sup>4</sup>.</p>
<p>Looking at the printed out ELBO, it’s pretty clear that there was no traction after the first ~1000 samples. A variational family this simple isn’t going to get much better, no matter how much time you give it.</p>
</section>
<section id="full-rank-approximation" class="level2">
<h2 class="anchored" data-anchor-id="full-rank-approximation">Full-Rank Approximation</h2>
<p>So if extend training time, but improvements don’t result, maybe the next option is ask whether we need something more sophisticated than a mean-field approximation. Instead of</p>
<p><img src="https://latex.codecogs.com/png.latex?q(z)%20=%20%5Cprod_%7Bj=1%7D%5E%7Bm%7D%20q_j(z_j)"></p>
<p>let’s now try the full-rank approximation. Gather than each <img src="https://latex.codecogs.com/png.latex?z_j"> getting it’s own independent Gaussian, this uses a single multivariate normal distribution- so we can now (roughly) learn correlation structure, fancy.</p>
<p><img src="https://latex.codecogs.com/png.latex?q(z)%20=%20%5Cmathcal%7BN%7D(z%7C%5Cmu,%5CSigma)"></p>
<div class="cell" data-warnings="false">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><span class="fu" style="color: #4758AB;">tic</span>()</span>
<span id="cb13-2">fit_60k_fullrank <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">stan_glmer</span>(abortion <span class="sc" style="color: #5E5E5E;">~</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> state) <span class="sc" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> eth) <span class="sc" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> educ) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb13-3">                                      male <span class="sc" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> male<span class="sc" style="color: #5E5E5E;">:</span>eth) <span class="sc" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> educ<span class="sc" style="color: #5E5E5E;">:</span>age) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb13-4">                                      (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> educ<span class="sc" style="color: #5E5E5E;">:</span>eth) <span class="sc" style="color: #5E5E5E;">+</span> repvote <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">factor</span>(region),</span>
<span id="cb13-5">  <span class="at" style="color: #657422;">family =</span> <span class="fu" style="color: #4758AB;">binomial</span>(<span class="at" style="color: #657422;">link =</span> <span class="st" style="color: #20794D;">"logit"</span>),</span>
<span id="cb13-6">  <span class="at" style="color: #657422;">data =</span> cces_all_df,</span>
<span id="cb13-7">  <span class="at" style="color: #657422;">prior =</span> <span class="fu" style="color: #4758AB;">normal</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">autoscale =</span> <span class="cn" style="color: #8f5902;">TRUE</span>),</span>
<span id="cb13-8">  <span class="at" style="color: #657422;">prior_covariance =</span> <span class="fu" style="color: #4758AB;">decov</span>(<span class="at" style="color: #657422;">scale =</span> <span class="fl" style="color: #AD0000;">0.50</span>),</span>
<span id="cb13-9">  <span class="at" style="color: #657422;">adapt_delta =</span> <span class="fl" style="color: #AD0000;">0.99</span>,</span>
<span id="cb13-10">  <span class="at" style="color: #657422;">tol_rel_obj =</span> <span class="fl" style="color: #AD0000;">1e-8</span>,</span>
<span id="cb13-11">  <span class="co" style="color: #5E5E5E;"># Printing the ELBO every 1k draws</span></span>
<span id="cb13-12">  <span class="at" style="color: #657422;">refresh =</span> <span class="dv" style="color: #AD0000;">1000</span>,</span>
<span id="cb13-13">  <span class="at" style="color: #657422;">algorithm =</span> <span class="st" style="color: #20794D;">"fullrank"</span>,</span>
<span id="cb13-14">  <span class="at" style="color: #657422;">QR =</span> <span class="cn" style="color: #8f5902;">TRUE</span>,</span>
<span id="cb13-15">  <span class="at" style="color: #657422;">seed =</span> <span class="dv" style="color: #AD0000;">605</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Chain 1: ------------------------------------------------------------
Chain 1: EXPERIMENTAL ALGORITHM:
Chain 1:   This procedure has not been thoroughly tested and may be unstable
Chain 1:   or buggy. The interface is subject to change.
Chain 1: ------------------------------------------------------------
Chain 1: 
Chain 1: 
Chain 1: 
Chain 1: Gradient evaluation took 0.025 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 250 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Begin eta adaptation.
Chain 1: Iteration:   1 / 250 [  0%]  (Adaptation)
Chain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)
Chain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)
Chain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)
Chain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)
Chain 1: Iteration: 250 / 250 [100%]  (Adaptation)
Chain 1: Success! Found best value [eta = 0.1].
Chain 1: 
Chain 1: Begin stochastic gradient ascent.
Chain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes 
Chain 1:    100      -248586.032             1.000            1.000
Chain 1:    200      -180460.369             0.689            1.000
Chain 1:    300      -121675.221             0.620            0.483
Chain 1:    400       -87431.017             0.563            0.483
Chain 1:    500      -120999.829             0.506            0.392
Chain 1:    600       -96768.296             0.463            0.392
Chain 1:    700       -93851.607             0.402            0.378
Chain 1:    800       -92494.273             0.353            0.378
Chain 1:    900       -74378.556             0.341            0.277
Chain 1:   1000       -77681.560             0.311            0.277
Chain 1:   1100       -77465.866             0.211            0.250
Chain 1:   1200       -68692.287             0.186            0.244
Chain 1:   1300       -75140.633             0.147            0.128
Chain 1:   1400       -49430.772             0.160            0.128
Chain 1:   1500       -59011.994             0.148            0.128
Chain 1:   1600       -57033.572             0.127            0.086
Chain 1:   1700       -56133.855             0.125            0.086
Chain 1:   1800       -46605.149             0.144            0.128
Chain 1:   1900       -47895.964             0.122            0.086
Chain 1:   2000       -44745.890             0.125            0.086
Chain 1:   2100       -43472.467             0.128            0.086
Chain 1:   2200       -43454.384             0.115            0.070
Chain 1:   2300       -41781.249             0.110            0.040
Chain 1:   2400       -42045.221             0.059            0.035
Chain 1:   2500       -41381.652             0.044            0.029
Chain 1:   2600       -40754.440             0.043            0.027
Chain 1:   2700       -41108.136             0.042            0.027
Chain 1:   2800       -40450.439             0.023            0.016
Chain 1:   2900       -40423.015             0.020            0.016
Chain 1:   3000       -40375.121             0.013            0.015
Chain 1:   3100       -40227.022             0.011            0.009
Chain 1:   3200       -40302.411             0.011            0.009
Chain 1:   3300       -40352.339             0.007            0.006
Chain 1:   3400       -40174.196             0.007            0.004
Chain 1:   3500       -40089.973             0.006            0.004
Chain 1:   3600       -40143.009             0.004            0.002
Chain 1:   3700       -40123.486             0.003            0.002
Chain 1:   3800       -40044.004             0.002            0.002
Chain 1:   3900       -39955.515             0.002            0.002
Chain 1:   4000       -40003.851             0.002            0.002
Chain 1:   4100       -39948.544             0.002            0.002
Chain 1:   4200       -40028.027             0.002            0.002
Chain 1:   4300       -39907.006             0.002            0.002
Chain 1:   4400       -39868.266             0.002            0.002
Chain 1:   4500       -39938.386             0.002            0.002
Chain 1:   4600       -39837.339             0.002            0.002
Chain 1:   4700       -39852.349             0.002            0.002
Chain 1:   4800       -39823.670             0.002            0.002
Chain 1:   4900       -39809.797             0.001            0.001
Chain 1:   5000       -39807.261             0.001            0.001
Chain 1:   5100       -39806.402             0.001            0.001
Chain 1:   5200       -39818.805             0.001            0.001
Chain 1:   5300       -39797.428             0.001            0.001
Chain 1:   5400       -39790.469             0.001            0.000
Chain 1:   5500       -39785.797             0.001            0.000
Chain 1:   5600       -39779.121             0.000            0.000
Chain 1:   5700       -39780.314             0.000            0.000
Chain 1:   5800       -39771.363             0.000            0.000
Chain 1:   5900       -39770.673             0.000            0.000
Chain 1:   6000       -39764.096             0.000            0.000
Chain 1:   6100       -39764.173             0.000            0.000
Chain 1:   6200       -39765.651             0.000            0.000
Chain 1:   6300       -39756.809             0.000            0.000
Chain 1:   6400       -39753.724             0.000            0.000
Chain 1:   6500       -39754.753             0.000            0.000
Chain 1:   6600       -39750.392             0.000            0.000
Chain 1:   6700       -39753.067             0.000            0.000
Chain 1:   6800       -39750.341             0.000            0.000
Chain 1:   6900       -39745.696             0.000            0.000
Chain 1:   7000       -39743.521             0.000            0.000
Chain 1:   7100       -39739.157             0.000            0.000
Chain 1:   7200       -39736.689             0.000            0.000
Chain 1:   7300       -39743.472             0.000            0.000
Chain 1:   7400       -39738.431             0.000            0.000
Chain 1:   7500       -39740.789             0.000            0.000
Chain 1:   7600       -39735.842             0.000            0.000
Chain 1:   7700       -39733.493             0.000            0.000
Chain 1:   7800       -39735.015             0.000            0.000
Chain 1:   7900       -39736.429             0.000            0.000
Chain 1:   8000       -39733.548             0.000            0.000
Chain 1:   8100       -39732.722             0.000            0.000
Chain 1:   8200       -39734.720             0.000            0.000
Chain 1:   8300       -39732.932             0.000            0.000
Chain 1:   8400       -39727.658             0.000            0.000
Chain 1:   8500       -39734.522             0.000            0.000
Chain 1:   8600       -39728.602             0.000            0.000
Chain 1:   8700       -39724.690             0.000            0.000
Chain 1:   8800       -39725.374             0.000            0.000
Chain 1:   8900       -39731.450             0.000            0.000
Chain 1:   9000       -39725.866             0.000            0.000
Chain 1:   9100       -39728.639             0.000            0.000
Chain 1:   9200       -39730.156             0.000            0.000
Chain 1:   9300       -39729.036             0.000            0.000
Chain 1:   9400       -39725.536             0.000            0.000
Chain 1:   9500       -39727.031             0.000            0.000
Chain 1:   9600       -39725.389             0.000            0.000
Chain 1:   9700       -39727.947             0.000            0.000
Chain 1:   9800       -39723.932             0.000            0.000
Chain 1:   9900       -39723.173             0.000            0.000
Chain 1:   10000       -39723.944             0.000            0.000
Chain 1: Informational Message: The maximum number of iterations is reached! The algorithm may not have converged.
Chain 1: This variational approximation is not guaranteed to be meaningful.
Chain 1: 
Chain 1: Drawing a sample of size 1000 from the approximate posterior... 
Chain 1: COMPLETED.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Pareto k diagnostic value is 2.95. Resampling is disabled. Decreasing
tol_rel_obj may help if variational algorithm has terminated prematurely.
Otherwise consider using sampling instead.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><span class="fu" style="color: #4758AB;">toc</span>()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>350.16 sec elapsed</code></pre>
</div>
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1">full_rank_draws <span class="ot" style="color: #003B4F;">&lt;-</span> poststrat_df_60k <span class="sc" style="color: #5E5E5E;">%&gt;%</span> <span class="fu" style="color: #4758AB;">add_epred_draws</span>(fit_60k_fullrank,</span>
<span id="cb18-2">                                                        <span class="at" style="color: #657422;">ndraws =</span> <span class="dv" style="color: #AD0000;">1000</span>)</span>
<span id="cb18-3"></span>
<span id="cb18-4">frvi_points <span class="ot" style="color: #003B4F;">&lt;-</span> full_rank_draws <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb18-5">                        <span class="fu" style="color: #4758AB;">group_by</span>(state,.draw) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb18-6">                        <span class="fu" style="color: #4758AB;">summarize</span>(<span class="at" style="color: #657422;">postrat_draw =</span> <span class="fu" style="color: #4758AB;">sum</span>(.epred<span class="sc" style="color: #5E5E5E;">*</span>(n<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sum</span>(n)))) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb18-7">                        <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">model =</span> <span class="st" style="color: #20794D;">"FR-VI"</span>)</span>
<span id="cb18-8"></span>
<span id="cb18-9">combined_points_w_frvi <span class="ot" style="color: #003B4F;">&lt;-</span> combined_points_w_lower_tol <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb18-10">                      <span class="fu" style="color: #4758AB;">bind_rows</span>(frvi_points) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb18-11">                      <span class="fu" style="color: #4758AB;">ungroup</span>()</span>
<span id="cb18-12"></span>
<span id="cb18-13">combined_points_w_frvi <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb18-14">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">ordered_state =</span> <span class="fu" style="color: #4758AB;">fct_reorder</span>(combined_points_w_frvi<span class="sc" style="color: #5E5E5E;">$</span>state,</span>
<span id="cb18-15">                                     combined_points_w_frvi<span class="sc" style="color: #5E5E5E;">$</span>postrat_draw)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb18-16">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">y =</span> ordered_state,</span>
<span id="cb18-17">             <span class="at" style="color: #657422;">x =</span> postrat_draw,</span>
<span id="cb18-18">             <span class="at" style="color: #657422;">color =</span> model)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb18-19">     <span class="fu" style="color: #4758AB;">stat_dots</span>(<span class="at" style="color: #657422;">quantiles =</span> <span class="dv" style="color: #AD0000;">100</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb18-20">     <span class="fu" style="color: #4758AB;">facet_wrap</span>(<span class="sc" style="color: #5E5E5E;">~</span>model) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb18-21">     <span class="fu" style="color: #4758AB;">theme</span>(<span class="at" style="color: #657422;">legend.position=</span><span class="st" style="color: #20794D;">"none"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb18-22">  <span class="fu" style="color: #4758AB;">xlab</span>(<span class="st" style="color: #20794D;">"Should employers be allowed to deny their employees abortion care?"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb18-23">  <span class="fu" style="color: #4758AB;">ylab</span>(<span class="st" style="color: #20794D;">"State"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt2/Variational_MRP_pt2_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>The first thing to note here is that unlike the mean-field approximation, fitting this model required some tinkering to get it to fit. I ended up needing to set <code>QR = TRUE</code> (ie, use a QR decomposition) to get this to fit at all (unless I set the initialization to 0, at which point the posterior collapsed to nearly a single point).</p>
<p>Unfortunately, this version has a similar spiky posterior distribution. In terms of uncertainty, it’s clearly worse than the mean-field implementation. The ELBO starts from higher, spends some time actually improving, but also quickly reaches a plateau. It doesn’t seem like this is a way out either.</p>
</section>
</section>
<section id="where-to-from-here-why-is-it-like-this" class="level1">
<h1>Where to from here? (Why is it like this?)</h1>
<p>We’ve seen that simple variational families like the mean-field and full-rank can approximately mirror the central tendencies of MCMC, but things fall apart as we attempt to consider uncertainty, either through simple credible intervals, or especially once we start to visualize the unrealistic, lumpy VI posterior distributions in their entirety.</p>
<p>This isn’t something we can solve with more training time: each of these algorithms had reached the lowest ELBO they could well before we produced final draws. If I had to guess, I think we need a fundamentally more expressive class of variational family to make progress.</p>
<p>While trying to fit models without digging too much into the theory of why VI approximations can be poor has been fun, it’s time to bring in some theory. In the next post, I’ll explore the literature on why the uncertainty behavior of VI can be so dubious. In the following one, I’ll illustrate some better diagnostics as well.</p>
<p>The code for this post can be found <a href="https://github.com/andytimm/andytimm.github.io/blob/main/posts/Variational%20MRP%20Pt2/Variational_MRP_pt2.qmd">here</a>. Thanks for reading.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Really, the worst type of wrong, completely unpredictable wrong. If you spend time staring to try to infer a causal pattern of which states we can’t estimate well, you’re likely just going to end up confused.↩︎</p></li>
<li id="fn2"><p>Some of these MFVI distributions are bad enough that you might reasonably wonder if some of the badness is just plotting weirdness. That was my intuition at first. Of course though, this is sufficient granularity to make the MCMC results look reasonable. But even if you zoom in on 1 or two states and add way more points, the improbably sharp spikes remain.↩︎</p></li>
<li id="fn3"><p>Phrase due to Richard McElreath. The magic of good visualizations like Kay et al.’s is that makes it trivial to let pattern recognition go to work, and be able to go “oh, that looks wrong”.↩︎</p></li>
<li id="fn4"><p>Also, apologies for showing every 100 iterations; the rstanarm parameter to set this, <code>refresh</code> doesn’t appear to work properly with non-MCMC models, so I can either not show the ELBO or blow up the post with this.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2022,
  author = {Andy Timm},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2022-11-20},
  url = {https://andytimm.github.io/Variational_MRP_pt2.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2022" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2022. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> November 20, 2022. <a href="https://andytimm.github.io/Variational_MRP_pt2.html">https://andytimm.github.io/Variational_MRP_pt2.html</a>.
</div></div></section></div> ]]></description>
  <category>MRP</category>
  <category>Variational Inference</category>
  <guid>https://andytimm.github.io/posts/Variational MRP Pt2/Variational_MRP_pt2.html</guid>
  <pubDate>Sun, 20 Nov 2022 05:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Variational MRP Pt2/images/cover_photo.png" medium="image" type="image/png" height="180" width="144"/>
</item>
<item>
  <title>Variational Inference for MRP with Reliable Posterior Distributions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Variational MRP Pt1/variational_mrp_pt1.html</link>
  <description><![CDATA[ 




<p>This post introduces a series I intend to write, exploring using <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">Variational Inference</a> to massively speed up running complex survey estimation models like variants of <a href="https://en.wikipedia.org/wiki/Multilevel_regression_with_poststratification">Multilevel Regression and Poststratification</a> while aiming to keep approximation error from completely ruining the model.</p>
<p>The rough plan for the series is as follows:</p>
<ol type="1">
<li><strong>(This post)</strong> Introducing the Problem- Why is VI useful, why VI can produce spherical cows</li>
<li>How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li>Some theory on why posterior approximation with VI can be so poor</li>
<li>Seeing if some more sophisticated techniques like normalizing flows help</li>
</ol>
<section id="motivation-for-series" class="level1">
<h1>Motivation for series</h1>
<p>I learn well by explaining things to others, and I’ve been particularly excited to learn about variational inference and ways to improve it over the past few months. There are lots of Bayesian models I would like to fit, especially in my political work, that I would categorize as being incredibly useful, but on the edge of practically acceptable run times. For example, the somewhat but not particularly complex model I’ll use as a running example for the series <strong>takes ~8 hours to fit on 60k observations</strong>.</p>
<p>Having a model run overnight or for a full work day can be fine sometimes, but what if there is a more urgent need for the results? What if we need to iterate to find the “right” model? What if the predictions from this model need to feed into a later one? How constrained do we feel about adding just a little bit more complexity to the model, or increasing our N size just a bit more?</p>
<p>If we can get VI to fit well, we can make complex Bayesian models a lot more practical to use in a wider variety of scenarios, and maybe even extend the complexity of what we can build given time and resource constraints.</p>
</section>
<section id="spherical-cow-sadness" class="level1">
<h1>Spherical Cow Sadness</h1>
<section id="ive-got-that" class="level5">
<h5 class="anchored" data-anchor-id="ive-got-that">I’ve got that…</h5>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 67.6%;justify-content: center;">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt1/rstanarm_disclaimer.png" class="img-fluid"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 5.4%;justify-content: center;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 27.0%;justify-content: center;">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt1/blei_vi_spherical.png" class="img-fluid"></p>
</div>
</div>
</div>
<p>If VI can make Bayesian inference much faster, what’s the catch? The above two images encapsulate the problem pretty well. First, as the left screenshot from <a href="https://mc-stan.org/rstanarm/reference/rstanarm-package.html#estimation-algorithms">rstanarm’s documentation</a> shows, variational inference requires a (bold text warning requiring) set of approximating distribution choices in order to be tractable to optimize. On the right, in their survey paper on VI, <a href="https://arxiv.org/pdf/1601.00670.pdf">Blei et al.&nbsp;(2018)</a> are showing one of the potential posterior distorting consequences of our choice to approximate.</p>
<p>So stepping back for a second, we’ve taken a problem for which there’s usually no closed form solution (Bayesian inference), where even the best approximation algorithm we can usually use (MCMC) isn’t always enough for valid inference without very careful validation and tinkering. Then we decided our approximation could do with being more approximate.</p>
<p>That was perhaps an overly bleak description, but it should give some intuition why this is a hard problem. We want to choose some method of approximating our posterior such that it is amenable to optimization-based solving instead of requiring sampling, but not trade away our ability to correctly understand the full complexity of the posterior distribution<sup>1</sup>.</p>
</section>
</section>
<section id="introducing-mrp-and-our-running-example" class="level1">
<h1>Introducing MRP and our running example</h1>
<section id="introducing-mrp" class="level2">
<h2 class="anchored" data-anchor-id="introducing-mrp">Introducing MRP</h2>
<p>While I’m mostly focused on the way we choose to actually fit a given model with this series, here’s a super quick review of the intuition in building a MRP model. If you want a more complete introduction, Kastellec’s <a href="https://scholar.princeton.edu/jkastellec/publications">MRP Primer</a> is a great starting point, as are the case studies I link a bit later.</p>
<p>MRP casts estimation of a population quantity of interest <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> as a prediction problem. That is, instead of the more traditional approach of building <a href="https://www.pewresearch.org/methods/2018/01/26/how-different-weighting-methods-work/#raking">simple raked weights</a> and using weighted estimators, MRP leans more heavily on modeling and then poststratification to make the estimates representative.</p>
<p>To sketch out the steps-</p>
<ol type="1">
<li>Either gather or run a survey or collection of surveys that collect both information on the outcome of interest, <img src="https://latex.codecogs.com/png.latex?y">, and a set of demographic and geographic predictors, <img src="https://latex.codecogs.com/png.latex?%5Cleft(X_%7B1%7D,%20X_%7B2%7D,%20X_%7B3%7D,%20%5Cldots,%20X_%7Bm%7D%5Cright)">.</li>
<li>Build a poststratification table, with population counts or estimated population counts <img src="https://latex.codecogs.com/png.latex?N_%7Bj%7D"> for each possible combination of the features gathered above. Each possible combination <img src="https://latex.codecogs.com/png.latex?j"> is called a cell, one of <img src="https://latex.codecogs.com/png.latex?J"> possible cells. For example, if we poststratified only on state, there would be <img src="https://latex.codecogs.com/png.latex?J=51"> (with DC) total cells; in practice, <img src="https://latex.codecogs.com/png.latex?J"> is often several thousand.</li>
<li>Build a model, usually a Bayesian multilevel regression, to predict <img src="https://latex.codecogs.com/png.latex?y"> using the demographic characteristic from the survey or set of surveys, estimating model parameters along the way.</li>
<li>Estimate <img src="https://latex.codecogs.com/png.latex?y"> for each cell in the poststratification table, using the model built on the sample.</li>
<li>Aggregate the cells to the population of interest, weighting by the <img src="https://latex.codecogs.com/png.latex?N_%7Bj%7D">’s to obtain population level estimates: <img src="https://latex.codecogs.com/png.latex?%5Ctheta_%7B%5Cmathrm%7BPOP%7D%7D=%5Cfrac%7B%5Csum_%7Bj%20%5Cin%20J%7D%20N_%7Bj%7D%20%5Ctheta_%7Bj%7D%7D%7B%5Csum_%7Bj%20%5Cin%20J%7D%20N_%7BJ%7D%7D"></li>
</ol>
<p>Why would we want to do this over building more typical survey weights? To the extent your new model has desirable properties like the ability to incorporate priors, can partially pool to manage rare subpopulations where you don’t have a lot of sample, and so on, you can get the benefits of that more efficient model through MRP. Raking in its simplest form is really just a linear model; we have plenty of methods that can do better. Outside of bayesian multilevel models which are the most common, there’s an increasing literature on using a wide variety of machine learning algorithms like BART<sup>2</sup> to do the estimation stage; Andrew Gelman calls this <a href="https://statmodeling.stat.columbia.edu/2018/05/19/regularized-prediction-poststratification-generalization-mister-p/">RRP</a>.</p>
</section>
<section id="introducing-the-running-example" class="level2">
<h2 class="anchored" data-anchor-id="introducing-the-running-example">Introducing the Running Example</h2>
<p>Rather than reinvent the wheel, I’ll follow the lead of the excellent <a href="https://bookdown.org/jl5522/MRP-case-studies/">Multilevel Regression and Poststratification Case Studies</a> by Lopez-Martin, Philips, and Gelman, and model survey binary responses from the <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ZSBZ7K">2018 CCES</a> for the following question:</p>
<blockquote class="blockquote">
<p>Allow employers to decline coverage of abortions in insurance plans (Support / Oppose)</p>
</blockquote>
<p>From the CCES, we get information on each participant’s state, age, gender, ethnicity, and education level. Supplementing this individual level data, we also include region flags for each state, and Republican vote share in the 2016 election- these state level predictors have been shown to be critical for getting strong MRP estimates by <a href="http://www.columbia.edu/~jhp2121/publications/HowShouldWeEstimateOpinion.pdf">Lax and Philips (2009)</a> and others. and If you’d like deeper detail on the dataset itself, I’d refer you to <a href="https://bookdown.org/jl5522/MRP-case-studies/introduction-to-mister-p.html#ref-2018CCES">this part</a> MRP case study.</p>
<p>Using these, we setup the model for <img src="https://latex.codecogs.com/png.latex?Pr(y_i%20=%201)"> the probability of supporting allowing employers to decline coverage of abortions in insurance plans as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0APr(y_i%20=%201)%20=&amp;%20logit%5E%7B-1%7D(%0A%5Cgamma%5E0%0A+%20%5Calpha_%7B%5Crm%20s%5Bi%5D%7D%5E%7B%5Crm%20state%7D%0A+%20%5Calpha_%7B%5Crm%20a%5Bi%5D%7D%5E%7B%5Crm%20age%7D%0A+%20%5Calpha_%7B%5Crm%20r%5Bi%5D%7D%5E%7B%5Crm%20eth%7D%0A+%20%5Calpha_%7B%5Crm%20e%5Bi%5D%7D%5E%7B%5Crm%20educ%7D%0A+%20%5Cbeta%5E%7B%5Crm%20male%7D%20%5Ccdot%20%7B%5Crm%20Male%7D_%7B%5Crm%20i%7D%20%5C%5C%0A&amp;+%20%5Calpha_%7B%5Crm%20g%5Bi%5D,%20r%5Bi%5D%7D%5E%7B%5Crm%20male.eth%7D%0A+%20%5Calpha_%7B%5Crm%20e%5Bi%5D,%20a%5Bi%5D%7D%5E%7B%5Crm%20educ.age%7D%0A+%20%5Calpha_%7B%5Crm%20e%5Bi%5D,%20r%5Bi%5D%7D%5E%7B%5Crm%20educ.eth%7D%0A+%20%5Cgamma%5E%7B%5Crm%20south%7D%20%5Ccdot%20%7B%5Crm%20South%7D_%7B%5Crm%20s%7D%20%5C%5C%0A&amp;+%20%5Cgamma%5E%7B%5Crm%20northcentral%7D%20%5Ccdot%20%7B%5Crm%20NorthCentral%7D_%7B%5Crm%20s%7D%0A+%20%5Cgamma%5E%7B%5Crm%20west%7D%20%5Ccdot%20%7B%5Crm%20West%7D_%7B%5Crm%20s%7D%0A+%20%5Cgamma%5E%7B%5Crm%20repvote%7D%20%5Ccdot%20%7B%5Crm%20RepVote%7D_%7B%5Crm%20s%7D)%0A%5Cend%7Baligned%7D%0A"></p>
<p>Where we incorporate pretty much all of our predictors as varying intercepts to allow for pooling across demographic and geographic characteristics:</p>
<ul>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20a%7D%5E%7B%5Crm%20age%7D">: The effect of subject <img src="https://latex.codecogs.com/png.latex?i">’s age on the probability of supporting the statement.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20r%7D%5E%7B%5Crm%20eth%7D">: The effect of subject <img src="https://latex.codecogs.com/png.latex?i">’s ethnicity on the probability of supporting the statement.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20e%7D%5E%7B%5Crm%20educ%7D">: The effect of subject <img src="https://latex.codecogs.com/png.latex?i">’s education on the probability of supporting the statement.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20s%7D%5E%7B%5Crm%20state%7D">: The effect of subject <img src="https://latex.codecogs.com/png.latex?i">’s state on the probability of supporting the statement.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Cbeta%5E%7B%5Crm%20male%7D">: The average effect of being male on the probability of supporting abortion. Note that it doesn’t really make much sense to model a two category<sup>3</sup> factor as a varying intercept.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20e,r%7D%5E%7B%5Crm%20male.eth%7D">, <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20e,r%7D%5E%7B%5Crm%20educ.age%7D">, <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7B%5Crm%20e,r%7D%5E%7B%5Crm%20educ.eth%7D">: Are several reasonable guesses at important interactions for this question. We could add many more two way, or even some three way interactions here, but this is enough for my testing here.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Cgamma%5E%7B%5Crm%20south%7D,%20%5Cgamma%5E%7B%5Crm%20northcentral%7D,%20%5Cgamma%5E%7B%5Crm%20west%7D,%5Cgamma%5E%7B%5Crm%20repvote%7D">: are the state level predictors which are not represented as varying intercepts. Following the case study, I use <img src="https://latex.codecogs.com/png.latex?%5Cgamma">’s for the state level coefficients, keeping <img src="https://latex.codecogs.com/png.latex?%5Cbeta">’s for individual coefficients. Note that Northeast is the base region of the region factor here, so it doesn’t get it’s own coefficient.</p></li>
</ul>
<p>Stepping back for a second, let’s describe the complexity of this model in more general terms. This certainly isn’t state of the art for MRP, and you could definitely add in things like a lot more interactions, some varying slopes, non-univariate prior and/or structured priors, or other elements to make this a more interesting model. That said, this is already clearly enough of a model to improve on simple raking in many cases, and it produces a nuanced enough posterior that we can feasibly imagine a bad approximation going all spherical cow shaped on us.</p>
<p>Why this dataset and this model for this series? The question we model itself isn’t super important- as long as we can expect some significant regional and demographic variation in the outcome we’ll be able to explore if VI smoothes away some posterior complexity that MCMC can capture. Drawing an example from the CCES is quite useful, as the 60k total sample is much larger than typical publicly available surveys, and so we can check behavior under larger N sizes. Practically, fitting this with <code>rstanarm</code> allows us to switch easily from a great MCMC implementation to a decent VI optimizer quickly for some early tests. Finally, the complexity and runtime of the model is a nice balance of being something that we can fit with MCMC in a not terrible amount of time for comparison’s sake, and something challenging enough that it should teach us something about VI’s ability to handle non-toy models of the world.</p>
<p>Fitting this<sup>4</sup> with MCMC in <code>rstanarm</code> is as simple as:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="co" style="color: #5E5E5E;"># Fit in stan_glmer</span></span>
<span id="cb1-2">fit <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">stan_glmer</span>(abortion <span class="sc" style="color: #5E5E5E;">~</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> state) <span class="sc" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> eth) <span class="sc" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> educ) <span class="sc" style="color: #5E5E5E;">+</span> male <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-3">                    (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> male<span class="sc" style="color: #5E5E5E;">:</span>eth) <span class="sc" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> educ<span class="sc" style="color: #5E5E5E;">:</span>age) <span class="sc" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> educ<span class="sc" style="color: #5E5E5E;">:</span>eth) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-4">                    repvote <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">factor</span>(region),</span>
<span id="cb1-5">  <span class="at" style="color: #657422;">family =</span> <span class="fu" style="color: #4758AB;">binomial</span>(<span class="at" style="color: #657422;">link =</span> <span class="st" style="color: #20794D;">"logit"</span>),</span>
<span id="cb1-6">  <span class="at" style="color: #657422;">data =</span> cces_df,</span>
<span id="cb1-7">  <span class="at" style="color: #657422;">prior =</span> <span class="fu" style="color: #4758AB;">normal</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">autoscale =</span> <span class="cn" style="color: #8f5902;">TRUE</span>),</span>
<span id="cb1-8">  <span class="at" style="color: #657422;">prior_covariance =</span> <span class="fu" style="color: #4758AB;">decov</span>(<span class="at" style="color: #657422;">scale =</span> <span class="fl" style="color: #AD0000;">0.50</span>),</span>
<span id="cb1-9">  <span class="at" style="color: #657422;">adapt_delta =</span> <span class="fl" style="color: #AD0000;">0.99</span>,</span>
<span id="cb1-10">  <span class="at" style="color: #657422;">refresh =</span> <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb1-11">  <span class="at" style="color: #657422;">seed =</span> <span class="dv" style="color: #AD0000;">605</span>)</span></code></pre></div>
</div>
<p>Since it isn’t relevant for the rest of my discussion here, I’ll summarize the model diagnostics here and say that this seems to be a pretty reasonable fit- no issues with divergences, and no issues with poor <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D">’s. Worth quickly pointing out that we did have to tune <code>adapt_delta</code> a bit to get no divergences though- even before getting to fitting this with VI, a model like this requires some adjustments to fit correctly.</p>
<p>With a model like this on just a 5k sample, we can produce pretty solid state level predictions that have clearly benefited from being fit with a Bayesian multilevel model:</p>
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt1/5k_sample_full_results.png" class="img-fluid"></p>
<p>With a 5k sample, MRP lands much closer to the complete weighted survey than a 5k unweighted sample: neat. That’s certainly not a fully fair comparison, but it gives some intution around the promise of this approach.</p>
<p>Somewhat less neat is that even a 5k sample here takes about 13 minutes to fit. How does this change as we fit on more and more of the data?</p>
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 75%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Sample Size</th>
<th style="text-align: left;">Runtime</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">5,000</td>
<td style="text-align: left;">13 minutes</td>
</tr>
<tr class="even">
<td style="text-align: left;">10,000</td>
<td style="text-align: left;">44 minutes</td>
</tr>
<tr class="odd">
<td style="text-align: left;">60,000</td>
<td style="text-align: left;">526 minutes (~8 hours!)</td>
</tr>
</tbody>
</table>
<p>As the table above should illustrate, if you’re fitting a decently complex Bayesian model on even somewhat large N sizes, you’re pretty quickly going to cap out what you can reasonably fit in a acceptable amount of time. If you’re scaling N past the above example, or deepening the modeling complexity, you’ll pretty quickly feel effectively locked out of using these models in fast-paced environments.</p>
<p>Hopefully fitting my running example has helped for building intuition here. Even a reasonably complex Bayesian model can have some pretty desirable estimation properties. To make iterating on modelling choices faster, to scale our N or model complexity higher, or just to use a model like this day to day when time matters, we’d really like to scale these fitting times back. Can Variational Inference help?</p>
</section>
</section>
<section id="introducing-variational-inference" class="level1">
<h1>Introducing Variational Inference</h1>
<p>I’ve gotten relatively far in this post without clearly explaining what Variational Inference is, and why it might provide a more efficient and scalable way to fix large Bayesian models. Let’s fully flesh that out here to ground the rest of the series.</p>
<p>In the bigger picture, pretty much all of our efforts in Bayesian inference are a form of approximate inference. Almost no models we care about for real world applications have closed form solutions- conjugate prior type situations are a math problem for stats classes, not a general tool for inference.</p>
<p>Following <a href="https://arxiv.org/abs/1601.00670">Blei et al.&nbsp;(2018)</a>’s notation, let’s setup the general problem first, describe (briefly) how MCMC solves it, and then more slowly demonstrate how VI does. Let’s say we have some observations <img src="https://latex.codecogs.com/png.latex?x_%7B1:N%7D">, and and some latent variables that define the model <img src="https://latex.codecogs.com/png.latex?z_%7B1:M%7D">. Note for concreteness these latent variables represent our quantities of interest: key parameters and so on- we’re calling them latent in the sense that we can’t go out and directly measure a <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> or <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> from the model above, we have to gather data that allows us to estimate them. We call <img src="https://latex.codecogs.com/png.latex?p(z)"> priors, and they define our model prior to contact with the data. The goal of Bayesian inference then is conditioning on our data in order to get the posterior:</p>
<p><img src="https://latex.codecogs.com/png.latex?p(z%7Cx)%20=%20%5Cfrac%7Bp(z,x)%7D%7Bp(x)%7D"></p>
<p>If you’re reading this post series, it’s likely you recognize that the denominator on the right here (often called the “evidence”) is the sticking point; the integral <img src="https://latex.codecogs.com/png.latex?p(x)%20=%20%5Cint%7Bp(z,x)dz%7D"> won’t have a closed form solution.</p>
<p>When we use Markov Chain Monte Carlo as we did above to estimate the model, we’re defining a Markov Chain on <img src="https://latex.codecogs.com/png.latex?z">, whose stationary distribution if we’ve done everything right is <img src="https://latex.codecogs.com/png.latex?p(z%7Cx)">. There are better and worse ways to do this certainly- the development of the <a href="https://mc-stan.org/">Stan</a> language, with associated <a href="https://mc-stan.org/docs/2_19/reference-manual/hamiltonian-monte-carlo.html">Hamiltonian Monte Carlo</a> with <a href="https://arxiv.org/abs/1111.4246">NUTS</a> sampler has massively expanded what was possible to fit in recent years. However, while actively improving the speed and scalability of sampling is an active area of research (for example, by using <a href="https://mc-stan.org/cmdstanr/articles/opencl.html">GPU compute</a> where possible), some of the speed challenges just seem a bit baked into the approach. For example, the sequential nature of markov chains makes parallelization within chains seem out of reach absent some as-yet unknown clever tricks.</p>
<p>Instead of sampling, variational inference asks what we’d need to figure out to treat the Bayesian inference problem as an <strong>optimization problem</strong>, where we could bring to bear all the tools for efficient, scalable, and parallelizable optimization we have developed.</p>
<p>Let’s start with the idea of a family of approximate densities <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BQ%7D"> over our latent variables<sup>5</sup>.</p>
<p>Within that <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BQ%7D">, we want to try the best <img src="https://latex.codecogs.com/png.latex?q(z)">, call it <img src="https://latex.codecogs.com/png.latex?q%5E*(z)">, that minimizes the Kullback-Leibler divergence to the true posterior:</p>
<p><img src="https://latex.codecogs.com/png.latex?q%5E*(z)%20=%20argmin_%7Bq(z)%20%5Cin%20%5Cmathscr%7BQ%7D%7D(q(z)%7C%7Cp(z%7Cx))"></p>
<p>If we choose a good <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BQ%7D">, managing the complexity so that it includes a density close to <img src="https://latex.codecogs.com/png.latex?p(z%7Cx)">, without becoming too slow or impossible to optimize, this approach may provide a significant speed boost.</p>
<p>To start working with this approach though, there’s one major remaining problem. Do you see it in the equation above?</p>
<section id="the-elbo" class="level2">
<h2 class="anchored" data-anchor-id="the-elbo">The ELBO</h2>
<p>If you haven’t seen it yet, this quick substitution should clarify a potential issue with VI as I’ve described it so far:</p>
<p><img src="https://latex.codecogs.com/png.latex?q%5E*(z)%20=%20argmin_%7Bq(z)%20%5Cin%20%5Cmathscr%7BQ%7D%7D(q(z)%7C%7C%5Cfrac%7Bp(z,x)%7D%7B%5Cbf%20p(x)%7D)%20=%20%5Cmathbb%7BE%7D%5Blogq(z)%5D%20-%20%5Cmathbb%7BE%7D%5Blogp(z,x)%5D%20+%20%7B%5Cbf%20logp(x)%7D"> Without some new trick, all I’ve said so far is to approximate a thing I can’t analytically calculate (the posterior, specially the issue evidence piece of it), I’m going to calculate the distance between my approximation and… the thing I said has a component can’t calculate?</p>
<p>Fortunately, a clever solution exists here that makes this strategy possible. Instead of trying to minimize the above KL divergence, we can optimize the alternative objective:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5Blogp(z,x)%5D%20-%20%5Cmathbb%7BE%7D%5Blogq(z)%5D"></p>
<p>This is just the negative of the first two terms above, leaving aside the <img src="https://latex.codecogs.com/png.latex?logp(x)">. Why can we treat maximizing this as minimizing the KL divergence? The <img src="https://latex.codecogs.com/png.latex?logp(x)"> term is just a constant (with respect to q), so regardless of how we vary q, this will still be a valid alternative objective. We call this the Evidence Lower Bound (ELBO)<sup>6</sup>.</p>
<p>If it’s helpful for intuition, play around with this great interactive ELBO optimizer by Felix Köhler:</p>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt1/elboplot.png" class="img-fluid"></p>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Variational MRP Pt1/elboeqs.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Link to demonstration <a href="https://englishprobabilistic-machine-learningelbo-interactive--or5u7m.streamlitapp.com/">here</a>; check out Felix’s Youtube explanation of the ELBO <a href="https://www.youtube.com/watch?v=HxQ94L8n0vU">also</a>!</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>By twiddling the knobs on <img src="https://latex.codecogs.com/png.latex?%5Cmu"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma"> for our approximating normal, we can get our surrogate distribution pretty close to the True Posterior (which we know for purposes of demonstration, so we can calculate the true KL, not just it’s ELBO component). No matter how we twiddle though, the evidence remains constant.</p>
<p>For further intuition- notice that we can only do this trick in one direction. The KL divergence isn’t symmetrical, and if we wanted to calculate the “reverse” KL, we couldn’t use this strategy as <img src="https://latex.codecogs.com/png.latex?logq(x)"> would not be a constant. Even if we thought that optimizing other direction of KL might have desirable properties like emphasizing <a href="https://agustinus.kristia.de/techblog/2016/12/21/forward-reverse-kl/">mass-seeking over mode-seeking behavior</a>, that simply isn’t an option.</p>
</section>
</section>
<section id="a-first-try-at-vi-on-this-dataset" class="level1">
<h1>A first try at VI on this dataset</h1>
<p>Ok, so we have an objective to optimize that should actually work. What’s a good <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BQ%7D">? The choice has been shown to matter a lot, but for purposes of a first swing here, let’s try one of the simpler ideas people have explored, the mean-field family. These latent variables will be assumed mutually independent<sup>7</sup> and each get it’s own distinct factor in the variational density. A member of this would look something like:</p>
<p><img src="https://latex.codecogs.com/png.latex?q(z)%20=%20%5Cprod_%7Bj=1%7D%5E%7Bm%7D%20q_j(z_j)"></p>
<p>Each latent <img src="https://latex.codecogs.com/png.latex?z_j"> get it’s own variational factor with density <img src="https://latex.codecogs.com/png.latex?q_j(z_j)">, whose knobs we play with to maximize the ELBO. In the particular implementation below normal distributions are used, plenty of other options like t distributions are common too.</p>
<p>Probably not the best we can do, but let’s give it a roll. Since we’ve been told this will scale really well too supposedly, let’s use all 60k of the observations just to get a sense how it’ll compare to our 8+ hours in that case.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="fu" style="color: #4758AB;">tic</span>()</span>
<span id="cb2-2">fit_60k <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">stan_glmer</span>(abortion <span class="sc" style="color: #5E5E5E;">~</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> state) <span class="sc" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> eth) <span class="sc" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> educ) <span class="sc" style="color: #5E5E5E;">+</span> male <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-3">                    (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> male<span class="sc" style="color: #5E5E5E;">:</span>eth) <span class="sc" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> educ<span class="sc" style="color: #5E5E5E;">:</span>age) <span class="sc" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">|</span> educ<span class="sc" style="color: #5E5E5E;">:</span>eth) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-4">                    repvote <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">factor</span>(region),</span>
<span id="cb2-5">  <span class="at" style="color: #657422;">family =</span> <span class="fu" style="color: #4758AB;">binomial</span>(<span class="at" style="color: #657422;">link =</span> <span class="st" style="color: #20794D;">"logit"</span>),</span>
<span id="cb2-6">  <span class="at" style="color: #657422;">data =</span> cces_all_df,</span>
<span id="cb2-7">  <span class="at" style="color: #657422;">prior =</span> <span class="fu" style="color: #4758AB;">normal</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">autoscale =</span> <span class="cn" style="color: #8f5902;">TRUE</span>),</span>
<span id="cb2-8">  <span class="at" style="color: #657422;">prior_covariance =</span> <span class="fu" style="color: #4758AB;">decov</span>(<span class="at" style="color: #657422;">scale =</span> <span class="fl" style="color: #AD0000;">0.50</span>),</span>
<span id="cb2-9">  <span class="at" style="color: #657422;">adapt_delta =</span> <span class="fl" style="color: #AD0000;">0.99</span>,</span>
<span id="cb2-10">  <span class="at" style="color: #657422;">refresh =</span> <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb2-11">  <span class="at" style="color: #657422;">algorithm =</span> <span class="st" style="color: #20794D;">"meanfield"</span>,</span>
<span id="cb2-12">  <span class="at" style="color: #657422;">seed =</span> <span class="dv" style="color: #AD0000;">605</span>)</span>
<span id="cb2-13"><span class="fu" style="color: #4758AB;">toc</span>()</span></code></pre></div>
</div>
<p>This finishes in a blazing <strong>144.03 seconds</strong>. Is this a good fit, or have we created a ridiculous spherical cow?</p>
<p>You’ll have to find out in the next post. Thanks for reading!</p>
<p><em>Typically, I’ll include links to code at the end of these posts, but since the only thing going on in this notebook is mentioning some runtimes of the models displayed inline at various sample sizes, I’m skipping that for now.</em></p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>If I were that type of Bayesian, this is where I’d complain that if we screw this up badly enough, we might as well be frequentists or worse, machine learning folk.↩︎</p></li>
<li id="fn2"><p>In grad school, using BART as the estimator (also combining it with some portions of the model being estimated as multilevel models) was the focus of my <a href="https://andytimm.github.io/posts/BART%20VI/2020-03-06-BART-vi.html">masters thesis</a>. This pairs the best parts of relatively black box machine learning sensibility with the advantages of still having a truly Bayesian model. With comparatively minimal iteration you can get a pretty decent set of MRP models that will be better than many basic versions of multilevel models fit early in the MRP literature. Of course, if you’re willing to spend a bunch of time iterating on the absolute best models for a given problem, and incorporate lots of problem specific knowledge into model forms you can and should do better than <a href="https://github.com/jbisbee1/BARP">BARP</a>. Also, a lot of pretty cool things you can do like jointly model multiple question responses at the same time aren’t going to be easily to implement unless you get way in the weeds of your own BART implementation.↩︎</p></li>
<li id="fn3"><p>Insert snark about CCES folks doing a poor job at gender inclusivity despite 80+ researchers working on it here.↩︎</p></li>
<li id="fn4"><p>Again, see the MRP case studies linked above if you want see all the data prep and draw manipulation here; I’ll be leaving out most such details that aren’t relevant for comparisons to fitting this model with VI from now on.↩︎</p></li>
<li id="fn5"><p>In grad school, I had a friend who insisted on calling this “spicy Q”. For a while we had a latex package that made <code>\spicy{}</code> equivalent to <code>\mathscr{}</code>. Apologies for the footnote for the dumb LaTeX joke, but now I’m pretty sure you won’t have a sudden moment of “what is that symbol again” discussing VI ever.↩︎</p></li>
<li id="fn6"><p>Why is this a lower bound? Notice that we could write the evidence from above equations as <img src="https://latex.codecogs.com/png.latex?logp(x)%20=%20KL(q(z)%7C%7Cp(z%7Cx))%20+%20ELBO(q)">. Since the KL divergence is non-negative (it’s zero when distributions <img src="https://latex.codecogs.com/png.latex?p"> and <img src="https://latex.codecogs.com/png.latex?q"> are identical), the ELBO is a lower bound of the evidence.↩︎</p></li>
<li id="fn7"><p>If this seems like it could go fully spherical cow, both literally in the sense that if we use a bunch of independent normals we make a sphere, and in the sense that this may not represent the full complexity of public opinion, you’re correct. Assuming independence here could very easily cause problems, and part of why this VI strategy is so challenging is the subset of things we can easily optimize doesn’t have the best overlap with fully realistic distributional assumptions over our latent variables.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2022,
  author = {Andy Timm},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2022-10-10},
  url = {https://andytimm.github.io/variational_mrp_pt1.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2022" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2022. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> October 10, 2022. <a href="https://andytimm.github.io/variational_mrp_pt1.html">https://andytimm.github.io/variational_mrp_pt1.html</a>.
</div></div></section></div> ]]></description>
  <category>MRP</category>
  <category>BART</category>
  <category>Variational Inference</category>
  <guid>https://andytimm.github.io/posts/Variational MRP Pt1/variational_mrp_pt1.html</guid>
  <pubDate>Mon, 10 Oct 2022 04:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Variational MRP Pt1/elboplot.png" medium="image" type="image/png" height="43" width="144"/>
</item>
<item>
  <title>BART with varying intercepts in the MRP framework</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/BART VI/2020-03-06-BART-vi.html</link>
  <description><![CDATA[ 




<p>This is the first of a few posts about some of the substantive and modeling findings from my master’s thesis, where I use Bayesian Additive Regression Trees (BART) and Poststratification to model support for a border wall from 2015-2019. In this post I explore some of the properties of using BART with varying intercepts (BART-vi) within the MRP framework.</p>
<!--more-->
<section id="choosing-a-prediction-model-for-mrp" class="level2">
<h2 class="anchored" data-anchor-id="choosing-a-prediction-model-for-mrp">Choosing a prediction model for MRP</h2>
<p>Multilevel Regression and Poststratification has seen huge success as a tool for obtaining accurate estimates of public opinion in small areas using national surveys. As the name would suggest, the most common tool used for the modeling step of these models are multilevel regressions, often Bayesian multilevel ones. The key intuition here is that pooling data across levels of predictors like state makes much more efficient use of the underlying data, which leads to more accurate estimates. However, there is no strict requirement that the models used here be multilevel regressions, it’s simply an efficient and stable way to get regularized predictions using quite a large set of predictors. Recently, academic work has begun to explore using a wide class of machine learning algorithms as the predictive component in this framework. Andrew Gelman calls this RRP: <a href="https://statmodeling.stat.columbia.edu/2018/05/19/regularized-prediction-poststratification-generalization-mister-p/">Regularized Regression and Poststratification</a>.</p>
<p>One particularly promising alternative prediction algorithm is <a href="https://arxiv.org/abs/0806.3286">BART</a>, which puts the high accuracy of tree-based algorithms like random forests or gradient boosting into a Bayesian framework. This has a number of appealing advantages compared to other machine learning options, especially for RRP. First, unlike other algorithms which might not have clear measures of their uncertainty or confidence intervals around their predictions, BART approximates a full posterior distribution. Second, BART has a number of prior and hyperparameter choices that have been shown to be highly effective in a wide variety of settings, somewhat reducing the need for parameter search. Finally, BART runs fast, especially when compared to the Bayesian multilevel models commonly used for MRP.</p>
<p>Of course, BART models are not without their disadvantages. First and foremost, there is currently only a small amount of recent work on BART models for categorical (as opposed to binary) response <a href="https://arxiv.org/abs/1701.01503">(Murray, 2019)</a>, and no public implementation of that model that I am aware of. In my case, this means modeling the border wall question as binary “support vs.&nbsp;oppose”, as opposed to the three categories “support, oppose, don’t know”. Given the salience of the issue, only 3.1% of people responded “Don’t Know”, so this is a relatively minor loss. However, for questions like the formerly crowded 2020 democratic primary, or a general election where third parties play a major role, this could be a much more serious loss.</p>
<p>While only a small amount of work has compared the two so far, estimates using BART appear to slightly outperform those using multilevel models. For example, <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12361">Montgomery &amp; Olivella (2018)</a> compared using BART to <a href="http://www.stat.columbia.edu/~gelman/research/published/misterp.pdf">Gelman &amp; Ghitza’s (2013)</a>’s fairly complex multilevel model, finding the predictions were incredibly similar, being as much as .97 correlated. As they note however, their BART model produced these results both without large amounts of iteration on model form (which has been a constant challenge for MRP), and producing such estimates orders of magnitude faster. Similarly, <a href="https://www.cambridge.org/core/journals/american-political-science-review/article/barp-improving-mister-p-using-bayesian-additive-regression-trees/630866EB47F9366EDB3C22CFD951BB6F">Bisbee (2019)</a> finds across 89 different datasets that BART and MRP produced very similar estimates, but BART’s were of slightly higher quality, both by Mean Absolute Error (MAE) and Interstate Correlation (a measure of how well the state level predictions from a model using national surveys line up with state level polls). Ending his article, Bisbee writes “One avenue of future research might focus on variants of Bayesian additive regression trees that embed a multilevel component, likely providing further improvements as the best of both worlds.”</p>
<p>This is exactly what I do in my thesis, using BART with varying intercepts by state to model support for a border wall by state. To present my findings around BART-vi, I’ll start by providing a brief overview of the MRP framework. Next, I’ll explain BART, and how BART-vi extends this model. Finally, I’ll build one model of each BART type, and compare them.</p>
</section>
<section id="a-quick-review-of-multilevel-regression-and-poststratification" class="level2">
<h2 class="anchored" data-anchor-id="a-quick-review-of-multilevel-regression-and-poststratification">A Quick Review of Multilevel Regression and Poststratification</h2>
<p>While I’m mostly focused on the modeling step with this post, here’s a quick review of the overall process in building a MRP/RRP model. If you want a more complete introduction, Kastellec’s <a href="https://scholar.princeton.edu/jkastellec/publications">MRP Primer</a> is a great starting point.</p>
<p>MRP or RRP cast estimation of a population quantity of interest <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> as a prediction problem. That is, instead of the more traditional approach of attempting to design the initial survey to be representative of the population, MRP leans more heavily on modeling and poststratification to make the estimates representative.</p>
<p>To sketch out the steps-</p>
<ol type="1">
<li>Either gather or run a survey or collection of surveys that collect both information on the outcome of interest, <img src="https://latex.codecogs.com/png.latex?y">, and a set of demographic and geographic predictors, <img src="https://latex.codecogs.com/png.latex?%5Cleft(X_%7B1%7D,%20X_%7B2%7D,%20X_%7B3%7D,%20%5Cldots,%20X_%7Bm%7D%5Cright)">.</li>
<li>Build a poststratification table, with population counts or estimated population counts <img src="https://latex.codecogs.com/png.latex?N_%7Bj%7D"> for each possible combination of the features gathered above. Each possible combination <img src="https://latex.codecogs.com/png.latex?j"> is called a cell, one of <img src="https://latex.codecogs.com/png.latex?J"> possible cells. For example, if we poststratified only on state, there would be <img src="https://latex.codecogs.com/png.latex?J=51"> (with DC) total cells; in practice, <img src="https://latex.codecogs.com/png.latex?J"> is often several thousand.</li>
<li>Build a model, usually a Bayesian multilevel regression, to predict <img src="https://latex.codecogs.com/png.latex?y"> using the demographic characteristic from the survey or set of surveys, estimating model parameters along the way.</li>
<li>Estimate <img src="https://latex.codecogs.com/png.latex?y"> for each cell in the poststratification table, using the model built on the sample.</li>
<li>Aggregate the cells to the population of interest, weighting by the <img src="https://latex.codecogs.com/png.latex?N_%7Bj%7D">’s to obtain population level estimates: <img src="https://latex.codecogs.com/png.latex?%5Ctheta_%7B%5Cmathrm%7BPOP%7D%7D=%5Cfrac%7B%5Csum_%7Bj%20%5Cin%20J%7D%20N_%7Bj%7D%20%5Ctheta_%7Bj%7D%7D%7B%5Csum_%7Bj%20%5Cin%20J%7D%20N_%7BJ%7D%7D"></li>
</ol>
</section>
<section id="bart" class="level2">
<h2 class="anchored" data-anchor-id="bart">BART</h2>
<p>In this section, I review the general BART model, and discuss the hyperparameter choices I use. Proposed by <a href="https://arxiv.org/abs/0806.3286">Chipman et al, (2008)</a>, BART is a Bayesian machine learning algorithm that has seen widespread success in a wide variety of both predictive and causal inference applications. Like most machine learning models, it treats the prediction task as modeling the outcome <img src="https://latex.codecogs.com/png.latex?y"> as an unknown function <img src="https://latex.codecogs.com/png.latex?f"> of the <img src="https://latex.codecogs.com/png.latex?k"> predictors <img src="https://latex.codecogs.com/png.latex?y%20=%20f(X_%7Bk%7D)">.</p>
<p>BART does with this a sum of decision trees:</p>
<p><img src="https://latex.codecogs.com/png.latex?Y_%7Bk%7D=%5Csum_%7Bj=1%7D%5E%7Bm%7D%20g%5Cleft(%5Cmathbf%7BX%7D_%7Bk%7D,%20T_%7Bj%7D,%20%5Cmathbf%7BM%7D_%7Bj%7D%5Cright)+%5Cepsilon_%7Bk%7D%20%5Cquad%20%5Cepsilon_%7Bk%7D%20%5Cstackrel%7Bi%20.%20i%20.%20d%7D%7B%5Csim%7D%20N%5Cleft(0,%20%5Csigma%5E%7B2%7D%5Cright)"></p>
<p>(To start with the continuous case, before generalizing to the binary case in a moment)</p>
<p>Each tree <img src="https://latex.codecogs.com/png.latex?T_j"> splits the data along a variety of predictors, seeking to improve the purity of outcomes in each group. For instance, in seeking to partition respondents into purer groups of support or opposition for a border wall, one natural split is that of white vs.&nbsp;nonwhite respondents, after which a further split by education might further partition the white node. At the end of fitting such a tree, there are <img src="https://latex.codecogs.com/png.latex?b_%7Bj%7D"> terminal nodes (nodes at the bottom of the tree), which contain groups where the average outcome <img src="https://latex.codecogs.com/png.latex?%5Cmu_%7Bj%7D"> should be purer due to iterative splitting. This iterative splitting is equivalent to the modeling of interaction effects, and combining many such trees allows for flexible and highly non-linear functions of the predictors to be calculated. Each data point <img src="https://latex.codecogs.com/png.latex?x"> is thought of as assigned to one such terminal node for each tree, which captures <img src="https://latex.codecogs.com/png.latex?E(y%20%5Cvert%20x)">, with the collection of <img src="https://latex.codecogs.com/png.latex?u_%7Bj%7D">’s referred to collectively as <img src="https://latex.codecogs.com/png.latex?M">. Together, <img src="https://latex.codecogs.com/png.latex?m"> such trees are fit to residual errors from an initial baseline prediction iteratively, ensuring that the trees are grown in varying structures that predict well for different parts of the covariate space, not just split on the same features producing identical predictions.</p>
<p>To fit such trees to the data and not overfit, BART utilizes a Bayesian framework, placing priors on tree structure, terminal node parameters, and variance, <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">. The prior for the <img src="https://latex.codecogs.com/png.latex?u_%7Bj%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2"> are:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%20%5Cmu_%7Bj%7D%20%7C%20T_%7Bj%7D%20&amp;%20%5Csim%20N%5Cleft(%5Cmu,%20%5Csigma_%7B%5Cmu%7D%5E%7B2%7D%5Cright)%20%5C%5C%20%5Csigma%5E%7B2%7D%20&amp;%20%5Csim%20I%20G%5Cleft(%5Cfrac%7B%5Cnu%7D%7B2%7D,%20%5Cfrac%7B%5Cnu%20%5Clambda%7D%7B2%7D%5Cright)%20%5Cend%7Baligned%7D"></p>
<p>Where <img src="https://latex.codecogs.com/png.latex?I%20G(%5Calpha,%20%5Cbeta)"> is the inverse gamma distribution with shape parameter <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and rate <img src="https://latex.codecogs.com/png.latex?%5Cbeta">. The priors on the tree structure can be thought of as having 3 components. First, there is a prior on the probability that a tree of depth <img src="https://latex.codecogs.com/png.latex?d%20=%200,1,2..."> is not terminal, which is <img src="https://latex.codecogs.com/png.latex?%5Calpha(1+d)%5E%7B-%5Cbeta%7D">, with <img src="https://latex.codecogs.com/png.latex?%5Calpha%20%5Cin(0,1)%20%5Ctext%20%7B%20and%20%7D%20%5Cbeta%20%5Cin%5B0,%20%5Cinfty)">. This <img src="https://latex.codecogs.com/png.latex?%5Calpha"> controls how likely a terminal node is to be split, with smaller values indicating a lower likelihood of split, and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> controls the number of terminal nodes, larger <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> implying more nodes. The second prior on the trees is on the distribution used to choose which covariate is split on. The final prior on the trees is on the value of the chosen splitting covariate at which to split. For both these later parameters, a common choice (and the one dbarts makes) is a simple discrete uniform distribution.</p>
<p>This set of priors also requires choosing the <img src="https://latex.codecogs.com/png.latex?m,%20%5Calpha,%20%5Cbeta,%20%5Cmu_%7B%5Cmu%7D,%20%5Csigma,%20%5Cnu"> and <img src="https://latex.codecogs.com/png.latex?%5Clambda"> hyperparameters, which can be chosen via cross-validation or simply set to defaults. In general, the past literature on BART finds that the defaults developed by Mculloch work quite well in a surprisingly large number of contexts <a href="https://deepblue.lib.umich.edu/handle/2027.42/147594">(Tan, 2018)</a>. More specifically in the MRP context, both <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12361">Montgomery &amp; Olivella (2018)</a> and <a href="https://www.cambridge.org/core/journals/american-political-science-review/article/barp-improving-mister-p-using-bayesian-additive-regression-trees/630866EB47F9366EDB3C22CFD951BB6F">Bisbee (2019)</a> found little reason to utilize non-default hyperparameter choices after reasonable search. For completeness, however, I ran a small number of hyperparameter searches on my complete records data, as recommended by the author of the <a href="https://cran.r-project.org/web/packages/dbarts/index.html">dbarts</a> package. Similar to prior work, I found little reason to diverge from the defaults suggested by Chipman et al, and implemented in dbarts, although I did ultimately go with <img src="https://latex.codecogs.com/png.latex?m%20=%20200"> trees as Chipman et al.&nbsp;suggest, not the <img src="https://latex.codecogs.com/png.latex?m%20=%2075"> default in dbarts. For a full derivation of these choices and their resultant properties, see <a href="https://arxiv.org/abs/0806.3286">Chipman et al.&nbsp;(2008)</a> or <a href="https://deepblue.lib.umich.edu/handle/2027.42/147594">(Tan, 2018)</a>.</p>
<p>A final modification of this formulation of BART is needed for binary outcomes. For binary outcomes, BART uses the probit link function to model the relationship between <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?y">:</p>
<p><img src="https://latex.codecogs.com/png.latex?P%5Cleft(Y_%7Bk%7D=1%20%7C%20%5Cmathbf%7BX%7D_%7Bk%7D%5Cright)=%5CPhi%5Cleft%5BG%5Cleft(%5Cmathbf%7BX%7D_%7Bk%7D%5Cright)%5Cright%5D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5CPhi%5B.%5D"> is the cumulative distribution function of a standard normal distribution, and <img src="https://latex.codecogs.com/png.latex?G"> is the full BART model we saw earlier. This slightly modifies steps for drawing from the posterior distribution, as discussed further in <a href="https://arxiv.org/abs/0806.3286">Chipman et al.&nbsp;(2008)</a>.</p>
</section>
<section id="bart-vi" class="level2">
<h2 class="anchored" data-anchor-id="bart-vi">BART-vi</h2>
<p>To the best of my knowledge, no prior work has utilized BART with varying intercepts (BART-vi) in the MRP framework. Given the huge amount of prior work on MRP that leverages a large set of varying intercepts, this seems like a natural extension. This modifies the BART predictor for binary outcomes to</p>
<p><img src="https://latex.codecogs.com/png.latex?P%5Cleft(Y_%7Bk%7D=1%20%7C%20%5Cmathbf%7BX%7D_%7Bk%7D%5Cright)=%5CPhi%5Cleft%5BG%5Cleft(%5Cmathbf%7BX%7D_%7Bk%7D%5Cright)%20+%20%5Calpha_%7Bk%7D%5Cright%5D"></p>
<p>with <img src="https://latex.codecogs.com/png.latex?a_%7Bk%7D%20%5Csim%20N%5Cleft(0,%20%5Ctau%5E%7B2%7D%5Cright)">. Critically, this also removes the varying intercept variable from the choice of possible features to split on, modeling it purely as a varying intercept. Given that the <a href="https://cran.r-project.org/web/packages/dbarts/index.html">dbarts</a> package which I use currently only supports 1 varying intercept, the natural choice is the state variable, as it both has the most categories and is one of the original motivations for varying intercepts in MRP work. All the old priors and hyperparameters remain the same, and dbarts places an additional cauchy prior on <img src="https://latex.codecogs.com/png.latex?%5Ctau%5E%7B2%7D">. While this cauchy prior is much less informative than the half-t, half-normal, or other priors typically used for MRP, at this time it is not possible to modify the prior choice except to a gamma distribution which is also not ideal. Future work could consider fitting this type of model with the more informative priors favored by the MRP literature for random effects, although such an improvement would require a time investment in learning to modify the c++ codebase of dbarts.</p>
</section>
<section id="comparing-the-predictions-of-the-two" class="level2">
<h2 class="anchored" data-anchor-id="comparing-the-predictions-of-the-two">Comparing the predictions of the two</h2>
<p>I provide two forms of evaluation for BART-vi vs regular BART, a quantitative assessment based on 10-fold cross validation, and a graphical/qualitative comparison of state level estimates resulting from the two.</p>
<p>To test the performance of this modification, I fit BART with and without varying intercept on state to the same <img src="https://latex.codecogs.com/png.latex?m%20=%2050"> imputed<sup>1</sup> datasets, using 10-fold cross validation within each dataset. Overall, while both models are extremely accurate, the BART-vi model slightly outperforms the regular BART model without varying intercepts in terms of RMSE, MSE, and AUC on average. Of course, given that this is a test of predictive accuracy before the final poststratification, this isn’t a full validation of BART-vi’s predictive superiority in the MRP context. However, this is consistent with <a href="https://deepblue.lib.umich.edu/handle/2027.42/147594">(Tan, 2018)</a>’s result in a more extensive set of simulation studies that there are small gains in accuracy to be had with BART-vi when random effects are used with an appropriate grouping variable. To make such a comparison completely rigorously, one would need to fit both types of models on a dataset with a ground truth such as vote share, poststratify, and then contrast their properties relative to that ground truth, not simply compare predictive accuracy on the initial set of surveys. However, as this is not possible for the border wall question, I take this as a rough suggestion that BART-vi may preform better in my context, and possibly in others.</p>
<p>Plotting a comparison of the state-level median prediction from the two models after poststratification shows a familiar pattern of pooling. The BART-vi estimates are pulled somewhat towards the grand mean, whereas the ones without varying intercepts are a bit more spread out. Note, however, that we don’t see the sort of <a href="https://twitter.com/rlmcelreath/status/878268413952634880/photo/1">idealized pooling</a> trend often shown in textbook examples of multilevel models, with non-multilevel predictions that are uniformly higher above the grand mean and uniformly lower below it compared to the multilevel predictions. This is due to the simple BART model modeling much more complex interactions based on the state variables than a single level regression.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/BART VI/bart-compare.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">BART models with and without pooling</figcaption><p></p>
</figure>
</div>
<p>One particularly interesting qualitative example to illustrate the differences between the models is that of DC, which is both a small state, and an incredibly liberal one. This presents a dilemma from the perspective of varying intercepts pooling: on the one hand, with only 94 observations in the full data, we should want some pooling on DC’s estimate. On the other, DC genuinely is exceptionally liberal, which suggests that pooling it too much could hurt predictive performance. While both already somewhat regularize the 13% raw approval for the wall in our aggregated polls, BART-vi does so much more. Thus, while the average predictions are of higher quality with BART-vi, the DC and other extreme state predictions are superior without random effects. Most prior MRP work has been happy to make this sort of trade off, and based on the rough accuracy comparisons I’ve made, this appears to work well for my data and my BART model as well.</p>
<p>Comparing the full posterior distributions of the two models below, we can also see BART-vi has noticably wider 50 and 90% intervals as well (the dot indicates the median, the thick bar is the 50% interval, and the thinnest bar is the 90% one). Like with my CV testing, a complete sense of which level of uncertainty provided here is appropriate will have to wait for future MRP work that leverages data with a ground truth. However, in many cases, the fixed effects intervals border on what I’d call concerningly small- I wouldn’t be suprised if the coverage properties of the BART-vi intervals are better.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/BART VI/full-post.jpeg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Full Posterior of the Two Models</figcaption><p></p>
</figure>
</div>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next steps</h2>
<p>Given that my work shows BART-vi having some desirable properties for RRP, what might be some extensions to explore next?</p>
<p>A first obvious step might be explore this type of model on data where we do have a ground truth like voter turnout, or vote share. For a more extensive comparison, one could leverage Bisbee (2019)’s replication data, which would hopefully provide a more complete answer to whether this strategy works well in general.</p>
<p>Probably the most theoretically interesting question would be how to handle the possibility of multiple random intercepts, if dbarts or another package eventually implements them. This represents a tradeoff between the benefits of flexible Bayesian non-parametrics in BART, and the pooling behavior of varying intercepts. Initially, I thought it was entirely feasible that the BART-vi I fit would have worse predictive accuracy, given the potential benefits of splitting on state. However, given that I utilize both 2012 vote share and region as predictors, it seems that the model still had ample state level information. However, as we pooled across more variables, this would increasingly weaken the non-parametric component of the BART-vi model. In this scenario, would pooling across demographic predictors that have many fewer categories make sense? While future work will have to tell, my guess is that the answer might be that only state or other geographic variables benefit from pooling.</p>
<hr>
<p><a name="imputationnote">1</a>: Given my data had a relatively large proportion of respondents who refused to answer at least 1 demographic question (10.54%), I also explored imputing the missing characteristics using a variety of different approaches. The full details of that are coming in another post, but I ran 10-fold CV on the imputations so that the evaluation would more fully mirror my final modeling scenario.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2019,
  author = {Andy Timm},
  title = {BART with Varying Intercepts in the {MRP} Framework},
  date = {2019-07-03},
  url = {https://andytimm.github.io/2020-03-06-BART-vi.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2019" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2019. <span>“BART with Varying Intercepts in the MRP
Framework.”</span> July 3, 2019. <a href="https://andytimm.github.io/2020-03-06-BART-vi.html">https://andytimm.github.io/2020-03-06-BART-vi.html</a>.
</div></div></section></div> ]]></description>
  <category>From Old Website</category>
  <category>MRP</category>
  <category>BART</category>
  <guid>https://andytimm.github.io/posts/BART VI/2020-03-06-BART-vi.html</guid>
  <pubDate>Wed, 03 Jul 2019 04:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/BART VI/bart-compare.png" medium="image" type="image/png" height="90" width="144"/>
</item>
<item>
  <title>Convention Prediction with a Bayesian Hierarchical Multinomial Model</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Convention Model/2019-07-13-convention-model.html</link>
  <description><![CDATA[ 




<p>Here, I use a Bayesian hierarchical multinomial model to predict the first ballot results at the 2018 DFL (Democratic) State Convention, with data aggregated to the Party Unit level (ex: State Senate district) to guarantee anonymity. While using aggregated data obviously isn’t ideal, this sort of strategy shows a lot of promise, especially if individual level predictors could be harnessed as another level of the hierarchical model. As it stands, this is mostly a proof of concept for Bayesian hierarchical models in this context. To use something like this in practice, one could use prior predictive simulation to game out the convention under various assumptions, or condition on the first ballot data and use it to analyze trends in support and predict subsequent ballots as your floor team collects further data.</p>
<p>At this convention, I was working for Erin Murphy (who ultimately won) on her data team, and so I have access to their database on the Delegates heading into the convention. A winner is declared if any candidate reaches 60% of the Delegate pool of 1307.5 Delegates, so 785 votes. For simplicity, I focus in this project solely on predicting the first ballot results.</p>
<!--more-->
<p>Briefly, my goals with this project are:</p>
<ol type="1">
<li>Represent my pre-convention uncertainty about outcomes given the data we have intelligently.</li>
<li>See how much we can improve predictions by exploiting the hierarchical nature of districts (Party Units within Congressional Districts).</li>
<li>See if this makes sense as a modeling strategy to keep developing by adding the individual level data.</li>
</ol>
<section id="the-dataset" class="level1">
<h1>The Dataset</h1>
<p>For each of the non-empty 121 Party Units in Minnesota, my response is the number of Delegate that each candidate (Erin Murphy, Rebecca Otto, and Tim Walz) received, along with a count for “No Endorsement” voters. Thus, summing across the 121 PUs would give the full first ballot results by candidate. I thus model these using a <strong>multinomial logit model</strong> in brms.</p>
<p>In terms of predictors, we have the Congressional district each party unit is in, which should explain some variation, as Otto/Walz are generally perceived to be more appealing to rural voters, while Murphy was from the Twin Cities. I also have the estimated proportion of Delegates the Murphy campaign believed they had the support of in each Party Unit, based on their field campaign, and on the subcaucuses the Delegates were elected out of. These proportions turned out to be quite accurate, and so my assumption is they’ll be strong predictors. Using the delegate level data (including issue and candidate IDs, subcaucuses, and voter file information like gender and age) would no doubt significantly improve things.</p>
<p>I exclude the data cleaning here, but it’s available in the .Rmd on my <a href="https://github.com/andytimm/ConventionPrediction/blob/master/Convention_Prediction_Final.Rmd">github</a>. One non-obvious transformation I make is to double all counts before modeling, but halve them before analysis, as some rural, low population areas are awarded “half delegates”, and I need integer outcomes to work with a multinomial model.</p>
</section>
<section id="prior-predictive-simulation-intercept-only" class="level1">
<h1>Prior Predictive Simulation: Intercept Only</h1>
<p>I start with an intercept only model, and plot realizations of the first ballot across draws. This helps explain my level of uncertainty before we include predictors and condition on the data. To keep this post short, I only include the final result of iterating to find suitably cautious priors at this early stage, not as I add further predictors to the model. With a multinomial model like this, even just enforcing the count constraint (vote counts in each party unit have to add to their total allocation of delegates) already produces a suprisingly reasonable model.</p>
<p>For context on these priors, there was a relatively large amount of uncertainty for our campaign and all the campaigns heading into first ballot for a variety of reasons. First, we had only ID’d about 2/3 of the delegate body by first ballot. Second, it was becoming increasingly clear that Rebecca Otto didn’t have the delegates to win the convention, so it was possible we’d see a decent portion of her delegates switch sides even before first ballot. Finally, a major statewide c4, ISAIAH, had been telling their delegates to hold off on committing, but the rumor was that they were going to endorse Erin Murphy’s or Otto’s campaign (whichever progressive was more viable), so a large number of Delegates had a preference they didn’t openly state.</p>
<p>All that said, while it was highly unlikely that anyone was going to reach a winning 60% of the delegate pool (785 Delegates) on the first ballot, I did want at least a bit of probability on those outcomes. From our ID data, it looked something like 40%-20%-40% was the most likely outcome for the first ballot, which is how I set the means.</p>
<p>As a final note, voting “No Endorsement” on an early ballot is extremely rare, which is correctly reflected.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="co" style="color: #5E5E5E;"># Base class is No-Endorsement</span></span>
<span id="cb1-2">initial_prior <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="fu" style="color: #4758AB;">prior</span>(<span class="fu" style="color: #4758AB;">normal</span>(<span class="dv" style="color: #AD0000;">4</span>,.<span class="dv" style="color: #AD0000;">25</span>), <span class="at" style="color: #657422;">class =</span> <span class="st" style="color: #20794D;">"Intercept"</span>, <span class="at" style="color: #657422;">dpar =</span> <span class="st" style="color: #20794D;">"mu2"</span>),</span>
<span id="cb1-3">              <span class="fu" style="color: #4758AB;">prior</span>(<span class="fu" style="color: #4758AB;">normal</span>(<span class="dv" style="color: #AD0000;">3</span>,.<span class="dv" style="color: #AD0000;">5</span>), <span class="at" style="color: #657422;">class =</span> <span class="st" style="color: #20794D;">"Intercept"</span>, <span class="at" style="color: #657422;">dpar =</span> <span class="st" style="color: #20794D;">"mu3"</span>),</span>
<span id="cb1-4">              <span class="fu" style="color: #4758AB;">prior</span>(<span class="fu" style="color: #4758AB;">normal</span>(<span class="dv" style="color: #AD0000;">4</span>,.<span class="dv" style="color: #AD0000;">25</span>), <span class="at" style="color: #657422;">class =</span> <span class="st" style="color: #20794D;">"Intercept"</span>, <span class="at" style="color: #657422;">dpar =</span> <span class="st" style="color: #20794D;">"mu4"</span>)</span>
<span id="cb1-5">              )</span>
<span id="cb1-6"></span>
<span id="cb1-7">int_only <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">brm</span>(<span class="fu" style="color: #4758AB;">bf</span>(y <span class="sc" style="color: #5E5E5E;">|</span> <span class="fu" style="color: #4758AB;">trials</span>(Total) <span class="sc" style="color: #5E5E5E;">~</span> <span class="dv" style="color: #AD0000;">1</span>), <span class="at" style="color: #657422;">family=</span><span class="fu" style="color: #4758AB;">multinomial</span>(),<span class="at" style="color: #657422;">data=</span>erin_data, <span class="at" style="color: #657422;">prior =</span> initial_prior, <span class="at" style="color: #657422;">sample_prior=</span><span class="st" style="color: #20794D;">"only"</span>)</span>
<span id="cb1-8"></span>
<span id="cb1-9">linpred <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">posterior_linpred</span>(int_only, <span class="at" style="color: #657422;">transform =</span> T)</span>
<span id="cb1-10"></span>
<span id="cb1-11"><span class="co" style="color: #5E5E5E;"># Divide by 2 after summing the totals to put back on original delegate count scale</span></span>
<span id="cb1-12"><span class="fu" style="color: #4758AB;">boxplot</span>(<span class="fu" style="color: #4758AB;">apply</span>(linpred, <span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">3</span>), <span class="at" style="color: #657422;">FUN =</span> sumdiv2), <span class="at" style="color: #657422;">pch =</span> <span class="st" style="color: #20794D;">"."</span>, <span class="at" style="color: #657422;">las =</span> <span class="dv" style="color: #AD0000;">1</span>,</span>
<span id="cb1-13">        <span class="at" style="color: #657422;">names =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"No Endorse"</span>, <span class="st" style="color: #20794D;">"Erin Murphy"</span>, <span class="st" style="color: #20794D;">"Rebeca Otto"</span>, <span class="st" style="color: #20794D;">"Tim Walz"</span>))</span></code></pre></div>
<p><img src="https://andytimm.github.io/posts/Convention Model/unnamed-chunk-2-1.png" class="img-fluid" alt="first test"><!-- --></p>
</section>
<section id="initial-posterior" class="level1">
<h1>Initial Posterior</h1>
<p>Now let’s add in our non-CD predictors and condition on our data. I add a <img src="https://latex.codecogs.com/png.latex?N(0,3)"> prior over all the <img src="https://latex.codecogs.com/png.latex?%5Cbeta">’s as a weakly informative prior to help the model fit. The model fits well; there are no divergences, all <img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D"> were 1, and I got a good number of effective samples for each parameter.</p>
<p>As we’d expect with so little data, the standard errors are very large compared to the coefficients, but generally point the right ways- the Strong/Lean Erin predictions have a positive influence on Erin’s support (mu2), for example, and similar with Walz (mu4), and Otto (mu2). Later, when I look at marginal plots, I’ll talk more about some of the more interesting coefficients, namely the ISAIAH and Unknown ones.</p>
<pre><code>##  Family: multinomial
##   Links: mu2 = logit; mu3 = logit; mu4 = logit
## Formula: y | trials(Total) ~ ISAIAH + Lean.Erin + Lean.Walz + Lean.Otto + Strong.Erin + Strong.Otto + Strong.Walz + Undecided + Unknown
##    Data: erin_data (Number of observations: 121)
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
##
## Population-Level Effects:
##                 Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## mu2_Intercept       4.15      1.07     2.07     6.27       4931 1.00
## mu3_Intercept       3.40      1.06     1.40     5.51       5520 1.00
## mu4_Intercept       4.19      1.03     2.14     6.23       5352 1.00
## mu2_ISAIAH          1.59      1.68    -1.71     4.96       5557 1.00
## mu2_Lean.Erin       1.00      1.95    -2.92     4.80       5198 1.00
## mu2_Lean.Walz       0.24      1.99    -3.72     4.10       6179 1.00
## mu2_Lean.Otto      -1.74      2.08    -5.67     2.44       5975 1.00
## mu2_Strong.Erin     2.18      1.69    -1.17     5.52       5448 1.00
## mu2_Strong.Otto    -1.91      1.71    -5.23     1.54       6238 1.00
## mu2_Strong.Walz    -1.70      1.65    -5.03     1.63       5958 1.00
## mu2_Undecided      -0.53      1.78    -4.15     3.03       5513 1.00
## mu2_Unknown         0.11      1.55    -2.94     3.17       5267 1.00
## mu3_ISAIAH         -0.64      1.74    -3.97     2.82       5673 1.00
## mu3_Lean.Erin       0.73      2.03    -3.34     4.69       4694 1.00
## mu3_Lean.Walz      -1.27      2.10    -5.35     2.89       6218 1.00
## mu3_Lean.Otto       3.26      2.14    -0.99     7.40       5679 1.00
## mu3_Strong.Erin    -1.22      1.68    -4.51     2.08       5648 1.00
## mu3_Strong.Otto     4.28      1.70     0.94     7.74       5737 1.00
## mu3_Strong.Walz    -2.46      1.66    -5.60     0.85       6205 1.00
## mu3_Undecided       0.63      1.74    -2.81     4.06       5264 1.00
## mu3_Unknown         0.42      1.56    -2.56     3.45       6235 1.00
## mu4_ISAIAH         -1.86      1.68    -5.10     1.57       5066 1.00
## mu4_Lean.Erin      -0.39      1.97    -4.21     3.44       5386 1.00
## mu4_Lean.Walz       1.51      1.98    -2.32     5.48       6163 1.00
## mu4_Lean.Otto      -1.28      2.09    -5.41     2.80       6302 1.00
## mu4_Strong.Erin    -1.48      1.68    -4.67     1.90       5654 1.00
## mu4_Strong.Otto    -2.00      1.70    -5.33     1.47       6029 1.00
## mu4_Strong.Walz     1.46      1.60    -1.65     4.61       5859 1.00
## mu4_Undecided       0.19      1.72    -3.22     3.59       5522 1.00
## mu4_Unknown         1.08      1.51    -1.94     4.07       6072 1.00
##
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample
## is a crude measure of effective sample size, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</section>
<section id="nesting-the-party-units-in-cd" class="level1">
<h1>Nesting the Party Units in CD</h1>
<p>A next logical step for the model would be to incorporate the hierarchical structure present in the data- Party Units nested within Congressional Districts. Given that the CD’s reflect both the progressive/moderate and rural/urban divides that defined the election, expecting some significant between group variation is reasonable.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">final_prior <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="fu" style="color: #4758AB;">prior</span>(<span class="fu" style="color: #4758AB;">normal</span>(<span class="dv" style="color: #AD0000;">4</span>,.<span class="dv" style="color: #AD0000;">25</span>), <span class="at" style="color: #657422;">class =</span> <span class="st" style="color: #20794D;">"Intercept"</span>, <span class="at" style="color: #657422;">dpar =</span> <span class="st" style="color: #20794D;">"mu2"</span>),</span>
<span id="cb3-2">              <span class="fu" style="color: #4758AB;">prior</span>(<span class="fu" style="color: #4758AB;">normal</span>(<span class="dv" style="color: #AD0000;">3</span>,.<span class="dv" style="color: #AD0000;">5</span>), <span class="at" style="color: #657422;">class =</span> <span class="st" style="color: #20794D;">"Intercept"</span>, <span class="at" style="color: #657422;">dpar =</span> <span class="st" style="color: #20794D;">"mu3"</span>),</span>
<span id="cb3-3">              <span class="fu" style="color: #4758AB;">prior</span>(<span class="fu" style="color: #4758AB;">normal</span>(<span class="dv" style="color: #AD0000;">4</span>,.<span class="dv" style="color: #AD0000;">25</span>), <span class="at" style="color: #657422;">class =</span> <span class="st" style="color: #20794D;">"Intercept"</span>, <span class="at" style="color: #657422;">dpar =</span> <span class="st" style="color: #20794D;">"mu4"</span>),</span>
<span id="cb3-4">              <span class="fu" style="color: #4758AB;">prior</span>(<span class="fu" style="color: #4758AB;">normal</span>(<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">3</span>),<span class="at" style="color: #657422;">class =</span> <span class="st" style="color: #20794D;">"b"</span>),</span>
<span id="cb3-5">              <span class="fu" style="color: #4758AB;">prior</span>(<span class="fu" style="color: #4758AB;">normal</span>(<span class="dv" style="color: #AD0000;">0</span>,.<span class="dv" style="color: #AD0000;">2</span>), <span class="at" style="color: #657422;">class =</span> <span class="st" style="color: #20794D;">"sd"</span>, <span class="at" style="color: #657422;">group =</span> <span class="st" style="color: #20794D;">"CD"</span>, <span class="at" style="color: #657422;">dpar =</span> <span class="st" style="color: #20794D;">"mu2"</span>),</span>
<span id="cb3-6">              <span class="fu" style="color: #4758AB;">prior</span>(<span class="fu" style="color: #4758AB;">normal</span>(<span class="dv" style="color: #AD0000;">0</span>,.<span class="dv" style="color: #AD0000;">2</span>), <span class="at" style="color: #657422;">class =</span> <span class="st" style="color: #20794D;">"sd"</span>, <span class="at" style="color: #657422;">group =</span> <span class="st" style="color: #20794D;">"CD"</span>, <span class="at" style="color: #657422;">dpar =</span> <span class="st" style="color: #20794D;">"mu3"</span>),</span>
<span id="cb3-7">              <span class="fu" style="color: #4758AB;">prior</span>(<span class="fu" style="color: #4758AB;">normal</span>(<span class="dv" style="color: #AD0000;">0</span>,.<span class="dv" style="color: #AD0000;">2</span>), <span class="at" style="color: #657422;">class =</span> <span class="st" style="color: #20794D;">"sd"</span>, <span class="at" style="color: #657422;">group =</span> <span class="st" style="color: #20794D;">"CD"</span>, <span class="at" style="color: #657422;">dpar =</span> <span class="st" style="color: #20794D;">"mu4"</span>)</span>
<span id="cb3-8">              )</span>
<span id="cb3-9"></span>
<span id="cb3-10">full_model <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">brm</span>(<span class="fu" style="color: #4758AB;">bf</span>(y <span class="sc" style="color: #5E5E5E;">|</span> <span class="fu" style="color: #4758AB;">trials</span>(Total) <span class="sc" style="color: #5E5E5E;">~</span> ISAIAH <span class="sc" style="color: #5E5E5E;">+</span> Lean.Erin <span class="sc" style="color: #5E5E5E;">+</span> Lean.Walz <span class="sc" style="color: #5E5E5E;">+</span> Lean.Otto <span class="sc" style="color: #5E5E5E;">+</span> Strong.Erin <span class="sc" style="color: #5E5E5E;">+</span> Strong.Otto <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb3-11">        Strong.Walz <span class="sc" style="color: #5E5E5E;">+</span> Undecided <span class="sc" style="color: #5E5E5E;">+</span> Unknown <span class="sc" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">|</span>CD)), <span class="at" style="color: #657422;">family=</span><span class="fu" style="color: #4758AB;">multinomial</span>(),<span class="at" style="color: #657422;">prior =</span> final_prior, <span class="at" style="color: #657422;">data=</span>erin_data)</span></code></pre></div>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><span class="fu" style="color: #4758AB;">summary</span>(full_model)</span></code></pre></div>
<pre><code>##  Family: multinomial
##   Links: mu2 = logit; mu3 = logit; mu4 = logit
## Formula: y | trials(Total) ~ ISAIAH + Lean.Erin + Lean.Walz + Lean.Otto + Strong.Erin + Strong.Otto + Strong.Walz + Undecided + Unknown + (1 | CD)
##    Data: erin_data (Number of observations: 121)
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
##
## Group-Level Effects:
## ~CD (Number of levels: 9)
##                   Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sd(mu2_Intercept)     0.08      0.06     0.00     0.23       2406 1.00
## sd(mu3_Intercept)     0.14      0.09     0.01     0.35       1760 1.00
## sd(mu4_Intercept)     0.09      0.07     0.00     0.24       2311 1.00
##
## Population-Level Effects:
##                 Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## mu2_Intercept       4.14      1.08     2.03     6.29       2895 1.00
## mu3_Intercept       3.33      1.09     1.24     5.46       2566 1.00
## mu4_Intercept       4.27      1.07     2.21     6.36       2919 1.00
## mu2_ISAIAH          1.69      1.66    -1.42     5.00       3289 1.00
## mu2_Lean.Erin       1.07      1.95    -2.71     4.96       3678 1.00
## mu2_Lean.Walz       0.17      2.06    -3.81     4.18       3957 1.00
## mu2_Lean.Otto      -1.38      2.10    -5.57     2.70       3972 1.00
## mu2_Strong.Erin     2.17      1.65    -1.03     5.52       3294 1.00
## mu2_Strong.Otto    -1.84      1.73    -5.14     1.55       3077 1.00
## mu2_Strong.Walz    -1.72      1.62    -4.81     1.40       3741 1.00
## mu2_Undecided      -0.52      1.80    -4.01     3.13       2834 1.00
## mu2_Unknown         0.11      1.53    -2.93     3.17       3079 1.00
## mu3_ISAIAH         -0.70      1.73    -4.01     2.66       2931 1.00
## mu3_Lean.Erin       0.68      2.05    -3.29     4.62       4101 1.00
## mu3_Lean.Walz      -1.01      2.11    -5.12     3.11       4077 1.00
## mu3_Lean.Otto       3.16      2.17    -1.11     7.43       4086 1.00
## mu3_Strong.Erin    -1.15      1.69    -4.40     2.23       3109 1.00
## mu3_Strong.Otto     4.15      1.76     0.80     7.64       2990 1.00
## mu3_Strong.Walz    -2.19      1.62    -5.38     0.99       3156 1.00
## mu3_Undecided       0.62      1.83    -2.95     4.16       3098 1.00
## mu3_Unknown         0.47      1.55    -2.56     3.48       2478 1.00
## mu4_ISAIAH         -1.91      1.73    -5.23     1.47       3164 1.00
## mu4_Lean.Erin      -0.42      1.98    -4.21     3.45       3632 1.00
## mu4_Lean.Walz       1.34      2.02    -2.61     5.35       3723 1.00
## mu4_Lean.Otto      -1.47      2.09    -5.46     2.66       3452 1.00
## mu4_Strong.Erin    -1.57      1.63    -4.74     1.57       3367 1.00
## mu4_Strong.Otto    -1.91      1.76    -5.34     1.64       2914 1.00
## mu4_Strong.Walz     1.38      1.58    -1.84     4.50       3574 1.00
## mu4_Undecided      -0.01      1.80    -3.44     3.57       3232 1.00
## mu4_Unknown         0.99      1.54    -2.00     3.99       3137 1.00
##
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample
## is a crude measure of effective sample size, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</section>
<section id="model-comparison" class="level1">
<h1>Model Comparison</h1>
<p>Intuitively, a model with pooling across CD’s should outperform a single level model using them, but let’s make sure. Below, the multilevel level model far outperforms the single level one, with it’s ELPD more than 2 standard errors better.</p>
<p>One limitation of the analysis below is that I wasn’t able refit without the final observation (the super delegates) to calculate ELPDs for the super delegates directly. This is because the “Super” level only exists in 1 example, and the loo package doesn’t allow new factor levels- holding it out would thus throw an error.</p>
<pre><code>##                  elpd_diff se_diff
## full_model         0.0       0.0
## not_pooled_model -10.7       4.9

## Method: stacking
## ------
##        weight
## model1 1.000
## model2 0.000</code></pre>
</section>
<section id="interesting-marginal-plots" class="level1">
<h1>Interesting Marginal Plots</h1>
<p>For the most part, the marginal plots were what I’d expect- for instance, PUs with a greater estimated strong support for Erin (“Strong Erin”), had increasingly greater support for Erin, and vice versa for Walz, and so I don’t show most of these.</p>
<p>Even though the predictions are obviously very noisy however, they did pick up on two important, more subtle trends. First, in the ISAIAH plot below, it’s beginning to appear that Murphy (2) does well in places with many ISAIAH delegates, whereas Walz (4) does progressively worse.</p>
<p><img src="https://andytimm.github.io/posts/Convention Model/unnamed-chunk-6-1.png" class="img-fluid"><!-- --></p>
<p>The other interesting trend the model picked up on was that it tends to be harder to ID your opponent’s supporters than your own- as your supporters want to contact and work with the campaign, but the opponents’ have no reason to do so, and help their candidate by not giving you much information. This is correctly reflected in the negative slope in Unknown for Erin (2), but positive one for Walz (4), given the predictor data comes from the Murphy campaign.</p>
<p>While these plots suggest a understandably high level of uncertainty, the fact that they’re correctly reflecting many of the relationships I believe to be true offers some level of face validity of the model.</p>
<p><img src="https://andytimm.github.io/posts/Convention Model/unnamed-chunk-7-1.png" class="img-fluid" alt="Marginal_Plot"><!-- --></p>
</section>
<section id="posterior-predictive-check" class="level1">
<h1>Posterior Predictive Check</h1>
<p>Plotting the predictions for Erin Murphy (blue) and Tim Walz (red) against the actual results, we can see the predictions track reasonably closely considering the limited data. While many point predictions fall outside the 25-75 quantile range, the vast majority stay within the boxplot’s whiskers. Again, stressing the limitations of the small dataset we have, this is a fairly reasonable range of outcomes to predict, and there’s no systematic pattern I can see to which Party Units the model struggles with.</p>
<p><img src="https://andytimm.github.io/posts/Convention Model/unnamed-chunk-8-1.png" class="img-fluid" alt="Murphy"><!-- --><img src="https://andytimm.github.io/posts/Convention Model/unnamed-chunk-8-2.png" class="img-fluid" alt="Walz"><!-- --></p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>To actually use a model like this on a convention floor, I’d definitely want to fully incorporate the individual level data that underlie what’s shown here, as attempting to model with 1 obs/district is challenging. However, this initial model is fairly promising- nesting PUs within CDs seems like a strong overall strategy, and even with limited data, the model already can pick up on relationships like that between estimated ISAIAH and Unknown support and Delegate returns. Some further details can be found in the .rmd <a href="https://github.com/andytimm/ConventionPrediction/blob/master/Convention_Prediction_Final.Rmd">here</a>.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2019,
  author = {Andy Timm},
  title = {Convention {Prediction} with a {Bayesian} {Hierarchical}
    {Multinomial} {Model}},
  date = {2019-07-03},
  url = {https://andytimm.github.io/2019-07-13-convention-model.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2019" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2019. <span>“Convention Prediction with a Bayesian
Hierarchical Multinomial Model.”</span> July 3, 2019. <a href="https://andytimm.github.io/2019-07-13-convention-model.html">https://andytimm.github.io/2019-07-13-convention-model.html</a>.
</div></div></section></div> ]]></description>
  <category>From Old Website</category>
  <category>Stan</category>
  <guid>https://andytimm.github.io/posts/Convention Model/2019-07-13-convention-model.html</guid>
  <pubDate>Wed, 03 Jul 2019 04:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Convention Model/unnamed-chunk-2-1.png" medium="image" type="image/png" height="103" width="144"/>
</item>
<item>
  <title>Is Voting Habit Forming? Replication, and additional robustness checks</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html</link>
  <description><![CDATA[ 




<p>This post walks through my replication of the fuzzy regression discontinuity portion of Coppock and Green’s 2016 paper <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12210">Is Voting Habit Forming? New Evidence from Experiments and Regression Discontinuities</a>, and details some additional robustness checks I conducted. While I was able to reproduce all of their estimates fairly easily due to great <a href="http://dx.doi.org/10.7910/DVN/ALZVAW">replication materials</a>, my additional robustness checks suggest that their results are more sensitive to bandwith choices than their testing suggests. Additionally, Coppock and Green argue the effects they find are likely due to habit alone, whereas I’m unconvinced that’s the sole mechanism involved. This is my work from Jennifer Hill and Joe Robinson-Cimpian’s Causal Inference class at NYU, and I’m grateful for both their feedback on the project.</p>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>When we first vote in an election, is that experience habit forming? If so, how strong is the effect of habit? This is a critical question of voting behavior. For example, it has implications for attempting to make the electorate more representative of the general population. If people continue to vote at a reasonable rate after their first ballot cast, then the burden of organizations doing GOTV work gets comparatively lighter; further, their work has dividends for elections to come. On the extreme other end, if almost no habit forming effect existed, then turnout work would be a project of individual races. Fortunately, practical experience from campaigns suggests there’s likely at least some habit effect. After all, the best predictor of voting in the current election is voting in the prior.</p>
<p>If we believe that there may be a habit effect on voting in an initial in future elections, how can we estimate a causal effect? Of course, we cannot use a randomized experiment: we cannot ethically randomize some citizens to vote, or more concerningly, randomize some citizens to not do so. Thus, the main observational strategies used to estimate such causal effects involve instrumental variable approaches, utilizing randomized experiments in the “upstream” election which attempt to mobilize voters in the treatment group as an instrument to estimate the Complier Average Causal Effect (CACE) in subsequent “downstream” elections. For example, <a href="https://isps.yale.edu/research/publications/isps12-024">Bedolla and Michelson (2012)</a>, utilizing a series of experiments that aimed to improve turnout in minority voters in California, find overall that voting in the upstream election results in a 23-percentage point increase in probability to vote in subsequent elections for compliers. However, these designs are frequently limited by somewhat weak instruments and low overall sample sizes, as increasing turnout through campaign intervention is extremely difficult and expensive per voter reached, especially in non-white and younger populations <a href="https://www.brookings.edu/book/get-out-the-vote-2/">(Gerber &amp; Green, 2012)</a>. The weakness of these instruments has prompted recent research using fuzzy regression discontinuity (FRD) designs, leveraging the fact that being just barely 18 or just too young to vote on the upstream election day should set voters on very different voting trajectories if such a habit effect exists <a href="https://www.sas.upenn.edu/~marcmere/workingpapers/PersistenceParticipation.pdf">(Meredith 2009</a>; <a href="https://www.tandfonline.com/doi/full/10.1080/17457289.2012.718280">Dinas (2012)</a>.</p>
<p>The latest and most comprehensive such fuzzy regression discontinuity paper is Coppock and Green’s 2016 paper <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12210">Is Voting Habit Forming? New Evidence from Experiments and Regression Discontinuities</a>, which considerably expands both the amount of data used and sophistication of modeling design used. Beyond just using the FRD design, they also incorporate results from several instrumental variable approaches in their very impressive paper.</p>
</section>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">Data</h2>
<p>States are required by the Help America Vote Act to make available to the public individual level data on every registered voter in their state, although states vary considerably in how much information about each voter they provide. For example, some states provide complete histories of which election a voter has participated in, whereas others provide only the most recent 8 elections. Similarly, some states provide birthdate, whereas others provide only age, or age buckets.</p>
<p>Given that the analysis they propose requires precise voting histories and a specific birthdate, Coppock and Green gathered the full voter file in 2013 from the 15 states where all three of</p>
<ol type="1">
<li>a complete history of which elections the individual had voted in,</li>
<li>information on that voter’s eligibility to vote (to rule out the otherwise ineligible such as felons), and</li>
<li>birthdate were available.</li>
</ol>
<p>In the replication file, these 15 voter files have been grouped by birthdate cohort and state, so that the unit of analysis is a group such as registered voters born on 11/6/1998, in Arkansas. This grouping sidesteps the potential problems arising from the fact the voter file only includes people registered to vote. While there is not a complete list of just eligible and just ineligible 17 and 18-year olds, through this cohort grouping, we can work with the 2008/2012 votes cast by cohorts above and below the eligibility threshold instead. Unfortunately, this also adds some additional complexity to interpreting results, as most potential violations of the fuzzy regression discontinuity design assumptions would occur at the individual level. Further, demographic profiles of the cohorts aren’t available, limiting our set of possible confounders to work with. The full dataset has 172,616 state and birthdate state cohorts, but for the purpose of my analysis, I work with the 11,680 birthdate state cohorts whose birthday fall within 365 days of eligibility to vote in the 2008 presidential election.</p>
<p>My outcome of interest is the number of votes cast in the downstream election, the 2012 presidential general election. The “treatment” is having voted in the upstream 2008 presidential election. The instrument is eligibility to participate in the 2008 election, which is determined by being 18 on election day, not 18 by the registration deadline as is sometimes commonly believed. The eligibility criteria being uniform across states greatly simplifies generating the remaining variables used. Due to this uniformity, the forcing variable is simply the cohort’s number of days above or below turning 18 on the upstream election day. Finally, to account for seasonal and day of the week birth trends which subsequently influence total votes cast by each cohort, Coppock and Green include a lagged downstream vote total for the birthdate cohort one year older. As an illustration of these quantities, here is an example row:</p>
<table class="table">
<colgroup>
<col style="width: 18%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 19%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Cohort</th>
<th>2008 Vote Total</th>
<th>2012 Vote Total</th>
<th>2008 Eligible?</th>
<th>Days to Eligible</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1988-11-06-AR</td>
<td>20</td>
<td>27</td>
<td>Yes</td>
<td>-364</td>
</tr>
</tbody>
</table>
<p>To show variation between states, I present below the total number of votes cast in each state for 2008 and 2012. As we’d expect with half the group ineligible in 2008, the vote totals rise considerably for 2012. The variation in state population carries through to the number of votes cast in each state in the sample, which will later influence the standard errors we are able to achieve. Overall, however, this is quite a large sample, both in the number of state-birthdate cohorts (11,680), and in total 2008/2012 votes cast (532,459 and 961,894 respectively).</p>
<table class="table">
<thead>
<tr class="header">
<th><strong>State</strong></th>
<th><strong>Sum 2008</strong></th>
<th><strong>Sum 2012</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AR</td>
<td>11,796</td>
<td>20,915</td>
</tr>
<tr class="even">
<td>CT</td>
<td>22,475</td>
<td>33,040</td>
</tr>
<tr class="odd">
<td>FL</td>
<td>88,704</td>
<td>173,999</td>
</tr>
<tr class="even">
<td>IA</td>
<td>5,336</td>
<td>10,600</td>
</tr>
<tr class="odd">
<td>IL</td>
<td>59,149</td>
<td>108,721</td>
</tr>
<tr class="even">
<td>KY</td>
<td>22,017</td>
<td>40,946</td>
</tr>
<tr class="odd">
<td>MO</td>
<td>35,603</td>
<td>63,381</td>
</tr>
<tr class="even">
<td>MT</td>
<td>6,782</td>
<td>11,030</td>
</tr>
<tr class="odd">
<td>NJ</td>
<td>52,125</td>
<td>87,395</td>
</tr>
<tr class="even">
<td>NV</td>
<td>12,494</td>
<td>24,502</td>
</tr>
<tr class="odd">
<td>NY</td>
<td>80,180</td>
<td>150,489</td>
</tr>
<tr class="even">
<td>OK</td>
<td>15,608</td>
<td>23,565</td>
</tr>
<tr class="odd">
<td>OR</td>
<td>17,900</td>
<td>36,843</td>
</tr>
<tr class="even">
<td>PA</td>
<td>95,412</td>
<td>165,622</td>
</tr>
<tr class="odd">
<td>RI</td>
<td>6,878</td>
<td>10,846</td>
</tr>
</tbody>
</table>
</section>
<section id="estimand" class="level2">
<h2 class="anchored" data-anchor-id="estimand">Estimand</h2>
<p>Given this data, we can estimate a Complier Average Causal Effect (CACE). Define <img src="https://latex.codecogs.com/png.latex?D"> to be the number of votes cast in the 2008 election, and <img src="https://latex.codecogs.com/png.latex?Y"> to be votes in the 20212 election. As a reminder, one cannot assign <img src="https://latex.codecogs.com/png.latex?D">, you can only leverage the encouragement to do so created by being just eligible, which I label <img src="https://latex.codecogs.com/png.latex?Z%20%5Cin%20%5B0,1%5D">, with 0 being too young, and 1 being old enough to vote in 2008 respectively. The forcing variable, days above or below being old enough to vote in 2008, I label <img src="https://latex.codecogs.com/png.latex?T">. <em>Lagged</em> refers to lagged downstream vote total for the birthdate cohort one year older, used to eliminate other temporal trends. For individual birthdate cohorts, I use subscript <img src="https://latex.codecogs.com/png.latex?i">’s.</p>
<p>The estimand is thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Clim%20_%7BT%20%5Cdownarrow%200%7D%20%5Csum_%7B1%7D%5E%7BN%7D%5Cleft%5BY_%7Bi%7D%20%7C%20D_%7B1%20i%7D=1%5Cright%5D-%20%5Clim%20_%7BT%20%5Cuparrow%200%7D%20%5Csum_%7B1%7D%5E%7BN%7D%5Cleft%5BY_%7Bi%7D%20%7C%20D_%7B1%20i%7D=0%5Cright%5D"></p>
<p>This is the additional number of votes in 2012 we would expect across cohorts if all voters did vote in 2008, beyond the number if each cohort’s subjects had not voted in 2008, among only the compliers, subjects who vote if and only if they receive the “encouragement” of being eligible in 2008. Following Coppock and Green, I present these as expected proportions of increase, ie .06 or 6% increase in 2012 turnout, as this makes it easier to compare amongst states with different vote totals. This must be restricted to compliers only because they are the only group in our analysis whose behavior changes just above or below being 18 on election day. For example, the encouragement provided by becoming eligible does not influence the behavior of “never takers”, those who would never vote. Our encouragement cannot create a difference in votes cast for this group or another other that is not the compliers: being just above or below the eligibility threshold only potentially alters the observed outcome of the compliers.</p>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">Methods</h2>
<p>To build up to the full fuzzy regression discontinuity method for estimating the CACE which comprises both an instrumental variable approach and a regression discontinuity, it is helpful to start by first introducing the instrumental variable approach to estimating the CACE in a simpler scenario, after which I will describe how utilizing the regression discontinuity modifies the problem.</p>
<p>For the moment, imagine if instead of a regression discontinuity, the encouragement to vote in 2008, <img src="https://latex.codecogs.com/png.latex?Z">, was a nonpartisan mailer encouraging voting. Further, voters were randomized into a control and treatment group, with some voters receiving the piece, while the other received either nothing or a placebo mailer. This is a classical randomized experiment, and if assumptions of SUTVA and ignorability hold, this allows us to estimate an average treatment effect. To allow us to estimate the effect of voting in 2008 (<img src="https://latex.codecogs.com/png.latex?D">) on voting in 2012 (<img src="https://latex.codecogs.com/png.latex?Y">) using this simple random experiment in the first election as an instrument, however, we need 3 additional assumptions. First, we must assume that any effect of the experimental encouragement on <img src="https://latex.codecogs.com/png.latex?Y"> operates only through <img src="https://latex.codecogs.com/png.latex?D">, referred to as excludability. This would be violated if the nonpartisan mailer in 2008 had such a profound effect on a voter that it still influenced the voter 4 years later, in 2012. However, as most campaign interventions exhibit relatively quick decay in effectiveness (<a href="https://www.brookings.edu/book/get-out-the-vote-2/">Gerber &amp; Green, 2012</a>), it is reasonable to make this assumption.</p>
<p>Second, we need to assume that the treatment is effective, and convinces some subjects who would have not have voted otherwise to vote. In other words, the experiment needs to generate at least some compliers, and more would improve our ability to estimate the CACE. Multiple studies have shown that such mail pieces are effective (<a href="https://www.brookings.edu/book/get-out-the-vote-2/">Gerber &amp; Green, 2012</a>), although the effect sizes are relatively weak.</p>
<p>Third, we need to make an assumption that our experimental encouragement creates no defiers, called the monotonicity assumption. To define defiers, partition the subjects into 4 groups, based on their potential outcomes arising from receiving the mail piece or not. “Always Takers” vote whether they receive the mailer or not: both potential outcomes have them voting. “Never Takers” are the reverse: they never vote regardless of the mailer. Compliers, as already discussed, are those who vote if they receive the encouragement, but don’t otherwise. Defiers invert the mailer’s intended influence, and vote only if not encouraged, refusing to vote if they are sent the mail piece. While one can easily imagine a registered voter frustrated with inundation of political mail, it seems unlikely that a single mail piece would entirely invert a subject’s attitudes towards voting.</p>
<p>Given this strong set of assumptions, it is finally possible to estimate the effect of <img src="https://latex.codecogs.com/png.latex?D"> on <img src="https://latex.codecogs.com/png.latex?Y">, through two stage least squares. First, regressing <img src="https://latex.codecogs.com/png.latex?D"> on <img src="https://latex.codecogs.com/png.latex?Z"> gives <img src="https://latex.codecogs.com/png.latex?E%5BD%20%5Cvert%20Z%5D=%5Calpha_%7B0%7D+%5Calpha_%7B1%7D%20Z+%5Cvarepsilon">. Then, using the predictions from the first stage, regressing <img src="https://latex.codecogs.com/png.latex?Y"> on <img src="https://latex.codecogs.com/png.latex?D"> gives <img src="https://latex.codecogs.com/png.latex?E%5BY%20%5Cvert%20D%5D=%5Cbeta_%7B0%7D+%5Ctau%20D+%5Cvarepsilon">, with tau being the instrumental variable estimate of voting in 2008’s effect on voting in 2012 for the compliers. Intuitively, we are utilizing the variation in 2008 turnout induced by the random experiment to isolate downstream variation in turnout for the compliers. This can be extended to include further covariates, however we will wait until the full fuzzy RD design to illustrate this.</p>
<p>In the full fuzzy regression discontinuity design, instead of the treatment being a randomly assigned treatment, Coppock and Green leverage the variability created by the fact that around election day, some subjects were just slightly too young or just old enough to vote. Informally, we replace the random assignment of an experiment in the upstream year with the as-if-random assignment of young voters near the age threshold to either ineligibility or eligibility to vote in 2008.</p>
<p>More formally, instead of the type of ignorability assumption of the random experiment, we require ignorability conditional on covariates within some bandwith of the eligibility cutoff, <img src="https://latex.codecogs.com/png.latex?Y(1),%20Y(0)%20%5Cperp%20Z%20%5Cvert%20x,%20x%20%5Cin(C-a,%20C+a)">. Also, we require that the eligibility cutoff and birthdates be determined independently of one another.</p>
<p>Finally, we must be able to model the two outcomes <img src="https://latex.codecogs.com/png.latex?Y(1)%20%5Cvert%20x,%20Y(0)%5Cvert%20x"> accurately in this region. One major challenge in modeling these are questions of how much of the data to use for estimation. While we can only estimate a causal effect right at the threshold (the “jump” created by being too young or just old enough), estimating the effects at that point can use different widths of data around the disconintuity.</p>
<p>Leaving further discussion of whether these assumptions are reasonable with Coppock and Green’s data to the next section, we can use a similar set of 2SLS regressions to estimate:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A&amp;D=%5Calpha_%7B0%7D+%5Calpha_%7B1%7D%20Z+%5Calpha_%7B2%7D%20T+%5Calpha_%7B3%7D%20T%20*%20Z+%5Calpha_%7B4%7D%20%5Ctext%20%7BLagged%7D+%5Cvarepsilon%5C%5C%0A&amp;Y=%5Cbeta_%7B0%7D+%5Cbeta_%7B1%7D%20D+%5Cbeta_%7B2%7D%20T+%5Cbeta_%7B3%7D%20T%20*%20D+%5Cbeta_%7B4%7D%20%5Ctext%20%7BLagged%7D+%5Cvarepsilon%0A%5Cend%7Baligned%7D"></p>
<p>With our fuzzy regression discontinuity estimate of the CACE as <img src="https://latex.codecogs.com/png.latex?%5Cbeta_%7B1%7D">, which is calculated separately for each state. <img src="https://latex.codecogs.com/png.latex?T"> is centered at 0, removing the need to complicate this definition by including the interaction term.</p>
<p>What properties does this CACE have? Given that is calculated using only the compliers not the full sample, its standard errors are much larger than the ITT ones we might expect from a simple random experiment, if we could run one in this scenario. The CACE also encompasses the full causal process that is set in motion by the upstream election – even effects resulting from voting in intermediate elections, or possible additional campaign outreach due to voting in 2008, although we cannot tease such effects apart. Despite these problems, the CACE from this fuzzy regression discontinuity has the potential to have much smaller standard errors than leveraging an upstream randomized experiment, given that the quasi-treatment is applied across the entire voter file, not just who an experiment would target.</p>
<p>As an extension to the methods used by Coppock and Green, I also consider bandwith selection algorithms for the regression discontinuity, the results of which I will discuss with the diagnostics. These tools attempt to find a bandwith that is optimal to some criterion, seeking to reduce the researcher degrees of freedom available in setting many possible bandwiths. For example, one such metric would be finding the bandwith that minimizes an approximation to the mean squared error (MSE) in estimating tau <a href="https://www.nber.org/papers/w14726">(Imbens and Kalyanaraman, 2009)</a>.</p>
</section>
<section id="assumptions" class="level2">
<h2 class="anchored" data-anchor-id="assumptions">Assumptions</h2>
<p>Having described the extensive assumptions needed above in order to explain the fuzzy regression discontinuity model, I now evaluate the plausibility of these assumptions for Coppock and Green's case.</p>
<p><strong>Exclusion:</strong> Is there any path other than through <img src="https://latex.codecogs.com/png.latex?D"> through which the "treatment" of being just eligible in 2008 could affect propensity to vote in 2012? Absent a history of actually participating, it is unlikely that someone who becomes eligible slightly earlier would be more likely to be targeted by a campaign to turn out. We can, however, imagine a subject who knew they would be eligible paying more attention in 2008 than someone who knew they would not be, which could spark more interest by the 2012 election. Similarly, a subject who missed voting in their first presidential election but was eligible might be more motivated in 2012 for a fear of missing out again ("I missed voting for Obama, but won't do so again"). With all such paths that imagine a more engaged citizen for their first presidential election, however, one has to imagine that most such citizens would want to put that energy towards voting in 2008. Thus, while we have no way to rule out such backdoor paths from being just eligible in 2008 to voting in 2012, we have to imagine such cases would be relatively rare.</p>
<p><strong>Effectiveness of the Instrument:</strong> While just being eligible alone likely has a relatively weak effect on 2008 turnout, given that we are working at the scale of full state voter files, we can be confident we have generated a reasonable number of compliers to work with out of over five hundred thousand ballots cast by the birthdate cohorts studied.</p>
<p><strong>Monotonicity:</strong> A defier in Coppock's and Green's design would be someone who votes despite not quite being old enough, or vice versa. Given that this constitutes a felony, and a hard to commit one with little benefit, we can be extremely confident in this assumption. Alternatively, someone could choose to not vote only if just eligible, which again implies an extremely unlikely stance towards election law.</p>
<p><strong>Ignorability:</strong> Here, we need to be clear that we mean ignorability within some cutoff of being just eligible, given our covariates: <img src="https://latex.codecogs.com/png.latex?Y(1),%20Y(0)%20%5Cperp%20Z%20%5Cvert%20x,%20x%20%5Cin(C-a,%20C+a)">. It seems plausible that being a month above or below 18 on election day shouldn't create any other major changes in a 17 or 18 year old's potential outcomes. However, as we get farther from the eligibility threshold it becomes more plausible that the groups could diverge through, for example, differences in maturity or differences in education due to birthdate.</p>
<p><strong>Cutoff and forcing variable determined independently:</strong> It seems unlikely that either Election Day or someone's recorded birthday could be moved to help a subject vote earlier. Federal Election Day has been fixed in the United States since 1945. Further, it seems exceedingly unlikely that a parent or hospital administrator would attempt to modify a birth certificate simply to allow their child to vote 1 year earlier.</p>
<p><strong>Estimation:</strong> As we will see in plots below, there seems to be almost no non-linear trend in Y above or below the cutoff, simplifying modeling <img src="https://latex.codecogs.com/png.latex?E%5BY%5Cvert%20X%5D">. Also, while there might be some concern with day or the week, seasonal, or other temporal trends in the number of dates on a given birthdate and thus the number of expected votes there, including the year lagged variable should arguably be sufficient to model them, provided any trends aren't particularly unique to 1990 births. While we'd ideally like more covariates than are available in the flattened data presented here, what we have seems sufficient to make at least a reasonable estimate.</p>
<p><strong>SUTVA:</strong> Lastly, there is little reason to believe that the encouragement to vote provided to one subject by turning 18 close to election day could influence another subject. While we can imagine students influencing each other's politics, it seems implausible that a friend turning 18 close to election day alone could change a student's propensity to vote meaningfully.</p>
</section>
<section id="diagnostics" class="level2">
<h2 class="anchored" data-anchor-id="diagnostics">Diagnostics</h2>
<p>Given that my greatest concern with Coppock and Green’s design is their large choice of bandwith at 365 days around 2008’s election day, I first briefly describe results of other diagnostics I replicated before focusing attention on a variety of ways to evaluate the bandwidth’s effect on the CACE. This emphasizes my work beyond the replication, most of which isn’t shown here- after the following two paragraphs and first table, diagnostics in this section are my extensions of Coppock and Green’s work.</p>
<p>First, being just eligible appears to have an effect on voting 2008, as checked through running just the first stage of a 2 stage least squares model, so our instrument generates compliers. There were no discontinuities in the year lagged vote counts around the 2008 eligibility cutoff, our covariate.</p>
<p>I was able to successfully reproduce Coppock and Green’s robustness check across different order polynomials, shown below. Note, however, that these estimates are from meta-analyses across states; I develop a similar table by state and bandwith later. Both 1st and 2nd order polynomials returned similar results, while a 3rd order polynomial estimates became extremely sensitive to choice of bandwith. As <a href="https://www.nber.org/papers/w20405">Gelman and Imbens (2014)</a> discuss however, cubic polynomials are problematic more generally in regression discontinuity designs, so this result was expected. Robust SE's are in parentheses below their corresponding estimate. <strong>The primary estimates presented by CG in the main paper are bolded.</strong></p>
<table class="table">
<colgroup>
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>90</th>
<th>180</th>
<th>270</th>
<th>365</th>
<th>455</th>
<th>545</th>
<th>635</th>
<th>730</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Difference-in-Means</td>
<td>0.108</td>
<td>0.118</td>
<td>0.124</td>
<td>0.119</td>
<td>0.118</td>
<td>0.117</td>
<td>0.11</td>
<td>0.103</td>
</tr>
<tr class="even">
<td></td>
<td>(0.01)</td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
</tr>
<tr class="odd">
<td>First-order Polynomial</td>
<td>0.058</td>
<td>0.089</td>
<td>0.101</td>
<td><strong>0.117</strong></td>
<td>0.118</td>
<td>0.121</td>
<td>0.13</td>
<td>0.136</td>
</tr>
<tr class="even">
<td></td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td><strong>(0.01)</strong></td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
<td>(0.00)</td>
</tr>
<tr class="odd">
<td>Second-order Polynomial</td>
<td>0.016</td>
<td>0.055</td>
<td>0.074</td>
<td>0.083</td>
<td>0.102</td>
<td>0.104</td>
<td>0.099</td>
<td>0.103</td>
</tr>
<tr class="even">
<td></td>
<td>(0.02)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
</tr>
<tr class="odd">
<td>Third-order Polynomial</td>
<td>0.012</td>
<td>0.035</td>
<td>0.05</td>
<td>0.056</td>
<td>0.06</td>
<td>0.086</td>
<td>0.095</td>
<td>0.097</td>
</tr>
<tr class="even">
<td></td>
<td>(0.02)</td>
<td>(0.02)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
<td>(0.01)</td>
</tr>
</tbody>
</table>
<p>Next, somewhat surprisingly, Coppock and Green never actually plot the discontinuity they work with either in the main paper or appendix. The plot of the discontinuity in Figure 1 below establishes several informative points. First, there is a noticeable dip in total votes cast in 2012 by cohort upon being slightly too young to vote in 2008, a good first validation of the design. Second, the trend in the data both to the left and right of the discontinuity appears to be extremely linear, which may influence my later bandwith selection algorithm findings. Finally, even in the binned plot the wide range of votes cast by birthdate state cohorts is obvious. Upon further inspection much of this variation is driven by state, which motivates my next analysis, seeing how each state's CACE changes as the cutoff varies.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Is Voting Habit Forming/discontinuity.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Discontinuity Plot</figcaption><p></p>
</figure>
</div>
<p>Note that the full 10,000+ birthdate cohorts are binned with their average plotted to make the graph legible. Lines plotted are first order polynomials.</p>
<p>While Coppock and Green present results at their final chosen bandwith by state, they conduct robustness checks only at the level of meta-analysis pooling across states. Given that some of the variation in vote total also seems to be driven by state, below are CACE estimates by state, by a variety of bandwiths. These are considerably more sensitive to the bandwith choice than the above table that pools across states, as we’d expect given the disaggregation. For example, the average across the estimates using 30-day bandwith (.01) is closer to 0 than the .1 resulting from 365 bandwith concreated on in the main paper. This somewhat lowers my confidence in the by-state estimates as Coppock and Green present them. Their claim that their results are insensitive to bandwith variation only really appear to hold when estimates are aggregated through meta-analysis, as in the earlier table.</p>
<table class="table">
<colgroup>
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>365</th>
<th></th>
<th>180</th>
<th></th>
<th>90</th>
<th></th>
<th>60</th>
<th></th>
<th>30</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AR</td>
<td>0.20</td>
<td>(0.03)</td>
<td>0.15</td>
<td>(0.04)</td>
<td>0.14</td>
<td>(0.07)</td>
<td>0.01</td>
<td>(0.10)</td>
<td>0.10</td>
<td>(0.16)</td>
</tr>
<tr class="even">
<td>CT</td>
<td>0.16</td>
<td>(0.02)</td>
<td>0.14</td>
<td>(0.02)</td>
<td>0.14</td>
<td>(0.03)</td>
<td>0.11</td>
<td>(0.03)</td>
<td>0.11</td>
<td>(0.05)</td>
</tr>
<tr class="odd">
<td>IA</td>
<td>0.08</td>
<td>(0.04)</td>
<td>0.03</td>
<td>(0.05)</td>
<td>0.03</td>
<td>(0.08)</td>
<td>-0.05</td>
<td>(0.11)</td>
<td>-0.03</td>
<td>(0.17)</td>
</tr>
<tr class="even">
<td>IL</td>
<td>0.08</td>
<td>(0.01)</td>
<td>0.05</td>
<td>(0.02)</td>
<td>0.02</td>
<td>(0.02)</td>
<td>0.01</td>
<td>(0.03)</td>
<td>0.04</td>
<td>(0.04)</td>
</tr>
<tr class="odd">
<td>FL</td>
<td>0.10</td>
<td>(0.01)</td>
<td>0.09</td>
<td>(0.02)</td>
<td>0.06</td>
<td>(0.03)</td>
<td>0.03</td>
<td>(0.04)</td>
<td>-0.01</td>
<td>(0.05)</td>
</tr>
<tr class="even">
<td>KY</td>
<td>0.08</td>
<td>(0.02)</td>
<td>0.08</td>
<td>(0.03)</td>
<td>0.05</td>
<td>(0.04)</td>
<td>0.05</td>
<td>(0.05)</td>
<td>0.08</td>
<td>(0.07)</td>
</tr>
<tr class="odd">
<td>MO</td>
<td>0.16</td>
<td>(0.02)</td>
<td>0.15</td>
<td>(0.03)</td>
<td>0.14</td>
<td>(0.04)</td>
<td>0.13</td>
<td>(0.06)</td>
<td>0.11</td>
<td>(0.08)</td>
</tr>
<tr class="even">
<td>MT</td>
<td>0.11</td>
<td>(0.03)</td>
<td>0.08</td>
<td>(0.04)</td>
<td>0.03</td>
<td>(0.07)</td>
<td>-0.01</td>
<td>(0.09)</td>
<td>-0.08</td>
<td>(0.12)</td>
</tr>
<tr class="odd">
<td>NJ</td>
<td>0.15</td>
<td>(0.01)</td>
<td>0.12</td>
<td>(0.02)</td>
<td>0.09</td>
<td>(0.03)</td>
<td>0.03</td>
<td>(0.03)</td>
<td>0.01</td>
<td>(0.05)</td>
</tr>
<tr class="even">
<td>NV</td>
<td>0.17</td>
<td>(0.03)</td>
<td>0.14</td>
<td>(0.04)</td>
<td>0.09</td>
<td>(0.06)</td>
<td>0.00</td>
<td>(0.08)</td>
<td>-0.01</td>
<td>(0.11)</td>
</tr>
<tr class="odd">
<td>NY</td>
<td>0.07</td>
<td>(0.01)</td>
<td>0.02</td>
<td>(0.02)</td>
<td>-0.03</td>
<td>(0.03)</td>
<td>-0.06</td>
<td>(0.03)</td>
<td>-0.03</td>
<td>(0.05)</td>
</tr>
<tr class="even">
<td>OK</td>
<td>0.14</td>
<td>(0.02)</td>
<td>0.11</td>
<td>(0.03)</td>
<td>0.09</td>
<td>(0.06)</td>
<td>0.01</td>
<td>(0.08)</td>
<td>0.04</td>
<td>(0.15)</td>
</tr>
<tr class="odd">
<td>OR</td>
<td>0.11</td>
<td>(0.02)</td>
<td>0.07</td>
<td>(0.04)</td>
<td>0.06</td>
<td>(0.06)</td>
<td>0.01</td>
<td>(0.08)</td>
<td>-0.07</td>
<td>(0.12)</td>
</tr>
<tr class="even">
<td>PA</td>
<td>0.12</td>
<td>(0.02)</td>
<td>0.08</td>
<td>(0.03)</td>
<td>0.01</td>
<td>(0.04)</td>
<td>-0.05</td>
<td>(0.05)</td>
<td>-0.04</td>
<td>(0.07)</td>
</tr>
<tr class="odd">
<td>RI</td>
<td>0.11</td>
<td>(0.03)</td>
<td>0.14</td>
<td>(0.05)</td>
<td>0.05</td>
<td>(0.08)</td>
<td>0.03</td>
<td>(0.10)</td>
<td>-0.15</td>
<td>(0.16)</td>
</tr>
</tbody>
</table>
<p>Bandwith Sensitivity for State estimates. Robust Standard errors are to the right of their respective estimate in parentheses.</p>
<p>As a final extension of the robustness to different bandwith checks Coppock and Green consider in their appendix, I also explored a variety of algorithms for bandwith selection. Across different order polynomials, kernels, and metrics to optimize for, all results suggested using the full 365-day bandwith. In retrospect, looking at the quite linear trend in the data, this makes some sense: given little difficult variation to model, choosing to include all the data could easily be the most effective strategy to reduce metrics such as the MSE in estimating <img src="https://latex.codecogs.com/png.latex?%5Ctau_%7BD%7D">.</p>
</section>
<section id="results-and-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="results-and-interpretation">Results and Interpretation</h2>
<p>While I was able to reproduce Coppock and Green’s results, I differ slightly in how I choose to interpret them. Their identification strategy overall makes sense. At worst, it seems possible the exclusion assumption could be subject to minor violations, but the frequency of such events would be low. Unlike Coppock and Green, however, I don’t think its fair to say that the results are completely robust to bandwith selection or result from habit alone.</p>
<p>While their meta-analysis level results are insensitive to variations in the order of polynomial used or bandwith both in their appendix and my replication, allowing the bandwith to vary by state revealed significant additional variability. Given that by state estimates are a core part of their results, I would argue it is best to present something closer to my bandwith by state table than the singular estimates resulting from a bandwith of 365. If one accepts their bandwith, either on its own or as a result of bandwith selection algorithms, it appears that the overall CACE across states is around .1, establishing that there does appear to be some causal effect on 2012 participation from 2008 participation for compliers. If a reader prefers a much narrower bandwith, this overall shrinks to roughly .06 for a 90 day bandwith, or close to 0 for a 30 day one.</p>
<p>A second concern is that Coppock and Green argue that these effects result mostly from habit, not campaigns in 2012 choosing to target those who voted in 2008. This would not represent a violation of exclusion, as campaign effects represent a valid path from <img src="https://latex.codecogs.com/png.latex?D"> to <img src="https://latex.codecogs.com/png.latex?Y">, not <img src="https://latex.codecogs.com/png.latex?Z"> to <img src="https://latex.codecogs.com/png.latex?Y">. Thus, this is a disagreement around causes of effects. To argue that habit, not campaigns, cause the CACEs they find, Coppock and Green note that battleground states do not have significantly higher estimates than non-battleground states. That is, if the cause was campaign activity, we would expect higher CACEs in battleground states in presidential years like 2012. While this would be somewhat convincing as an argument, the ability to tease out the pattern they describe is dependent on a large bandwith to generate small enough standard errors to tell states apart. Beyond this, during campaign years, even in non-battleground states, most people with some voting history can expect campaign outreach. Further, in non-battleground states, this contact sometimes prioritizes young voters and others whose turnout is more ambiguous. Overall, there does appear to be some evidence that the effects Coppock and Green find aren’t mainly due to campaign effects, however this isn’t sufficient to fully support their claim that most of what they find is habit alone.</p>
<p>To provide an interpretation of the individual CACEs, consider the Florida estimate at a bandwith of 90 days on either side of the 2008 election. If our assumptions hold, the CACE of .06 suggests that for compliers, participation in the 2008 election caused a 6% increase in total 2012 votes cast in Florida, above the total votes cast for those compliers who did not participate in 2008.</p>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<p>Overall, reproducing this paper convinced me that Coppock and Green have a strong method for estimating the causal effect of voting in the 2008 election on voting in the 2012 election for compliers. However, these estimates are sensitive to bandwith variation at the state level, with effects shrinking towards zero as the bandwith becomes tighter. Also, it is unclear if their results are due to habit alone, although the estimates do support this somewhat. Thus, I think a narrower interpretation of the 2008-2012 election pair analysis is that there is some suggestion of an effect of voting in 2008 on 2012 votes cast for compliers, provided you accept a large bandwith, and the effect is most likely not due to campaign effects.</p>
<p>Of course, replicating such a colossal paper and set of analyses has many limitations. First, I restricted my analysis and robustness checks to the 2008-2012 election pair, in order to replicate the pair the original authors used in their robustness checks. I also didn’t study or replicate their instrumental variables CACE estimates resulting from using older randomized experiments as instruments. Finally, given the flattened form of the data provided, I was unable to provide fully satisfying descriptive profiles of the birthdate-state cohorts, for example about their demographic composition.</p>
<p>Future work could consider replicating the rest of Coppock and Green’s study with additional robustness checks, or conducting the same analysis using recent advances in voter file linking. For example, given that midterm elections have much lower turnout, and thus fewer compliers in a fuzzy regression discontinuity design, it would be interesting to see if an election pair like 2006-2010 exhibits even higher sensitivity to variation in bandwith. In the past few years, companies such as Catalist and Movement Cooperative have also improved the quality of data available in voter files. These provide a number of advantages that could be helpful for a future study like Coppock and Green’s, including better cleaning of the underlying data, and richer information on the individual voters, either through integration of additional data sources, or through modeling. Coppock and Green’s analysis takes an incredible step towards identifying whether voting is habit forming, but further analysis is needed to show that their results are deeply robust to electoral context and modeling choices. Some of these steps will likely form my project for my Advanced Causal Inference course this semester, although I’m still deciding which points are the most interesting to look at.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2019,
  author = {Andy Timm},
  title = {Is {Voting} {Habit} {Forming?} {Replication,} and Additional
    Robustness Checks},
  date = {2019-07-03},
  url = {https://andytimm.github.io/2020-03-09-Voting-Habit-FRD.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2019" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2019. <span>“Is Voting Habit Forming? Replication, and
Additional Robustness Checks.”</span> July 3, 2019. <a href="https://andytimm.github.io/2020-03-09-Voting-Habit-FRD.html">https://andytimm.github.io/2020-03-09-Voting-Habit-FRD.html</a>.
</div></div></section></div> ]]></description>
  <category>From Old Website</category>
  <category>causal inference</category>
  <guid>https://andytimm.github.io/posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html</guid>
  <pubDate>Wed, 03 Jul 2019 04:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Is Voting Habit Forming/discontinuity.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Type S/M errors in R with retrodesign()</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html</link>
  <description><![CDATA[ 




<p>This is a online version of the vignette for my r package <strong>retrodesign</strong>. It can be installed with:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;">install.packages</span>(<span class="st" style="color: #20794D;">"retrodesign"</span>)</span></code></pre></div>
<p>In light of the replication and reproducibility crisis, researchers across many fields have been reexamining their relationship with the Null Hypothesis Significance Testing (NHST) framework, and developing tools to more fully understand the implications of their research designs for replicable and reproducible science. One group of papers in this vein are works by Andrew Gelman, John Carlin, Francis Tuerlinckx, and others which develop two new metrics for better understanding hypothesis testing in noisy samples. For example, Gelman and Carlin’s <a href="http://www.stat.columbia.edu/~gelman/research/published/retropower20.pdf">Assessing Type S and Type M Errors</a> (2014) argues that looking at power, type I errors, and type II errors are insufficient to fully capture the risks of NHST analyses, in that such analysis focuses excessively on statistical significance. Instead, they argue for consideration of the probability you’ll get the sign on your effect wrong, or <strong>type S error</strong>, and the factor by which your effect size might be exaggerated, or <strong>type M</strong> error. Together, these additional statistics more fully explain the dangers of working in the NHST framework, especially in noisy, small sample environments.</p>
<p><strong>retrodesign</strong> is a package designed to help researchers better understand type S and M errors and their implications for their research. In this vignette, I introduce both the need for the type S/M error metrics, and the tools retrodesign provides for examining them. I assume only a basic familiarity with hypothesis testing, and provide definitional reminders along the way.</p>
<!--more-->
<section id="an-initial-example" class="level2">
<h2 class="anchored" data-anchor-id="an-initial-example">An Initial Example</h2>
<p>To nail down the assumptions we’re working with, we’ll start with an abstract example from <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/bmsp.12132">Lu et al.</a> (2018); the second will draw on a real world scenario and follow Gelman and Carlin’s suggested workflow for design analysis more closely.</p>
<p>Let’s say we’re testing whether a true effect size is zero or not, in a two tailed test.</p>
<p><img src="https://latex.codecogs.com/png.latex?H%20_%20%7B%200%20%7D%20:%20%5Cmu%20=%200%20%5Cquad%20%5Ctext%20%7B%20vs.%20%7D%20%5Cquad%20H%20_%20%7B%201%20%7D%20:%20%5Cmu%20%5Cneq%200"></p>
<p>We’re assuming that the test statistic here is normally distributed. As <a href="http://www.stat.columbia.edu/~gelman/research/published/francis8.pdf">Gelman and Tuerlinckx</a> (2009) note, this is a pretty widely applicable assumption; through the Central Limit Theorem, it applies to common settings like differences between test and control groups in a randomized experiment, averages, or linear regression coefficients. If it helps, imagine the following in the context of one of those from your field.</p>
<p>Traditionally, we’d focus on Type I/II errors. <strong>Type I error</strong> is rejecting the null hypothesis, when it is true. <strong>Type II error</strong> is failing to reject the null hypothesis, when it is not true. The <strong>power</strong>, then, is just is the probability that the test correctly rejects the null hypothesis when a specific alternative hypothesis is true.</p>
<p>Beyond those, we’ll also consider:</p>
<ol type="1">
<li><strong>Type S (sign)</strong>: the test statistic is in the opposite direction of the true effect size, given that the statistic is statistically significant;</li>
<li><strong>Type M (magnitude or exaggeration ratio)</strong>: the test statistic in magnitude exaggerates the true effect size, given that the statistic is statistically significant.</li>
</ol>
<p>Notice that both of these are conditional on the test statistic being statistically significant; we’ll come back to this fact several times.</p>
<p>To visualize these, we’ll draw 5000 samples from a normal distribution with mean .5, and standard deviation 1. We’ll then analyze these in a NHST setting where we have a standard error of 1. We can use <code>sim_plot()</code> to do so, with the first parameter being our postulated effect size .5, and the second being our hypothetical standard error of 1. If you prefer to not use ggplot graphics like I do here, set the <code>gg</code> argument to FALSE.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="fu" style="color: #4758AB;">sim_plot</span>(.<span class="dv" style="color: #AD0000;">5</span>,<span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
<p><img src="https://andytimm.github.io/posts/Intro to Retrodesign/unnamed-chunk-2-1.png" class="img-fluid" alt="Visual of what type S and M errors are"><!-- --></p>
<p>Here, the dotted line is the true effect size, and the full lines are where the statistic becomes statistically significantly different from 0, given our standard error of 1. The greyed out points aren’t statistically significant, the squares are type M errors, and the triangles are type S errors.</p>
<p>Even though the full distribution is faithfully centered around the true effect, once we filter using statistical significance, we will both exaggerate the effect and get its sign wrong often.</p>
<p>Of course, trying to find a true effect size of .5 with a standard error of 1 is extremely underpowered. More precisely, the power, Type S and type M error here are:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><span class="fu" style="color: #4758AB;">retro_design</span>(.<span class="dv" style="color: #AD0000;">5</span>,<span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb3-2"><span class="co" style="color: #5E5E5E;">#&gt; $power</span></span>
<span id="cb3-3"><span class="co" style="color: #5E5E5E;">#&gt; [1] 0.07909753</span></span>
<span id="cb3-4"><span class="co" style="color: #5E5E5E;">#&gt;</span></span>
<span id="cb3-5"><span class="co" style="color: #5E5E5E;">#&gt; $typeS</span></span>
<span id="cb3-6"><span class="co" style="color: #5E5E5E;">#&gt; [1] 0.0878352</span></span>
<span id="cb3-7"><span class="co" style="color: #5E5E5E;">#&gt;</span></span>
<span id="cb3-8"><span class="co" style="color: #5E5E5E;">#&gt; $typeM</span></span>
<span id="cb3-9"><span class="co" style="color: #5E5E5E;">#&gt; [1] 4.788594</span></span></code></pre></div>
<p>The function arguments here the same as those for <code>sim_plot()</code> above. If you want to work with a t-distribution instead, you’ll need to use <code>retrodesign()</code> instead, and provide the degrees of freedom for the <code>df</code> argument. <code>retrodesign()</code> is the original function provided by Gelman &amp; Carlin (2014), which uses simulation rather than an exact solution to calculate the errors. It’s thus slower, but can work with t-distributions as well (the closed form solution only applies in the normal case).</p>
<p>A power of .07 isn’t considered good research in most fields, so most cases won’t be this severe. However, it does illustrate two important points. In a underpowered example like this, we will hugely overestimate our effect size (by a factor of 4.7x! on average), or even get its sign wrong if we’re unlucky (around 8% of the time). Further, these are practical measures to focus on; getting the sign wrong could mean recommending the wrong drug treatment, exaggerating the treatment effect could mean undertreating someone once the drug goes to market.</p>
</section>
<section id="a-severe-example" class="level2">
<h2 class="anchored" data-anchor-id="a-severe-example">A Severe Example</h2>
<p>Now that you hopefully have a sense of what type S/M error add to our understanding of NHST in the context of noisy, small sample studies, we’ll move onto a real world example, where we’ll focus on following Gelman and Carlin’s suggested design analysis through one of their examples.</p>
<p>We’ll be working with <a href="https://www.ncbi.nlm.nih.gov/pubmed/16949101">Kanazawa</a> (2007), which claims to have found that the most attractive people are more likely to have girls. To be more specific, each of their ~3,000 people surveyed had been assigned a “attractiveness” score from 1-5, and they then compared the first born children of the most attractive to other groups; 56% were girls compared to 48% in the other groups. They thus obtained a difference estimate of 8%, with a p-value of .015, so significant at the traditional <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%20.05">.</p>
<p>Stepping back for a second, this is fishy in numerous ways. First, comparing the first borne children is an oddly specific comparison to have run- at worst, this might be a case of p-hacking. Or maybe they saw a strong comparison, and decided to test it, and in doing so fell into a <a href="http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf">Garden of Forking Paths</a> problem. Second, if attractive people had many more girls, it seems unlikely that gender balance would be as even as it is. So again, this example has a likely spurious result, is likely to have low power, and a high chance of S/M errors.</p>
<p>To do a design analysis of this, Gelman and Carlin’s first step is to review the literature for a posited true effect size. There is plenty of prior research on variation in sex ratio of human births. A huge variety of factors have been studied such as race, parental age, season of birth, and so on, only finding effects from .3% to 2%. In the most extreme cases (conditions like extreme poverty or famine), these effects only rise to 3%. (If you’re interested, the causal reasoning seems to be that male fetuses are more likely than female ones to die under adverse conditions.)</p>
<p>Like traditional design analyses, we’ll posit a wide range of effects here, and see how our power, type S error, and type M error rates change correspondingly. Gelman and Carlin end up looking at .1-1%, reasoning that sex ratios vary very little in general, and that the subject attractiveness rating is quite noisy as well. Even if we compare their effect size to the effect sizes found in the most extreme scenarios in prior literature, it doesn’t look good.</p>
<p>We can infer their standard error of the difference to be 3.3% from their reported 8% estimate and p-value of .015; we now have everything we need.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><span class="co" style="color: #5E5E5E;"># The posited effects Gelman and Carlin consider</span></span>
<span id="cb4-2"><span class="fu" style="color: #4758AB;">retro_design</span>(<span class="fu" style="color: #4758AB;">list</span>(.<span class="dv" style="color: #AD0000;">1</span>,.<span class="dv" style="color: #AD0000;">3</span>,<span class="dv" style="color: #AD0000;">1</span>),<span class="fl" style="color: #AD0000;">3.3</span>)</span>
<span id="cb4-3"><span class="co" style="color: #5E5E5E;">#&gt;   effect_size      power    type_s   type_m</span></span>
<span id="cb4-4"><span class="co" style="color: #5E5E5E;">#&gt; 1         0.1  0.0501052 0.4646377 77.15667</span></span>
<span id="cb4-5"><span class="co" style="color: #5E5E5E;">#&gt; 2         0.3 0.05094724 0.3953041 25.74305</span></span>
<span id="cb4-6"><span class="co" style="color: #5E5E5E;">#&gt; 3           1 0.06058446 0.1950669 7.795564</span></span>
<span id="cb4-7"></span>
<span id="cb4-8"><span class="co" style="color: #5E5E5E;"># A particularly charitable set of posited effects</span></span>
<span id="cb4-9"><span class="fu" style="color: #4758AB;">retro_design</span>(<span class="fu" style="color: #4758AB;">list</span>(.<span class="dv" style="color: #AD0000;">1</span>,.<span class="dv" style="color: #AD0000;">3</span>,<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">2</span>,<span class="dv" style="color: #AD0000;">3</span>),<span class="fl" style="color: #AD0000;">3.3</span>)</span>
<span id="cb4-10"><span class="co" style="color: #5E5E5E;">#&gt;   effect_size      power     type_s   type_m</span></span>
<span id="cb4-11"><span class="co" style="color: #5E5E5E;">#&gt; 1         0.1  0.0501052  0.4646377 77.15667</span></span>
<span id="cb4-12"><span class="co" style="color: #5E5E5E;">#&gt; 2         0.3 0.05094724  0.3953041 25.74305</span></span>
<span id="cb4-13"><span class="co" style="color: #5E5E5E;">#&gt; 3           1 0.06058446  0.1950669 7.795564</span></span>
<span id="cb4-14"><span class="co" style="color: #5E5E5E;">#&gt; 4           2 0.09302718 0.05529112 3.982141</span></span>
<span id="cb4-15"><span class="co" style="color: #5E5E5E;">#&gt; 5           3  0.1487169 0.01384174 2.719244</span></span></code></pre></div>
<p>By providing a list for our first argument <code>A</code>, we get a dataframe with a posited effect size, and corresponding power, type S, and type M errors in each row. If you just want the lists of power/type S/type M, you can just feed in a vector, ie <code>c(.1,.3,1)</code>.</p>
<p>So if we go with a high but fairly reasonable posited effect of 1%, there’s a nearly 1 in 5 chance that such an experiment would get the direction of the effect wrong. Even if we assume that being the daughter of a highly attractive person has equivalent effect to being born in a famine (effect size 3%), this experiment would exaggerate the true effect size by a factor of 2.7x on average.</p>
<p>This analysis has given us added information beyond what we’d get from the point estimate, the confidence interval, and the p-value. Under reasonable assumptions about the true effect size, this study simply seems too small to be informative: Under most assumptions, we’re quite likely to get the sign wrong, and even with a quite generous assumption of true effect size we’ll greatly exaggerate the effect size.</p>
</section>
<section id="assessing-type-sm-errors-when-you-dont-have-prior-information" class="level2">
<h2 class="anchored" data-anchor-id="assessing-type-sm-errors-when-you-dont-have-prior-information">Assessing Type S/M errors when you don’t have prior information</h2>
<p>One obvious objection you might have to this framework is that you may not have a clear sense of what your effect size will be. As extreme as it sounds, you may not even have a sense of the right order of magnitude. In these cases, it makes even more sense to calculate type s/m errors across a variety of posited effect sizes and see how they influence your research.</p>
</section>
<section id="so-how-worried-should-we-be-in-more-reasonable-studies" class="level2">
<h2 class="anchored" data-anchor-id="so-how-worried-should-we-be-in-more-reasonable-studies">So how worried should we be in more reasonable studies?</h2>
<p>Another concern might be that my first two examples were severely underpowered. However, even with powers that are publishable in many fields, we should still be worried about type M errors, but not type S errors.</p>
<p>To sketch out the relationship between possible effect sizes and these errors, we adopt the standard error from the prior example, but greatly expand the posited effect sizes, to max out at 10, where the power would be a perfectly reasonable .85. We’ll use <code>type_s</code> and <code>type_m</code>, both of which take a single or list of possible <code>A</code>’s, and a standard error, similar to <code>retrodesign</code>, but only return the respective error.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">possible_effects <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">seq</span>(.<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">10</span>, <span class="at" style="color: #657422;">by =</span> .<span class="dv" style="color: #AD0000;">5</span>)</span>
<span id="cb5-2">effect_s_pairs <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">data.frame</span>(possible_effects,<span class="fu" style="color: #4758AB;">type_s</span>(possible_effects,<span class="fl" style="color: #AD0000;">3.3</span>))</span>
<span id="cb5-3">effect_m_pairs <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">data.frame</span>(possible_effects,<span class="fu" style="color: #4758AB;">type_m</span>(possible_effects,<span class="fl" style="color: #AD0000;">3.3</span>))</span>
<span id="cb5-4"></span>
<span id="cb5-5">s_plot<span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">ggplot</span>(effect_s_pairs, <span class="fu" style="color: #4758AB;">aes</span>(possible_effects, type_s)) <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">geom_point</span>()</span>
<span id="cb5-6">m_plot <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">ggplot</span>(effect_m_pairs, <span class="fu" style="color: #4758AB;">aes</span>(possible_effects, type_m)) <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">geom_point</span>()</span>
<span id="cb5-7"></span>
<span id="cb5-8"><span class="fu" style="color: #4758AB;">grid.arrange</span>(s_plot, m_plot, <span class="at" style="color: #657422;">ncol=</span><span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
<p><img src="https://andytimm.github.io/posts/Intro to Retrodesign/unnamed-chunk-5-1.png" class="img-fluid" alt="Comparison of Type S/M error across effect sizes"><!-- --></p>
<p>As <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/bmsp.12132">Lu et al.</a> (2018) note, the type S and M error shrink at very different rates as power rises.</p>
<p>They find the probability of type S error decreases rapidly. To ensure that $s $ and <img src="https://latex.codecogs.com/png.latex?s%5Cleq%200.01">, we only need <img src="https://latex.codecogs.com/png.latex?power%20=%200.08"> and <img src="https://latex.codecogs.com/png.latex?power%20=%200.17">, respectively. Thus, unless your study is severely underpowered, you shouldn’t need to worry about type s errors very often.</p>
<p>On the other hand, The type m error decreases relatively slowly. To ensure that <img src="https://latex.codecogs.com/png.latex?m%20%5Cleq%201.5"> and <img src="https://latex.codecogs.com/png.latex?m%20%5Cleq%201.1">, we need <img src="https://latex.codecogs.com/png.latex?power%20=%200.52"> and <img src="https://latex.codecogs.com/png.latex?power%20=%200.85">, respectively. Whereas even moderately powered studies make type s errors relatively improbable, only very high powered studies keep exaggeration of effect sizes down. If your field requires a power of .80, you should thus be cognizant that effect sizes are likely somewhat inflated.</p>
</section>
<section id="solutions-outside-nhst" class="level2">
<h2 class="anchored" data-anchor-id="solutions-outside-nhst">Solutions outside NHST</h2>
<p>For the majority of this vignette, I’ve avoided questioning whether we should be working in the NHST framework. However, <code>retrodesign</code> is a package to help address some limitations of NHST work as it’s traditionally practiced, so it makes sense to address these solutions.</p>
<p>Going back to the plot we started with, remember that the underlying gaussian does actually faithfully center around it’s mean of .5.</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><span class="fu" style="color: #4758AB;">sim_plot</span>(.<span class="dv" style="color: #AD0000;">5</span>,<span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
<p><img src="https://andytimm.github.io/posts/Intro to Retrodesign/unnamed-chunk-6-1.png" class="img-fluid" alt="Return to showing first plot"><!-- --></p>
<p>Type S and M errors are an artifact of the hard thresholding implicit in a NHST environment, where an arbitrary p-value (usually .05) decides what is and isn’t noteworthy.</p>
<p>If you have to work in the NHST framework because it’s what your discipline publishes, you can better understand some problems it might cause by exploring type S and M errors. If you’re able to choose how you analyze your experiments though, you can avoid these errors (and many others) by abandoning statistical significance as a hard filter entirely. If learning more about problems with NHST beyond type S and M errors, and suggestions for alternative strategies is interesting to you, check out the <strong>Abandon Statistical Significance</strong> paper linked in the further reading below.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<ol type="1">
<li><strong>Gelman and Tuerlinckx’s <a href="http://www.stat.columbia.edu/~gelman/research/published/francis8.pdf">Type S error rates for classical and Bayesian single and multiple comparisons procedures</a> (2000)</strong>: A comparison of the properties of Type S errors of frequentist and Bayesian confidence statements. Useful for how this all plays out in a Bayesian context. Bayesian confidence statements have the desirable property of being more conservative than frequentist ones.</li>
<li><strong>Gelman and Carlin’s <a href="http://www.stat.columbia.edu/~gelman/research/published/retropower20.pdf">Assessing Type S and Type M Errors</a> (2014)</strong>: Gelman and Carlin compare their suggested design analysis, as we’ve written about above, to more traditional design analysis, through several examples, and discuss the desirable properties it has in more depth than I do here. It is also the source of the original retrodesign() function, which I re-use in the package with permission.</li>
<li><strong>Lu et al’s <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/bmsp.12132">A note on Type S/M errors in hypothesis testing</a> (2018)</strong>: Lu and coauthors go further into the mathematical properties of Type S/M errors, and prove the closed form solutions implimented in <code>retrodesign</code>.</li>
<li><strong>McShane et al’s <a href="https://arxiv.org/abs/1709.07588">Abandon Statistical Significance</a> (2017)</strong>: If you want a starting point on the challenges with NHST that have led many statisticians to argue for abandoning NHST all together, and starting points for alternative ways of doing science.</li>
</ol>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2018,
  author = {Andy Timm},
  title = {Type {S/M} Errors in {R} with Retrodesign()},
  date = {2018-05-11},
  url = {https://andytimm.github.io/2019-02-05-Intro_To_retrodesign.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2018" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2018. <span>“Type S/M Errors in R with Retrodesign().”</span>
May 11, 2018. <a href="https://andytimm.github.io/2019-02-05-Intro_To_retrodesign.html">https://andytimm.github.io/2019-02-05-Intro_To_retrodesign.html</a>.
</div></div></section></div> ]]></description>
  <category>From Old Website</category>
  <guid>https://andytimm.github.io/posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html</guid>
  <pubDate>Fri, 11 May 2018 04:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Intro to Retrodesign/unnamed-chunk-2-1.png" medium="image" type="image/png" height="103" width="144"/>
</item>
<item>
  <title>Why the normal distribution?</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Maximum Entropy Normal/2018-05-11-maxentropynormal.html</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Maximum Entropy Normal/boring_normal.jpg" class="img-fluid figure-img"></p>
</figure>
</div>
<p>When I was first studying probability theory as an undergrad, I had a bit of a conceptual hang-up with the Central Limit Theorem. <a href="http://www.rpubs.com/christopher_castle/137490">Simulating it in R</a> gave a nice visual of how each additional random variable smoothed out some of the original distribution’s individuality, and asymptotically we were left with a more generic shape. The proofs were relatively straightforward. One part, however, didn’t really make sense to me. My problem was this: <strong>Of all the many possible distributions, why is the normal distribution in particular that our i.i.d random variables converge to in distribution?</strong></p>
<!--more-->
<p>The normal distribution has lots of interesting properties that I looked at to gain some intuition, but many of the them seem to follow from the CLT, rather than explaining it. In fact, it wasn’t until a later course in machine learning that integrated some <a href="https://en.wikipedia.org/wiki/Information_theory">information theory</a> that I found a satisfactory answer to my question.</p>
<p>In this post, I’ll explain an insight from the principle of maximum entropy that conceptually justifies the normal distribution’s role in the CLT. To do so, we’ll first build up a basic introduction to entropy, how probability and entropy interact, and then explain the entropy property of the normal distribution that helped me understand the question above. For this post, all you’ll really need is a rough idea of what a random variable is, and some familiarity with common probability distributions; we’ll build up the rest from scratch.</p>
<section id="entropy" class="level2">
<h2 class="anchored" data-anchor-id="entropy">Entropy</h2>
<p>Imagine two situations in which you’ve lost your keys at home. In the first scenario, you’re well and truly confused as to where they are. As far as you’re concerned, any location within your home is equally likely. In the second scenario, you remember having them by your kitchen counter, so you’re pretty sure they’re there, or at least somewhere near there. Intuitively, the knowledge that they’re probably in your kitchen is much, much more informative that the idea that they’re equally likely to be anywhere in your house. In fact, once you accept the constraint that they’re within the bounds of your house, it’s pretty hard to imagine a more useless, uninformative statement than “they’re equally likely to be anywhere”.</p>
<p>Information theory helps makes rigorous many of our informal ideas about how much information or uncertainty is contained in situations like the above. Let’s start then, by defining <strong>entropy</strong>, a measure of the uncertainty of a random variable:</p>
<p><img src="https://latex.codecogs.com/png.latex?H(X)%20=%20-%20%5Cint_%7BI%7D%20p(x)%20%5Clog%7Bp(x)%7D%20dx"></p>
<p>Where p(x) is our continuous probability distribution over some interval I. It can be analogously defined for the discrete case by replacing the integral with a summation. To stress the intuition here, the higher the entropy, the less we know about the value of the random variable. As an example, a uniform distribution over the bounds <img src="https://latex.codecogs.com/png.latex?%5Ba,b%5D"> is analogous to our “keys could be anywhere” scenario- the entropy of our variable is high. In fact, once you accept the constraint that the distribution is supported on [a,b], one can prove that the uniform distribution is the maximum entropy distribution among all continuous distributions on that interval. We won’t include the proof here, but showing a similar maximum entropy property about the normal distribution is where we’re headed.</p>
<p>Before we get back to the normal distribution and CLT though, let’s think a little bit more about the concept of maximum entropy. Why would we want to use something specifically because it’s minimally informative? Let’s think about how we’d express or model our belief that our keys are equally likely to be anywhere. If we’re truly of the belief that they’re equally likely to be anywhere in our house, then some sort of model based on a uniform distribution over the space likely makes sense. If they’re equally likely to be anywhere, placing high probability in a specific room would be wrong, and making that additional assumption would probably slow down our search. This is Jaynes’ principle of maximum entropy- we should use the distribution with the highest entropy, subject to any prior constraints we already have.</p>
<p>This principle, then, is about epistemic modesty. We can want to choose the distribution that meets our constraints, and assumes as little additional information as possible.</p>
</section>
<section id="the-normal-distribution" class="level2">
<h2 class="anchored" data-anchor-id="the-normal-distribution">The Normal Distribution</h2>
<p>Once you specify a variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E%7B2%7D">, and that the distribution be supported on the reals from <img src="https://latex.codecogs.com/png.latex?(-%20%5Cinfty,%20%5Cinfty)">, the normal distribution is the maximum entropy distribution! <a href="http://notstatschat.tumblr.com/post/146886495511/how-do-we-prove-the-central-limit-theorem">Thomas Lumley</a> calls this a “a precise characterization of the normal’s boringness”. In our mental image of the CLT at work then, we’re approaching the normal because of it’s generic-ness, it’s lack of information. If we were mixing colors, the normal would be a nondescript grey.</p>
<p>In one sense, this result may be counter intuitive. As statisticians, when we find out that a distribution we’re working with is roughly normal, we tend to feel like we have a lot to work with. Many of our favorite tools like maximum likelihood will work well, and we have straightforward ways to estimate most quantities of interest. However, this result illustrates a subtle point: ease of use and information content aren’t the same thing.</p>
<p>If you found this use of information theory improved your intuition for probability and want more, here are some suggestions for further reading-</p>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources:</h2>
<ul>
<li>If you’re wondering how we actually prove that the normal is the maximum entropy distribution for a specified variance, there’s a self-contained proof in section <a href="http://www.deeplearningbook.org/contents/inference.html">19.4.2 of Deep Learning</a>. I excluded it in this post because it required a lot of extra math, and didn’t add much to the intuitive point I was trying to show.</li>
<li><a href="http://colah.github.io/posts/2015-09-Visual-Information/">Chris Olah’s Visual Information Theory</a> is a great introduction to the information compression and distribution comparison parts of information theory.</li>
</ul>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2018,
  author = {Andy Timm},
  title = {Why the Normal Distribution?},
  date = {2018-05-11},
  url = {https://andytimm.github.io/2018-05-11-maxentropynormal.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2018" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2018. <span>“Why the Normal Distribution?”</span> May 11,
2018. <a href="https://andytimm.github.io/2018-05-11-maxentropynormal.html">https://andytimm.github.io/2018-05-11-maxentropynormal.html</a>.
</div></div></section></div> ]]></description>
  <category>From Old Website</category>
  <guid>https://andytimm.github.io/posts/Maximum Entropy Normal/2018-05-11-maxentropynormal.html</guid>
  <pubDate>Fri, 11 May 2018 04:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Maximum Entropy Normal/boring_normal.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Predicting race part 1- Bayes’ rule method and extensions</title>
  <dc:creator>Andy Timm</dc:creator>
  <link>https://andytimm.github.io/posts/Race Models Pt 1/race_models_part1.html</link>
  <description><![CDATA[ 




<p>Race is a defining part of political identity in the United States, and so it should be no surprise that accurately modeling race can be beneficial for many political campaign activities. For instance, many organizations work to improve turnout in specific communities of color, or want to target persuasion on a given issue to certain racial group. Alternatively, race and ethnicity might be desired as an input to a larger voting or support likelihood model, given that race is generally predictive of both <a href="http://www.electproject.org/home/voter-turnout/demographics">voting likelihood</a> and <a href="https://www.nytimes.com/interactive/2016/11/08/us/politics/election-exit-polls.html">candidate support</a>.</p>
<p>Unfortunately, complete self-reported race data is only available in 7 states, where it is required by law: Alabama, Florida, Georgia, Louisiana, Mississippi, North Carolina, and South Carolina. Pennsylvania and Tennessee also have an optional field on voter registration forms. Outside of these states, race and ethnicity need to be collected individually, or modeled. These models commonly take advantage of the name (especially surname) of the individual voter, and other information in the voter file to do so.</p>
<p>In this post, I’ll explore a Bayes’ rule based method for modeling racial identity, and how it can be extended with additional information from state voter files where available. Obviously, it won’t be as predictive as something like Catalist or Civis Analytics’ ones (which have the benefit of huges swathes of additional survey and other data, not to mention more sophisticated modeling), but it can help illustrate the benfits and limitations of such tools. In the next posts, I’ll explain how natural language processing models can achieve still higher accuracy by extracting more information from names themselves. Since the Florida voter file has self-reported race data and is <a href="http://flvoters.com/downloads.html">easy to access</a>, we’ll test our models’ effectiveness against that ground truth.</p>
<section id="a-simple-first-method" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-first-method">A simple first method</h2>
<p>The <a href="https://www.census.gov/data/developers/data-sets/surnames.html">census surname files</a> are a an incredible source of information on how race correlates with names, and are the starting point of our model. In these files, the census provides race percentage breakdowns for any surname with more than 100 occurrences in the United States, except where redacted for privacy reasons. The data is available only aggregated at the national level. In practice, this means coverage of about 90% of the population.</p>
<p>For many voters, surname is the only information we need to make an accurate classification: names such as Carlson, Meyer, and Hanson are in excess of 90% white, with similar surnames existing for other races. Unfortunately, many names aren’t as clear cut, and some names like Chavis are less than 40% likely to be any race. This is a good starting point, but we can do better.</p>
<p>As our first improvement, <a href="https://link.springer.com/article/10.1007/s10742-009-0047-1">Elliot et al.&nbsp;(2009)</a> incorporates census geolocation (county, tract, or block) data, using Bayes’ theorem.</p>
<p>Bayes’ theorem is a natural way to integrate the new census evidence we have, updating our initial beliefs with more data. <img src="https://latex.codecogs.com/png.latex?P(A)"> and <img src="https://latex.codecogs.com/png.latex?P(B)"> are the probabilities of events A and B occurring independently of each other, and <img src="https://latex.codecogs.com/png.latex?P(A%20%5Cvert%20B)"> and <img src="https://latex.codecogs.com/png.latex?P(B%20%5Cvert%20A)"> are conditional probabilities, such as the probability a given voter is white given their surname, or <img src="https://latex.codecogs.com/png.latex?P(white%20%5Cvert%20surname)">. Bayes’ theorem appears in it’s simplest form below, giving us an updated probability utilizing the new information:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20P(A%7CB)%20=%20%5Cfrac%7BP(B%7CA)%20P(A)%7D%7BP(B)%7D%20"></p>
<p>Of course, we’re interested in integrating multiple pieces of evidence (surname/geolocation for now, with more coming later), and applying it to multiple voters. The equations will be much more involved, but remember that we’re essentially just extending the above equation to work with multiple inputs, over multiple voters. Note that I’ll be using the notation from <a href="https://imai.princeton.edu/research/race.html">Imai and Khanna (2016)</a>, which is somewhat clearer than the notion from Elliot et al.</p>
<p>We want <img src="https://latex.codecogs.com/png.latex?P(R_i%20=%20r%20%5Cvert%20S_i%20=%20s,%20G_i%20=%20g)">, the conditional probability that the voter <img src="https://latex.codecogs.com/png.latex?i"> belongs to the racial group <img src="https://latex.codecogs.com/png.latex?r"> given their surname <img src="https://latex.codecogs.com/png.latex?s"> and geolocation <img src="https://latex.codecogs.com/png.latex?g">. <img src="https://latex.codecogs.com/png.latex?R">, <img src="https://latex.codecogs.com/png.latex?S">, and <img src="https://latex.codecogs.com/png.latex?G"> are the sets of all racial groups, all surnames, and all geolocations respectively. Thus, as a final expression we’ll get:</p>
<p><img src="https://latex.codecogs.com/png.latex?P(R_i%20=%20r%20%5Cvert%20S_i%20=%20s,%20G_i%20=%20g)%20=%20%5Cfrac%7BP(G_i%20=%20g%20%5Cvert%20R_i%20=%20r)%20P(R_i%20=%20r%7CS_i%20=%20s)%7D%7B%5Csum_%7Br'%5Cin%20R%7D%20P(G_i%20=%20g%7CR_i%20=%20r)%20P(R_i%20=%20r%7CS_i%20=%20s)%20%7D"></p>
<p>We already have almost all these probabilities between the census surname list and census demographic data. <img src="https://latex.codecogs.com/png.latex?P(R_i%20=%20r%20%5Cvert%20S_i%20=%20s)"> is the racial composition of surnames from the surname list. But what about <img src="https://latex.codecogs.com/png.latex?P(G_i%20=%20g%20%5Cvert%20R_i%20=%20r)">? As an intermediate step, we first need to calculate <img src="https://latex.codecogs.com/png.latex?P(G_i%20=%20g%20%5Cvert%20R_i%20=%20r)"> , which we can calculate using Bayes’ rule again. <img src="https://latex.codecogs.com/png.latex?P(G_i%20=%20g%20%5Cvert%20R_i%20=%20r)"> is then <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BP(R_i%20=%20r%20%5Cvert%20G_i%20=%20g)%20P(G_i%20=%20g)%7D%20%7B%5Csum_%7Br'%5Cin%20R%7D%20P(R_i%20=%20r%20%5Cvert%20G_i%20=%20g')%20P(G_i%20=%20g)%7D">, completing everything we need to produce our second model.</p>
<p>This model, like the surname list, produces probabilistic predictions of race, for instance, a given voter is 94.3% likely to be white, given their name and geolocation. The probabilities sum to 1 across the races.</p>
<p>We’ll get to how to use and evaluate such models soon, but first: given that we’ve started to include information from the voter file to improve our predictions, why don’t we use other fields we have such as age, party registration, and gender? They all are likely to contain some conditional information about a voter’s race, and while party registration isn’t reported in every state, it is in Florida.</p>
<p>That’s exactly the proposal of <a href="https://imai.princeton.edu/research/race.html">Imai and Khanna (2016)</a>, and it works well. The equations get slightly more complicated, but extending Bayes’ rule to include more and more variables doesn’t change all that much. We have to calculate more intermediate probabilities, but the essential process and reasoning of incorporating new information to update our belief is the same. If you want to see the full model written out, with space for an arbitrary number of new variables <img src="https://latex.codecogs.com/png.latex?X_i">, you can find it in the linked paper.</p>
</section>
<section id="use-and-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="use-and-evaluation">Use and Evaluation</h2>
<p>Now that we have a probability distribution over the racial groups, how do we utilize them?</p>
<p>We might take the highest probability race and use that as our prediction. Alternatively, as an input in a later model, we might choose to simply incorporate all 5 probabilities, allowing our following model as much information as possible about the racial identity of a voter. Catalist, the democratic data vendor, turns probabilities into simple categories such as “likely white” or “possibly black”, which simplify working with the results on a campaign.</p>
<p>As a final idea, we might have an application in mind where you want to only predict a certain race when you’re very confident in your prediction. For example, you might be hoping to target a turnout mailer written in Spanish to only Hispanics, and as few non-Hispanics as possible. To do this, we’d utilize only high probability predictions- say, above 85% likely to be Hispanic. By changing that threshold, you could optimize the size of your mail universe versus the specificity and efficiency of it, finding the best balance for your campaign.</p>
<p>As our last example showed, when utilizing such probabilistic predictions, there is naturally an accuracy tradeoff involved in selecting what type of threshold to use. We could misleadingly say our model is extremely accurate if we only use a high threshold, but a fairer, more systematic evaluation would require looking at how it preforms over multiple such thresholds. That’s what we’ll develop next: Area Under the Curve (AUC), a systematic method for evaluating classifiers.</p>
<p>There are 4 possible outcomes to making a classification: a True Positive (TP), False Positive (FP), True negative (TN), and False Negative (FN). As an example then, a true positive is when we predicted positive, and the ground truth label was actually positive.</p>
<p>Rather than building a table of these 4 outcomes, called a confusion matrix, we’ll be working with summary statistics of these outcomes. The True Positive Rate (or sensitivity or recall) is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BTP%7D%7BTP+FN%7D">. In other words, out of all the points that are (ground truth) positive, how many did we correctly classify? Similarly, the False Positive Rate is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BFP%7D%7BFP+TN%7D">. High True Positive Rate is good: it means we’ll miss relatively few positive examples. On the other end of things, low False Positive Rate is what we want: it means fewer negative points will be misclassified.</p>
<p>By calculating these two statistics at a variety of thresholds, then plotting a curve with the FPR on the x-axis and TPR on the y-axis, we can get a deeper understanding of the tradeoff. This is called an Receiver Operating Characteristic. Given what I’ve said about the meaning of the TPR and FPR, can you figure out quality of the 3 classifiers that are graphed below?</p>
<p><img src="https://andytimm.github.io/posts/Race Models Pt 1/example_curves.jpg" class="img-fluid" alt="Example ROCs"> The first is a perfect classifier: at all tradeoff points, it has a TPR of 1, and FPR of 0. The second is pretty good: at most tradeoff points it does well. The third, a straight line from (0, 0) to (1,1) is a random classifier: it’s equivalent to guessing.</p>
<p>As an overall summary then, the area under this curve (AUC) is of our one number summary of these graphs. The shown classifiers have AUC 1, .8, and .5 respectively.</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>Now that we’ve built up a relatively complex model, and learned how to use and evaluate it, let’s plot some ROC curves, and look at AUCs for our models.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://andytimm.github.io/posts/Race Models Pt 1/ROC_for_wru.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">ROC graphs for 4 models</figcaption><p></p>
</figure>
</div>
<p>And here’s the AUC table: bold is the highest overall for each race.</p>
<table class="table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>White</strong></th>
<th><strong>Black</strong></th>
<th><strong>Hispanic</strong></th>
<th><strong>Asian</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Surname</strong></td>
<td>0.8414369</td>
<td>0.8562679</td>
<td>0.9325489</td>
<td>0.8606716</td>
</tr>
<tr class="even">
<td><strong>Geo/Surname</strong></td>
<td>0.8792853</td>
<td>0.8918873</td>
<td><strong>0.9492531</strong></td>
<td><strong>0.8717517</strong></td>
</tr>
<tr class="odd">
<td><strong>Kitchen Sink</strong></td>
<td><strong>0.8903922</strong></td>
<td><strong>0.8979003</strong></td>
<td>0.9362794</td>
<td>0.7648270</td>
</tr>
</tbody>
</table>
<p>Looking at these, there are a lot of interesting patterns!</p>
<p><strong>White:</strong> The White models’ results are perhaps what we most expected. With more data, each subsequent model does better, and the overall result is strong.</p>
<p><strong>Black:</strong> Interestingly, the kitchen sink model still does the best overall with black voters, but by a much smaller margin than with whites. Also, once the true positive rate gets to around .90, curves cross! This is a good illustration of the importance of plotting ROC curves when doing classification- depending on your campaign, you might correctly choose either the Geo/Surname model or the Kitchen Sink one, depending on what type of cutoff you need.</p>
<p><strong>Hispanic:</strong> The Hispanic models are all very close together: with surname information being so effective in classifying Hispanics (more effective than any model for the other races), there isn’t much room for census or voter file data to improve things.</p>
<p><strong>Asian:</strong> These models are significantly weaker than all the others, but still reasonable. The unexpected trend though, is that the kitchen sink model preforms much worse than the other two. Given how few Asian voters there are overall in Florida, this downturn is probably explained by the relatively low density of any Asians across the other inputs, like age, sex, or party. Thus, while geographic information might slightly improve things, Floridians are so unlikely to be Asian overall that all of those extra variables don’t carry any useful information about who might be Asian.</p>
<p>Overall, these Bayes’ Theorem models have a lot of attractive features. While the predictiveness of names, and what variables you have from the voter file might vary state to state, the models can used anywhere in the US. They’re also quite a strong baseline for accuracy as well- far, far better than random, even for Asian voters. Unlike models we’ll discuss in the next post, these models require no training data. Finally, they’re transparent- if you want to check how surname, geolocation, and party weighed in to a particular decision, it’s only a few calculations with Bayes’ rule away.</p>
<p>Of course, we’d love higher accuracy, and we can get there with natural language processing. What about those ~10% of voters whose name aren’t in the census surname file? If a name starts with “Mc”, but isn’t in the census surname list, I personally would guess they’re of Irish descent, and therefore white. And what about using first names, middle names, and name suffixes? It’s linguistic patterns like these that we’ll hope to exploit with NLP, in the next post.</p>
<p>You can find the code used to write this post <a href="https://github.com/andytimm/CampaignBlog/tree/master/Race_Prediction/part1_wru">here</a>.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2018,
  author = {Andy Timm},
  title = {Predicting Race Part 1- {Bayes’} Rule Method and Extensions},
  date = {2018-04-10},
  url = {https://andytimm.github.io/race_models_part1.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2018" class="csl-entry quarto-appendix-citeas">
Andy Timm. 2018. <span>“Predicting Race Part 1- Bayes’ Rule Method and
Extensions.”</span> April 10, 2018. <a href="https://andytimm.github.io/race_models_part1.html">https://andytimm.github.io/race_models_part1.html</a>.
</div></div></section></div> ]]></description>
  <category>From Old Website</category>
  <guid>https://andytimm.github.io/posts/Race Models Pt 1/race_models_part1.html</guid>
  <pubDate>Tue, 10 Apr 2018 04:00:00 GMT</pubDate>
  <media:content url="https://andytimm.github.io/posts/Race Models Pt 1/example_curves.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
