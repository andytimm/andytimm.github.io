<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Andy Timm">
<meta name="dcterms.date" content="2023-05-02">

<title>Andy Timm - Variational Inference for MRP with Reliable Posterior Distributions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Andy Timm - Variational Inference for MRP with Reliable Posterior Distributions">
<meta property="og:description" content="Part 3- Alternatives to KL Divergence">
<meta property="og:image" content="https://andytimm.github.io/posts/Variational MRP Pt3/variational_mrp_3_files/figure-html/unnamed-chunk-1-1.png">
<meta property="og:site-name" content="Andy Timm">
<meta property="og:image:height" content="960">
<meta property="og:image:width" content="1344">
<meta name="twitter:title" content="Andy Timm - Variational Inference for MRP with Reliable Posterior Distributions">
<meta name="twitter:description" content="Part 3- Alternatives to KL Divergence">
<meta name="twitter:image" content="https://andytimm.github.io/posts/Variational MRP Pt3/variational_mrp_3_files/figure-html/unnamed-chunk-1-1.png">
<meta name="twitter:creator" content="@andy_timm">
<meta name="twitter:site" content="@andy_timm">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image-height" content="960">
<meta name="twitter:image-width" content="1344">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Andy Timm</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects.html" rel="" target="">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../software.html" rel="" target="">
 <span class="menu-text">Software</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html" rel="" target="">
 <span class="menu-text">Rarely Updated Blog</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Variational Inference for MRP with Reliable Posterior Distributions</h1>
            <p class="subtitle lead">Part 3- Alternatives to KL Divergence</p>
                                <div class="quarto-categories">
                <div class="quarto-category">MRP</div>
                <div class="quarto-category">Variational Inference</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://andytimm.github.io">Andy Timm</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 2, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#inclusive-versus-exclusive-kl-divergence" id="toc-inclusive-versus-exclusive-kl-divergence" class="nav-link active" data-scroll-target="#inclusive-versus-exclusive-kl-divergence">Inclusive versus Exclusive KL-divergence</a>
  <ul>
  <li><a href="#chi2-variational-inference-chivi-and-the-cubo-bound" id="toc-chi2-variational-inference-chivi-and-the-cubo-bound" class="nav-link" data-scroll-target="#chi2-variational-inference-chivi-and-the-cubo-bound"><span class="math inline">\chi^{2}</span> Variational Inference (CHIVI) and the CUBO bound</a></li>
  <li><a href="#but-can-we-estimate-it" id="toc-but-can-we-estimate-it" class="nav-link" data-scroll-target="#but-can-we-estimate-it">… But can we estimate it?</a></li>
  <li><a href="#but-can-we-calculate-gradients-efficiently" id="toc-but-can-we-calculate-gradients-efficiently" class="nav-link" data-scroll-target="#but-can-we-calculate-gradients-efficiently">… But can we calculate gradients efficiently?</a></li>
  </ul></li>
  <li><a href="#the-bigger-picture-again" id="toc-the-bigger-picture-again" class="nav-link" data-scroll-target="#the-bigger-picture-again">The Bigger Picture Again</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p><strong>Note:</strong> I’ve gotten a lot more pessimistic about how generally useful the alternatives to simple KL-Divergence are on their own since writing this post. I still think these are really useful ideas to think to build intuition about VI, and techniques like CHIVI are useful for some lower dimensional problems or as part of an ensemble of techniques for high dimensional ones. However, <a href="https://arxiv.org/abs/2103.01085">this paper</a> from Dhaka et al.&nbsp;is very convincing that CHIVI and currently available similar algorithms are in practice very hard to optimize for high dimensional posteriors, and that some of the intuitive benefits shown about CHIVI below in low dimensions don’t really generalize the way we’d expect to higher dimensions.</p>
<hr>
<p>This is section 3 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.</p>
<p>In the <a href="https://andytimm.github.io/posts/Variational%20MRP%20Pt2/Variational_MRP_pt2.html">last post</a> we threw caution to the wind, and tried out some simple variational inference implementations, to build up some intuition about what bad VI might look like. Just pulling a simple variational inference implementation off the shelf and whacking run perhaps unsurprisingly produced dubious models, so in this post we’ll bring in long overdue theory to understand why VI is so difficult, and what we can do about it.</p>
<p>The general structure for the next couple of posts will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in the next three posts will go a long way towards more robust variational inference. I’ll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.</p>
<p>The rough plan for the series is as follows:</p>
<ol type="1">
<li>Introducing the Problem- Why is VI useful, why VI can produce spherical cows</li>
<li>How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li><strong>(This post)</strong> Problem 1: KL-D prefers exclusive solutions; are there alternatives?</li>
<li>Problem 2: Not all samples are of equal utility; can we weight them cleverly?</li>
<li>Problem 3: How can we know when VI is wrong? Are there useful error bounds?</li>
<li>Better grounded diagnostics and workflow</li>
<li>Seeing if some more sophisticated techniques like normalizing flows add much</li>
</ol>
<section id="inclusive-versus-exclusive-kl-divergence" class="level1">
<h1>Inclusive versus Exclusive KL-divergence</h1>
<p>Like I mentioned in the first post in the series, the Evidence Lower Bound (ELBO), our optimization objective we’re working with is a tractable approximation of the Kullback-Leibler Divergence between our choice of approximating distribution <span class="math inline">q(z)</span> to our true posterior <span class="math inline">p(z)</span>.</p>
<p>The KL divergence is asymmetric: in general, <span class="math inline">KL(p||q) \neq KL(q||p)</span>. Previously, we saw that this asymmetry mattered quite a bit for our ELBO idea:</p>
<p><span class="math display">argmin_{q(z) \in \mathscr{Q}}(q(z)||\frac{p(z,x)}{\bf p(x)}) = \mathbb{E}[logq(z)] - \mathbb{E}[logp(z,x)] + {\bf logp(x)}</span> We can’t calculate the bolded term <span class="math inline">logp(x)</span>; if we could we wouldn’t be finding this inference thing so hard in the first place. The way we sidestepped that with the ELBO is to note that the term is constant with respect to <span class="math inline">q</span>; so we can go on our merry way minimizing the above without it.</p>
<p>If we flip the divergence around though, we’ve got an issue. That term would then be a <span class="math inline">logq(x)</span> … which we can’t write off in the same way- it varies as we optimize. So if we’re doing this ELBO minimizing version of variational inference, we’re obligated to use this “reverse” KL divergence, the second option below.</p>
<p><span class="math display">
\begin{align}
KL(p||q) = \sum_{x \in X}{p(x)}log[\frac{p(x)}{q(x)}]  \\
KL(q||p) = \sum_{x \in X}{q(x)}log[\frac{q(x)}{p(x)}]
\end{align}
</span></p>
<p>Unfortunately, this choice to optimize the “reverse” KL divergence bakes in preference for a certain type of solution<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>I found I built better intuition for this encoded preference after seeing it presented many different overlapping ways, so here are a few of my favorites.</p>
<p>One way to see the difference is through a variety of labels for each direction. One could call Forward KL (1) vs.&nbsp;Reverse KL (2):</p>
<ol type="1">
<li>Inclusive vs.&nbsp;Exclusive (my favorite, and so what I’m using for the section header)</li>
<li>Mean Seeking vs.&nbsp;Mode Seeking</li>
<li>Zero Avoiding vs.&nbsp;Zero Forcing</li>
</ol>
<p>let’s quickly sketch what this might look like in the case of a simple mixture of normals with a single normal as a variational family:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'ggplot2' was built under R version 4.2.3</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'tidyverse' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'tibble' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'tidyr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'readr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'purrr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'dplyr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'stringr' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'forcats' was built under R version 4.2.3</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'lubridate' was built under R version 4.2.3</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>mixture <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">normals =</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(<span class="dv">1000</span>,<span class="dv">3</span>,<span class="dv">1</span>),<span class="fu">rnorm</span>(<span class="dv">1000</span>,<span class="dv">15</span>,<span class="dv">2</span>)),</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>                      <span class="at">mode_seeking_kl =</span> <span class="fu">rnorm</span>(<span class="dv">2000</span>,<span class="fl">3.5</span>,<span class="dv">2</span>),</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>                      <span class="at">mean_seeking_kl =</span> <span class="fu">rnorm</span>(<span class="dv">2000</span>,<span class="dv">9</span>,<span class="dv">4</span>))</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>rkl_plot <span class="ot">&lt;-</span> mixture <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> normals)) <span class="sc">+</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="fu">aes</span>(<span class="at">x =</span> normals), <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="fu">aes</span>(<span class="at">x =</span> mode_seeking_kl), <span class="at">color =</span> <span class="st">"green"</span>) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">"Exclusive KL"</span>) <span class="sc">+</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>fkl_plot <span class="ot">&lt;-</span> mixture <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> normals)) <span class="sc">+</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="fu">aes</span>(<span class="at">x =</span> normals), <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="fu">aes</span>(<span class="at">x =</span> mean_seeking_kl), <span class="at">color =</span> <span class="st">"green"</span>) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">"Inclusive KL"</span>) <span class="sc">+</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(rkl_plot,fkl_plot)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="variational_mrp_3_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>To approximate the same exact red distribution <span class="math inline">p(x)</span>, Inclusive KL (1) and Exclusive KL (2) would optimize the green <span class="math inline">q(p)</span> in quite different manner.</p>
<p>To spell out the ways to describe this above: Inclusive KL will try to cover all the probability mass in <span class="math inline">p(x)</span>, even if it means a peak at a unfortunate middle ground. Exclusive KL, on the other hand, will try to concentrate it’s mass on the largest mode, even if it means missing much of the mixture of normals. Alternatively, we could describe the top graph as mode seeking, and the bottom as mean seeking. Finally, we could say the top graph shows “Zero Forcing” behavior- it will heavily favor putting zero mass on some parts of the graph to avoid any weight where <span class="math inline">p(x)</span> has no mass, even if it means missing an entire mode. Conversely, Inclusive KL will aim to cover all the mass of <span class="math inline">p(x)</span> in full even if the result is an odd solution, in order to avoid having zero mass where <span class="math inline">p(x)</span> has some.</p>
<p>How does this follow from the form of the divergence?</p>
<p>To start with, notice that for inclusive KL we could think of the <span class="math inline">log(\frac{p(x)}{q(x)})</span> part of the term being weighted by <span class="math inline">p(x)</span>- if in some range of <span class="math inline">x</span> <span class="math inline">p(x)</span> is 0, we don’t pay a penalty if <span class="math inline">q(x)</span> puts mass. The reverse is not true however- if our <span class="math inline">q(x)</span> is zero where there should be mass in our true distribution, our Inclusive KL divergence is infinite<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p><span class="math display">
\begin{align}
KL(p||q) = \sum_{x \in X}{p(x)}log[\frac{p(x)}{q(x)}]  \\
KL(q||p) = \sum_{x \in X}{q(x)}log[\frac{q(x)}{p(x)}]
\end{align}
</span></p>
<p>And if we change the direction of the divergence, the opposite zeros and infinities show up, enforcing strong preferences for a specific type of solution.</p>
<p>When the example is a simple mix of two gaussians approximated with a single gaussian, it’s fairly easy to intuit how the choice of KL divergence will influence the optimization solution. This all gets a bit more opaque on harder problems- like we saw with the example last post, ELBO based VI will tend to underestimate the support of <span class="math inline">p(x)</span> but whether the solution is narrow but overall reasonable, or pretty much degenerate, is hard to predict. However, this exploration of how the form of the divergence influences the results still gives a rough intuition for why our ELBO optimized posteriors might collapse.</p>
<p>If we want to try the opposite direction of KL divergence, it isn’t immediately obvious there’s a global objective we can choose that favors overdispersed solutions. Like I mentioned above, if we try to make an ELBO-esque target but reverse the KL divergence, the <span class="math inline">logp(x)</span> which is constant with respect to the <span class="math inline">q(x)</span> we’re optimizing becomes a <span class="math inline">logq(x)</span> which we can’t so easily work around.</p>
<p>Let’s look first at a solution in the spirit of VI<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> to the above problem which requires us to pick up a new divergence, the <span class="math inline">\chi^{2}</span>-divergence, and optimizes a new bound. Let’s take a look at it.</p>
<section id="chi2-variational-inference-chivi-and-the-cubo-bound" class="level2">
<h2 class="anchored" data-anchor-id="chi2-variational-inference-chivi-and-the-cubo-bound"><span class="math inline">\chi^{2}</span> Variational Inference (CHIVI) and the CUBO bound</h2>
<p>The <span class="math inline">\chi^{2}</span>-divergence has form:</p>
<p><span class="math display">
D_{\chi^2}(p||q) = \mathbb{E}_{q(z;\lambda)}[(\frac{p(z|x)}{q(z;\lambda)})^2 -1]
</span> For simplicity and comparability, I’m switching here to using <a href="https://arxiv.org/abs/1611.00328">Dieng et Al. (2017)</a>’s notation here- they use <span class="math inline">q(z;\lambda)</span> to refer to the variational family we’re using, indexed by parameters <span class="math inline">\lambda</span>.</p>
<p>This divergence has the properties we wanted when we tried to use Inclusive KL Divergence- it tends to be mean seeking instead of mode seeking.</p>
<p>Like with the ELBO, we need to show that we have a bound here independent of <span class="math inline">logp(x)</span>, and that we have a way to estimate that bound efficiently.</p>
<p>Let’s first move around a few pieces of the first term above:</p>
<p><span class="math display">
\begin{align}
\mathbb{E}_{q(z;\lambda)}[(\frac{p(z|x)}{q(z;\lambda)})^2&amp; = 1 + D_{\chi^2}(p(z|x)|q(z;\lambda)) \\
&amp;= p(x)^2[1 + D_{\chi^2}(p(z|x)|q(z;\lambda))]
\end{align}
</span> Then we can take the log of both sides of the equation, which gives us:</p>
<p><span class="math display">
\frac{1}{2}log(1 + D_{\chi^2}(p(z|x)|q(z;\lambda))) = -logp(x) + \frac{1}{2}log\mathbb{E}_{q(z;\lambda)}[(\frac{p(z|x)}{q(z;\lambda)})^2]  
</span> …and this is starting to feel a lot like the ELBO derivation. Log is monotonic, and the <span class="math inline">-logp(x)</span> term is constant as we optimize <span class="math inline">q</span>, so we’ve found something that we’re close to able to minimize:</p>
<p><span class="math display">
CUBO_{2}(\lambda) = \frac{1}{2}log\mathbb{E}_{q(z;\lambda)}[(\frac{p(z|x)}{q(z;\lambda)})^2]
</span> Since this new divergence is non-negative as well, this is a upper bound of the model evidence. This is thus named <span class="math inline">\chi</span> upper bound (CUBO)<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
</section>
<section id="but-can-we-estimate-it" class="level2">
<h2 class="anchored" data-anchor-id="but-can-we-estimate-it">… But can we estimate it?</h2>
<p>One other issue here: how do we estimate this? The CUBO objective got rid of the <span class="math inline">logp(x)</span> we were worried about, but it seems like that expectation is going to be difficult to estimate in general.</p>
<p>Your first idea might be to Monte Carlo (not MCMC) estimate it roughly like this:</p>
<p><span class="math display">
CUBO_2(\lambda) = \frac{1}{2}log\frac{1}{S}\sum_{s=1}^{S}[(\frac{p(x,z^{s})}{q(z^{s};\lambda)})^2]
</span> Unfortunately, the <span class="math inline">log</span> transform here means our Monte Carlo estimator will be biased: we can see this by applying Jensen’s inequality to the above. To make this stably act as an upper bound, we can apply a clever transformation:</p>
<p><span class="math display">
\bf{L} = exp(n* CUBO_2(\lambda))
</span></p>
<p>Since exp is monotonic, this has the same objective as the CUBO, but we can Monte Carlo estimate it unbiasedly. Is that the last problem to solve?</p>
</section>
<section id="but-can-we-calculate-gradients-efficiently" class="level2">
<h2 class="anchored" data-anchor-id="but-can-we-calculate-gradients-efficiently">… But can we calculate gradients efficiently?</h2>
<p>Wait, wait no. Sorry to keep saying there’s one more step here, but there’s a lot that goes into making a full, convenient, general use algorithm here. The last step (for real this time) is that we need to figure out how to get gradients for the estimate of <span class="math inline">\bf{L}</span> above, <span class="math inline">\bf{\hat{L}}</span>. The issue is that we don’t have any guarantee that a unbiased Monte Carlo estimator of <span class="math inline">\bf{\hat{L}}</span> gets us a Monte Carlo way to estimate <span class="math inline">\nabla_\lambda\bf{\hat{L}}</span>- we can’t guarantee that the gradient of the expectation is equal to the expectation of the gradient.</p>
<p>For this, we need to pull out a trick from the variational autoencoder literature. This is usually referred to as the “Reparameterization Trick”<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, but the original CHIVI paper refers to them as “reparmeterization gradients”. We will assume we can rewrite the generative process of our model as <span class="math inline">z = g(\lambda,\epsilon)</span>, where <span class="math inline">\epsilon \sim p(\epsilon)</span> and g being a deterministic function. Then we have a new estimator for both <span class="math inline">\bf{\hat{L}}</span> and it’s gradient:</p>
<p><span class="math display">
\begin{align}
\bf{\hat{L}} &amp;= \frac{1}{B}\sum_{b=1}^B(\frac{p(x,g(\lambda,\epsilon^{(b)}))}{q(g(\lambda,\epsilon^{(b)};\lambda))})^{2} \\
\nabla_\lambda\bf{\hat{L}}  &amp;= \frac{2}{B}\sum_{b=1}^B(\frac{p(x,g(\lambda,\epsilon^{(b)}))}{q(g(\lambda,\epsilon^{(b)};\lambda))})^2 \nabla_\lambda log(\frac{p(x,g(\lambda,\epsilon^{(b)}))}{q(g(\lambda,\epsilon^{(b)};\lambda))})
\end{align}
</span> There are one or two more neat computational tricks in the paper I won’t explain here (essentially: how do we extend this to work in minibatch fashion, and how do we avoid numerical underflow issues), but this is now essentially functional. The whole algorithm, which they dubbed CHIVI is below:</p>
<p><img src="images/CHIVI_algorithm.png" class="img-fluid"></p>
</section>
</section>
<section id="the-bigger-picture-again" class="level1">
<h1>The Bigger Picture Again</h1>
<p>Stepping back, let’s talk about some practical properties of the algorithm we’ve been stepping through.</p>
<p>First, and probably most exciting given what we saw in the last post, CHIVI’s objective has the property that it is inclusive, unlike the ELBO we were using earlier. This won’t be the right choice for all variational inference problems, but given our prior issues with very narrow posteriors this will be exciting to test<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. And as we’ll see in the next section, this overdispersion tendency in the posterior will often have a beneficial interaction with importance sampling which can improve our estimates further.</p>
<p>Another nice thing here is that if we were to estimate both the ELBO and CUBO for a given problem, we’d get both a upper and lower bound on the model evidence. This is theoretically convenient in that we now have a sandwich estimator, which actually obtains reasonably tight bounds. We’ll even be able use the fact we have both later to get some bounds on practical bounds on quantities we tend to report in practice like means and covariances!</p>
<p>A final neat benefit here is that to the extent we are willing to consider ensembling models (again, more on that soon), this CHIVI framework will produce estimates that succeed (and fail) in less similar ways that the the ELBO based estimators we looked at last post. Expanding our available set of tools is always good, but it’s even better when we’re ensembling because we can lean more heavily on each model for the tasks it succeeds on.</p>
<p>One potential downside here is that we introduced a solution that partially relies on a Monte Carlo estimator. That said, this is pretty cheap in practice; if we’re using VI as a drop in for MCMC, this is still going to be much much faster than MCMC for any big problem. We’ll need to think about a reasonable number of samples in a given case, but realistically this isn’t going to be a driving factor in determining compute time.</p>
<p>Another final problem is that the estimator we built out for the CUBO that we could actually estimate tends to end up having pretty high variance. Exponentiating the objective isn’t free in that sense; but this problem of variance reduction in estimators is something that feels like a tractable problem to iterate on.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>In truth, both KL divergences encode structural preferences for the type of optimization solution they admit- neither will be the right choice for every problem and variational family combination. But as we’ll see, being able to choose will give us more options to fit models we believe.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>This is the footnote for those of you that are annoyed because you tried to write out how this would happen, and got something like <span class="math inline">p(x) log \frac{p(x)}{0}</span>, which should be undefined if we’re following normal math rules. But this is information theory, and in this strange land we say <span class="math inline">p(x) log \frac{p(x)}{0} = \infty</span>. I don’t have a strong intuition for why this is the best solution, but a information encoding perspective makes it make more sense at least: if we know the distribution of <span class="math inline">p</span>, we can construct a code for it with average description length <span class="math inline">H(x)</span>. One way to understand the KL divergence is as what happens when we try to use the code for a distribution <span class="math inline">q</span> to describe <span class="math inline">p</span>, we’d need <span class="math inline">H(p) + KL(p||q)</span> bits on average to describe <span class="math inline">p</span>. In the code for <span class="math inline">q</span> has no way to represent some element of <span class="math inline">p</span>, then requiring… infinite bits feels like the right way to describe the breakdown of meaning? All this to say this condition is something our optimizer will try hard to avoid.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>I’ll mention an alternative approach, Expectation Propagation, that takes a different (not global objective based) approach further down.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>This approach actually defines a family of <span class="math inline">n</span> new divergences, where you replace the <span class="math inline">\frac{1}{2}</span> with <span class="math inline">\frac{1}{n}</span> and similarly replace the square an exponent with n.&nbsp;Fully stepping through why this is neat wasn’t worth how far afield it’d take us, but the original <span class="math inline">CHIVI</span> paper has some cool derivations based on this, one of which I’ll discuss on the next section on importance sampling.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>I thought about a section to explain the reparameterization trick, but there are enough good explanations online of the trick. If you’re interested in learning more about why this is important for optimization through stocastic models, I’d recommend starting with Gregory Gundersen’s explanation <a href="https://gregorygundersen.com/blog/2018/04/29/reparameterization/">here</a> and then move on to the original Kingma &amp; Welling, 2013 paper. As general advice on understanding it better though, I’ll echo Greg’s point that some of the online explanations I’ve seen are a bit loose- the key is that we want to express a gradient of an expectation (can’t MC estimate for sure) as an expectation of a gradient (which we can MC estimate provided our convenient deterministic function <span class="math inline">g</span> is differentiable).<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>In a few posts, we’re theoryposting for a bit.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div id="quarto-reuse" class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2023,
  author = {Timm, Andy},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2023-05-02},
  url = {https://andytimm.github.io/posts/Variational MRP Pt3/variational_mrp_3.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2023" class="csl-entry quarto-appendix-citeas" role="listitem">
Timm, Andy. 2023. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> May 2, 2023. <a href="https://andytimm.github.io/posts/Variational MRP Pt3/variational_mrp_3.html">https://andytimm.github.io/posts/Variational
MRP Pt3/variational_mrp_3.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>