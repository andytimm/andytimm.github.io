<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Andy Timm">
<meta name="dcterms.date" content="2023-05-27">

<title>Andy Timm - Variational Inference for MRP with Reliable Posterior Distributions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Andy Timm - Variational Inference for MRP with Reliable Posterior Distributions">
<meta property="og:description" content="Part 5- Normalizing Flows">
<meta property="og:site-name" content="Andy Timm">
<meta name="twitter:title" content="Andy Timm - Variational Inference for MRP with Reliable Posterior Distributions">
<meta name="twitter:description" content="Part 5- Normalizing Flows">
<meta name="twitter:creator" content="@andy_timm">
<meta name="twitter:site" content="@andy_timm">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Andy Timm</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects.html">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../software.html">
 <span class="menu-text">Software</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html">
 <span class="menu-text">Rarely Updated Blog</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Variational Inference for MRP with Reliable Posterior Distributions</h1>
            <p class="subtitle lead">Part 5- Normalizing Flows</p>
                                <div class="quarto-categories">
                <div class="quarto-category">MRP</div>
                <div class="quarto-category">Variational Inference</div>
                <div class="quarto-category">Normalizing Flows</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Andy Timm </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 27, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#a-problem-adaptive-variational-family-with-less-tinkering" id="toc-a-problem-adaptive-variational-family-with-less-tinkering" class="nav-link active" data-scroll-target="#a-problem-adaptive-variational-family-with-less-tinkering">A problem adaptive variational family with less tinkering?</a></li>
  <li><a href="#what-is-a-normalizing-flow" id="toc-what-is-a-normalizing-flow" class="nav-link" data-scroll-target="#what-is-a-normalizing-flow">What is a normalizing flow?</a>
  <ul>
  <li><a href="#normalizing-flows-for-variational-inference-versus-other-applications" id="toc-normalizing-flows-for-variational-inference-versus-other-applications" class="nav-link" data-scroll-target="#normalizing-flows-for-variational-inference-versus-other-applications">Normalizing Flows for variational inference versus other applications</a></li>
  </ul></li>
  <li><a href="#how-to-train-your-neural-net" id="toc-how-to-train-your-neural-net" class="nav-link" data-scroll-target="#how-to-train-your-neural-net">How to train your neural net</a></li>
  <li><a href="#a-basic-flow" id="toc-a-basic-flow" class="nav-link" data-scroll-target="#a-basic-flow">A basic flow</a></li>
  <li><a href="#what-more-complicated-nns-look-like" id="toc-what-more-complicated-nns-look-like" class="nav-link" data-scroll-target="#what-more-complicated-nns-look-like">What more complicated NNs look like</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>This is section 5 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.</p>
<p>The general structure for this post and the around it will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in these four posts will go a long way towards more robust variational inference. I’ll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.</p>
<p>In the <a href="https://andytimm.github.io/posts/Variational%20MRP%20Pt4/variational_mrp_4.html">last post</a> we saw a variety of different ways importance sampling can be used to improve VI and make it more robust, from defining a tighter bound to optimize in the importance weighted ELBO, to weighting <span class="math inline">q(x)</span> samples together efficiently to look more like <span class="math inline">p(x)</span>, to combining entirely different variational approximations together to cover different parts of the posterior with multiple importance sampling.</p>
<p>In this post, we’ll tackle the problem of how to define a deeply flexible variational family <span class="math inline">\mathscr{Q}</span> that can adapt to each problem while still being easy to sample from. To do this, we’ll draw on normalizing flows, a technique for defining a composition of invertible transformations on top of a simple base distribution like a normal distribution. We’ll build our way up to using increasingly complex neural networks to define those transformations, allowing for for truly complex variational families that are problem adaptive, training as we train our variational model.</p>
<p>The rough plan for the series is as follows:</p>
<ol type="1">
<li>Introducing the Problem- Why is VI useful, why VI can produce spherical cows</li>
<li>How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li>Problem 1: KL-D prefers exclusive solutions; are there alternatives?</li>
<li>Problem 2: Not all VI samples are of equal utility; can we weight them cleverly?</li>
<li><strong>(This post)</strong> Problem 3: How can we get deeply flexible variational approximations; are Normalizing Flows the answer?</li>
<li>Problem 4: How can we know when VI is wrong? Are there useful error bounds?</li>
<li>Better grounded diagnostics and workflow</li>
</ol>
<section id="a-problem-adaptive-variational-family-with-less-tinkering" class="level1">
<h1>A problem adaptive variational family with less tinkering?</h1>
<p><img src="images/flows_stairs_meme.png" class="img-fluid" alt="Something about NNs makes me meme more"></p>
<p>Jumping from mean-field or full-rank Gaussians and similar distributions to neural networks feels a little… dramatic<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, so I want to spend some time justifying why this is a good idea.</p>
<p>For VI to work well, we need something that’s still simple to sample from, but capable of, in aggregate, representing a posterior that is probably pretty complex. Certainly, some problems are amenable to the simple variational families <span class="math inline">\mathscr{Q}</span> we’ve tried so far, but it’s worth re-emphasizing that we’re probably trying to represent something complex, and even moderate success at that using a composition of normals should be a little surprising, not the expected outcome.</p>
<p>If we need <span class="math inline">\mathscr{Q}</span> to be more complex, aren’t there choices between what we’ve seen and a neural network? There’s a whole literature of them- from using mixture distributions as variational distributions to inducing some additional structure into a mean-field type solution if you have some specific knowledge about your target posterior you can use. By and large though, this type of class of solutions has been surpassed by normalizing flows in much of modern use for more complex posteriors.</p>
<p>Why? A first reason is described in the paper that started the normalizing flows for VI literature, Rezende and Mohamed’s <a href="https://arxiv.org/pdf/1505.05770.pdf"><strong>Variational Inference with Normalizing Flows </strong></a>: making our base variational distribution more complex adds a variety of different computational costs, which add up quickly. This isn’t the most face-valid argument when I’m claiming a neural network is a good alternative, but it gets more plausible when you think through how poorly it’d scale to keep making your mixture distribution more and more complex as your posteriors get harder to handle. So this is a <em>scalability</em> argument- it might sound extreme to bring in a neural net, but as problems get bigger, scaling matters.</p>
<p>The other point I’d raise is that all these other tools aren’t very black box at all- if we can make things work with a problem-adapted version of mean-field with some structure based on the knowledge of a specific problem we have, that sounds like it gets time consuming fast. If I’m going to have to find a particular, problem-specific solution each time I want to use variational inference, that feels fragile and fiddly as well- that’s a poor user experience.</p>
<p>The novel idea with normalizing flows is that we’ll start with a simple base density like a normal distribution that is easy to sample from, but instead of only optimizing the parameters of that normal distribution, we’ll also use the training on our ELBO or other objective to learn a transformation that reshapes that normal distribution to look like our posterior. By having that transforming component be partially composed of a neural network, we give ourselves access to an incredibly expressive, automatically problem adaptive, and heavily scalable variant of variational inference that is quite widely used.</p>
<p>And if the approximation isn’t expressive enough? Deep Learning researchers have an unfussy, general purpose innovation for that: MORE LAYERS!<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p><img src="images/more_layers.png" class="img-fluid" alt="Wow such estimator, very deep"></p>
</section>
<section id="what-is-a-normalizing-flow" class="level1">
<h1>What is a normalizing flow?</h1>
<p>A normalizing flow transforms a simple base density into a complex one through a sequence of invertible transformations. By stacking more and more of these invertible transformations (having the density “flow” through them), we can create arbitrarily complex distributions that remain valid probability distributions. Since it isn’t universal in the flows literature, let me be explicit that I’ll consider “forward” to be the direction flowing from the base density to the posterior, and the “backward” or “normalizing” direction as towards the base density.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/normalizing-flow.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Image Credit to <a href="https://siboehm.com/articles/19/normalizing-flow-network">Simon Boehm</a> here</figcaption><p></p>
</figure>
</div>
<p>If we have a random variable <span class="math inline">x</span>, with distribution <span class="math inline">q(x)</span>, some function <span class="math inline">f</span> with an inverse <span class="math inline">f^{-1} = g, g \circ f(x) = x</span>, then the distribution of the result of one iteration of x through, <span class="math inline">q^\prime(x)</span> is:</p>
<p><span class="math display">
q\prime(z) = q(x) \lvert det \frac{\partial f^{-1}}{\partial x^\prime} \rvert = q(x) \lvert \frac{\partial f}{\partial x} \rvert^{-1}
</span> I won’t derive this identity<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, but it follows from the chain rule and the properties of Jacobians of invertible functions.</p>
<p>The real power comes in here when we see that these transformations stack. If we’ve got a chain of transformations (eg <span class="math inline">f_K(...(f_2f_1(x))</span>:</p>
<p><span class="math display">
x_K = f(x) \circ ... \circ f_2 \circ f_1(x_0)
</span></p>
<p>then the resulting density <span class="math inline">q_K(x)</span> looks like:</p>
<p><span class="math display">
ln q_K (x_K) = lnq_0(x_0) - \sum \limits_{K = 1}\limits^{K} ln  \lvert \frac{\partial f_k}{\partial x_{k-1}} \rvert^{-1}
</span></p>
<p>Neat, and surprisingly simple! If the terms above are all easy to calculate, we can very efficiently stack a bunch of these transformations and make an expressive model.</p>
<section id="normalizing-flows-for-variational-inference-versus-other-applications" class="level2">
<h2 class="anchored" data-anchor-id="normalizing-flows-for-variational-inference-versus-other-applications">Normalizing Flows for variational inference versus other applications</h2>
<p>One source of confusion when I was learning about normalizing flows for variational inference was that variational inference makes up a fairly small proportion of the use cases for normalizing flows, and thus the academic literature and online discussion. More common applications include density estimation, image generation, representation learning, and reinforcement learning. In addition to making specifically applicable discussions harder to find, often resources will make strong claims about properties of a given flow structure, that really only holding in some subset of the above applications<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
<p>By taking a second to explain this crisply and compare different application’s needs, hopefully I can save you some confusion and make engaging with the broader literature easier.</p>
<p>To start, consider the relevant operations we’ve introduced so far:</p>
<ol type="1">
<li>computing <span class="math inline">f</span>, that is pushing a sample through the transformations</li>
<li>computing <span class="math inline">g</span>, <span class="math inline">f</span>’s inverse which undoes the manipulations</li>
<li>computing the (log) determinant of the Jacobian</li>
</ol>
<p>1 and 3 definitely need to be efficient for our use case, since we need to be able to sample and push through using the formula above efficiently to calculate an ELBO and train our model. 2 is where things get more subtle: we definitely need <span class="math inline">f</span> to be invertible, since our formulas above are dependent on a property of Jacobians of invertible functions. But we don’t actually really need to explicitly compute <span class="math inline">g</span> for variational inference. Even knowing the inverse exists but not having a formula might be fine for us!</p>
<p>Contrast this with density estimation, where the goal would not to sample from the distribution, but instead to estimate the density. In this case, most of the time would be spent going in the opposite direction, so that they can evaluate the log-likliehood of the data, and maximize it to improve the model<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. The need for an expressive transformation of densities unite these two cases, but the goal is quite different!</p>
<p>This level of goal disagreement also shows it face in what direction papers choose to call forward: Most papers outside of variational inference applications consider forward to be the opposite of what I do here, the direction towards the base density, the “normalizing” direction.</p>
<p>For our use, hopefully this short digression has clarified which operations we need to be fast versus just exist. If you dive deeper into further work on normalizing flows, hopefully recognizing there are two different ways to point this thing helps you more quickly orient yourself to how other work describe flows.</p>
</section>
</section>
<section id="how-to-train-your-neural-net" class="level1">
<h1>How to train your neural net</h1>
<p>Now, let’s turn to how we actually fit a normalizing flow. Since this would be a bit hard to grok a code presentation if I took advantage of the full flexibility and abstraction that something like <a href="https://github.com/abhiagwl/vistan/tree/master"><code>vistan</code></a> provides, before heading into general purpose tools I’ll talk through a bit more explicit implementation of a simpler flow called a planar flow <code>PyTorch</code> for illustration. Rather than reinventing the wheel, I’ll leverage Edvard Hulten’s implementation <a href="https://github.com/e-hulten/planar-flows">here</a>.</p>
<p>In this section, I’ll define conceptually how we’re fitting the model, and build out a fun target distribution and loss`- since I expect many people reading this may moderately new to PyTorch, I’ll explain in detail than normal what each operation is doing and why we need it.</p>
<p>Let’s first make a fun target posterior distribution from an image to model. I think it’d be a fun preview gif for the post to watch the model say Hi:</p>
<p><img src="images/hard_to_draw_posterior.png" class="img-fluid" alt="Wow such estimator, very deep"></p>
<p>It’s quick to turn the 300x300 pixel image above into a 300x300 PyTorch tensor. To represent this as a 2-D density we can fit models against, we’ll read in the image, collapse along the color dimension, and transform it into a torch tensor:</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>raw_img <span class="op">=</span> Image.<span class="bu">open</span>(<span class="st">"images/hard_to_draw_posterior.png"</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Sum the 3 color channels</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>greyscale_img <span class="op">=</span> np.array(raw_img).<span class="bu">sum</span>(axis <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace white values (1020), with 0, so density is all at letters</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> greyscale_img <span class="op">==</span> <span class="dv">1020</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace the selected values with 0</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>greyscale_img[indices] <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize values to help with fitting</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>normalized_image  <span class="op">=</span> (greyscale_img <span class="op">-</span> greyscale_img.<span class="bu">min</span>()) <span class="op">/</span> (<span class="dv">10000000</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Make a torch tensor of the target to use</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>torch_posterior <span class="op">=</span> torch.from_numpy(normalized_image).to(device)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co">#300 x 300 px, normalized grayscale representation of the above image</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>torch_posterior.<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>tensor(0.9065, device='cuda:0', dtype=torch.float64)</code></pre>
</div>
</div>
<p>Now let’s define our loss for training, which will just be a slight reformulation of our ELBO:</p>
<p><span class="math display">
\mathbb{E}[logp(z,x)] - \mathbb{E}[logq(z)]
</span></p>
<p>To do this, we’ll define a class for the loss.</p>
<p>First, we pick a simple base distribution to push through our flow, here a 2-D Normal distribution called <code>base_distr</code>. We’ll also include the interesting target we just made above, <code>distr</code>.</p>
<p>Next, the forward pass structure. The <code>forward</code> method is the is the core of the computational graph structure in PyTorch. It defines operations that are applied to the input tensors to compute the output, and gives PyTorch the needed information for automatic differentiation, which allows smooth calculation and backpropogation of loss through the model to train it. This <code>VariationalLoss</code> module will run at the end of the forward pass to calculate the loss and allow us to pass it back through the graph for training.</p>
<p>Keeping with the structure above of numbering successive stages of the flow, <code>z0</code> here is our base distribution, and <code>z</code> will be the learned approximation to the target. In addition to the terms you’d expect in the ELBO, we’re also tracking and making use of the sum of the log determinant of the Jacobians to a handle on the distortion of the base density the flows apply.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># https://github.com/e-hulten/planar-flows/blob/master/loss.py</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VariationalLoss(nn.Module):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,distribution):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>      <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.distr <span class="op">=</span> distribution</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.base_distr <span class="op">=</span> MultivariateNormal(torch.zeros(<span class="dv">2</span>), torch.eye(<span class="dv">2</span>))</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> forward(<span class="va">self</span>, z0: Tensor, z: Tensor, sum_log_det_J: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>      base_log_prob <span class="op">=</span> <span class="va">self</span>.base_distr.log_prob(z0)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>      target_density_log_prob <span class="op">=</span> <span class="op">-</span><span class="va">self</span>.distr(z)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span> (base_log_prob <span class="op">-</span> target_density_log_prob <span class="op">-</span> sum_log_det_J).mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="a-basic-flow" class="level1">
<h1>A basic flow</h1>
<p>Next, let’s define the structure of the actual flow. To do this, we’ll first describe a single layer of the flow, then we’ll show structure to stack the flow in layers.</p>
<p>Our first flow we look at will be the <strong>planar flow</strong> from the original Normalizing Flows for variational Inference paper mentioned above. The name comes from how the function defines a (hyper)plane, and compress or expand the density around it:</p>
<p><span class="math display">
f(x) = x + u*tanh(w^Tx + b), w, u \in   \mathbb{R}^d, b \in \mathbb{R}
</span></p>
<p><span class="math inline">w</span> and <span class="math inline">b</span> define the hyperplane and u specifies the direction and strength of the expansion. I’ll show a visualization of just one layer of that below.</p>
<p>If you’re more used to working with neural nets, you might wonder why we choose the non-linearity <span class="math inline">tanh</span> here, which generally isn’t as popular as something like <span class="math inline">relu</span> or it’s variants in more recent years due to it’s more unstable gradient flows. As the authors show in appendix <span class="math inline">A.1</span>, functions like the above aren’t actually always invertible, and choosing <span class="math inline">tanh</span> allows them to impose some constraints that make things reliably invertible. See the Appendix for more details about how that works, or take a careful look at Edvard’s implementation of the single function below:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># From https://github.com/e-hulten/planar-flows/blob/master/planar_transform.py</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PlanarTransform(nn.Module):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""Implementation of the invertible transformation used in planar flow:</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">      f(z) = z + u * h(dot(w.T, z) + b)</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">  See Section 4.1 in https://arxiv.org/pdf/1505.05770.pdf. </span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">  """</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2</span>):</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>      <span class="co">"""Initialise weights and bias.</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">      </span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">      Args:</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">          dim: Dimensionality of the distribution to be estimated.</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">      """</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>      <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.w <span class="op">=</span> nn.Parameter(torch.randn(<span class="dv">1</span>, dim).normal_(<span class="dv">0</span>, <span class="fl">0.1</span>))</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.b <span class="op">=</span> nn.Parameter(torch.randn(<span class="dv">1</span>).normal_(<span class="dv">0</span>, <span class="fl">0.1</span>))</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.u <span class="op">=</span> nn.Parameter(torch.randn(<span class="dv">1</span>, dim).normal_(<span class="dv">0</span>, <span class="fl">0.1</span>))</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> forward(<span class="va">self</span>, z: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> torch.mm(<span class="va">self</span>.u, <span class="va">self</span>.w.T) <span class="op">&lt;</span> <span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>          <span class="va">self</span>.get_u_hat()</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span> z <span class="op">+</span> <span class="va">self</span>.u <span class="op">*</span> nn.Tanh()(torch.mm(z, <span class="va">self</span>.w.T) <span class="op">+</span> <span class="va">self</span>.b)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> log_det_J(<span class="va">self</span>, z: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> torch.mm(<span class="va">self</span>.u, <span class="va">self</span>.w.T) <span class="op">&lt;</span> <span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>          <span class="va">self</span>.get_u_hat()</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>      a <span class="op">=</span> torch.mm(z, <span class="va">self</span>.w.T) <span class="op">+</span> <span class="va">self</span>.b</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>      psi <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> nn.Tanh()(a) <span class="op">**</span> <span class="dv">2</span>) <span class="op">*</span> <span class="va">self</span>.w</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>      abs_det <span class="op">=</span> (<span class="dv">1</span> <span class="op">+</span> torch.mm(<span class="va">self</span>.u, psi.T)).<span class="bu">abs</span>()</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>      log_det <span class="op">=</span> torch.log(<span class="fl">1e-4</span> <span class="op">+</span> abs_det)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span> log_det</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> get_u_hat(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>      <span class="co">"""Enforce w^T u &gt;= -1. When using h(.) = tanh(.), this is a sufficient condition </span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a><span class="co">      for invertibility of the transformation f(z). See Appendix A.1.</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="co">      """</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>      wtu <span class="op">=</span> torch.mm(<span class="va">self</span>.u, <span class="va">self</span>.w.T)</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>      m_wtu <span class="op">=</span> <span class="op">-</span><span class="dv">1</span> <span class="op">+</span> torch.log(<span class="dv">1</span> <span class="op">+</span> torch.exp(wtu))</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.u.data <span class="op">=</span> (</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>          <span class="va">self</span>.u <span class="op">+</span> (m_wtu <span class="op">-</span> wtu) <span class="op">*</span> <span class="va">self</span>.w <span class="op">/</span> torch.norm(<span class="va">self</span>.w, p<span class="op">=</span><span class="dv">2</span>, dim<span class="op">=</span><span class="dv">1</span>) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>      )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Where things will start to get exciting is multiple layers of the flow; here’s how we can make an abstraction that allows us to stack up <span class="math inline">K</span> layers of the flow to control the flexibility of our approximation.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PlanarFlow(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2</span>, K: <span class="bu">int</span> <span class="op">=</span> <span class="dv">6</span>):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Make a planar flow by stacking planar transformations in sequence.</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co">            dim: Dimensionality of the distribution to be estimated.</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">            K: Number of transformations in the flow. </span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> [PlanarTransform(dim) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(K)]</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> nn.Sequential(<span class="op">*</span><span class="va">self</span>.layers)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, z: Tensor) <span class="op">-&gt;</span> Tuple[Tensor, <span class="bu">float</span>]:</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        log_det_J <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>            log_det_J <span class="op">+=</span> layer.log_det_J(z)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> layer(z)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z, log_det_J</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s run this for a single layer to introduce the training loop, and build some intuition on the planar flow. Note that I’m hiding setting up the plot code.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">#From https://github.com/e-hulten/planar-flows/blob/master/train.py</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>target_distr <span class="op">=</span> <span class="st">"ring"</span>  <span class="co"># U_1, U_2, U_3, U_4, ring</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>flow_length <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>num_batches <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">6e-4</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>axlim <span class="op">=</span> xlim <span class="op">=</span> ylim <span class="op">=</span> <span class="dv">5</span>  <span class="co"># 5 for U_1 to U_4, 7 for ring</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>density <span class="op">=</span> TargetDistribution(target_distr)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> PlanarFlow(dim, K<span class="op">=</span>flow_length)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> VariationalLoss(density)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>optimiser <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Train model.</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch_num <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num_batches <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get batch from N(0,I).</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    batch <span class="op">=</span> torch.zeros(size<span class="op">=</span>(batch_size, <span class="dv">2</span>)).normal_(mean<span class="op">=</span><span class="dv">0</span>, std<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Pass batch through flow.</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    zk, log_jacobians <span class="op">=</span> model(batch)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute loss under target distribution.</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> bound(batch, zk, log_jacobians)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    optimiser.zero_grad()</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    optimiser.step()</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> batch_num <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"(batch_num </span><span class="sc">{</span>batch_num<span class="sc">:05d}</span><span class="ss">/</span><span class="sc">{</span>num_batches<span class="sc">}</span><span class="ss">) loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(log_jacobians)</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> batch_num <span class="op">==</span> <span class="dv">1</span> <span class="kw">or</span> batch_num <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save plots during training. Plots are saved to the 'train_plots' folder.</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>        plot_training(model, flow_length, batch_num, lr, axlim) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Python311\Lib\site-packages\torch\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\aten\src\ATen\native\TensorShape.cpp:3484.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 00100/20000) loss: 23.53183937072754</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\timma\AppData\Local\Temp\ipykernel_1480\4251992244.py:69: UserWarning: The input coordinates to pcolormesh are interpreted as cell centers, but are not monotonically increasing or decreasing. This may lead to incorrectly calculated cell edges, in which case, please supply explicit cell edges to pcolormesh.
  ax.pcolormesh(</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 00200/20000) loss: 20.994998931884766</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 00300/20000) loss: 20.90053939819336</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 00400/20000) loss: 18.84044647216797</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 00500/20000) loss: 16.77310562133789</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 00600/20000) loss: 16.30998992919922</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 00700/20000) loss: 11.962639808654785</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 00800/20000) loss: 11.01216983795166</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 00900/20000) loss: 10.37910270690918</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 01000/20000) loss: 8.418041229248047</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 01100/20000) loss: 6.021894931793213</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 01200/20000) loss: 5.717703819274902</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 01300/20000) loss: 3.8855597972869873</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 01400/20000) loss: 3.86718487739563</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 01500/20000) loss: 3.907670021057129</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 01600/20000) loss: 1.847822904586792</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 01700/20000) loss: 4.638023853302002</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 01800/20000) loss: 3.2167813777923584</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 01900/20000) loss: 1.8979766368865967</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 02000/20000) loss: 2.2743802070617676</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 02100/20000) loss: 1.3806898593902588</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 02200/20000) loss: 1.5938010215759277</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 02300/20000) loss: 2.74418044090271</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 02400/20000) loss: 3.166958808898926</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 02500/20000) loss: 2.990528106689453</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 02600/20000) loss: 1.1335718631744385</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 02700/20000) loss: 1.306429386138916</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 02800/20000) loss: 1.2546987533569336</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 02900/20000) loss: 0.30904117226600647</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 03000/20000) loss: 1.9958691596984863</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 03100/20000) loss: 1.4669091701507568</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 03200/20000) loss: 1.7111603021621704</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 03300/20000) loss: 1.8202102184295654</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 03400/20000) loss: 1.8184399604797363</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 03500/20000) loss: 1.2064381837844849</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 03600/20000) loss: 0.8316801190376282</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 03700/20000) loss: 0.12350399792194366</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 03800/20000) loss: 0.7493021488189697</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 03900/20000) loss: 1.421569585800171</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 04000/20000) loss: 0.44843626022338867</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 04100/20000) loss: 0.6824027895927429</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 04200/20000) loss: 0.7544111013412476</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 04300/20000) loss: 0.8090195655822754</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 04400/20000) loss: 0.5537429451942444</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 04500/20000) loss: 0.04105612635612488</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 04600/20000) loss: 0.6974716186523438</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 04700/20000) loss: 0.19744260609149933</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 04800/20000) loss: 0.20983920991420746</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 04900/20000) loss: -0.1809486746788025</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 05000/20000) loss: 0.37607288360595703</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 05100/20000) loss: 0.9134331941604614</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 05200/20000) loss: 0.3730510175228119</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 05300/20000) loss: -0.47192516922950745</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 05400/20000) loss: 0.8290470838546753</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 05500/20000) loss: 1.3974835872650146</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 05600/20000) loss: 0.09541454911231995</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 05700/20000) loss: -0.13839580118656158</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 05800/20000) loss: 0.11648982018232346</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 05900/20000) loss: -0.6386630535125732</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 06000/20000) loss: 0.9324575662612915</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 06100/20000) loss: -0.24909564852714539</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 06200/20000) loss: 1.4798192977905273</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 06300/20000) loss: 0.43216240406036377</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 06400/20000) loss: -0.7252155542373657</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 06500/20000) loss: 0.5799909234046936</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 06600/20000) loss: -0.21017222106456757</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 06700/20000) loss: -0.3734526038169861</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 06800/20000) loss: 0.5551584959030151</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 06900/20000) loss: 0.1255159080028534</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 07000/20000) loss: -0.5630631446838379</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 07100/20000) loss: -0.030557140707969666</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 07200/20000) loss: -0.7508930563926697</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 07300/20000) loss: -0.5233619213104248</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 07400/20000) loss: 0.28095123171806335</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 07500/20000) loss: 0.07843410968780518</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 07600/20000) loss: 0.5511898994445801</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 07700/20000) loss: -0.1848641335964203</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 07800/20000) loss: 0.11113755404949188</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 07900/20000) loss: 0.008256569504737854</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 08000/20000) loss: -0.19947832822799683</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 08100/20000) loss: -0.45735034346580505</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 08200/20000) loss: 0.5538589954376221</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 08300/20000) loss: -0.10944157838821411</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 08400/20000) loss: -0.4294120669364929</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 08500/20000) loss: -0.25491130352020264</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 08600/20000) loss: -0.5831103324890137</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 08700/20000) loss: 0.04871252179145813</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 08800/20000) loss: -0.4299813508987427</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 08900/20000) loss: 0.2146015167236328</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 09000/20000) loss: 0.1435202658176422</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 09100/20000) loss: -0.716264009475708</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 09200/20000) loss: -0.6629137396812439</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 09300/20000) loss: -0.30255424976348877</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 09400/20000) loss: -0.09136298298835754</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 09500/20000) loss: -0.39034590125083923</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 09600/20000) loss: -0.4760989844799042</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 09700/20000) loss: 0.29063454270362854</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 09800/20000) loss: -0.5586217641830444</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 09900/20000) loss: -0.43544644117355347</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 10000/20000) loss: -0.9282410144805908</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 10100/20000) loss: 0.19365458190441132</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 10200/20000) loss: 0.0816102921962738</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 10300/20000) loss: -1.0993659496307373</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 10400/20000) loss: -0.21989655494689941</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 10500/20000) loss: -0.5877478718757629</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 10600/20000) loss: -0.40770483016967773</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 10700/20000) loss: -1.0903253555297852</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 10800/20000) loss: -0.5532143712043762</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 10900/20000) loss: -0.16332033276557922</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 11000/20000) loss: -0.4744631052017212</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 11100/20000) loss: -0.09918088465929031</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 11200/20000) loss: -0.567780077457428</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 11300/20000) loss: -0.7111999988555908</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 11400/20000) loss: -0.7287379503250122</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 11500/20000) loss: -0.0649578869342804</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 11600/20000) loss: 0.25484105944633484</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 11700/20000) loss: -0.5245593786239624</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 11800/20000) loss: -0.6941218376159668</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 11900/20000) loss: 0.5740460157394409</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 12000/20000) loss: -0.20360657572746277</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 12100/20000) loss: -0.7829161882400513</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 12200/20000) loss: -0.08825202286243439</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 12300/20000) loss: -1.2766939401626587</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 12400/20000) loss: -0.7791504263877869</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 12500/20000) loss: -0.39922642707824707</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 12600/20000) loss: -0.15501278638839722</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 12700/20000) loss: -0.3032105565071106</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 12800/20000) loss: -0.5128021240234375</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 12900/20000) loss: -0.8823572993278503</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 13000/20000) loss: -0.6840925216674805</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 13100/20000) loss: -0.12149892747402191</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 13200/20000) loss: -0.666481614112854</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 13300/20000) loss: -0.8941734433174133</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 13400/20000) loss: -1.0827946662902832</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 13500/20000) loss: -1.045516848564148</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 13600/20000) loss: -0.36946970224380493</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 13700/20000) loss: -1.2924668788909912</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 13800/20000) loss: -0.9714236259460449</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 13900/20000) loss: -0.13862578570842743</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 14000/20000) loss: -0.6488382816314697</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 14100/20000) loss: -0.8653355836868286</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 14200/20000) loss: 0.01034960150718689</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 14300/20000) loss: -0.2832479178905487</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 14400/20000) loss: -0.6680377721786499</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 14500/20000) loss: -0.8564409017562866</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 14600/20000) loss: -0.06087149679660797</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 14700/20000) loss: -0.5138503909111023</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 14800/20000) loss: -0.7603799104690552</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 14900/20000) loss: -0.7985144853591919</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 15000/20000) loss: -0.7117128372192383</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 15100/20000) loss: -0.8555217981338501</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 15200/20000) loss: -0.22392134368419647</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 15300/20000) loss: -0.27232199907302856</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 15400/20000) loss: -0.9488778114318848</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 15500/20000) loss: -0.5534208416938782</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 15600/20000) loss: -0.5648906826972961</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 15700/20000) loss: -0.7468466758728027</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 15800/20000) loss: -0.3864515721797943</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 15900/20000) loss: 0.24845197796821594</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 16000/20000) loss: -1.1112899780273438</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 16100/20000) loss: -0.3495236039161682</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 16200/20000) loss: -0.842694878578186</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 16300/20000) loss: -0.7514711618423462</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 16400/20000) loss: -0.956265389919281</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 16500/20000) loss: -0.6691103577613831</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 16600/20000) loss: -0.6804472208023071</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 16700/20000) loss: -1.0454661846160889</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 16800/20000) loss: -0.8576145172119141</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 16900/20000) loss: -0.7169828414916992</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 17000/20000) loss: -0.4931994378566742</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 17100/20000) loss: -0.8349542617797852</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 17200/20000) loss: -0.6177186369895935</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 17300/20000) loss: -0.8593571186065674</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 17400/20000) loss: -1.1710288524627686</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 17500/20000) loss: -0.45717254281044006</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 17600/20000) loss: -0.7136792540550232</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 17700/20000) loss: -0.30101335048675537</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 17800/20000) loss: -0.7737146019935608</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 17900/20000) loss: -0.7789114713668823</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 18000/20000) loss: -0.40764886140823364</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 18100/20000) loss: -0.46933722496032715</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 18200/20000) loss: -0.5195136070251465</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 18300/20000) loss: -0.29774248600006104</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 18400/20000) loss: -0.45873814821243286</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 18500/20000) loss: -0.9430521726608276</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 18600/20000) loss: -0.8074394464492798</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 18700/20000) loss: -0.6546366214752197</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 18800/20000) loss: -0.6442867517471313</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 18900/20000) loss: -1.2402918338775635</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 19000/20000) loss: -0.9669927358627319</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 19100/20000) loss: -0.6719714403152466</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 19200/20000) loss: -0.6797963976860046</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 19300/20000) loss: -0.45326051115989685</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 19400/20000) loss: -0.6933085322380066</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 19500/20000) loss: -0.8500641584396362</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 19600/20000) loss: -1.0281943082809448</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 19700/20000) loss: -0.7677603363990784</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 19800/20000) loss: -0.24082374572753906</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 19900/20000) loss: -0.24140426516532898</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>(batch_num 20000/20000) loss: -0.9767911434173584</code></pre>
</div>
</div>
</section>
<section id="what-more-complicated-nns-look-like" class="level1">
<h1>What more complicated NNs look like</h1>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>It also almost has a bit of “no brain no pain” ML guy energy, in the sense that we’re really pulling out the biggest algorithm possible. It really is a funny trajectory to me to go from “I’d like to still be Bayesian, but avoid MCMC because it’s slow” to “screw subtle design, let’s throw a NN at it”.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>This is mostly a joke, but it really is a tremendous convenience that there’s such a straight forward knob to turn for “expressivity” in this context. We’ll get into the ways that isn’t completely true soon, but NNs provide fantastic convenience in terms of workflow for improving model flexibility.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>You can see it in the original Normalizing Flows paper linked above, or combined with a nice matrix calc review by <a href="https://lilianweng.github.io/posts/2018-10-13-flow-models/">Lilian Weng</a>. As a more general note, since this is a common topic on a few different talented people’s blogs, I’ll try to focus on covering material I think I can provide more intuition for, or that are most relevant for variational inference.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>A great example of this is Lilian Weng’s <a href="https://lilianweng.github.io/posts/2018-10-13-flow-models/">NF walkthrough</a> which I reccomended above- It has a fantastic review of the needed linear algebra and covers a lot of different flow types, but is a bit overly general about what properties are most desirable in a flow, and therefore initially a bit fuzzy on the value different flows have.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Deriving precisely how this works would take us too far afield, but see <a href="https://arxiv.org/abs/1908.09257">Kobyzev et al.&nbsp;(2020)</a> if you’re interested. It’s a great review paper that does a lot of work to recognize there are multiple different possible applications of normalizing flows, and thus different notations and framings that they very successfully bridge. are many different implicit and explicit objectives and<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2023,
  author = {Andy Timm},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2023-05-27},
  url = {https://andytimm.github.io/variational_mrp_pt5.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2023" class="csl-entry quarto-appendix-citeas" role="doc-biblioentry">
Andy Timm. 2023. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> May 27, 2023. <a href="https://andytimm.github.io/variational_mrp_pt5.html">https://andytimm.github.io/variational_mrp_pt5.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>