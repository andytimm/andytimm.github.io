<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Andy Timm">
<meta name="dcterms.date" content="2023-05-02">

<title>Andy Timm - Variational Inference for MRP with Reliable Posterior Distributions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Andy Timm - Variational Inference for MRP with Reliable Posterior Distributions">
<meta property="og:description" content="Part 4- Some theory on why VI is hard">
<meta property="og:site-name" content="Andy Timm">
<meta name="twitter:title" content="Andy Timm - Variational Inference for MRP with Reliable Posterior Distributions">
<meta name="twitter:description" content="Part 4- Some theory on why VI is hard">
<meta name="twitter:creator" content="@andy_timm">
<meta name="twitter:site" content="@andy_timm">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Andy Timm</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects.html">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../software.html">
 <span class="menu-text">Software</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html">
 <span class="menu-text">Rarely Updated Blog</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Variational Inference for MRP with Reliable Posterior Distributions</h1>
            <p class="subtitle lead">Part 4- Some theory on why VI is hard</p>
                                <div class="quarto-categories">
                <div class="quarto-category">MRP</div>
                <div class="quarto-category">Variational Inference</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Andy Timm </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 2, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#not-all-samples-are-equally-good" id="toc-not-all-samples-are-equally-good" class="nav-link active" data-scroll-target="#not-all-samples-are-equally-good">Not all samples are equally good</a>
  <ul>
  <li><a href="#importance-weighted-variational-inference" id="toc-importance-weighted-variational-inference" class="nav-link" data-scroll-target="#importance-weighted-variational-inference">Importance Weighted Variational Inference</a>
  <ul class="collapse">
  <li><a href="#how-does-iw-elbo-change-the-vi-problem-conceptually" id="toc-how-does-iw-elbo-change-the-vi-problem-conceptually" class="nav-link" data-scroll-target="#how-does-iw-elbo-change-the-vi-problem-conceptually">How does IW-ELBO change the VI problem conceptually?</a></li>
  </ul></li>
  <li><a href="#solving-our-is-problems-with-pareto-smoothed-importance-sampling" id="toc-solving-our-is-problems-with-pareto-smoothed-importance-sampling" class="nav-link" data-scroll-target="#solving-our-is-problems-with-pareto-smoothed-importance-sampling">Solving our IS problems with Pareto-Smoothed Importance Sampling</a></li>
  <li><a href="#multiple-proposal-distributions-with-multiple-importance-sampling" id="toc-multiple-proposal-distributions-with-multiple-importance-sampling" class="nav-link" data-scroll-target="#multiple-proposal-distributions-with-multiple-importance-sampling">Multiple Proposal Distributions with Multiple Importance Sampling</a></li>
  </ul></li>
  <li><a href="#can-we-bound-error-in-terms-of-elbo-or-cubo" id="toc-can-we-bound-error-in-terms-of-elbo-or-cubo" class="nav-link" data-scroll-target="#can-we-bound-error-in-terms-of-elbo-or-cubo">Can we bound error in terms of ELBO or CUBO?</a>
  <ul>
  <li><a href="#wasserstein-bounds" id="toc-wasserstein-bounds" class="nav-link" data-scroll-target="#wasserstein-bounds">Wasserstein Bounds</a></li>
  </ul></li>
  <li><a href="#conclusions-bonus-context" id="toc-conclusions-bonus-context" class="nav-link" data-scroll-target="#conclusions-bonus-context">Conclusions + Bonus Context</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>This is section 4 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.</p>
<p>The general structure for this post and the one before and after it will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in these three posts will go a long way towards more robust variational inference. I’ll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.</p>
<p>In the <a href="https://andytimm.github.io/posts/Variational%20MRP%20Pt3/variational_mrp_3.html">last post</a> we took a look at how our ELBO objective requires specific version of KL Divergence (the “Exclusive” formulation of KLD), and saw that it encoded a preference for a certain type of solution to the VI problem. Then we looked at CUBO and CHIVI, an alternative bound and algorithm that avoid this problem, often leading to a more useful posterior distribution by pursuing a more “inclusive” solution.</p>
<p>In this post, we’ll leverage importance sampling to make the most of the samples we do have, emphasizing the parts of our <span class="math inline">q(x)</span> that look like <span class="math inline">p(x)</span> and de-emphasizing the parts that do not.</p>
<p>The rough plan for the series is as follows:</p>
<ol type="1">
<li>Introducing the Problem- Why is VI useful, why VI can produce spherical cows</li>
<li>How far does iteration on classic VI algorithms like mean-field and full-rank get us?</li>
<li>Problem 1: KL-D prefers exclusive solutions; are there alternatives?</li>
<li><strong>(This post)</strong> Problem 2: Not all VI samples are of equal utility; can we weight them cleverly?</li>
<li>Problem 3: How can we know when VI is wrong? Are there useful error bounds?</li>
<li>Better grounded diagnostics and workflow</li>
<li>Seeing if some more sophisticated techniques like normalizing flows add much</li>
</ol>
<section id="not-all-samples-are-equally-good" class="level1">
<h1>Not all samples are equally good</h1>
<p>So we’ve made an approximation <span class="math inline">q(x)</span> that’s cheap to sample from, and is somewhat close to <span class="math inline">p(x)</span>, our true posterior. The way to improve the approximation we’ve focused on so far is to just go back to the start and make <span class="math inline">q(x)</span> better; for example, through changing up the variational family, or to switching to a different optimization objective like the CUBO. That’s one solution that’s often necessary, but can we work with a particular <span class="math inline">q(x)</span> we have and make better use of the parts of it that are the closest to being right?</p>
<p>… Phased this way, this sounds a lot like importance sampling. If you haven’t seen them before, an importance sampling estimator allows us to take draws from a (preferably) easy to sample from distribution<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> and reweight the samples to look more like our true target distribution. The weight <span class="math inline">w_i</span> (or ratio, <span class="math inline">r_i</span>) for each sample <span class="math inline">i</span> take form:</p>
<p><span class="math display">
w_i = \frac{p(x_i)}{q(x_i)}
</span> Before you get worried that we don’t have <span class="math inline">p(x_i)</span> because of the normalizing constant like every time we talk about having <span class="math inline">p(x)</span> in this series, there’s a clever estimator that “self-normalizes” such that this can be a reasonable strategy. Intuitively, we’re just placing more weight on samples likely <span class="math inline">p(x)</span>.</p>
<p>This footenote<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> has a selection of some of my favorite resources for learning more or refreshing your memory about importance sampling, but for the main discussion let me pull out some particularly important sub-problems to solve in making a good importance sampling estimator, and good important sampling estimator for VI.</p>
<p>First, our choice of the “proposal” distribution we’re reweighting to be more like <span class="math inline">p(x)</span> matters for making this process practically feasible. We need the proposal distribution to be close enough to <span class="math inline">p(x)</span> that a realistic number of the draws get non-negligible weights. It might be true that we could draw proposals from a big <span class="math inline">N</span> dimensional uniform distribution for every problem, but if we want to be done sampling enough this century we need to at least get fairly close with our initial <span class="math inline">q(x)</span>.</p>
<p>A second, but related problem is that it’s quite common for the unmodified importance sampling estimator to have some weights which are orders and orders of magnitude higher than the average weight, blowing up the variance of the estimator. Dan Simpson’s slides I linked above has an instructive example with not too weird <span class="math inline">p(x)</span> and <span class="math inline">q(x)</span>’s that has a max weight ~1.4 million (!) times the average. If that happens, our estimator will essentially ignore most samples without gigantic weights, and it’ll take ages for that estimator to tell us anything remotely reliable.</p>
<p>So with those points we need to address, here are the next topics in this post:</p>
<ol type="1">
<li>Importance Weighted Variational Inference</li>
<li>Robust importance sampling with built in diagnostics via Pareto-Smoothed Importance Sampling</li>
<li>Combining multiple proposal distributions via Multiple Importance Sampling</li>
</ol>
<section id="importance-weighted-variational-inference" class="level2">
<h2 class="anchored" data-anchor-id="importance-weighted-variational-inference">Importance Weighted Variational Inference</h2>
<p>Importance Weighting for VI in it’s simplest form is pretty intuitive (draw samples from an already trained <span class="math inline">q(x)</span>, weight them…), but let’s derive the new Importance Weighted Variational Inference (IWVI) estimator first since some nice intuition will come with it.</p>
<p>I want to emphasize something that wasn’t clear to me for a good while- these two ideas are not equivalent. While both are useful tools, the “train time”, objective-modifying IWVI estimator is a distinct approach from the “test time” importance sampling approach that takes draws from <span class="math inline">q(x)</span> as already fixed and reweights them as best it can.</p>
<p>We’ll aim to show that we can get a tighter ELBO by using importance sampling. This type of tighter ELBO was first shown by <a href="https://arxiv.org/abs/1509.00519">Burda et Al. (2015)</a> in the context of Variational Autoencoders after which is was fairly clear this could apply to variational inference, but <a href="https://arxiv.org/abs/1808.09034">Domke and Sheldon (2018)</a> fleshed out some details of that extension- I’ll be explaining some of the latter group’s main results first.</p>
<p>To start, imagine a random variable <span class="math inline">R</span>, such that <span class="math inline">\mathbb{E}{R} = p(x)</span>, which we’ll think of as a estimator of <span class="math inline">p(x)</span>. Then by Jensen’s Inequality:</p>
<p><span class="math display">
logp(x) = \mathbb{E}logR + \mathbb{E}log\frac{p(x)}{R}
</span></p>
<p>The first term is the bound, which will be tighter if <span class="math inline">R</span> is highly concentrated.</p>
<p>This is a more general form of the ELBO; we can make it quite familiar looking by having our <span class="math inline">R</span> above be:</p>
<p><span class="math display">
R = \frac{p(z,x)}{q(z)}, z \sim q
</span></p>
<p>The reason for pointing out this fairly simple generalization is helpful is that it frames how to tighten our ELBO on <span class="math inline">logp(x)</span> via alternative estimators <span class="math inline">R</span>.</p>
<p>By drawing <span class="math inline">M</span> samples and averaging them as in importance sampling, we get:</p>
<p><span class="math display">
R_M = \frac{1}{M}\sum_{m=1}^{M}\frac{p(z_m,x)}{q(z_m)}, z_m \sim q
</span> From there, we can derive a tighter bound on <span class="math inline">logp(x)</span>, referred to as the IW-ELBO:</p>
<p><span class="math display">
IW-ELBO_M[q(z)||p(z,x)] := \mathbb{E}_{q(z_{1:M})}log\frac{1}{M} \sum_{m=1}^{M}\frac{p(z_m,x)}{q(z_m)}
</span> Where we’re using the <span class="math inline">1:M</span> as a shorthand for <span class="math inline">q(z_{1:M}) = q(z_1)...q(z_M)</span>.</p>
<p>It’s worth noting that the last few lines don’t specify a particular form of importance sampling- we’re getting the tighter theoretical bounding behavior from the averaging of samples from <span class="math inline">q</span>. We’ll see a particularly good form of importance sampling with desirable practical properties in a moment.</p>
<section id="how-does-iw-elbo-change-the-vi-problem-conceptually" class="level3">
<h3 class="anchored" data-anchor-id="how-does-iw-elbo-change-the-vi-problem-conceptually">How does IW-ELBO change the VI problem conceptually?</h3>
<p>The tighter bound is nice, but importance sampling also has the side effect (done right, side benefit) of modifying our incentives in choosing a variational family. To see what I mean, we can re-use the example distributions from last post we used to build intuition for KL Divergence, where red was the true distribution, and green were our potential approximations. If we’re not going to draw multiple samples and weight them, it makes sense to choose something like the first plot below. Every draw in the middle of the two target modes is expensive per our ELBO objective, so better to choose a mode.</p>
<div class="cell">

</div>
<div class="cell">

</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>rkl_plot <span class="ot">&lt;-</span> mixture <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> normals)) <span class="sc">+</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="fu">aes</span>(<span class="at">x =</span> normals), <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="fu">aes</span>(<span class="at">x =</span> mode_seeking_kl), <span class="at">color =</span> <span class="st">"green"</span>) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">"Without weighting, we prefer to capture a mode"</span>) <span class="sc">+</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>fkl_plot <span class="ot">&lt;-</span> mixture <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> normals)) <span class="sc">+</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="fu">aes</span>(<span class="at">x =</span> normals), <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="fu">aes</span>(<span class="at">x =</span> mean_seeking_kl), <span class="at">color =</span> <span class="st">"green"</span>) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">"With importance sampling, weights allow us to prefer coverage"</span>) <span class="sc">+</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">""</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(rkl_plot,fkl_plot)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="variational_mrp_pt4_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>If we can use importance sampling though, quite the opposite is be true! Note that we’re still using the ELBO, a reverse-KL based metric- that hasn’t changed. What has changed is our ability to mitigate the objective costs of those samples between the two extremes. Via this “train time” implementation of IS, points outside the two target modes will get lower importance weights, and points within the modes will get higher ones, so as long as we’re covering the modes with some reasonable amount of probability mass, and drawing enough samples we can actually do better with the distribution centered between the modes.</p>
<p>To further drive home the point about how a “train time” and “test time” implementations of IS differ, could “test time” IS do this? Not really- because the ability to better minimize the ELBO via sampling requires the IW-ELBO variant and associated training process. If we hard-coded <span class="math inline">q(x)</span> as the green <span class="math inline">N(9,4)</span> shown above, “test time” IS could weight the right samples up to better approximate <span class="math inline">p(x)</span>, but it doesn’t fundamentally alter our optimization problem the way the IWVI estimand does.</p>
<p>We can also imagine how varying the number of samples might effect optimization. Between <span class="math inline">M=1</span> and “enough draws to get all the benefits of IS”, we can imagine there’s a slow transition from “just stick with 1 mode” and “go with IS”. So it seems like we should be worried about getting the number of samples right, but fortunately as we’ll see in the next section there are great rules of thumb in some variants of IS. We’ll still need to bear the cost of sampling (which gets higher as <span class="math inline">q(x)</span> becomes “further” from <span class="math inline">p(x)</span>, as we’ll need more samples to weight into a good approximation), but the cost of sampling for most VI implementations will often be pretty manageable if our proposal distribution is somewhat close to <span class="math inline">p(x)</span>.</p>
<p>Another way to think about how importance sampling changes our task with variational inference is to think about what sorts of distributions make sense to have as our variational family, and even which objective might be better given IS. On choice of a variational family, if we’re aiming for coverage, moving towards thicker-tailed distributions like t distributions makes a lot of sense. While we explored the IW-ELBO above to build intuition, there’s no reason not to apply VI to the CUBO and thus CHIVI- this also naturally produces nicely overdispersed distributions which can be importance sampled closer to the true <span class="math inline">p(x)</span>. This idea of aiming for a wide proposal to sample from is referred to in the importance sampling literature (eg <a href="https://artowen.su.domains/mc/">Owen, 2013</a>) as “defensive sampling”, with <a href="https://arxiv.org/abs/1808.09034">Domke and Sheldon (2018)</a> exploring the VI connection more fully. For intuition, by ensuring most of <span class="math inline">p(x)</span> is covered by some reasonable mass makes it easier to efficiently get draws that can be weighted into a final posterior, even if the unweighted posterior might be too wide.</p>
</section>
</section>
<section id="solving-our-is-problems-with-pareto-smoothed-importance-sampling" class="level2">
<h2 class="anchored" data-anchor-id="solving-our-is-problems-with-pareto-smoothed-importance-sampling">Solving our IS problems with Pareto-Smoothed Importance Sampling</h2>
<p>As we’ve been talking about importance sampling, we’ve been leaving some of the messier details aside (how many samples to draw, how to deal with the cases when some of the weights get huge, how to know when our proposal distribution is “close” enough).</p>
<p>While the Importance Sampling Literature is huge and there are a lot of possible solutions here, I’ll next introduce <a href="https://arxiv.org/abs/1507.02646">Vehtari et Al. (2015)</a>’s Pareto-Smoothed Importance Sampling. I’m a huge fan of this paper- it’s a really elegant and powerful tool, derived from taking Bayesian principles seriously.</p>
<p>Above, I described a common failure mode for IS estimators, where some weights are orders of magnitude larger than others, with this long right tail of ratios dominating the weighted average and blowing up the variance of the estimator. Pareto-Smoothed Importance Sampling proposes to model those tail values as coming from a Generalized Pareto Distribution, a distribution for describing extreme values, and replace the most extreme weights with modeled (and more stable) values.</p>
<p>For concreteness, let’s introduce a simple 1-D example. We’ll aim to use importance sampling to approximate distributions <span style="color:red;"><span class="math inline">\mathcal{T}(\mu = 0,\sigma = 1,t =5</span>)</span> and <span style="color:blue;"><span class="math inline">\mathcal{C}(x_0= 0,\gamma = 10)</span></span> with a <span style="color:green;"><span class="math inline">\mathcal{N}(\mu = 0,\sigma = 1</span>)</span> distribution. If that sounds like the opposite of preferring wide tails on <span class="math inline">q(x)</span>’s I described above, you’re right, but using a poor choice here will illustrate some useful properties.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>simulated_data <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="at">q_x =</span> <span class="fu">rnorm</span>(<span class="dv">100000</span>),</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="at">manageable_p_x =</span> <span class="fu">rt</span>(<span class="dv">100000</span>,<span class="dv">5</span>),</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="at">unmanageable_p_x =</span> <span class="fu">rcauchy</span>(<span class="dv">100000</span>),</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="at">manageable_ratios =</span> <span class="fu">dt</span>(q_x,<span class="dv">5</span>)<span class="sc">/</span><span class="fu">dnorm</span>(q_x),</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="at">unmanageable_ratios =</span> <span class="fu">dcauchy</span>(q_x,<span class="dv">0</span>,<span class="dv">10</span>)<span class="sc">/</span><span class="fu">dnorm</span>(q_x)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>simulated_data <span class="sc">%&gt;%</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>            <span class="fu">pivot_longer</span>(<span class="fu">c</span>(q_x,manageable_p_x,unmanageable_p_x),</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>                         <span class="at">values_to =</span> <span class="st">"draws"</span>,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>                         <span class="at">names_to =</span> <span class="st">"distributions"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>            <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> draws, <span class="at">color =</span> distributions)) <span class="sc">+</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>            <span class="fu">geom_density</span>() <span class="sc">+</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>            <span class="co"># If you wanted to show the full reach of the Cauchy, it'd be</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>            <span class="co"># hard to see the shape of the T vs N; it's that wide.</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Hence the 6k values removed</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>            <span class="fu">xlim</span>(<span class="sc">-</span><span class="dv">10</span>,<span class="dv">10</span>) <span class="sc">+</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>            <span class="fu">ggtitle</span>(<span class="st">"Visualizing the distributions in question"</span>) <span class="sc">+</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>            <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">"none"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Removed 6245 rows containing non-finite values (stat_density).</code></pre>
</div>
<div class="cell-output-display">
<p><img src="variational_mrp_pt4_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The tails on that Cauchy distribution are super, super wide compared to our normal, so the samples far, far out in the tails of the normal will need massive weights to approximate the cauchy. The t-distribution is wider too, so we’ll need some higher weights, but not nearly as many. As a way to visualize this, you can see that just a handful of draws have weights away from ~1, but these weights are as much as 5000x higher than the mean ratio, and will dominate any average we make of them.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>simulated_data <span class="sc">%&gt;%</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(unmanageable_ratios) <span class="sc">%&gt;%</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">n =</span> <span class="fu">seq</span>(<span class="dv">1</span>,<span class="dv">100000</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> n,<span class="at">y =</span> unmanageable_ratios)) <span class="sc">+</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"A pretty typical 'unsaveable' set of importance ratios"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="variational_mrp_pt4_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The t-distribution ratio plot would look similar, but with a much smaller y-scale. The max weight would still be much larger than the average, but more than an order of magnitude or so less large:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>mean_t <span class="ot">&lt;-</span> <span class="fu">mean</span>(simulated_data<span class="sc">$</span>manageable_ratios)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>max_t <span class="ot">&lt;-</span> <span class="fu">max</span>(simulated_data<span class="sc">$</span>manageable_ratios)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>mean_c <span class="ot">&lt;-</span> <span class="fu">mean</span>(simulated_data<span class="sc">$</span>unmanageable_ratios)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>max_c <span class="ot">&lt;-</span><span class="fu">max</span>(simulated_data<span class="sc">$</span>unmanageable_ratios)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">"the mean of the t is: "</span>,mean_t,<span class="st">" compared to a max of "</span>,max_t,<span class="st">";"</span>,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>             <span class="st">"The cauchy cause is more extreme- the mean of the cauchy is: "</span>,mean_c,<span class="st">" compared to a max of "</span>,max_c))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "the mean of the t is: 0.995904306317625 compared to a max of 164.448932152079;The cauchy cause is more extreme- the mean of the cauchy is: 0.283655933763642 compared to a max of 1428.22324178729"</code></pre>
</div>
</div>
<p>So let’s bring this back to Pareto smoothing here. We want to model and smooth that long tail of the ratio distribution. It turns out there’s plenty of study of the distribution of extreme events, and there’s some classical limit results showing:</p>
<p><span class="math display">
r(x_i) | r(x_i) &gt; \tau \rightarrow GPD(\tau,\sigma,k), \tau \rightarrow \infty
</span></p>
<p>where <span class="math inline">\tau</span> is a lower bound parameter, which in our case defines how many ratios from the tail we’ll actually model. <span class="math inline">\sigma</span> is a scale parameter, and <span class="math inline">k</span> is a unconstrained shape parameter. The Generalized Pareto Distribution has form:</p>
<p><span class="math display">
\frac{1}{\sigma} \left(1 + k\frac{r - \tau}{\sigma} \right)^{-1/k-1}
</span> One of the best things about this approximation is that also provides a practical</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>manageable_psis <span class="ot">&lt;-</span> <span class="fu">psis</span>(<span class="fu">log</span>(simulated_data<span class="sc">$</span>manageable_ratios),</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>                       <span class="at">r_eff =</span> <span class="cn">NA</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Some Pareto k diagnostic values are slightly high. See help('pareto-k-diagnostic') for details.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>manageable_psis<span class="sc">$</span>diagnostics</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$pareto_k
[1] 0.5963495

$n_eff
[1] 62963.93</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>unmanageable_psis <span class="ot">&lt;-</span> <span class="fu">psis</span>(<span class="fu">log</span>(simulated_data<span class="sc">$</span>unmanageable_ratios),</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>                          <span class="at">r_eff =</span> <span class="cn">NA</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>unmanageable_psis<span class="sc">$</span>diagnostics</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$pareto_k
[1] 0.8533127

$n_eff
[1] 249.2052</code></pre>
</div>
</div>
</section>
<section id="multiple-proposal-distributions-with-multiple-importance-sampling" class="level2">
<h2 class="anchored" data-anchor-id="multiple-proposal-distributions-with-multiple-importance-sampling">Multiple Proposal Distributions with Multiple Importance Sampling</h2>
</section>
</section>
<section id="can-we-bound-error-in-terms-of-elbo-or-cubo" class="level1">
<h1>Can we bound error in terms of ELBO or CUBO?</h1>
<section id="wasserstein-bounds" class="level2">
<h2 class="anchored" data-anchor-id="wasserstein-bounds">Wasserstein Bounds</h2>
</section>
</section>
<section id="conclusions-bonus-context" class="level1">
<h1>Conclusions + Bonus Context</h1>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>we’ll call it q(x) here to make the application super clear, but often I see the “proposal” distribution called f(x) and the the distribution we want to approximate called g(x).<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>If you’re looking to learn about importance sampling for the first time, a great place to start is Ben Lambert’s video introductions to the basic idea: <a href="https://www.youtube.com/watch?v=V8f8ueBc9sY">video 1</a>, and <a href="https://www.youtube.com/watch?v=F5PdIQxMA28">video 2</a>. For building more intuition about why we need all these variance reducing modifications to general IS, Dan Simpson has some great <a href="https://dpsimpson.github.io/pages/talks/Importance_sampling_unsw_2019.pdf">slides</a> which have a side benefit of being hilarious. Those slides will mention a lot of the books/papers I find most instructive, but it’s worth calling out especially Vehtari et Al’s Pareto Smoothed Importance Sampling <a href="https://arxiv.org/abs/1507.02646">paper</a> as particularly well written and paradigm shaping. Finally, Elvira et Al’s (2019) Multiple Importance Sampling <a href="https://projecteuclid.org/journals/statistical-science/volume-34/issue-1/Generalized-Multiple-Importance-Sampling/10.1214/18-STS668.full">paper</a> is the most thorough I know, but isn’t particularly approachable. Instead, for MIS I’d recommend starting with the first few minutes of <a href="https://www.youtube.com/watch?v=dxFSwplfdpk">this talk</a> (although the main topic of their talk is less relevant, the visualizations are super helpful), and the first ~8 pages of <a href="https://arxiv.org/pdf/2102.05407.pdf">this paper</a>, also by Elvira et Al. (2021) (I especially like that it spends a bit more time on notation; since multiple importance sampling comes from/comes up in computer graphics, the notational choices sometimes feel a bit annoying to me). Finally, the <a href="https://dl.acm.org/doi/10.1145/218380.218498">original MIS paper itself</a>, Veach &amp; Guibas (1995) is quite readable, but requires a bit of reading around or reading into computer graphics to grok their examples and notational choices.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{timm2023,
  author = {Andy Timm},
  title = {Variational {Inference} for {MRP} with {Reliable} {Posterior}
    {Distributions}},
  date = {2023-05-02},
  url = {https://andytimm.github.io/variational_mrp_pt4.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-timm2023" class="csl-entry quarto-appendix-citeas" role="doc-biblioentry">
Andy Timm. 2023. <span>“Variational Inference for MRP with Reliable
Posterior Distributions.”</span> May 2, 2023. <a href="https://andytimm.github.io/variational_mrp_pt4.html">https://andytimm.github.io/variational_mrp_pt4.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>