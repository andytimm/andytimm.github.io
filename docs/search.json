[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Rarely Updated Blog",
    "section": "",
    "text": "Andy Timm\n\n\n\n\n\n\n  \n\n\n\n\nMy talk on Regularized Raking at NYOSP\n\n\n\n\n\n\n\nsurveys\n\n\nweighting\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nAndy Timm\n\n\n\n\n\n\n  \n\n\n\n\nVariational Inference for MRP with Reliable Posterior Distributions\n\n\nPart 6- Diagnostics\n\n\n\n\nMRP\n\n\nVariational Inference\n\n\nDiagnostics\n\n\n\n\n\n\n\n\n\n\n\nJun 17, 2023\n\n\nAndy Timm\n\n\n\n\n\n\n  \n\n\n\n\nVariational Inference for MRP with Reliable Posterior Distributions\n\n\nPart 5- Normalizing Flows\n\n\n\n\nMRP\n\n\nVariational Inference\n\n\nNormalizing Flows\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\nAndy Timm\n\n\n\n\n\n\n  \n\n\n\n\nVariational Inference for MRP with Reliable Posterior Distributions\n\n\nPart 4- Importance Sampling\n\n\n\n\nMRP\n\n\nVariational Inference\n\n\n\n\n\n\n\n\n\n\n\nMay 27, 2023\n\n\nAndy Timm\n\n\n\n\n\n\n  \n\n\n\n\nVariational Inference for MRP with Reliable Posterior Distributions\n\n\nPart 3- Alternatives to KL Divergence\n\n\n\n\nMRP\n\n\nVariational Inference\n\n\n\n\n\n\n\n\n\n\n\nMay 2, 2023\n\n\nAndy Timm\n\n\n\n\n\n\n  \n\n\n\n\nVariational Inference for MRP with Reliable Posterior Distributions\n\n\nPart 2- The errors of our ways\n\n\n\n\nMRP\n\n\nVariational Inference\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\nAndy Timm\n\n\n\n\n\n\n  \n\n\n\n\nVariational Inference for MRP with Reliable Posterior Distributions\n\n\nIntroductions- things to do, places to be\n\n\n\n\nMRP\n\n\nBART\n\n\nVariational Inference\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2022\n\n\nAndy Timm\n\n\n\n\n\n\n  \n\n\n\n\nBART with varying intercepts in the MRP framework\n\n\n\n\n\n\n\nFrom Old Website\n\n\nMRP\n\n\nBART\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2019\n\n\nAndy Timm\n\n\n\n\n\n\n  \n\n\n\n\nConvention Prediction with a Bayesian Hierarchical Multinomial Model\n\n\n\n\n\n\n\nFrom Old Website\n\n\nStan\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2019\n\n\nAndy Timm\n\n\n\n\n\n\n  \n\n\n\n\nIs Voting Habit Forming? Replication, and additional robustness checks\n\n\n\n\n\n\n\nFrom Old Website\n\n\ncausal inference\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2019\n\n\nAndy Timm\n\n\n\n\n\n\n  \n\n\n\n\nType S/M errors in R with retrodesign()\n\n\n\n\n\n\n\nFrom Old Website\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2018\n\n\nAndy Timm\n\n\n\n\n\n\n  \n\n\n\n\nWhy the normal distribution?\n\n\n\n\n\n\n\nFrom Old Website\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2018\n\n\nAndy Timm\n\n\n\n\n\n\n  \n\n\n\n\nPredicting race part 1- Bayes’ rule method and extensions\n\n\n\n\n\n\n\nFrom Old Website\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2018\n\n\nAndy Timm\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andy Timm",
    "section": "",
    "text": "About\nI was previously Manager of Data Science at 605, where I worked on problems of causal inference, forecasting, and persuadability modeling, helping our corporate and political clients to better understand the impact of their advertising campaigns and optimize their targeting. Before that, I was a grad student at NYU Gallatin, where I was lucky to have the freedom to study a wide variety of topics in political science, statistics, and computer science.\nI like leveraging modern statistical and machine learning tools to more effectively answer social science flavored questions. Some of my substantive interests include the increasing prominence of white identity in American politics, and the extent to which voting is habit forming.\nBefore grad school, I was a field and data staffer up and down the ballot in Minnesota. When not thinking about politics, I can most often be found running, reading, or playing chess.\nMy aim with this space is to show off some of my larger side projects, especially those that would benefit from some expository writing. I’ll also include occasional career updates.\nThis new website is still very much a WIP. Thanks to Drew Dimmery for starting point for this and post on how to make a quick Quarto website!\n\n    \n\n\n\n    \n\n\n\n    \n\n\n\n    \n\n\n\n\n\n\n\nMy Bookshelf as of January 2023. Hopefully more useful than squinting on zoom!"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Andy Timm",
    "section": "",
    "text": "Attribution 4.0 International\n=======================================================================\nCreative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.\nUsing Creative Commons Public Licenses\nCreative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.\n Considerations for licensors: Our public licenses are\n intended for use by those authorized to give the public\n permission to use material in ways otherwise restricted by\n copyright and certain other rights. Our licenses are\n irrevocable. Licensors should read and understand the terms\n and conditions of the license they choose before applying it.\n Licensors should also secure all rights necessary before\n applying our licenses so that the public can reuse the\n material as expected. Licensors should clearly mark any\n material not subject to the license. This includes other CC-\n licensed material, or material used under an exception or\n limitation to copyright. More considerations for licensors:\nwiki.creativecommons.org/Considerations_for_licensors\n\n Considerations for the public: By using one of our public\n licenses, a licensor grants the public permission to use the\n licensed material under specified terms and conditions. If\n the licensor's permission is not necessary for any reason--for\n example, because of any applicable exception or limitation to\n copyright--then that use is not regulated by the license. Our\n licenses grant only permissions under copyright and certain\n other rights that a licensor has authority to grant. Use of\n the licensed material may still be restricted for other\n reasons, including because others have copyright or other\n rights in the material. A licensor may make special requests,\n such as asking that all changes be marked or described.\n Although not required by our licenses, you are encouraged to\n respect those requests where reasonable. More_considerations\n for the public:\nwiki.creativecommons.org/Considerations_for_licensees\n=======================================================================\nCreative Commons Attribution 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)\n\nnever produces Adapted Material.\n\nDownstream recipients.\n\nOffer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nNo downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\n\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\nIf You Share Adapted Material You produce, the Adapter’s License You apply must not prevent recipients of the Adapted Material from complying with this Public License.\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.\nTO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION, NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT, INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES, COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n=======================================================================\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "posts/BART VI/2020-03-06-BART-vi.html",
    "href": "posts/BART VI/2020-03-06-BART-vi.html",
    "title": "BART with varying intercepts in the MRP framework",
    "section": "",
    "text": "This is the first of a few posts about some of the substantive and modeling findings from my master’s thesis, where I use Bayesian Additive Regression Trees (BART) and Poststratification to model support for a border wall from 2015-2019. In this post I explore some of the properties of using BART with varying intercepts (BART-vi) within the MRP framework."
  },
  {
    "objectID": "posts/BART VI/2020-03-06-BART-vi.html#choosing-a-prediction-model-for-mrp",
    "href": "posts/BART VI/2020-03-06-BART-vi.html#choosing-a-prediction-model-for-mrp",
    "title": "BART with varying intercepts in the MRP framework",
    "section": "Choosing a prediction model for MRP",
    "text": "Choosing a prediction model for MRP\nMultilevel Regression and Poststratification has seen huge success as a tool for obtaining accurate estimates of public opinion in small areas using national surveys. As the name would suggest, the most common tool used for the modeling step of these models are multilevel regressions, often Bayesian multilevel ones. The key intuition here is that pooling data across levels of predictors like state makes much more efficient use of the underlying data, which leads to more accurate estimates. However, there is no strict requirement that the models used here be multilevel regressions, it’s simply an efficient and stable way to get regularized predictions using quite a large set of predictors. Recently, academic work has begun to explore using a wide class of machine learning algorithms as the predictive component in this framework. Andrew Gelman calls this RRP: Regularized Regression and Poststratification.\nOne particularly promising alternative prediction algorithm is BART, which puts the high accuracy of tree-based algorithms like random forests or gradient boosting into a Bayesian framework. This has a number of appealing advantages compared to other machine learning options, especially for RRP. First, unlike other algorithms which might not have clear measures of their uncertainty or confidence intervals around their predictions, BART approximates a full posterior distribution. Second, BART has a number of prior and hyperparameter choices that have been shown to be highly effective in a wide variety of settings, somewhat reducing the need for parameter search. Finally, BART runs fast, especially when compared to the Bayesian multilevel models commonly used for MRP.\nOf course, BART models are not without their disadvantages. First and foremost, there is currently only a small amount of recent work on BART models for categorical (as opposed to binary) response (Murray, 2019), and no public implementation of that model that I am aware of. In my case, this means modeling the border wall question as binary “support vs. oppose”, as opposed to the three categories “support, oppose, don’t know”. Given the salience of the issue, only 3.1% of people responded “Don’t Know”, so this is a relatively minor loss. However, for questions like the formerly crowded 2020 democratic primary, or a general election where third parties play a major role, this could be a much more serious loss.\nWhile only a small amount of work has compared the two so far, estimates using BART appear to slightly outperform those using multilevel models. For example, Montgomery & Olivella (2018) compared using BART to Gelman & Ghitza’s (2013)’s fairly complex multilevel model, finding the predictions were incredibly similar, being as much as .97 correlated. As they note however, their BART model produced these results both without large amounts of iteration on model form (which has been a constant challenge for MRP), and producing such estimates orders of magnitude faster. Similarly, Bisbee (2019) finds across 89 different datasets that BART and MRP produced very similar estimates, but BART’s were of slightly higher quality, both by Mean Absolute Error (MAE) and Interstate Correlation (a measure of how well the state level predictions from a model using national surveys line up with state level polls). Ending his article, Bisbee writes “One avenue of future research might focus on variants of Bayesian additive regression trees that embed a multilevel component, likely providing further improvements as the best of both worlds.”\nThis is exactly what I do in my thesis, using BART with varying intercepts by state to model support for a border wall by state. To present my findings around BART-vi, I’ll start by providing a brief overview of the MRP framework. Next, I’ll explain BART, and how BART-vi extends this model. Finally, I’ll build one model of each BART type, and compare them."
  },
  {
    "objectID": "posts/BART VI/2020-03-06-BART-vi.html#a-quick-review-of-multilevel-regression-and-poststratification",
    "href": "posts/BART VI/2020-03-06-BART-vi.html#a-quick-review-of-multilevel-regression-and-poststratification",
    "title": "BART with varying intercepts in the MRP framework",
    "section": "A Quick Review of Multilevel Regression and Poststratification",
    "text": "A Quick Review of Multilevel Regression and Poststratification\nWhile I’m mostly focused on the modeling step with this post, here’s a quick review of the overall process in building a MRP/RRP model. If you want a more complete introduction, Kastellec’s MRP Primer is a great starting point.\nMRP or RRP cast estimation of a population quantity of interest \\theta as a prediction problem. That is, instead of the more traditional approach of attempting to design the initial survey to be representative of the population, MRP leans more heavily on modeling and poststratification to make the estimates representative.\nTo sketch out the steps-\n\nEither gather or run a survey or collection of surveys that collect both information on the outcome of interest, y, and a set of demographic and geographic predictors, \\left(X_{1}, X_{2}, X_{3}, \\ldots, X_{m}\\right).\nBuild a poststratification table, with population counts or estimated population counts N_{j} for each possible combination of the features gathered above. Each possible combination j is called a cell, one of J possible cells. For example, if we poststratified only on state, there would be J=51 (with DC) total cells; in practice, J is often several thousand.\nBuild a model, usually a Bayesian multilevel regression, to predict y using the demographic characteristic from the survey or set of surveys, estimating model parameters along the way.\nEstimate y for each cell in the poststratification table, using the model built on the sample.\nAggregate the cells to the population of interest, weighting by the N_{j}’s to obtain population level estimates: \\theta_{\\mathrm{POP}}=\\frac{\\sum_{j \\in J} N_{j} \\theta_{j}}{\\sum_{j \\in J} N_{J}}"
  },
  {
    "objectID": "posts/BART VI/2020-03-06-BART-vi.html#bart",
    "href": "posts/BART VI/2020-03-06-BART-vi.html#bart",
    "title": "BART with varying intercepts in the MRP framework",
    "section": "BART",
    "text": "BART\nIn this section, I review the general BART model, and discuss the hyperparameter choices I use. Proposed by Chipman et al, (2008), BART is a Bayesian machine learning algorithm that has seen widespread success in a wide variety of both predictive and causal inference applications. Like most machine learning models, it treats the prediction task as modeling the outcome y as an unknown function f of the k predictors y = f(X_{k}).\nBART does with this a sum of decision trees:\nY_{k}=\\sum_{j=1}^{m} g\\left(\\mathbf{X}_{k}, T_{j}, \\mathbf{M}_{j}\\right)+\\epsilon_{k} \\quad \\epsilon_{k} \\stackrel{i . i . d}{\\sim} N\\left(0, \\sigma^{2}\\right)\n(To start with the continuous case, before generalizing to the binary case in a moment)\nEach tree T_j splits the data along a variety of predictors, seeking to improve the purity of outcomes in each group. For instance, in seeking to partition respondents into purer groups of support or opposition for a border wall, one natural split is that of white vs. nonwhite respondents, after which a further split by education might further partition the white node. At the end of fitting such a tree, there are b_{j} terminal nodes (nodes at the bottom of the tree), which contain groups where the average outcome \\mu_{j} should be purer due to iterative splitting. This iterative splitting is equivalent to the modeling of interaction effects, and combining many such trees allows for flexible and highly non-linear functions of the predictors to be calculated. Each data point x is thought of as assigned to one such terminal node for each tree, which captures E(y \\vert x), with the collection of u_{j}’s referred to collectively as M. Together, m such trees are fit to residual errors from an initial baseline prediction iteratively, ensuring that the trees are grown in varying structures that predict well for different parts of the covariate space, not just split on the same features producing identical predictions.\nTo fit such trees to the data and not overfit, BART utilizes a Bayesian framework, placing priors on tree structure, terminal node parameters, and variance, \\sigma^2. The prior for the u_{j} and \\sigma^2 are:\n\\begin{aligned} \\mu_{j} | T_{j} & \\sim N\\left(\\mu, \\sigma_{\\mu}^{2}\\right) \\\\ \\sigma^{2} & \\sim I G\\left(\\frac{\\nu}{2}, \\frac{\\nu \\lambda}{2}\\right) \\end{aligned}\nWhere I G(\\alpha, \\beta) is the inverse gamma distribution with shape parameter \\alpha and rate \\beta. The priors on the tree structure can be thought of as having 3 components. First, there is a prior on the probability that a tree of depth d = 0,1,2... is not terminal, which is \\alpha(1+d)^{-\\beta}, with \\alpha \\in(0,1) \\text { and } \\beta \\in[0, \\infty). This \\alpha controls how likely a terminal node is to be split, with smaller values indicating a lower likelihood of split, and \\beta controls the number of terminal nodes, larger \\beta implying more nodes. The second prior on the trees is on the distribution used to choose which covariate is split on. The final prior on the trees is on the value of the chosen splitting covariate at which to split. For both these later parameters, a common choice (and the one dbarts makes) is a simple discrete uniform distribution.\nThis set of priors also requires choosing the m, \\alpha, \\beta, \\mu_{\\mu}, \\sigma, \\nu and \\lambda hyperparameters, which can be chosen via cross-validation or simply set to defaults. In general, the past literature on BART finds that the defaults developed by Mculloch work quite well in a surprisingly large number of contexts (Tan, 2018). More specifically in the MRP context, both Montgomery & Olivella (2018) and Bisbee (2019) found little reason to utilize non-default hyperparameter choices after reasonable search. For completeness, however, I ran a small number of hyperparameter searches on my complete records data, as recommended by the author of the dbarts package. Similar to prior work, I found little reason to diverge from the defaults suggested by Chipman et al, and implemented in dbarts, although I did ultimately go with m = 200 trees as Chipman et al. suggest, not the m = 75 default in dbarts. For a full derivation of these choices and their resultant properties, see Chipman et al. (2008) or (Tan, 2018).\nA final modification of this formulation of BART is needed for binary outcomes. For binary outcomes, BART uses the probit link function to model the relationship between X and y:\nP\\left(Y_{k}=1 | \\mathbf{X}_{k}\\right)=\\Phi\\left[G\\left(\\mathbf{X}_{k}\\right)\\right]\nwhere \\Phi[.] is the cumulative distribution function of a standard normal distribution, and G is the full BART model we saw earlier. This slightly modifies steps for drawing from the posterior distribution, as discussed further in Chipman et al. (2008)."
  },
  {
    "objectID": "posts/BART VI/2020-03-06-BART-vi.html#bart-vi",
    "href": "posts/BART VI/2020-03-06-BART-vi.html#bart-vi",
    "title": "BART with varying intercepts in the MRP framework",
    "section": "BART-vi",
    "text": "BART-vi\nTo the best of my knowledge, no prior work has utilized BART with varying intercepts (BART-vi) in the MRP framework. Given the huge amount of prior work on MRP that leverages a large set of varying intercepts, this seems like a natural extension. This modifies the BART predictor for binary outcomes to\nP\\left(Y_{k}=1 | \\mathbf{X}_{k}\\right)=\\Phi\\left[G\\left(\\mathbf{X}_{k}\\right) + \\alpha_{k}\\right]\nwith a_{k} \\sim N\\left(0, \\tau^{2}\\right). Critically, this also removes the varying intercept variable from the choice of possible features to split on, modeling it purely as a varying intercept. Given that the dbarts package which I use currently only supports 1 varying intercept, the natural choice is the state variable, as it both has the most categories and is one of the original motivations for varying intercepts in MRP work. All the old priors and hyperparameters remain the same, and dbarts places an additional cauchy prior on \\tau^{2}. While this cauchy prior is much less informative than the half-t, half-normal, or other priors typically used for MRP, at this time it is not possible to modify the prior choice except to a gamma distribution which is also not ideal. Future work could consider fitting this type of model with the more informative priors favored by the MRP literature for random effects, although such an improvement would require a time investment in learning to modify the c++ codebase of dbarts."
  },
  {
    "objectID": "posts/BART VI/2020-03-06-BART-vi.html#comparing-the-predictions-of-the-two",
    "href": "posts/BART VI/2020-03-06-BART-vi.html#comparing-the-predictions-of-the-two",
    "title": "BART with varying intercepts in the MRP framework",
    "section": "Comparing the predictions of the two",
    "text": "Comparing the predictions of the two\nI provide two forms of evaluation for BART-vi vs regular BART, a quantitative assessment based on 10-fold cross validation, and a graphical/qualitative comparison of state level estimates resulting from the two.\nTo test the performance of this modification, I fit BART with and without varying intercept on state to the same m = 50 imputed1 datasets, using 10-fold cross validation within each dataset. Overall, while both models are extremely accurate, the BART-vi model slightly outperforms the regular BART model without varying intercepts in terms of RMSE, MSE, and AUC on average. Of course, given that this is a test of predictive accuracy before the final poststratification, this isn’t a full validation of BART-vi’s predictive superiority in the MRP context. However, this is consistent with (Tan, 2018)’s result in a more extensive set of simulation studies that there are small gains in accuracy to be had with BART-vi when random effects are used with an appropriate grouping variable. To make such a comparison completely rigorously, one would need to fit both types of models on a dataset with a ground truth such as vote share, poststratify, and then contrast their properties relative to that ground truth, not simply compare predictive accuracy on the initial set of surveys. However, as this is not possible for the border wall question, I take this as a rough suggestion that BART-vi may preform better in my context, and possibly in others.\nPlotting a comparison of the state-level median prediction from the two models after poststratification shows a familiar pattern of pooling. The BART-vi estimates are pulled somewhat towards the grand mean, whereas the ones without varying intercepts are a bit more spread out. Note, however, that we don’t see the sort of idealized pooling trend often shown in textbook examples of multilevel models, with non-multilevel predictions that are uniformly higher above the grand mean and uniformly lower below it compared to the multilevel predictions. This is due to the simple BART model modeling much more complex interactions based on the state variables than a single level regression.\n\n\n\nBART models with and without pooling\n\n\nOne particularly interesting qualitative example to illustrate the differences between the models is that of DC, which is both a small state, and an incredibly liberal one. This presents a dilemma from the perspective of varying intercepts pooling: on the one hand, with only 94 observations in the full data, we should want some pooling on DC’s estimate. On the other, DC genuinely is exceptionally liberal, which suggests that pooling it too much could hurt predictive performance. While both already somewhat regularize the 13% raw approval for the wall in our aggregated polls, BART-vi does so much more. Thus, while the average predictions are of higher quality with BART-vi, the DC and other extreme state predictions are superior without random effects. Most prior MRP work has been happy to make this sort of trade off, and based on the rough accuracy comparisons I’ve made, this appears to work well for my data and my BART model as well.\nComparing the full posterior distributions of the two models below, we can also see BART-vi has noticably wider 50 and 90% intervals as well (the dot indicates the median, the thick bar is the 50% interval, and the thinnest bar is the 90% one). Like with my CV testing, a complete sense of which level of uncertainty provided here is appropriate will have to wait for future MRP work that leverages data with a ground truth. However, in many cases, the fixed effects intervals border on what I’d call concerningly small- I wouldn’t be suprised if the coverage properties of the BART-vi intervals are better.\n\n\n\nFull Posterior of the Two Models"
  },
  {
    "objectID": "posts/BART VI/2020-03-06-BART-vi.html#next-steps",
    "href": "posts/BART VI/2020-03-06-BART-vi.html#next-steps",
    "title": "BART with varying intercepts in the MRP framework",
    "section": "Next steps",
    "text": "Next steps\nGiven that my work shows BART-vi having some desirable properties for RRP, what might be some extensions to explore next?\nA first obvious step might be explore this type of model on data where we do have a ground truth like voter turnout, or vote share. For a more extensive comparison, one could leverage Bisbee (2019)’s replication data, which would hopefully provide a more complete answer to whether this strategy works well in general.\nProbably the most theoretically interesting question would be how to handle the possibility of multiple random intercepts, if dbarts or another package eventually implements them. This represents a tradeoff between the benefits of flexible Bayesian non-parametrics in BART, and the pooling behavior of varying intercepts. Initially, I thought it was entirely feasible that the BART-vi I fit would have worse predictive accuracy, given the potential benefits of splitting on state. However, given that I utilize both 2012 vote share and region as predictors, it seems that the model still had ample state level information. However, as we pooled across more variables, this would increasingly weaken the non-parametric component of the BART-vi model. In this scenario, would pooling across demographic predictors that have many fewer categories make sense? While future work will have to tell, my guess is that the answer might be that only state or other geographic variables benefit from pooling.\n\n1: Given my data had a relatively large proportion of respondents who refused to answer at least 1 demographic question (10.54%), I also explored imputing the missing characteristics using a variety of different approaches. The full details of that are coming in another post, but I ran 10-fold CV on the imputations so that the evaluation would more fully mirror my final modeling scenario."
  },
  {
    "objectID": "posts/Convention Model/2019-07-13-convention-model.html",
    "href": "posts/Convention Model/2019-07-13-convention-model.html",
    "title": "Convention Prediction with a Bayesian Hierarchical Multinomial Model",
    "section": "",
    "text": "Here, I use a Bayesian hierarchical multinomial model to predict the first ballot results at the 2018 DFL (Democratic) State Convention, with data aggregated to the Party Unit level (ex: State Senate district) to guarantee anonymity. While using aggregated data obviously isn’t ideal, this sort of strategy shows a lot of promise, especially if individual level predictors could be harnessed as another level of the hierarchical model. As it stands, this is mostly a proof of concept for Bayesian hierarchical models in this context. To use something like this in practice, one could use prior predictive simulation to game out the convention under various assumptions, or condition on the first ballot data and use it to analyze trends in support and predict subsequent ballots as your floor team collects further data.\nAt this convention, I was working for Erin Murphy (who ultimately won) on her data team, and so I have access to their database on the Delegates heading into the convention. A winner is declared if any candidate reaches 60% of the Delegate pool of 1307.5 Delegates, so 785 votes. For simplicity, I focus in this project solely on predicting the first ballot results.\n\nBriefly, my goals with this project are:\n\nRepresent my pre-convention uncertainty about outcomes given the data we have intelligently.\nSee how much we can improve predictions by exploiting the hierarchical nature of districts (Party Units within Congressional Districts).\nSee if this makes sense as a modeling strategy to keep developing by adding the individual level data.\n\n\nThe Dataset\nFor each of the non-empty 121 Party Units in Minnesota, my response is the number of Delegate that each candidate (Erin Murphy, Rebecca Otto, and Tim Walz) received, along with a count for “No Endorsement” voters. Thus, summing across the 121 PUs would give the full first ballot results by candidate. I thus model these using a multinomial logit model in brms.\nIn terms of predictors, we have the Congressional district each party unit is in, which should explain some variation, as Otto/Walz are generally perceived to be more appealing to rural voters, while Murphy was from the Twin Cities. I also have the estimated proportion of Delegates the Murphy campaign believed they had the support of in each Party Unit, based on their field campaign, and on the subcaucuses the Delegates were elected out of. These proportions turned out to be quite accurate, and so my assumption is they’ll be strong predictors. Using the delegate level data (including issue and candidate IDs, subcaucuses, and voter file information like gender and age) would no doubt significantly improve things.\nI exclude the data cleaning here, but it’s available in the .Rmd on my github. One non-obvious transformation I make is to double all counts before modeling, but halve them before analysis, as some rural, low population areas are awarded “half delegates”, and I need integer outcomes to work with a multinomial model.\n\n\nPrior Predictive Simulation: Intercept Only\nI start with an intercept only model, and plot realizations of the first ballot across draws. This helps explain my level of uncertainty before we include predictors and condition on the data. To keep this post short, I only include the final result of iterating to find suitably cautious priors at this early stage, not as I add further predictors to the model. With a multinomial model like this, even just enforcing the count constraint (vote counts in each party unit have to add to their total allocation of delegates) already produces a suprisingly reasonable model.\nFor context on these priors, there was a relatively large amount of uncertainty for our campaign and all the campaigns heading into first ballot for a variety of reasons. First, we had only ID’d about 2/3 of the delegate body by first ballot. Second, it was becoming increasingly clear that Rebecca Otto didn’t have the delegates to win the convention, so it was possible we’d see a decent portion of her delegates switch sides even before first ballot. Finally, a major statewide c4, ISAIAH, had been telling their delegates to hold off on committing, but the rumor was that they were going to endorse Erin Murphy’s or Otto’s campaign (whichever progressive was more viable), so a large number of Delegates had a preference they didn’t openly state.\nAll that said, while it was highly unlikely that anyone was going to reach a winning 60% of the delegate pool (785 Delegates) on the first ballot, I did want at least a bit of probability on those outcomes. From our ID data, it looked something like 40%-20%-40% was the most likely outcome for the first ballot, which is how I set the means.\nAs a final note, voting “No Endorsement” on an early ballot is extremely rare, which is correctly reflected.\n# Base class is No-Endorsement\ninitial_prior <- c(prior(normal(4,.25), class = \"Intercept\", dpar = \"mu2\"),\n              prior(normal(3,.5), class = \"Intercept\", dpar = \"mu3\"),\n              prior(normal(4,.25), class = \"Intercept\", dpar = \"mu4\")\n              )\n\nint_only <- brm(bf(y | trials(Total) ~ 1), family=multinomial(),data=erin_data, prior = initial_prior, sample_prior=\"only\")\n\nlinpred <- posterior_linpred(int_only, transform = T)\n\n# Divide by 2 after summing the totals to put back on original delegate count scale\nboxplot(apply(linpred, c(1,3), FUN = sumdiv2), pch = \".\", las = 1,\n        names = c(\"No Endorse\", \"Erin Murphy\", \"Rebeca Otto\", \"Tim Walz\"))\n\n\n\nInitial Posterior\nNow let’s add in our non-CD predictors and condition on our data. I add a N(0,3) prior over all the \\beta’s as a weakly informative prior to help the model fit. The model fits well; there are no divergences, all \\hat{R} were 1, and I got a good number of effective samples for each parameter.\nAs we’d expect with so little data, the standard errors are very large compared to the coefficients, but generally point the right ways- the Strong/Lean Erin predictions have a positive influence on Erin’s support (mu2), for example, and similar with Walz (mu4), and Otto (mu2). Later, when I look at marginal plots, I’ll talk more about some of the more interesting coefficients, namely the ISAIAH and Unknown ones.\n##  Family: multinomial\n##   Links: mu2 = logit; mu3 = logit; mu4 = logit\n## Formula: y | trials(Total) ~ ISAIAH + Lean.Erin + Lean.Walz + Lean.Otto + Strong.Erin + Strong.Otto + Strong.Walz + Undecided + Unknown\n##    Data: erin_data (Number of observations: 121)\n## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup samples = 4000\n##\n## Population-Level Effects:\n##                 Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat\n## mu2_Intercept       4.15      1.07     2.07     6.27       4931 1.00\n## mu3_Intercept       3.40      1.06     1.40     5.51       5520 1.00\n## mu4_Intercept       4.19      1.03     2.14     6.23       5352 1.00\n## mu2_ISAIAH          1.59      1.68    -1.71     4.96       5557 1.00\n## mu2_Lean.Erin       1.00      1.95    -2.92     4.80       5198 1.00\n## mu2_Lean.Walz       0.24      1.99    -3.72     4.10       6179 1.00\n## mu2_Lean.Otto      -1.74      2.08    -5.67     2.44       5975 1.00\n## mu2_Strong.Erin     2.18      1.69    -1.17     5.52       5448 1.00\n## mu2_Strong.Otto    -1.91      1.71    -5.23     1.54       6238 1.00\n## mu2_Strong.Walz    -1.70      1.65    -5.03     1.63       5958 1.00\n## mu2_Undecided      -0.53      1.78    -4.15     3.03       5513 1.00\n## mu2_Unknown         0.11      1.55    -2.94     3.17       5267 1.00\n## mu3_ISAIAH         -0.64      1.74    -3.97     2.82       5673 1.00\n## mu3_Lean.Erin       0.73      2.03    -3.34     4.69       4694 1.00\n## mu3_Lean.Walz      -1.27      2.10    -5.35     2.89       6218 1.00\n## mu3_Lean.Otto       3.26      2.14    -0.99     7.40       5679 1.00\n## mu3_Strong.Erin    -1.22      1.68    -4.51     2.08       5648 1.00\n## mu3_Strong.Otto     4.28      1.70     0.94     7.74       5737 1.00\n## mu3_Strong.Walz    -2.46      1.66    -5.60     0.85       6205 1.00\n## mu3_Undecided       0.63      1.74    -2.81     4.06       5264 1.00\n## mu3_Unknown         0.42      1.56    -2.56     3.45       6235 1.00\n## mu4_ISAIAH         -1.86      1.68    -5.10     1.57       5066 1.00\n## mu4_Lean.Erin      -0.39      1.97    -4.21     3.44       5386 1.00\n## mu4_Lean.Walz       1.51      1.98    -2.32     5.48       6163 1.00\n## mu4_Lean.Otto      -1.28      2.09    -5.41     2.80       6302 1.00\n## mu4_Strong.Erin    -1.48      1.68    -4.67     1.90       5654 1.00\n## mu4_Strong.Otto    -2.00      1.70    -5.33     1.47       6029 1.00\n## mu4_Strong.Walz     1.46      1.60    -1.65     4.61       5859 1.00\n## mu4_Undecided       0.19      1.72    -3.22     3.59       5522 1.00\n## mu4_Unknown         1.08      1.51    -1.94     4.07       6072 1.00\n##\n## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample\n## is a crude measure of effective sample size, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNesting the Party Units in CD\nA next logical step for the model would be to incorporate the hierarchical structure present in the data- Party Units nested within Congressional Districts. Given that the CD’s reflect both the progressive/moderate and rural/urban divides that defined the election, expecting some significant between group variation is reasonable.\nfinal_prior <- c(prior(normal(4,.25), class = \"Intercept\", dpar = \"mu2\"),\n              prior(normal(3,.5), class = \"Intercept\", dpar = \"mu3\"),\n              prior(normal(4,.25), class = \"Intercept\", dpar = \"mu4\"),\n              prior(normal(0,3),class = \"b\"),\n              prior(normal(0,.2), class = \"sd\", group = \"CD\", dpar = \"mu2\"),\n              prior(normal(0,.2), class = \"sd\", group = \"CD\", dpar = \"mu3\"),\n              prior(normal(0,.2), class = \"sd\", group = \"CD\", dpar = \"mu4\")\n              )\n\nfull_model <- brm(bf(y | trials(Total) ~ ISAIAH + Lean.Erin + Lean.Walz + Lean.Otto + Strong.Erin + Strong.Otto +\n        Strong.Walz + Undecided + Unknown + (1|CD)), family=multinomial(),prior = final_prior, data=erin_data)\nsummary(full_model)\n##  Family: multinomial\n##   Links: mu2 = logit; mu3 = logit; mu4 = logit\n## Formula: y | trials(Total) ~ ISAIAH + Lean.Erin + Lean.Walz + Lean.Otto + Strong.Erin + Strong.Otto + Strong.Walz + Undecided + Unknown + (1 | CD)\n##    Data: erin_data (Number of observations: 121)\n## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup samples = 4000\n##\n## Group-Level Effects:\n## ~CD (Number of levels: 9)\n##                   Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat\n## sd(mu2_Intercept)     0.08      0.06     0.00     0.23       2406 1.00\n## sd(mu3_Intercept)     0.14      0.09     0.01     0.35       1760 1.00\n## sd(mu4_Intercept)     0.09      0.07     0.00     0.24       2311 1.00\n##\n## Population-Level Effects:\n##                 Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat\n## mu2_Intercept       4.14      1.08     2.03     6.29       2895 1.00\n## mu3_Intercept       3.33      1.09     1.24     5.46       2566 1.00\n## mu4_Intercept       4.27      1.07     2.21     6.36       2919 1.00\n## mu2_ISAIAH          1.69      1.66    -1.42     5.00       3289 1.00\n## mu2_Lean.Erin       1.07      1.95    -2.71     4.96       3678 1.00\n## mu2_Lean.Walz       0.17      2.06    -3.81     4.18       3957 1.00\n## mu2_Lean.Otto      -1.38      2.10    -5.57     2.70       3972 1.00\n## mu2_Strong.Erin     2.17      1.65    -1.03     5.52       3294 1.00\n## mu2_Strong.Otto    -1.84      1.73    -5.14     1.55       3077 1.00\n## mu2_Strong.Walz    -1.72      1.62    -4.81     1.40       3741 1.00\n## mu2_Undecided      -0.52      1.80    -4.01     3.13       2834 1.00\n## mu2_Unknown         0.11      1.53    -2.93     3.17       3079 1.00\n## mu3_ISAIAH         -0.70      1.73    -4.01     2.66       2931 1.00\n## mu3_Lean.Erin       0.68      2.05    -3.29     4.62       4101 1.00\n## mu3_Lean.Walz      -1.01      2.11    -5.12     3.11       4077 1.00\n## mu3_Lean.Otto       3.16      2.17    -1.11     7.43       4086 1.00\n## mu3_Strong.Erin    -1.15      1.69    -4.40     2.23       3109 1.00\n## mu3_Strong.Otto     4.15      1.76     0.80     7.64       2990 1.00\n## mu3_Strong.Walz    -2.19      1.62    -5.38     0.99       3156 1.00\n## mu3_Undecided       0.62      1.83    -2.95     4.16       3098 1.00\n## mu3_Unknown         0.47      1.55    -2.56     3.48       2478 1.00\n## mu4_ISAIAH         -1.91      1.73    -5.23     1.47       3164 1.00\n## mu4_Lean.Erin      -0.42      1.98    -4.21     3.45       3632 1.00\n## mu4_Lean.Walz       1.34      2.02    -2.61     5.35       3723 1.00\n## mu4_Lean.Otto      -1.47      2.09    -5.46     2.66       3452 1.00\n## mu4_Strong.Erin    -1.57      1.63    -4.74     1.57       3367 1.00\n## mu4_Strong.Otto    -1.91      1.76    -5.34     1.64       2914 1.00\n## mu4_Strong.Walz     1.38      1.58    -1.84     4.50       3574 1.00\n## mu4_Undecided      -0.01      1.80    -3.44     3.57       3232 1.00\n## mu4_Unknown         0.99      1.54    -2.00     3.99       3137 1.00\n##\n## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample\n## is a crude measure of effective sample size, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nModel Comparison\nIntuitively, a model with pooling across CD’s should outperform a single level model using them, but let’s make sure. Below, the multilevel level model far outperforms the single level one, with it’s ELPD more than 2 standard errors better.\nOne limitation of the analysis below is that I wasn’t able refit without the final observation (the super delegates) to calculate ELPDs for the super delegates directly. This is because the “Super” level only exists in 1 example, and the loo package doesn’t allow new factor levels- holding it out would thus throw an error.\n##                  elpd_diff se_diff\n## full_model         0.0       0.0\n## not_pooled_model -10.7       4.9\n\n## Method: stacking\n## ------\n##        weight\n## model1 1.000\n## model2 0.000\n\n\nInteresting Marginal Plots\nFor the most part, the marginal plots were what I’d expect- for instance, PUs with a greater estimated strong support for Erin (“Strong Erin”), had increasingly greater support for Erin, and vice versa for Walz, and so I don’t show most of these.\nEven though the predictions are obviously very noisy however, they did pick up on two important, more subtle trends. First, in the ISAIAH plot below, it’s beginning to appear that Murphy (2) does well in places with many ISAIAH delegates, whereas Walz (4) does progressively worse.\n\nThe other interesting trend the model picked up on was that it tends to be harder to ID your opponent’s supporters than your own- as your supporters want to contact and work with the campaign, but the opponents’ have no reason to do so, and help their candidate by not giving you much information. This is correctly reflected in the negative slope in Unknown for Erin (2), but positive one for Walz (4), given the predictor data comes from the Murphy campaign.\nWhile these plots suggest a understandably high level of uncertainty, the fact that they’re correctly reflecting many of the relationships I believe to be true offers some level of face validity of the model.\n\n\n\nPosterior Predictive Check\nPlotting the predictions for Erin Murphy (blue) and Tim Walz (red) against the actual results, we can see the predictions track reasonably closely considering the limited data. While many point predictions fall outside the 25-75 quantile range, the vast majority stay within the boxplot’s whiskers. Again, stressing the limitations of the small dataset we have, this is a fairly reasonable range of outcomes to predict, and there’s no systematic pattern I can see to which Party Units the model struggles with.\n\n\n\nConclusion\nTo actually use a model like this on a convention floor, I’d definitely want to fully incorporate the individual level data that underlie what’s shown here, as attempting to model with 1 obs/district is challenging. However, this initial model is fairly promising- nesting PUs within CDs seems like a strong overall strategy, and even with limited data, the model already can pick up on relationships like that between estimated ISAIAH and Unknown support and Delegate returns. Some further details can be found in the .rmd here.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{timm2019,\n  author = {Andy Timm},\n  title = {Convention {Prediction} with a {Bayesian} {Hierarchical}\n    {Multinomial} {Model}},\n  date = {2019-07-03},\n  url = {https://andytimm.github.io/2019-07-13-convention-model.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndy Timm. 2019. “Convention Prediction with a Bayesian\nHierarchical Multinomial Model.” July 3, 2019. https://andytimm.github.io/2019-07-13-convention-model.html."
  },
  {
    "objectID": "posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html",
    "href": "posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html",
    "title": "Type S/M errors in R with retrodesign()",
    "section": "",
    "text": "This is a online version of the vignette for my r package retrodesign. It can be installed with:\nIn light of the replication and reproducibility crisis, researchers across many fields have been reexamining their relationship with the Null Hypothesis Significance Testing (NHST) framework, and developing tools to more fully understand the implications of their research designs for replicable and reproducible science. One group of papers in this vein are works by Andrew Gelman, John Carlin, Francis Tuerlinckx, and others which develop two new metrics for better understanding hypothesis testing in noisy samples. For example, Gelman and Carlin’s Assessing Type S and Type M Errors (2014) argues that looking at power, type I errors, and type II errors are insufficient to fully capture the risks of NHST analyses, in that such analysis focuses excessively on statistical significance. Instead, they argue for consideration of the probability you’ll get the sign on your effect wrong, or type S error, and the factor by which your effect size might be exaggerated, or type M error. Together, these additional statistics more fully explain the dangers of working in the NHST framework, especially in noisy, small sample environments.\nretrodesign is a package designed to help researchers better understand type S and M errors and their implications for their research. In this vignette, I introduce both the need for the type S/M error metrics, and the tools retrodesign provides for examining them. I assume only a basic familiarity with hypothesis testing, and provide definitional reminders along the way."
  },
  {
    "objectID": "posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html#an-initial-example",
    "href": "posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html#an-initial-example",
    "title": "Type S/M errors in R with retrodesign()",
    "section": "An Initial Example",
    "text": "An Initial Example\nTo nail down the assumptions we’re working with, we’ll start with an abstract example from Lu et al. (2018); the second will draw on a real world scenario and follow Gelman and Carlin’s suggested workflow for design analysis more closely.\nLet’s say we’re testing whether a true effect size is zero or not, in a two tailed test.\nH _ { 0 } : \\mu = 0 \\quad \\text { vs. } \\quad H _ { 1 } : \\mu \\neq 0\nWe’re assuming that the test statistic here is normally distributed. As Gelman and Tuerlinckx (2009) note, this is a pretty widely applicable assumption; through the Central Limit Theorem, it applies to common settings like differences between test and control groups in a randomized experiment, averages, or linear regression coefficients. If it helps, imagine the following in the context of one of those from your field.\nTraditionally, we’d focus on Type I/II errors. Type I error is rejecting the null hypothesis, when it is true. Type II error is failing to reject the null hypothesis, when it is not true. The power, then, is just is the probability that the test correctly rejects the null hypothesis when a specific alternative hypothesis is true.\nBeyond those, we’ll also consider:\n\nType S (sign): the test statistic is in the opposite direction of the true effect size, given that the statistic is statistically significant;\nType M (magnitude or exaggeration ratio): the test statistic in magnitude exaggerates the true effect size, given that the statistic is statistically significant.\n\nNotice that both of these are conditional on the test statistic being statistically significant; we’ll come back to this fact several times.\nTo visualize these, we’ll draw 5000 samples from a normal distribution with mean .5, and standard deviation 1. We’ll then analyze these in a NHST setting where we have a standard error of 1. We can use sim_plot() to do so, with the first parameter being our postulated effect size .5, and the second being our hypothetical standard error of 1. If you prefer to not use ggplot graphics like I do here, set the gg argument to FALSE.\nsim_plot(.5,1)\n\nHere, the dotted line is the true effect size, and the full lines are where the statistic becomes statistically significantly different from 0, given our standard error of 1. The greyed out points aren’t statistically significant, the squares are type M errors, and the triangles are type S errors.\nEven though the full distribution is faithfully centered around the true effect, once we filter using statistical significance, we will both exaggerate the effect and get its sign wrong often.\nOf course, trying to find a true effect size of .5 with a standard error of 1 is extremely underpowered. More precisely, the power, Type S and type M error here are:\nretro_design(.5,1)\n#> $power\n#> [1] 0.07909753\n#>\n#> $typeS\n#> [1] 0.0878352\n#>\n#> $typeM\n#> [1] 4.788594\nThe function arguments here the same as those for sim_plot() above. If you want to work with a t-distribution instead, you’ll need to use retrodesign() instead, and provide the degrees of freedom for the df argument. retrodesign() is the original function provided by Gelman & Carlin (2014), which uses simulation rather than an exact solution to calculate the errors. It’s thus slower, but can work with t-distributions as well (the closed form solution only applies in the normal case).\nA power of .07 isn’t considered good research in most fields, so most cases won’t be this severe. However, it does illustrate two important points. In a underpowered example like this, we will hugely overestimate our effect size (by a factor of 4.7x! on average), or even get its sign wrong if we’re unlucky (around 8% of the time). Further, these are practical measures to focus on; getting the sign wrong could mean recommending the wrong drug treatment, exaggerating the treatment effect could mean undertreating someone once the drug goes to market."
  },
  {
    "objectID": "posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html#a-severe-example",
    "href": "posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html#a-severe-example",
    "title": "Type S/M errors in R with retrodesign()",
    "section": "A Severe Example",
    "text": "A Severe Example\nNow that you hopefully have a sense of what type S/M error add to our understanding of NHST in the context of noisy, small sample studies, we’ll move onto a real world example, where we’ll focus on following Gelman and Carlin’s suggested design analysis through one of their examples.\nWe’ll be working with Kanazawa (2007), which claims to have found that the most attractive people are more likely to have girls. To be more specific, each of their ~3,000 people surveyed had been assigned a “attractiveness” score from 1-5, and they then compared the first born children of the most attractive to other groups; 56% were girls compared to 48% in the other groups. They thus obtained a difference estimate of 8%, with a p-value of .015, so significant at the traditional \\alpha = .05.\nStepping back for a second, this is fishy in numerous ways. First, comparing the first borne children is an oddly specific comparison to have run- at worst, this might be a case of p-hacking. Or maybe they saw a strong comparison, and decided to test it, and in doing so fell into a Garden of Forking Paths problem. Second, if attractive people had many more girls, it seems unlikely that gender balance would be as even as it is. So again, this example has a likely spurious result, is likely to have low power, and a high chance of S/M errors.\nTo do a design analysis of this, Gelman and Carlin’s first step is to review the literature for a posited true effect size. There is plenty of prior research on variation in sex ratio of human births. A huge variety of factors have been studied such as race, parental age, season of birth, and so on, only finding effects from .3% to 2%. In the most extreme cases (conditions like extreme poverty or famine), these effects only rise to 3%. (If you’re interested, the causal reasoning seems to be that male fetuses are more likely than female ones to die under adverse conditions.)\nLike traditional design analyses, we’ll posit a wide range of effects here, and see how our power, type S error, and type M error rates change correspondingly. Gelman and Carlin end up looking at .1-1%, reasoning that sex ratios vary very little in general, and that the subject attractiveness rating is quite noisy as well. Even if we compare their effect size to the effect sizes found in the most extreme scenarios in prior literature, it doesn’t look good.\nWe can infer their standard error of the difference to be 3.3% from their reported 8% estimate and p-value of .015; we now have everything we need.\n# The posited effects Gelman and Carlin consider\nretro_design(list(.1,.3,1),3.3)\n#>   effect_size      power    type_s   type_m\n#> 1         0.1  0.0501052 0.4646377 77.15667\n#> 2         0.3 0.05094724 0.3953041 25.74305\n#> 3           1 0.06058446 0.1950669 7.795564\n\n# A particularly charitable set of posited effects\nretro_design(list(.1,.3,1,2,3),3.3)\n#>   effect_size      power     type_s   type_m\n#> 1         0.1  0.0501052  0.4646377 77.15667\n#> 2         0.3 0.05094724  0.3953041 25.74305\n#> 3           1 0.06058446  0.1950669 7.795564\n#> 4           2 0.09302718 0.05529112 3.982141\n#> 5           3  0.1487169 0.01384174 2.719244\nBy providing a list for our first argument A, we get a dataframe with a posited effect size, and corresponding power, type S, and type M errors in each row. If you just want the lists of power/type S/type M, you can just feed in a vector, ie c(.1,.3,1).\nSo if we go with a high but fairly reasonable posited effect of 1%, there’s a nearly 1 in 5 chance that such an experiment would get the direction of the effect wrong. Even if we assume that being the daughter of a highly attractive person has equivalent effect to being born in a famine (effect size 3%), this experiment would exaggerate the true effect size by a factor of 2.7x on average.\nThis analysis has given us added information beyond what we’d get from the point estimate, the confidence interval, and the p-value. Under reasonable assumptions about the true effect size, this study simply seems too small to be informative: Under most assumptions, we’re quite likely to get the sign wrong, and even with a quite generous assumption of true effect size we’ll greatly exaggerate the effect size."
  },
  {
    "objectID": "posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html#assessing-type-sm-errors-when-you-dont-have-prior-information",
    "href": "posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html#assessing-type-sm-errors-when-you-dont-have-prior-information",
    "title": "Type S/M errors in R with retrodesign()",
    "section": "Assessing Type S/M errors when you don’t have prior information",
    "text": "Assessing Type S/M errors when you don’t have prior information\nOne obvious objection you might have to this framework is that you may not have a clear sense of what your effect size will be. As extreme as it sounds, you may not even have a sense of the right order of magnitude. In these cases, it makes even more sense to calculate type s/m errors across a variety of posited effect sizes and see how they influence your research."
  },
  {
    "objectID": "posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html#so-how-worried-should-we-be-in-more-reasonable-studies",
    "href": "posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html#so-how-worried-should-we-be-in-more-reasonable-studies",
    "title": "Type S/M errors in R with retrodesign()",
    "section": "So how worried should we be in more reasonable studies?",
    "text": "So how worried should we be in more reasonable studies?\nAnother concern might be that my first two examples were severely underpowered. However, even with powers that are publishable in many fields, we should still be worried about type M errors, but not type S errors.\nTo sketch out the relationship between possible effect sizes and these errors, we adopt the standard error from the prior example, but greatly expand the posited effect sizes, to max out at 10, where the power would be a perfectly reasonable .85. We’ll use type_s and type_m, both of which take a single or list of possible A’s, and a standard error, similar to retrodesign, but only return the respective error.\npossible_effects <- seq(.1,10, by = .5)\neffect_s_pairs <- data.frame(possible_effects,type_s(possible_effects,3.3))\neffect_m_pairs <- data.frame(possible_effects,type_m(possible_effects,3.3))\n\ns_plot<- ggplot(effect_s_pairs, aes(possible_effects, type_s)) + geom_point()\nm_plot <- ggplot(effect_m_pairs, aes(possible_effects, type_m)) + geom_point()\n\ngrid.arrange(s_plot, m_plot, ncol=2)\n\nAs Lu et al. (2018) note, the type S and M error shrink at very different rates as power rises.\nThey find the probability of type S error decreases rapidly. To ensure that $s $ and s\\leq 0.01, we only need power = 0.08 and power = 0.17, respectively. Thus, unless your study is severely underpowered, you shouldn’t need to worry about type s errors very often.\nOn the other hand, The type m error decreases relatively slowly. To ensure that m \\leq 1.5 and m \\leq 1.1, we need power = 0.52 and power = 0.85, respectively. Whereas even moderately powered studies make type s errors relatively improbable, only very high powered studies keep exaggeration of effect sizes down. If your field requires a power of .80, you should thus be cognizant that effect sizes are likely somewhat inflated."
  },
  {
    "objectID": "posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html#solutions-outside-nhst",
    "href": "posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html#solutions-outside-nhst",
    "title": "Type S/M errors in R with retrodesign()",
    "section": "Solutions outside NHST",
    "text": "Solutions outside NHST\nFor the majority of this vignette, I’ve avoided questioning whether we should be working in the NHST framework. However, retrodesign is a package to help address some limitations of NHST work as it’s traditionally practiced, so it makes sense to address these solutions.\nGoing back to the plot we started with, remember that the underlying gaussian does actually faithfully center around it’s mean of .5.\nsim_plot(.5,1)\n\nType S and M errors are an artifact of the hard thresholding implicit in a NHST environment, where an arbitrary p-value (usually .05) decides what is and isn’t noteworthy.\nIf you have to work in the NHST framework because it’s what your discipline publishes, you can better understand some problems it might cause by exploring type S and M errors. If you’re able to choose how you analyze your experiments though, you can avoid these errors (and many others) by abandoning statistical significance as a hard filter entirely. If learning more about problems with NHST beyond type S and M errors, and suggestions for alternative strategies is interesting to you, check out the Abandon Statistical Significance paper linked in the further reading below."
  },
  {
    "objectID": "posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html#further-reading",
    "href": "posts/Intro to Retrodesign/2019-02-05-Intro_To_retrodesign.html#further-reading",
    "title": "Type S/M errors in R with retrodesign()",
    "section": "Further Reading",
    "text": "Further Reading\n\nGelman and Tuerlinckx’s Type S error rates for classical and Bayesian single and multiple comparisons procedures (2000): A comparison of the properties of Type S errors of frequentist and Bayesian confidence statements. Useful for how this all plays out in a Bayesian context. Bayesian confidence statements have the desirable property of being more conservative than frequentist ones.\nGelman and Carlin’s Assessing Type S and Type M Errors (2014): Gelman and Carlin compare their suggested design analysis, as we’ve written about above, to more traditional design analysis, through several examples, and discuss the desirable properties it has in more depth than I do here. It is also the source of the original retrodesign() function, which I re-use in the package with permission.\nLu et al’s A note on Type S/M errors in hypothesis testing (2018): Lu and coauthors go further into the mathematical properties of Type S/M errors, and prove the closed form solutions implimented in retrodesign.\nMcShane et al’s Abandon Statistical Significance (2017): If you want a starting point on the challenges with NHST that have led many statisticians to argue for abandoning NHST all together, and starting points for alternative ways of doing science."
  },
  {
    "objectID": "posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html",
    "href": "posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html",
    "title": "Is Voting Habit Forming? Replication, and additional robustness checks",
    "section": "",
    "text": "This post walks through my replication of the fuzzy regression discontinuity portion of Coppock and Green’s 2016 paper Is Voting Habit Forming? New Evidence from Experiments and Regression Discontinuities, and details some additional robustness checks I conducted. While I was able to reproduce all of their estimates fairly easily due to great replication materials, my additional robustness checks suggest that their results are more sensitive to bandwith choices than their testing suggests. Additionally, Coppock and Green argue the effects they find are likely due to habit alone, whereas I’m unconvinced that’s the sole mechanism involved. This is my work from Jennifer Hill and Joe Robinson-Cimpian’s Causal Inference class at NYU, and I’m grateful for both their feedback on the project."
  },
  {
    "objectID": "posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html#motivation",
    "href": "posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html#motivation",
    "title": "Is Voting Habit Forming? Replication, and additional robustness checks",
    "section": "Motivation",
    "text": "Motivation\nWhen we first vote in an election, is that experience habit forming? If so, how strong is the effect of habit? This is a critical question of voting behavior. For example, it has implications for attempting to make the electorate more representative of the general population. If people continue to vote at a reasonable rate after their first ballot cast, then the burden of organizations doing GOTV work gets comparatively lighter; further, their work has dividends for elections to come. On the extreme other end, if almost no habit forming effect existed, then turnout work would be a project of individual races. Fortunately, practical experience from campaigns suggests there’s likely at least some habit effect. After all, the best predictor of voting in the current election is voting in the prior.\nIf we believe that there may be a habit effect on voting in an initial in future elections, how can we estimate a causal effect? Of course, we cannot use a randomized experiment: we cannot ethically randomize some citizens to vote, or more concerningly, randomize some citizens to not do so. Thus, the main observational strategies used to estimate such causal effects involve instrumental variable approaches, utilizing randomized experiments in the “upstream” election which attempt to mobilize voters in the treatment group as an instrument to estimate the Complier Average Causal Effect (CACE) in subsequent “downstream” elections. For example, Bedolla and Michelson (2012), utilizing a series of experiments that aimed to improve turnout in minority voters in California, find overall that voting in the upstream election results in a 23-percentage point increase in probability to vote in subsequent elections for compliers. However, these designs are frequently limited by somewhat weak instruments and low overall sample sizes, as increasing turnout through campaign intervention is extremely difficult and expensive per voter reached, especially in non-white and younger populations (Gerber & Green, 2012). The weakness of these instruments has prompted recent research using fuzzy regression discontinuity (FRD) designs, leveraging the fact that being just barely 18 or just too young to vote on the upstream election day should set voters on very different voting trajectories if such a habit effect exists (Meredith 2009; Dinas (2012).\nThe latest and most comprehensive such fuzzy regression discontinuity paper is Coppock and Green’s 2016 paper Is Voting Habit Forming? New Evidence from Experiments and Regression Discontinuities, which considerably expands both the amount of data used and sophistication of modeling design used. Beyond just using the FRD design, they also incorporate results from several instrumental variable approaches in their very impressive paper."
  },
  {
    "objectID": "posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html#data",
    "href": "posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html#data",
    "title": "Is Voting Habit Forming? Replication, and additional robustness checks",
    "section": "Data",
    "text": "Data\nStates are required by the Help America Vote Act to make available to the public individual level data on every registered voter in their state, although states vary considerably in how much information about each voter they provide. For example, some states provide complete histories of which election a voter has participated in, whereas others provide only the most recent 8 elections. Similarly, some states provide birthdate, whereas others provide only age, or age buckets.\nGiven that the analysis they propose requires precise voting histories and a specific birthdate, Coppock and Green gathered the full voter file in 2013 from the 15 states where all three of\n\na complete history of which elections the individual had voted in,\ninformation on that voter’s eligibility to vote (to rule out the otherwise ineligible such as felons), and\nbirthdate were available.\n\nIn the replication file, these 15 voter files have been grouped by birthdate cohort and state, so that the unit of analysis is a group such as registered voters born on 11/6/1998, in Arkansas. This grouping sidesteps the potential problems arising from the fact the voter file only includes people registered to vote. While there is not a complete list of just eligible and just ineligible 17 and 18-year olds, through this cohort grouping, we can work with the 2008/2012 votes cast by cohorts above and below the eligibility threshold instead. Unfortunately, this also adds some additional complexity to interpreting results, as most potential violations of the fuzzy regression discontinuity design assumptions would occur at the individual level. Further, demographic profiles of the cohorts aren’t available, limiting our set of possible confounders to work with. The full dataset has 172,616 state and birthdate state cohorts, but for the purpose of my analysis, I work with the 11,680 birthdate state cohorts whose birthday fall within 365 days of eligibility to vote in the 2008 presidential election.\nMy outcome of interest is the number of votes cast in the downstream election, the 2012 presidential general election. The “treatment” is having voted in the upstream 2008 presidential election. The instrument is eligibility to participate in the 2008 election, which is determined by being 18 on election day, not 18 by the registration deadline as is sometimes commonly believed. The eligibility criteria being uniform across states greatly simplifies generating the remaining variables used. Due to this uniformity, the forcing variable is simply the cohort’s number of days above or below turning 18 on the upstream election day. Finally, to account for seasonal and day of the week birth trends which subsequently influence total votes cast by each cohort, Coppock and Green include a lagged downstream vote total for the birthdate cohort one year older. As an illustration of these quantities, here is an example row:\n\n\n\n\n\n\n\n\n\n\nCohort\n2008 Vote Total\n2012 Vote Total\n2008 Eligible?\nDays to Eligible\n\n\n\n\n1988-11-06-AR\n20\n27\nYes\n-364\n\n\n\nTo show variation between states, I present below the total number of votes cast in each state for 2008 and 2012. As we’d expect with half the group ineligible in 2008, the vote totals rise considerably for 2012. The variation in state population carries through to the number of votes cast in each state in the sample, which will later influence the standard errors we are able to achieve. Overall, however, this is quite a large sample, both in the number of state-birthdate cohorts (11,680), and in total 2008/2012 votes cast (532,459 and 961,894 respectively).\n\n\n\nState\nSum 2008\nSum 2012\n\n\n\n\nAR\n11,796\n20,915\n\n\nCT\n22,475\n33,040\n\n\nFL\n88,704\n173,999\n\n\nIA\n5,336\n10,600\n\n\nIL\n59,149\n108,721\n\n\nKY\n22,017\n40,946\n\n\nMO\n35,603\n63,381\n\n\nMT\n6,782\n11,030\n\n\nNJ\n52,125\n87,395\n\n\nNV\n12,494\n24,502\n\n\nNY\n80,180\n150,489\n\n\nOK\n15,608\n23,565\n\n\nOR\n17,900\n36,843\n\n\nPA\n95,412\n165,622\n\n\nRI\n6,878\n10,846"
  },
  {
    "objectID": "posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html#estimand",
    "href": "posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html#estimand",
    "title": "Is Voting Habit Forming? Replication, and additional robustness checks",
    "section": "Estimand",
    "text": "Estimand\nGiven this data, we can estimate a Complier Average Causal Effect (CACE). Define D to be the number of votes cast in the 2008 election, and Y to be votes in the 20212 election. As a reminder, one cannot assign D, you can only leverage the encouragement to do so created by being just eligible, which I label Z \\in [0,1], with 0 being too young, and 1 being old enough to vote in 2008 respectively. The forcing variable, days above or below being old enough to vote in 2008, I label T. Lagged refers to lagged downstream vote total for the birthdate cohort one year older, used to eliminate other temporal trends. For individual birthdate cohorts, I use subscript i’s.\nThe estimand is thus:\n\\lim _{T \\downarrow 0} \\sum_{1}^{N}\\left[Y_{i} | D_{1 i}=1\\right]- \\lim _{T \\uparrow 0} \\sum_{1}^{N}\\left[Y_{i} | D_{1 i}=0\\right]\nThis is the additional number of votes in 2012 we would expect across cohorts if all voters did vote in 2008, beyond the number if each cohort’s subjects had not voted in 2008, among only the compliers, subjects who vote if and only if they receive the “encouragement” of being eligible in 2008. Following Coppock and Green, I present these as expected proportions of increase, ie .06 or 6% increase in 2012 turnout, as this makes it easier to compare amongst states with different vote totals. This must be restricted to compliers only because they are the only group in our analysis whose behavior changes just above or below being 18 on election day. For example, the encouragement provided by becoming eligible does not influence the behavior of “never takers”, those who would never vote. Our encouragement cannot create a difference in votes cast for this group or another other that is not the compliers: being just above or below the eligibility threshold only potentially alters the observed outcome of the compliers."
  },
  {
    "objectID": "posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html#methods",
    "href": "posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html#methods",
    "title": "Is Voting Habit Forming? Replication, and additional robustness checks",
    "section": "Methods",
    "text": "Methods\nTo build up to the full fuzzy regression discontinuity method for estimating the CACE which comprises both an instrumental variable approach and a regression discontinuity, it is helpful to start by first introducing the instrumental variable approach to estimating the CACE in a simpler scenario, after which I will describe how utilizing the regression discontinuity modifies the problem.\nFor the moment, imagine if instead of a regression discontinuity, the encouragement to vote in 2008, Z, was a nonpartisan mailer encouraging voting. Further, voters were randomized into a control and treatment group, with some voters receiving the piece, while the other received either nothing or a placebo mailer. This is a classical randomized experiment, and if assumptions of SUTVA and ignorability hold, this allows us to estimate an average treatment effect. To allow us to estimate the effect of voting in 2008 (D) on voting in 2012 (Y) using this simple random experiment in the first election as an instrument, however, we need 3 additional assumptions. First, we must assume that any effect of the experimental encouragement on Y operates only through D, referred to as excludability. This would be violated if the nonpartisan mailer in 2008 had such a profound effect on a voter that it still influenced the voter 4 years later, in 2012. However, as most campaign interventions exhibit relatively quick decay in effectiveness (Gerber & Green, 2012), it is reasonable to make this assumption.\nSecond, we need to assume that the treatment is effective, and convinces some subjects who would have not have voted otherwise to vote. In other words, the experiment needs to generate at least some compliers, and more would improve our ability to estimate the CACE. Multiple studies have shown that such mail pieces are effective (Gerber & Green, 2012), although the effect sizes are relatively weak.\nThird, we need to make an assumption that our experimental encouragement creates no defiers, called the monotonicity assumption. To define defiers, partition the subjects into 4 groups, based on their potential outcomes arising from receiving the mail piece or not. “Always Takers” vote whether they receive the mailer or not: both potential outcomes have them voting. “Never Takers” are the reverse: they never vote regardless of the mailer. Compliers, as already discussed, are those who vote if they receive the encouragement, but don’t otherwise. Defiers invert the mailer’s intended influence, and vote only if not encouraged, refusing to vote if they are sent the mail piece. While one can easily imagine a registered voter frustrated with inundation of political mail, it seems unlikely that a single mail piece would entirely invert a subject’s attitudes towards voting.\nGiven this strong set of assumptions, it is finally possible to estimate the effect of D on Y, through two stage least squares. First, regressing D on Z gives E[D \\vert Z]=\\alpha_{0}+\\alpha_{1} Z+\\varepsilon. Then, using the predictions from the first stage, regressing Y on D gives E[Y \\vert D]=\\beta_{0}+\\tau D+\\varepsilon, with tau being the instrumental variable estimate of voting in 2008’s effect on voting in 2012 for the compliers. Intuitively, we are utilizing the variation in 2008 turnout induced by the random experiment to isolate downstream variation in turnout for the compliers. This can be extended to include further covariates, however we will wait until the full fuzzy RD design to illustrate this.\nIn the full fuzzy regression discontinuity design, instead of the treatment being a randomly assigned treatment, Coppock and Green leverage the variability created by the fact that around election day, some subjects were just slightly too young or just old enough to vote. Informally, we replace the random assignment of an experiment in the upstream year with the as-if-random assignment of young voters near the age threshold to either ineligibility or eligibility to vote in 2008.\nMore formally, instead of the type of ignorability assumption of the random experiment, we require ignorability conditional on covariates within some bandwith of the eligibility cutoff, Y(1), Y(0) \\perp Z \\vert x, x \\in(C-a, C+a). Also, we require that the eligibility cutoff and birthdates be determined independently of one another.\nFinally, we must be able to model the two outcomes Y(1) \\vert x, Y(0)\\vert x accurately in this region. One major challenge in modeling these are questions of how much of the data to use for estimation. While we can only estimate a causal effect right at the threshold (the “jump” created by being too young or just old enough), estimating the effects at that point can use different widths of data around the disconintuity.\nLeaving further discussion of whether these assumptions are reasonable with Coppock and Green’s data to the next section, we can use a similar set of 2SLS regressions to estimate:\n\\begin{aligned}\n&D=\\alpha_{0}+\\alpha_{1} Z+\\alpha_{2} T+\\alpha_{3} T * Z+\\alpha_{4} \\text {Lagged}+\\varepsilon\\\\\n&Y=\\beta_{0}+\\beta_{1} D+\\beta_{2} T+\\beta_{3} T * D+\\beta_{4} \\text {Lagged}+\\varepsilon\n\\end{aligned}\nWith our fuzzy regression discontinuity estimate of the CACE as \\beta_{1}, which is calculated separately for each state. T is centered at 0, removing the need to complicate this definition by including the interaction term.\nWhat properties does this CACE have? Given that is calculated using only the compliers not the full sample, its standard errors are much larger than the ITT ones we might expect from a simple random experiment, if we could run one in this scenario. The CACE also encompasses the full causal process that is set in motion by the upstream election – even effects resulting from voting in intermediate elections, or possible additional campaign outreach due to voting in 2008, although we cannot tease such effects apart. Despite these problems, the CACE from this fuzzy regression discontinuity has the potential to have much smaller standard errors than leveraging an upstream randomized experiment, given that the quasi-treatment is applied across the entire voter file, not just who an experiment would target.\nAs an extension to the methods used by Coppock and Green, I also consider bandwith selection algorithms for the regression discontinuity, the results of which I will discuss with the diagnostics. These tools attempt to find a bandwith that is optimal to some criterion, seeking to reduce the researcher degrees of freedom available in setting many possible bandwiths. For example, one such metric would be finding the bandwith that minimizes an approximation to the mean squared error (MSE) in estimating tau (Imbens and Kalyanaraman, 2009)."
  },
  {
    "objectID": "posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html#assumptions",
    "href": "posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html#assumptions",
    "title": "Is Voting Habit Forming? Replication, and additional robustness checks",
    "section": "Assumptions",
    "text": "Assumptions\nHaving described the extensive assumptions needed above in order to explain the fuzzy regression discontinuity model, I now evaluate the plausibility of these assumptions for Coppock and Green's case.\nExclusion: Is there any path other than through D through which the \"treatment\" of being just eligible in 2008 could affect propensity to vote in 2012? Absent a history of actually participating, it is unlikely that someone who becomes eligible slightly earlier would be more likely to be targeted by a campaign to turn out. We can, however, imagine a subject who knew they would be eligible paying more attention in 2008 than someone who knew they would not be, which could spark more interest by the 2012 election. Similarly, a subject who missed voting in their first presidential election but was eligible might be more motivated in 2012 for a fear of missing out again (\"I missed voting for Obama, but won't do so again\"). With all such paths that imagine a more engaged citizen for their first presidential election, however, one has to imagine that most such citizens would want to put that energy towards voting in 2008. Thus, while we have no way to rule out such backdoor paths from being just eligible in 2008 to voting in 2012, we have to imagine such cases would be relatively rare.\nEffectiveness of the Instrument: While just being eligible alone likely has a relatively weak effect on 2008 turnout, given that we are working at the scale of full state voter files, we can be confident we have generated a reasonable number of compliers to work with out of over five hundred thousand ballots cast by the birthdate cohorts studied.\nMonotonicity: A defier in Coppock's and Green's design would be someone who votes despite not quite being old enough, or vice versa. Given that this constitutes a felony, and a hard to commit one with little benefit, we can be extremely confident in this assumption. Alternatively, someone could choose to not vote only if just eligible, which again implies an extremely unlikely stance towards election law.\nIgnorability: Here, we need to be clear that we mean ignorability within some cutoff of being just eligible, given our covariates: Y(1), Y(0) \\perp Z \\vert x, x \\in(C-a, C+a). It seems plausible that being a month above or below 18 on election day shouldn't create any other major changes in a 17 or 18 year old's potential outcomes. However, as we get farther from the eligibility threshold it becomes more plausible that the groups could diverge through, for example, differences in maturity or differences in education due to birthdate.\nCutoff and forcing variable determined independently: It seems unlikely that either Election Day or someone's recorded birthday could be moved to help a subject vote earlier. Federal Election Day has been fixed in the United States since 1945. Further, it seems exceedingly unlikely that a parent or hospital administrator would attempt to modify a birth certificate simply to allow their child to vote 1 year earlier.\nEstimation: As we will see in plots below, there seems to be almost no non-linear trend in Y above or below the cutoff, simplifying modeling E[Y\\vert X]. Also, while there might be some concern with day or the week, seasonal, or other temporal trends in the number of dates on a given birthdate and thus the number of expected votes there, including the year lagged variable should arguably be sufficient to model them, provided any trends aren't particularly unique to 1990 births. While we'd ideally like more covariates than are available in the flattened data presented here, what we have seems sufficient to make at least a reasonable estimate.\nSUTVA: Lastly, there is little reason to believe that the encouragement to vote provided to one subject by turning 18 close to election day could influence another subject. While we can imagine students influencing each other's politics, it seems implausible that a friend turning 18 close to election day alone could change a student's propensity to vote meaningfully."
  },
  {
    "objectID": "posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html#diagnostics",
    "href": "posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html#diagnostics",
    "title": "Is Voting Habit Forming? Replication, and additional robustness checks",
    "section": "Diagnostics",
    "text": "Diagnostics\nGiven that my greatest concern with Coppock and Green’s design is their large choice of bandwith at 365 days around 2008’s election day, I first briefly describe results of other diagnostics I replicated before focusing attention on a variety of ways to evaluate the bandwidth’s effect on the CACE. This emphasizes my work beyond the replication, most of which isn’t shown here- after the following two paragraphs and first table, diagnostics in this section are my extensions of Coppock and Green’s work.\nFirst, being just eligible appears to have an effect on voting 2008, as checked through running just the first stage of a 2 stage least squares model, so our instrument generates compliers. There were no discontinuities in the year lagged vote counts around the 2008 eligibility cutoff, our covariate.\nI was able to successfully reproduce Coppock and Green’s robustness check across different order polynomials, shown below. Note, however, that these estimates are from meta-analyses across states; I develop a similar table by state and bandwith later. Both 1st and 2nd order polynomials returned similar results, while a 3rd order polynomial estimates became extremely sensitive to choice of bandwith. As Gelman and Imbens (2014) discuss however, cubic polynomials are problematic more generally in regression discontinuity designs, so this result was expected. Robust SE's are in parentheses below their corresponding estimate. The primary estimates presented by CG in the main paper are bolded.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n90\n180\n270\n365\n455\n545\n635\n730\n\n\n\n\nDifference-in-Means\n0.108\n0.118\n0.124\n0.119\n0.118\n0.117\n0.11\n0.103\n\n\n\n(0.01)\n(0.00)\n(0.00)\n(0.00)\n(0.00)\n(0.00)\n(0.00)\n(0.00)\n\n\nFirst-order Polynomial\n0.058\n0.089\n0.101\n0.117\n0.118\n0.121\n0.13\n0.136\n\n\n\n(0.01)\n(0.01)\n(0.01)\n(0.01)\n(0.00)\n(0.00)\n(0.00)\n(0.00)\n\n\nSecond-order Polynomial\n0.016\n0.055\n0.074\n0.083\n0.102\n0.104\n0.099\n0.103\n\n\n\n(0.02)\n(0.01)\n(0.01)\n(0.01)\n(0.01)\n(0.01)\n(0.01)\n(0.01)\n\n\nThird-order Polynomial\n0.012\n0.035\n0.05\n0.056\n0.06\n0.086\n0.095\n0.097\n\n\n\n(0.02)\n(0.02)\n(0.01)\n(0.01)\n(0.01)\n(0.01)\n(0.01)\n(0.01)\n\n\n\nNext, somewhat surprisingly, Coppock and Green never actually plot the discontinuity they work with either in the main paper or appendix. The plot of the discontinuity in Figure 1 below establishes several informative points. First, there is a noticeable dip in total votes cast in 2012 by cohort upon being slightly too young to vote in 2008, a good first validation of the design. Second, the trend in the data both to the left and right of the discontinuity appears to be extremely linear, which may influence my later bandwith selection algorithm findings. Finally, even in the binned plot the wide range of votes cast by birthdate state cohorts is obvious. Upon further inspection much of this variation is driven by state, which motivates my next analysis, seeing how each state's CACE changes as the cutoff varies.\n\n\n\nDiscontinuity Plot\n\n\nNote that the full 10,000+ birthdate cohorts are binned with their average plotted to make the graph legible. Lines plotted are first order polynomials.\nWhile Coppock and Green present results at their final chosen bandwith by state, they conduct robustness checks only at the level of meta-analysis pooling across states. Given that some of the variation in vote total also seems to be driven by state, below are CACE estimates by state, by a variety of bandwiths. These are considerably more sensitive to the bandwith choice than the above table that pools across states, as we’d expect given the disaggregation. For example, the average across the estimates using 30-day bandwith (.01) is closer to 0 than the .1 resulting from 365 bandwith concreated on in the main paper. This somewhat lowers my confidence in the by-state estimates as Coppock and Green present them. Their claim that their results are insensitive to bandwith variation only really appear to hold when estimates are aggregated through meta-analysis, as in the earlier table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n365\n\n180\n\n90\n\n60\n\n30\n\n\n\n\n\nAR\n0.20\n(0.03)\n0.15\n(0.04)\n0.14\n(0.07)\n0.01\n(0.10)\n0.10\n(0.16)\n\n\nCT\n0.16\n(0.02)\n0.14\n(0.02)\n0.14\n(0.03)\n0.11\n(0.03)\n0.11\n(0.05)\n\n\nIA\n0.08\n(0.04)\n0.03\n(0.05)\n0.03\n(0.08)\n-0.05\n(0.11)\n-0.03\n(0.17)\n\n\nIL\n0.08\n(0.01)\n0.05\n(0.02)\n0.02\n(0.02)\n0.01\n(0.03)\n0.04\n(0.04)\n\n\nFL\n0.10\n(0.01)\n0.09\n(0.02)\n0.06\n(0.03)\n0.03\n(0.04)\n-0.01\n(0.05)\n\n\nKY\n0.08\n(0.02)\n0.08\n(0.03)\n0.05\n(0.04)\n0.05\n(0.05)\n0.08\n(0.07)\n\n\nMO\n0.16\n(0.02)\n0.15\n(0.03)\n0.14\n(0.04)\n0.13\n(0.06)\n0.11\n(0.08)\n\n\nMT\n0.11\n(0.03)\n0.08\n(0.04)\n0.03\n(0.07)\n-0.01\n(0.09)\n-0.08\n(0.12)\n\n\nNJ\n0.15\n(0.01)\n0.12\n(0.02)\n0.09\n(0.03)\n0.03\n(0.03)\n0.01\n(0.05)\n\n\nNV\n0.17\n(0.03)\n0.14\n(0.04)\n0.09\n(0.06)\n0.00\n(0.08)\n-0.01\n(0.11)\n\n\nNY\n0.07\n(0.01)\n0.02\n(0.02)\n-0.03\n(0.03)\n-0.06\n(0.03)\n-0.03\n(0.05)\n\n\nOK\n0.14\n(0.02)\n0.11\n(0.03)\n0.09\n(0.06)\n0.01\n(0.08)\n0.04\n(0.15)\n\n\nOR\n0.11\n(0.02)\n0.07\n(0.04)\n0.06\n(0.06)\n0.01\n(0.08)\n-0.07\n(0.12)\n\n\nPA\n0.12\n(0.02)\n0.08\n(0.03)\n0.01\n(0.04)\n-0.05\n(0.05)\n-0.04\n(0.07)\n\n\nRI\n0.11\n(0.03)\n0.14\n(0.05)\n0.05\n(0.08)\n0.03\n(0.10)\n-0.15\n(0.16)\n\n\n\nBandwith Sensitivity for State estimates. Robust Standard errors are to the right of their respective estimate in parentheses.\nAs a final extension of the robustness to different bandwith checks Coppock and Green consider in their appendix, I also explored a variety of algorithms for bandwith selection. Across different order polynomials, kernels, and metrics to optimize for, all results suggested using the full 365-day bandwith. In retrospect, looking at the quite linear trend in the data, this makes some sense: given little difficult variation to model, choosing to include all the data could easily be the most effective strategy to reduce metrics such as the MSE in estimating \\tau_{D}."
  },
  {
    "objectID": "posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html#results-and-interpretation",
    "href": "posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html#results-and-interpretation",
    "title": "Is Voting Habit Forming? Replication, and additional robustness checks",
    "section": "Results and Interpretation",
    "text": "Results and Interpretation\nWhile I was able to reproduce Coppock and Green’s results, I differ slightly in how I choose to interpret them. Their identification strategy overall makes sense. At worst, it seems possible the exclusion assumption could be subject to minor violations, but the frequency of such events would be low. Unlike Coppock and Green, however, I don’t think its fair to say that the results are completely robust to bandwith selection or result from habit alone.\nWhile their meta-analysis level results are insensitive to variations in the order of polynomial used or bandwith both in their appendix and my replication, allowing the bandwith to vary by state revealed significant additional variability. Given that by state estimates are a core part of their results, I would argue it is best to present something closer to my bandwith by state table than the singular estimates resulting from a bandwith of 365. If one accepts their bandwith, either on its own or as a result of bandwith selection algorithms, it appears that the overall CACE across states is around .1, establishing that there does appear to be some causal effect on 2012 participation from 2008 participation for compliers. If a reader prefers a much narrower bandwith, this overall shrinks to roughly .06 for a 90 day bandwith, or close to 0 for a 30 day one.\nA second concern is that Coppock and Green argue that these effects result mostly from habit, not campaigns in 2012 choosing to target those who voted in 2008. This would not represent a violation of exclusion, as campaign effects represent a valid path from D to Y, not Z to Y. Thus, this is a disagreement around causes of effects. To argue that habit, not campaigns, cause the CACEs they find, Coppock and Green note that battleground states do not have significantly higher estimates than non-battleground states. That is, if the cause was campaign activity, we would expect higher CACEs in battleground states in presidential years like 2012. While this would be somewhat convincing as an argument, the ability to tease out the pattern they describe is dependent on a large bandwith to generate small enough standard errors to tell states apart. Beyond this, during campaign years, even in non-battleground states, most people with some voting history can expect campaign outreach. Further, in non-battleground states, this contact sometimes prioritizes young voters and others whose turnout is more ambiguous. Overall, there does appear to be some evidence that the effects Coppock and Green find aren’t mainly due to campaign effects, however this isn’t sufficient to fully support their claim that most of what they find is habit alone.\nTo provide an interpretation of the individual CACEs, consider the Florida estimate at a bandwith of 90 days on either side of the 2008 election. If our assumptions hold, the CACE of .06 suggests that for compliers, participation in the 2008 election caused a 6% increase in total 2012 votes cast in Florida, above the total votes cast for those compliers who did not participate in 2008."
  },
  {
    "objectID": "posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html#discussion",
    "href": "posts/Is Voting Habit Forming/2020-03-09-Voting-Habit-FRD.html#discussion",
    "title": "Is Voting Habit Forming? Replication, and additional robustness checks",
    "section": "Discussion",
    "text": "Discussion\nOverall, reproducing this paper convinced me that Coppock and Green have a strong method for estimating the causal effect of voting in the 2008 election on voting in the 2012 election for compliers. However, these estimates are sensitive to bandwith variation at the state level, with effects shrinking towards zero as the bandwith becomes tighter. Also, it is unclear if their results are due to habit alone, although the estimates do support this somewhat. Thus, I think a narrower interpretation of the 2008-2012 election pair analysis is that there is some suggestion of an effect of voting in 2008 on 2012 votes cast for compliers, provided you accept a large bandwith, and the effect is most likely not due to campaign effects.\nOf course, replicating such a colossal paper and set of analyses has many limitations. First, I restricted my analysis and robustness checks to the 2008-2012 election pair, in order to replicate the pair the original authors used in their robustness checks. I also didn’t study or replicate their instrumental variables CACE estimates resulting from using older randomized experiments as instruments. Finally, given the flattened form of the data provided, I was unable to provide fully satisfying descriptive profiles of the birthdate-state cohorts, for example about their demographic composition.\nFuture work could consider replicating the rest of Coppock and Green’s study with additional robustness checks, or conducting the same analysis using recent advances in voter file linking. For example, given that midterm elections have much lower turnout, and thus fewer compliers in a fuzzy regression discontinuity design, it would be interesting to see if an election pair like 2006-2010 exhibits even higher sensitivity to variation in bandwith. In the past few years, companies such as Catalist and Movement Cooperative have also improved the quality of data available in voter files. These provide a number of advantages that could be helpful for a future study like Coppock and Green’s, including better cleaning of the underlying data, and richer information on the individual voters, either through integration of additional data sources, or through modeling. Coppock and Green’s analysis takes an incredible step towards identifying whether voting is habit forming, but further analysis is needed to show that their results are deeply robust to electoral context and modeling choices. Some of these steps will likely form my project for my Advanced Causal Inference course this semester, although I’m still deciding which points are the most interesting to look at."
  },
  {
    "objectID": "posts/Maximum Entropy Normal/2018-05-11-maxentropynormal.html",
    "href": "posts/Maximum Entropy Normal/2018-05-11-maxentropynormal.html",
    "title": "Why the normal distribution?",
    "section": "",
    "text": "When I was first studying probability theory as an undergrad, I had a bit of a conceptual hang-up with the Central Limit Theorem. Simulating it in R gave a nice visual of how each additional random variable smoothed out some of the original distribution’s individuality, and asymptotically we were left with a more generic shape. The proofs were relatively straightforward. One part, however, didn’t really make sense to me. My problem was this: Of all the many possible distributions, why is the normal distribution in particular that our i.i.d random variables converge to in distribution?\nThe normal distribution has lots of interesting properties that I looked at to gain some intuition, but many of the them seem to follow from the CLT, rather than explaining it. In fact, it wasn’t until a later course in machine learning that integrated some information theory that I found a satisfactory answer to my question.\nIn this post, I’ll explain an insight from the principle of maximum entropy that conceptually justifies the normal distribution’s role in the CLT. To do so, we’ll first build up a basic introduction to entropy, how probability and entropy interact, and then explain the entropy property of the normal distribution that helped me understand the question above. For this post, all you’ll really need is a rough idea of what a random variable is, and some familiarity with common probability distributions; we’ll build up the rest from scratch."
  },
  {
    "objectID": "posts/Maximum Entropy Normal/2018-05-11-maxentropynormal.html#entropy",
    "href": "posts/Maximum Entropy Normal/2018-05-11-maxentropynormal.html#entropy",
    "title": "Why the normal distribution?",
    "section": "Entropy",
    "text": "Entropy\nImagine two situations in which you’ve lost your keys at home. In the first scenario, you’re well and truly confused as to where they are. As far as you’re concerned, any location within your home is equally likely. In the second scenario, you remember having them by your kitchen counter, so you’re pretty sure they’re there, or at least somewhere near there. Intuitively, the knowledge that they’re probably in your kitchen is much, much more informative that the idea that they’re equally likely to be anywhere in your house. In fact, once you accept the constraint that they’re within the bounds of your house, it’s pretty hard to imagine a more useless, uninformative statement than “they’re equally likely to be anywhere”.\nInformation theory helps makes rigorous many of our informal ideas about how much information or uncertainty is contained in situations like the above. Let’s start then, by defining entropy, a measure of the uncertainty of a random variable:\nH(X) = - \\int_{I} p(x) \\log{p(x)} dx\nWhere p(x) is our continuous probability distribution over some interval I. It can be analogously defined for the discrete case by replacing the integral with a summation. To stress the intuition here, the higher the entropy, the less we know about the value of the random variable. As an example, a uniform distribution over the bounds [a,b] is analogous to our “keys could be anywhere” scenario- the entropy of our variable is high. In fact, once you accept the constraint that the distribution is supported on [a,b], one can prove that the uniform distribution is the maximum entropy distribution among all continuous distributions on that interval. We won’t include the proof here, but showing a similar maximum entropy property about the normal distribution is where we’re headed.\nBefore we get back to the normal distribution and CLT though, let’s think a little bit more about the concept of maximum entropy. Why would we want to use something specifically because it’s minimally informative? Let’s think about how we’d express or model our belief that our keys are equally likely to be anywhere. If we’re truly of the belief that they’re equally likely to be anywhere in our house, then some sort of model based on a uniform distribution over the space likely makes sense. If they’re equally likely to be anywhere, placing high probability in a specific room would be wrong, and making that additional assumption would probably slow down our search. This is Jaynes’ principle of maximum entropy- we should use the distribution with the highest entropy, subject to any prior constraints we already have.\nThis principle, then, is about epistemic modesty. We can want to choose the distribution that meets our constraints, and assumes as little additional information as possible."
  },
  {
    "objectID": "posts/Maximum Entropy Normal/2018-05-11-maxentropynormal.html#the-normal-distribution",
    "href": "posts/Maximum Entropy Normal/2018-05-11-maxentropynormal.html#the-normal-distribution",
    "title": "Why the normal distribution?",
    "section": "The Normal Distribution",
    "text": "The Normal Distribution\nOnce you specify a variance \\sigma^{2}, and that the distribution be supported on the reals from (- \\infty, \\infty), the normal distribution is the maximum entropy distribution! Thomas Lumley calls this a “a precise characterization of the normal’s boringness”. In our mental image of the CLT at work then, we’re approaching the normal because of it’s generic-ness, it’s lack of information. If we were mixing colors, the normal would be a nondescript grey.\nIn one sense, this result may be counter intuitive. As statisticians, when we find out that a distribution we’re working with is roughly normal, we tend to feel like we have a lot to work with. Many of our favorite tools like maximum likelihood will work well, and we have straightforward ways to estimate most quantities of interest. However, this result illustrates a subtle point: ease of use and information content aren’t the same thing.\nIf you found this use of information theory improved your intuition for probability and want more, here are some suggestions for further reading-"
  },
  {
    "objectID": "posts/Maximum Entropy Normal/2018-05-11-maxentropynormal.html#resources",
    "href": "posts/Maximum Entropy Normal/2018-05-11-maxentropynormal.html#resources",
    "title": "Why the normal distribution?",
    "section": "Resources:",
    "text": "Resources:\n\nIf you’re wondering how we actually prove that the normal is the maximum entropy distribution for a specified variance, there’s a self-contained proof in section 19.4.2 of Deep Learning. I excluded it in this post because it required a lot of extra math, and didn’t add much to the intuitive point I was trying to show.\nChris Olah’s Visual Information Theory is a great introduction to the information compression and distribution comparison parts of information theory."
  },
  {
    "objectID": "posts/Race Models Pt 1/2018-04-10-race-models-pt1.html",
    "href": "posts/Race Models Pt 1/2018-04-10-race-models-pt1.html",
    "title": "Andy Timm",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{timm,\n  author = {Andy Timm},\n  url = {https://andytimm.github.io/2018-04-10-race-models-pt1.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndy Timm. n.d. https://andytimm.github.io/2018-04-10-race-models-pt1.html."
  },
  {
    "objectID": "posts/Race Models Pt 1/race_models_part1.html",
    "href": "posts/Race Models Pt 1/race_models_part1.html",
    "title": "Predicting race part 1- Bayes’ rule method and extensions",
    "section": "",
    "text": "Race is a defining part of political identity in the United States, and so it should be no surprise that accurately modeling race can be beneficial for many political campaign activities. For instance, many organizations work to improve turnout in specific communities of color, or want to target persuasion on a given issue to certain racial group. Alternatively, race and ethnicity might be desired as an input to a larger voting or support likelihood model, given that race is generally predictive of both voting likelihood and candidate support.\nUnfortunately, complete self-reported race data is only available in 7 states, where it is required by law: Alabama, Florida, Georgia, Louisiana, Mississippi, North Carolina, and South Carolina. Pennsylvania and Tennessee also have an optional field on voter registration forms. Outside of these states, race and ethnicity need to be collected individually, or modeled. These models commonly take advantage of the name (especially surname) of the individual voter, and other information in the voter file to do so.\nIn this post, I’ll explore a Bayes’ rule based method for modeling racial identity, and how it can be extended with additional information from state voter files where available. Obviously, it won’t be as predictive as something like Catalist or Civis Analytics’ ones (which have the benefit of huges swathes of additional survey and other data, not to mention more sophisticated modeling), but it can help illustrate the benfits and limitations of such tools. In the next posts, I’ll explain how natural language processing models can achieve still higher accuracy by extracting more information from names themselves. Since the Florida voter file has self-reported race data and is easy to access, we’ll test our models’ effectiveness against that ground truth."
  },
  {
    "objectID": "posts/Race Models Pt 1/race_models_part1.html#a-simple-first-method",
    "href": "posts/Race Models Pt 1/race_models_part1.html#a-simple-first-method",
    "title": "Predicting race part 1- Bayes’ rule method and extensions",
    "section": "A simple first method",
    "text": "A simple first method\nThe census surname files are a an incredible source of information on how race correlates with names, and are the starting point of our model. In these files, the census provides race percentage breakdowns for any surname with more than 100 occurrences in the United States, except where redacted for privacy reasons. The data is available only aggregated at the national level. In practice, this means coverage of about 90% of the population.\nFor many voters, surname is the only information we need to make an accurate classification: names such as Carlson, Meyer, and Hanson are in excess of 90% white, with similar surnames existing for other races. Unfortunately, many names aren’t as clear cut, and some names like Chavis are less than 40% likely to be any race. This is a good starting point, but we can do better.\nAs our first improvement, Elliot et al. (2009) incorporates census geolocation (county, tract, or block) data, using Bayes’ theorem.\nBayes’ theorem is a natural way to integrate the new census evidence we have, updating our initial beliefs with more data. P(A) and P(B) are the probabilities of events A and B occurring independently of each other, and P(A \\vert B) and P(B \\vert A) are conditional probabilities, such as the probability a given voter is white given their surname, or P(white \\vert surname). Bayes’ theorem appears in it’s simplest form below, giving us an updated probability utilizing the new information:\n P(A|B) = \\frac{P(B|A) P(A)}{P(B)} \nOf course, we’re interested in integrating multiple pieces of evidence (surname/geolocation for now, with more coming later), and applying it to multiple voters. The equations will be much more involved, but remember that we’re essentially just extending the above equation to work with multiple inputs, over multiple voters. Note that I’ll be using the notation from Imai and Khanna (2016), which is somewhat clearer than the notion from Elliot et al.\nWe want P(R_i = r \\vert S_i = s, G_i = g), the conditional probability that the voter i belongs to the racial group r given their surname s and geolocation g. R, S, and G are the sets of all racial groups, all surnames, and all geolocations respectively. Thus, as a final expression we’ll get:\nP(R_i = r \\vert S_i = s, G_i = g) = \\frac{P(G_i = g \\vert R_i = r) P(R_i = r|S_i = s)}{\\sum_{r'\\in R} P(G_i = g|R_i = r) P(R_i = r|S_i = s) }\nWe already have almost all these probabilities between the census surname list and census demographic data. P(R_i = r \\vert S_i = s) is the racial composition of surnames from the surname list. But what about P(G_i = g \\vert R_i = r)? As an intermediate step, we first need to calculate P(G_i = g \\vert R_i = r) , which we can calculate using Bayes’ rule again. P(G_i = g \\vert R_i = r) is then \\frac{P(R_i = r \\vert G_i = g) P(G_i = g)} {\\sum_{r'\\in R} P(R_i = r \\vert G_i = g') P(G_i = g)}, completing everything we need to produce our second model.\nThis model, like the surname list, produces probabilistic predictions of race, for instance, a given voter is 94.3% likely to be white, given their name and geolocation. The probabilities sum to 1 across the races.\nWe’ll get to how to use and evaluate such models soon, but first: given that we’ve started to include information from the voter file to improve our predictions, why don’t we use other fields we have such as age, party registration, and gender? They all are likely to contain some conditional information about a voter’s race, and while party registration isn’t reported in every state, it is in Florida.\nThat’s exactly the proposal of Imai and Khanna (2016), and it works well. The equations get slightly more complicated, but extending Bayes’ rule to include more and more variables doesn’t change all that much. We have to calculate more intermediate probabilities, but the essential process and reasoning of incorporating new information to update our belief is the same. If you want to see the full model written out, with space for an arbitrary number of new variables X_i, you can find it in the linked paper."
  },
  {
    "objectID": "posts/Race Models Pt 1/race_models_part1.html#use-and-evaluation",
    "href": "posts/Race Models Pt 1/race_models_part1.html#use-and-evaluation",
    "title": "Predicting race part 1- Bayes’ rule method and extensions",
    "section": "Use and Evaluation",
    "text": "Use and Evaluation\nNow that we have a probability distribution over the racial groups, how do we utilize them?\nWe might take the highest probability race and use that as our prediction. Alternatively, as an input in a later model, we might choose to simply incorporate all 5 probabilities, allowing our following model as much information as possible about the racial identity of a voter. Catalist, the democratic data vendor, turns probabilities into simple categories such as “likely white” or “possibly black”, which simplify working with the results on a campaign.\nAs a final idea, we might have an application in mind where you want to only predict a certain race when you’re very confident in your prediction. For example, you might be hoping to target a turnout mailer written in Spanish to only Hispanics, and as few non-Hispanics as possible. To do this, we’d utilize only high probability predictions- say, above 85% likely to be Hispanic. By changing that threshold, you could optimize the size of your mail universe versus the specificity and efficiency of it, finding the best balance for your campaign.\nAs our last example showed, when utilizing such probabilistic predictions, there is naturally an accuracy tradeoff involved in selecting what type of threshold to use. We could misleadingly say our model is extremely accurate if we only use a high threshold, but a fairer, more systematic evaluation would require looking at how it preforms over multiple such thresholds. That’s what we’ll develop next: Area Under the Curve (AUC), a systematic method for evaluating classifiers.\nThere are 4 possible outcomes to making a classification: a True Positive (TP), False Positive (FP), True negative (TN), and False Negative (FN). As an example then, a true positive is when we predicted positive, and the ground truth label was actually positive.\nRather than building a table of these 4 outcomes, called a confusion matrix, we’ll be working with summary statistics of these outcomes. The True Positive Rate (or sensitivity or recall) is \\frac{TP}{TP+FN}. In other words, out of all the points that are (ground truth) positive, how many did we correctly classify? Similarly, the False Positive Rate is \\frac{FP}{FP+TN}. High True Positive Rate is good: it means we’ll miss relatively few positive examples. On the other end of things, low False Positive Rate is what we want: it means fewer negative points will be misclassified.\nBy calculating these two statistics at a variety of thresholds, then plotting a curve with the FPR on the x-axis and TPR on the y-axis, we can get a deeper understanding of the tradeoff. This is called an Receiver Operating Characteristic. Given what I’ve said about the meaning of the TPR and FPR, can you figure out quality of the 3 classifiers that are graphed below?\n The first is a perfect classifier: at all tradeoff points, it has a TPR of 1, and FPR of 0. The second is pretty good: at most tradeoff points it does well. The third, a straight line from (0, 0) to (1,1) is a random classifier: it’s equivalent to guessing.\nAs an overall summary then, the area under this curve (AUC) is of our one number summary of these graphs. The shown classifiers have AUC 1, .8, and .5 respectively."
  },
  {
    "objectID": "posts/Race Models Pt 1/race_models_part1.html#results",
    "href": "posts/Race Models Pt 1/race_models_part1.html#results",
    "title": "Predicting race part 1- Bayes’ rule method and extensions",
    "section": "Results",
    "text": "Results\nNow that we’ve built up a relatively complex model, and learned how to use and evaluate it, let’s plot some ROC curves, and look at AUCs for our models.\n\n\n\nROC graphs for 4 models\n\n\nAnd here’s the AUC table: bold is the highest overall for each race.\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\n\nSurname\n0.8414369\n0.8562679\n0.9325489\n0.8606716\n\n\nGeo/Surname\n0.8792853\n0.8918873\n0.9492531\n0.8717517\n\n\nKitchen Sink\n0.8903922\n0.8979003\n0.9362794\n0.7648270\n\n\n\nLooking at these, there are a lot of interesting patterns!\nWhite: The White models’ results are perhaps what we most expected. With more data, each subsequent model does better, and the overall result is strong.\nBlack: Interestingly, the kitchen sink model still does the best overall with black voters, but by a much smaller margin than with whites. Also, once the true positive rate gets to around .90, curves cross! This is a good illustration of the importance of plotting ROC curves when doing classification- depending on your campaign, you might correctly choose either the Geo/Surname model or the Kitchen Sink one, depending on what type of cutoff you need.\nHispanic: The Hispanic models are all very close together: with surname information being so effective in classifying Hispanics (more effective than any model for the other races), there isn’t much room for census or voter file data to improve things.\nAsian: These models are significantly weaker than all the others, but still reasonable. The unexpected trend though, is that the kitchen sink model preforms much worse than the other two. Given how few Asian voters there are overall in Florida, this downturn is probably explained by the relatively low density of any Asians across the other inputs, like age, sex, or party. Thus, while geographic information might slightly improve things, Floridians are so unlikely to be Asian overall that all of those extra variables don’t carry any useful information about who might be Asian.\nOverall, these Bayes’ Theorem models have a lot of attractive features. While the predictiveness of names, and what variables you have from the voter file might vary state to state, the models can used anywhere in the US. They’re also quite a strong baseline for accuracy as well- far, far better than random, even for Asian voters. Unlike models we’ll discuss in the next post, these models require no training data. Finally, they’re transparent- if you want to check how surname, geolocation, and party weighed in to a particular decision, it’s only a few calculations with Bayes’ rule away.\nOf course, we’d love higher accuracy, and we can get there with natural language processing. What about those ~10% of voters whose name aren’t in the census surname file? If a name starts with “Mc”, but isn’t in the census surname list, I personally would guess they’re of Irish descent, and therefore white. And what about using first names, middle names, and name suffixes? It’s linguistic patterns like these that we’ll hope to exploit with NLP, in the next post.\nYou can find the code used to write this post here."
  },
  {
    "objectID": "posts/Variational MRP Pt1/variational_mrp_pt1.html",
    "href": "posts/Variational MRP Pt1/variational_mrp_pt1.html",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "",
    "text": "This post introduces a series I intend to write, exploring using Variational Inference to massively speed up running complex survey estimation models like variants of Multilevel Regression and Poststratification while aiming to keep approximation error from completely ruining the model.\nThe rough plan for the series is as follows:"
  },
  {
    "objectID": "posts/Variational MRP Pt1/variational_mrp_pt1.html#introducing-mrp",
    "href": "posts/Variational MRP Pt1/variational_mrp_pt1.html#introducing-mrp",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "Introducing MRP",
    "text": "Introducing MRP\nWhile I’m mostly focused on the way we choose to actually fit a given model with this series, here’s a super quick review of the intuition in building a MRP model. If you want a more complete introduction, Kastellec’s MRP Primer is a great starting point, as are the case studies I link a bit later.\nMRP casts estimation of a population quantity of interest \\theta as a prediction problem. That is, instead of the more traditional approach of building simple raked weights and using weighted estimators, MRP leans more heavily on modeling and then poststratification to make the estimates representative.\nTo sketch out the steps-\n\nEither gather or run a survey or collection of surveys that collect both information on the outcome of interest, y, and a set of demographic and geographic predictors, \\left(X_{1}, X_{2}, X_{3}, \\ldots, X_{m}\\right).\nBuild a poststratification table, with population counts or estimated population counts N_{j} for each possible combination of the features gathered above. Each possible combination j is called a cell, one of J possible cells. For example, if we poststratified only on state, there would be J=51 (with DC) total cells; in practice, J is often several thousand.\nBuild a model, usually a Bayesian multilevel regression, to predict y using the demographic characteristic from the survey or set of surveys, estimating model parameters along the way.\nEstimate y for each cell in the poststratification table, using the model built on the sample.\nAggregate the cells to the population of interest, weighting by the N_{j}’s to obtain population level estimates: \\theta_{\\mathrm{POP}}=\\frac{\\sum_{j \\in J} N_{j} \\theta_{j}}{\\sum_{j \\in J} N_{J}}\n\nWhy would we want to do this over building more typical survey weights? To the extent your new model has desirable properties like the ability to incorporate priors, can partially pool to manage rare subpopulations where you don’t have a lot of sample, and so on, you can get the benefits of that more efficient model through MRP. Raking in its simplest form is really just a linear model; we have plenty of methods that can do better. Outside of bayesian multilevel models which are the most common, there’s an increasing literature on using a wide variety of machine learning algorithms like BART2 to do the estimation stage; Andrew Gelman calls this RRP."
  },
  {
    "objectID": "posts/Variational MRP Pt1/variational_mrp_pt1.html#introducing-the-running-example",
    "href": "posts/Variational MRP Pt1/variational_mrp_pt1.html#introducing-the-running-example",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "Introducing the Running Example",
    "text": "Introducing the Running Example\nRather than reinvent the wheel, I’ll follow the lead of the excellent Multilevel Regression and Poststratification Case Studies by Lopez-Martin, Philips, and Gelman, and model survey binary responses from the 2018 CCES for the following question:\n\nAllow employers to decline coverage of abortions in insurance plans (Support / Oppose)\n\nFrom the CCES, we get information on each participant’s state, age, gender, ethnicity, and education level. Supplementing this individual level data, we also include region flags for each state, and Republican vote share in the 2016 election- these state level predictors have been shown to be critical for getting strong MRP estimates by Lax and Philips (2009) and others. and If you’d like deeper detail on the dataset itself, I’d refer you to this part MRP case study.\nUsing these, we setup the model for Pr(y_i = 1) the probability of supporting allowing employers to decline coverage of abortions in insurance plans as:\n\n\\begin{aligned}\nPr(y_i = 1) =& logit^{-1}(\n\\gamma^0\n+ \\alpha_{\\rm s[i]}^{\\rm state}\n+ \\alpha_{\\rm a[i]}^{\\rm age}\n+ \\alpha_{\\rm r[i]}^{\\rm eth}\n+ \\alpha_{\\rm e[i]}^{\\rm educ}\n+ \\beta^{\\rm male} \\cdot {\\rm Male}_{\\rm i} \\\\\n&+ \\alpha_{\\rm g[i], r[i]}^{\\rm male.eth}\n+ \\alpha_{\\rm e[i], a[i]}^{\\rm educ.age}\n+ \\alpha_{\\rm e[i], r[i]}^{\\rm educ.eth}\n+ \\gamma^{\\rm south} \\cdot {\\rm South}_{\\rm s} \\\\\n&+ \\gamma^{\\rm northcentral} \\cdot {\\rm NorthCentral}_{\\rm s}\n+ \\gamma^{\\rm west} \\cdot {\\rm West}_{\\rm s}\n+ \\gamma^{\\rm repvote} \\cdot {\\rm RepVote}_{\\rm s})\n\\end{aligned}\n\nWhere we incorporate pretty much all of our predictors as varying intercepts to allow for pooling across demographic and geographic characteristics:\n\n\\alpha_{\\rm a}^{\\rm age}: The effect of subject i’s age on the probability of supporting the statement.\n\\alpha_{\\rm r}^{\\rm eth}: The effect of subject i’s ethnicity on the probability of supporting the statement.\n\\alpha_{\\rm e}^{\\rm educ}: The effect of subject i’s education on the probability of supporting the statement.\n\\alpha_{\\rm s}^{\\rm state}: The effect of subject i’s state on the probability of supporting the statement.\n\\beta^{\\rm male}: The average effect of being male on the probability of supporting abortion. Note that it doesn’t really make much sense to model a two category3 factor as a varying intercept.\n\\alpha_{\\rm e,r}^{\\rm male.eth}, \\alpha_{\\rm e,r}^{\\rm educ.age}, \\alpha_{\\rm e,r}^{\\rm educ.eth}: Are several reasonable guesses at important interactions for this question. We could add many more two way, or even some three way interactions here, but this is enough for my testing here.\n\\gamma^{\\rm south}, \\gamma^{\\rm northcentral}, \\gamma^{\\rm west},\\gamma^{\\rm repvote}: are the state level predictors which are not represented as varying intercepts. Following the case study, I use \\gamma’s for the state level coefficients, keeping \\beta’s for individual coefficients. Note that Northeast is the base region of the region factor here, so it doesn’t get it’s own coefficient.\n\nStepping back for a second, let’s describe the complexity of this model in more general terms. This certainly isn’t state of the art for MRP, and you could definitely add in things like a lot more interactions, some varying slopes, non-univariate prior and/or structured priors, or other elements to make this a more interesting model. That said, this is already clearly enough of a model to improve on simple raking in many cases, and it produces a nuanced enough posterior that we can feasibly imagine a bad approximation going all spherical cow shaped on us.\nWhy this dataset and this model for this series? The question we model itself isn’t super important- as long as we can expect some significant regional and demographic variation in the outcome we’ll be able to explore if VI smoothes away some posterior complexity that MCMC can capture. Drawing an example from the CCES is quite useful, as the 60k total sample is much larger than typical publicly available surveys, and so we can check behavior under larger N sizes. Practically, fitting this with rstanarm allows us to switch easily from a great MCMC implementation to a decent VI optimizer quickly for some early tests. Finally, the complexity and runtime of the model is a nice balance of being something that we can fit with MCMC in a not terrible amount of time for comparison’s sake, and something challenging enough that it should teach us something about VI’s ability to handle non-toy models of the world.\nFitting this4 with MCMC in rstanarm is as simple as:\n\n# Fit in stan_glmer\nfit <- stan_glmer(abortion ~ (1 | state) + (1 | eth) + (1 | educ) + male +\n                    (1 | male:eth) + (1 | educ:age) + (1 | educ:eth) +\n                    repvote + factor(region),\n  family = binomial(link = \"logit\"),\n  data = cces_df,\n  prior = normal(0, 1, autoscale = TRUE),\n  prior_covariance = decov(scale = 0.50),\n  adapt_delta = 0.99,\n  refresh = 0,\n  seed = 605)\n\nSince it isn’t relevant for the rest of my discussion here, I’ll summarize the model diagnostics here and say that this seems to be a pretty reasonable fit- no issues with divergences, and no issues with poor \\hat{r}’s. Worth quickly pointing out that we did have to tune adapt_delta a bit to get no divergences though- even before getting to fitting this with VI, a model like this requires some adjustments to fit correctly.\nWith a model like this on just a 5k sample, we can produce pretty solid state level predictions that have clearly benefited from being fit with a Bayesian multilevel model:\n\nWith a 5k sample, MRP lands much closer to the complete weighted survey than a 5k unweighted sample: neat. That’s certainly not a fully fair comparison, but it gives some intution around the promise of this approach.\nSomewhat less neat is that even a 5k sample here takes about 13 minutes to fit. How does this change as we fit on more and more of the data?\n\n\n\n\n\n\n\nSample Size\nRuntime\n\n\n\n\n5,000\n13 minutes\n\n\n10,000\n44 minutes\n\n\n60,000\n526 minutes (~8 hours!)\n\n\n\nAs the table above should illustrate, if you’re fitting a decently complex Bayesian model on even somewhat large N sizes, you’re pretty quickly going to cap out what you can reasonably fit in a acceptable amount of time. If you’re scaling N past the above example, or deepening the modeling complexity, you’ll pretty quickly feel effectively locked out of using these models in fast-paced environments.\nHopefully fitting my running example has helped for building intuition here. Even a reasonably complex Bayesian model can have some pretty desirable estimation properties. To make iterating on modelling choices faster, to scale our N or model complexity higher, or just to use a model like this day to day when time matters, we’d really like to scale these fitting times back. Can Variational Inference help?"
  },
  {
    "objectID": "posts/Variational MRP Pt1/variational_mrp_pt1.html#the-elbo",
    "href": "posts/Variational MRP Pt1/variational_mrp_pt1.html#the-elbo",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "The ELBO",
    "text": "The ELBO\nIf you haven’t seen it yet, this quick substitution should clarify a potential issue with VI as I’ve described it so far:\nq^*(z) = argmin_{q(z) \\in \\mathscr{Q}}(q(z)||\\frac{p(z,x)}{\\bf p(x)}) = \\mathbb{E}[logq(z)] - \\mathbb{E}[logp(z,x)] + {\\bf logp(x)} Without some new trick, all I’ve said so far is to approximate a thing I can’t analytically calculate (the posterior, specially the issue evidence piece of it), I’m going to calculate the distance between my approximation and… the thing I said has a component can’t calculate?\nFortunately, a clever solution exists here that makes this strategy possible. Instead of trying to minimize the above KL divergence, we can optimize the alternative objective:\n\\mathbb{E}[logp(z,x)] - \\mathbb{E}[logq(z)]\nThis is just the negative of the first two terms above, leaving aside the logp(x). Why can we treat maximizing this as minimizing the KL divergence? The logp(x) term is just a constant (with respect to q), so regardless of how we vary q, this will still be a valid alternative objective. We call this the Evidence Lower Bound (ELBO)6.\nIf it’s helpful for intuition, play around with this great interactive ELBO optimizer by Felix Köhler:\n\n\n\n\n\n\n\n\n\n\n\nLink to demonstration here; check out Felix’s Youtube explanation of the ELBO also!\n\n\n\n\n\nBy twiddling the knobs on \\mu and \\sigma for our approximating normal, we can get our surrogate distribution pretty close to the True Posterior (which we know for purposes of demonstration, so we can calculate the true KL, not just it’s ELBO component). No matter how we twiddle though, the evidence remains constant.\nFor further intuition- notice that we can only do this trick in one direction. The KL divergence isn’t symmetrical, and if we wanted to calculate the “reverse” KL, we couldn’t use this strategy as logq(x) would not be a constant. Even if we thought that optimizing other direction of KL might have desirable properties like emphasizing mass-seeking over mode-seeking behavior, that simply isn’t an option."
  },
  {
    "objectID": "posts/Variational MRP Pt2/Variational_MRP_pt2.html",
    "href": "posts/Variational MRP Pt2/Variational_MRP_pt2.html",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "",
    "text": "This is the second post in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.\nIn the last post, I laid out why such reformulating the Bayesian inference problem as optimization might be desirable, but previewed why this might be quite hard to find high quality approximations amenable to optimization. I then introduced our running example (predicting national/sub-national opinion on an abortion question from the CCES using MRP), and gave an initial introduction to a version of Variational Inference where we maximize the Evidence Lower Bound (ELBO) as an objective, and do so using a mean-field Gaussian approximation. We saw that with 60k examples, this took about 8 hours to fit with MCMC, but 144 seconds (!) with VI.\nIn this post, we’ll explore the shortcomings of this initial approximation, and take a first pass at trying to better with a more complex (full rank) variational approximation. The goal is to get a better feel for what failing models could look like, at least in this relatively simple case.\nThe rough plan for the series is as follows:"
  },
  {
    "objectID": "posts/Variational MRP Pt2/Variational_MRP_pt2.html#lowering-the-tolerance",
    "href": "posts/Variational MRP Pt2/Variational_MRP_pt2.html#lowering-the-tolerance",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "Lowering the tolerance",
    "text": "Lowering the tolerance\nSo we managed to structure our Bayesian inference problem as an optimization problem. Can’t we just optimize better? Maybe with more training the result will be less bad?\nthe tol_rel_obj parameter control’s the convergence tolerance on the relative norm of the objective. In other words, it controls what (change in the) Evidence Lower Bound value we consider accurate enough to stop at. The default is 0.01, which feels a bit opaque, but let’s try setting it way down to 1e-8 (1Mx lower). Then we can plot it alongside the MCMC estimates and original MF-VI attempt.\n\ntic()\nfit_60k_1e8 <- stan_glmer(abortion ~ (1 | state) + (1 | eth) + (1 | educ) +\n                                      male + (1 | male:eth) + (1 | educ:age) +\n                                      (1 | educ:eth) + repvote + factor(region),\n  family = binomial(link = \"logit\"),\n  data = cces_all_df,\n  prior = normal(0, 1, autoscale = TRUE),\n  prior_covariance = decov(scale = 0.50),\n  adapt_delta = 0.99,\n  # Printing the ELBO every 1k draws\n  refresh = 1000,\n  tol_rel_obj = 1e-8,\n  algorithm = \"meanfield\",\n  seed = 605)\n\nChain 1: ------------------------------------------------------------\nChain 1: EXPERIMENTAL ALGORITHM:\nChain 1:   This procedure has not been thoroughly tested and may be unstable\nChain 1:   or buggy. The interface is subject to change.\nChain 1: ------------------------------------------------------------\nChain 1: \nChain 1: \nChain 1: \nChain 1: Gradient evaluation took 0.032 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 320 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Begin eta adaptation.\nChain 1: Iteration:   1 / 250 [  0%]  (Adaptation)\nChain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)\nChain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)\nChain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)\nChain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)\nChain 1: Success! Found best value [eta = 1] earlier than expected.\nChain 1: \nChain 1: Begin stochastic gradient ascent.\nChain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes \nChain 1:    100       -40291.889             1.000            1.000\nChain 1:    200       -39947.669             0.504            1.000\nChain 1:    300       -39802.182             0.337            0.009\nChain 1:    400       -39776.283             0.253            0.009\nChain 1:    500       -39733.863             0.203            0.004\nChain 1:    600       -39733.198             0.169            0.004\nChain 1:    700       -39728.255             0.145            0.001\nChain 1:    800       -39784.557             0.127            0.001\nChain 1:    900       -39724.366             0.113            0.001\nChain 1:   1000       -39732.042             0.102            0.001\nChain 1:   1100       -39731.525             0.002            0.001\nChain 1:   1200       -39732.049             0.001            0.001\nChain 1:   1300       -39728.119             0.001            0.000\nChain 1:   1400       -39740.928             0.000            0.000\nChain 1:   1500       -39726.114             0.000            0.000\nChain 1:   1600       -39734.740             0.000            0.000\nChain 1:   1700       -39734.129             0.000            0.000\nChain 1:   1800       -39740.719             0.000            0.000\nChain 1:   1900       -39743.591             0.000            0.000\nChain 1:   2000       -39737.155             0.000            0.000\nChain 1:   2100       -39720.432             0.000            0.000\nChain 1:   2200       -39738.138             0.000            0.000\nChain 1:   2300       -39731.045             0.000            0.000\nChain 1:   2400       -39716.393             0.000            0.000\nChain 1:   2500       -39729.189             0.000            0.000\nChain 1:   2600       -39722.239             0.000            0.000\nChain 1:   2700       -39719.508             0.000            0.000\nChain 1:   2800       -39718.709             0.000            0.000\nChain 1:   2900       -39735.110             0.000            0.000\nChain 1:   3000       -39725.900             0.000            0.000\nChain 1:   3100       -39726.123             0.000            0.000\nChain 1:   3200       -39718.736             0.000            0.000\nChain 1:   3300       -39718.141             0.000            0.000\nChain 1:   3400       -39717.147             0.000            0.000\nChain 1:   3500       -39725.738             0.000            0.000\nChain 1:   3600       -39732.190             0.000            0.000\nChain 1:   3700       -39723.666             0.000            0.000\nChain 1:   3800       -39725.470             0.000            0.000\nChain 1:   3900       -39741.504             0.000            0.000\nChain 1:   4000       -39722.951             0.000            0.000\nChain 1:   4100       -39721.852             0.000            0.000\nChain 1:   4200       -39717.894             0.000            0.000\nChain 1:   4300       -39717.474             0.000            0.000\nChain 1:   4400       -39716.244             0.000            0.000\nChain 1:   4500       -39727.542             0.000            0.000\nChain 1:   4600       -39716.670             0.000            0.000\nChain 1:   4700       -39723.714             0.000            0.000\nChain 1:   4800       -39727.123             0.000            0.000\nChain 1:   4900       -39722.517             0.000            0.000\nChain 1:   5000       -39722.485             0.000            0.000\nChain 1:   5100       -39719.107             0.000            0.000\nChain 1:   5200       -39722.873             0.000            0.000\nChain 1:   5300       -39720.153             0.000            0.000\nChain 1:   5400       -39718.807             0.000            0.000\nChain 1:   5500       -39719.687             0.000            0.000\nChain 1:   5600       -39730.850             0.000            0.000\nChain 1:   5700       -39719.315             0.000            0.000\nChain 1:   5800       -39717.985             0.000            0.000\nChain 1:   5900       -39715.943             0.000            0.000\nChain 1:   6000       -39721.574             0.000            0.000\nChain 1:   6100       -39716.072             0.000            0.000\nChain 1:   6200       -39715.947             0.000            0.000\nChain 1:   6300       -39716.325             0.000            0.000\nChain 1:   6400       -39716.206             0.000            0.000\nChain 1:   6500       -39720.508             0.000            0.000\nChain 1:   6600       -39717.566             0.000            0.000\nChain 1:   6700       -39718.903             0.000            0.000\nChain 1:   6800       -39716.766             0.000            0.000\nChain 1:   6900       -39724.482             0.000            0.000\nChain 1:   7000       -39717.376             0.000            0.000\nChain 1:   7100       -39721.566             0.000            0.000\nChain 1:   7200       -39725.641             0.000            0.000\nChain 1:   7300       -39717.909             0.000            0.000\nChain 1:   7400       -39720.096             0.000            0.000\nChain 1:   7500       -39716.243             0.000            0.000\nChain 1:   7600       -39738.451             0.000            0.000\nChain 1:   7700       -39715.841             0.000            0.000\nChain 1:   7800       -39716.561             0.000            0.000\nChain 1:   7900       -39716.865             0.000            0.000\nChain 1:   8000       -39721.972             0.000            0.000\nChain 1:   8100       -39723.864             0.000            0.000\nChain 1:   8200       -39716.157             0.000            0.000\nChain 1:   8300       -39720.235             0.000            0.000\nChain 1:   8400       -39718.693             0.000            0.000\nChain 1:   8500       -39727.325             0.000            0.000\nChain 1:   8600       -39716.809             0.000            0.000\nChain 1:   8700       -39716.760             0.000            0.000\nChain 1:   8800       -39721.577             0.000            0.000\nChain 1:   8900       -39716.910             0.000            0.000\nChain 1:   9000       -39721.631             0.000            0.000\nChain 1:   9100       -39721.102             0.000            0.000\nChain 1:   9200       -39718.303             0.000            0.000\nChain 1:   9300       -39715.759             0.000            0.000\nChain 1:   9400       -39719.769             0.000            0.000\nChain 1:   9500       -39719.046             0.000            0.000\nChain 1:   9600       -39720.854             0.000            0.000\nChain 1:   9700       -39717.968             0.000            0.000\nChain 1:   9800       -39721.396             0.000            0.000\nChain 1:   9900       -39728.139             0.000            0.000\nChain 1:   10000       -39715.367             0.000            0.000\nChain 1: Informational Message: The maximum number of iterations is reached! The algorithm may not have converged.\nChain 1: This variational approximation is not guaranteed to be meaningful.\nChain 1: \nChain 1: Drawing a sample of size 1000 from the approximate posterior... \nChain 1: COMPLETED.\n\n\nWarning: Pareto k diagnostic value is 2.05. Resampling is disabled. Decreasing\ntol_rel_obj may help if variational algorithm has terminated prematurely.\nOtherwise consider using sampling instead.\n\n\nSetting 'QR' to TRUE can often be helpful when using one of the variational inference algorithms. See the documentation for the 'QR' argument.\n\ntoc()\n\n298.73 sec elapsed\n\nlower_tol_draws <- poststrat_df_60k %>% add_epred_draws(fit_60k_1e8, ndraws = 1000)\n\nmfvi_lower_tol_points <- lower_tol_draws %>% \n                        group_by(state,.draw) %>%\n                        summarize(postrat_draw = sum(.epred*(n/sum(n)))) %>%\n                        mutate(model = \"MF-VI 1e-8\")\n\ncombined_points_w_lower_tol <- combined_points %>%\n                      bind_rows(mfvi_lower_tol_points) %>%\n                      ungroup()\n\ncombined_points_w_lower_tol %>%\n  mutate(ordered_state = fct_reorder(combined_points_w_lower_tol$state,\n                                     combined_points_w_lower_tol$postrat_draw)) %>%\n  ggplot(aes(y = ordered_state,\n             x = postrat_draw,\n             color = model)) +\n     stat_dots(quantiles = 100) +\n     facet_wrap(~model) +\n     theme(legend.position=\"none\") +\n  xlab(\"Should employers be allowed to deny their employees abortion care?\") +\n  ylab(\"State\")\n\n\n\n\n… That certainly looks different, but I don’t really think I’d say it looks meaningfully better4.\nLooking at the printed out ELBO, it’s pretty clear that there was no traction after the first ~1000 samples. A variational family this simple isn’t going to get much better, no matter how much time you give it."
  },
  {
    "objectID": "posts/Variational MRP Pt2/Variational_MRP_pt2.html#full-rank-approximation",
    "href": "posts/Variational MRP Pt2/Variational_MRP_pt2.html#full-rank-approximation",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "Full-Rank Approximation",
    "text": "Full-Rank Approximation\nSo if extend training time, but improvements don’t result, maybe the next option is ask whether we need something more sophisticated than a mean-field approximation. Instead of\nq(z) = \\prod_{j=1}^{m} q_j(z_j)\nlet’s now try the full-rank approximation. Gather than each z_j getting it’s own independent Gaussian, this uses a single multivariate normal distribution- so we can now (roughly) learn correlation structure, fancy.\nq(z) = \\mathcal{N}(z|\\mu,\\Sigma)\n\ntic()\nfit_60k_fullrank <- stan_glmer(abortion ~ (1 | state) + (1 | eth) + (1 | educ) +\n                                      male + (1 | male:eth) + (1 | educ:age) +\n                                      (1 | educ:eth) + repvote + factor(region),\n  family = binomial(link = \"logit\"),\n  data = cces_all_df,\n  prior = normal(0, 1, autoscale = TRUE),\n  prior_covariance = decov(scale = 0.50),\n  adapt_delta = 0.99,\n  tol_rel_obj = 1e-8,\n  # Printing the ELBO every 1k draws\n  refresh = 1000,\n  algorithm = \"fullrank\",\n  QR = TRUE,\n  seed = 605)\n\nChain 1: ------------------------------------------------------------\nChain 1: EXPERIMENTAL ALGORITHM:\nChain 1:   This procedure has not been thoroughly tested and may be unstable\nChain 1:   or buggy. The interface is subject to change.\nChain 1: ------------------------------------------------------------\nChain 1: \nChain 1: \nChain 1: \nChain 1: Gradient evaluation took 0.025 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 250 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Begin eta adaptation.\nChain 1: Iteration:   1 / 250 [  0%]  (Adaptation)\nChain 1: Iteration:  50 / 250 [ 20%]  (Adaptation)\nChain 1: Iteration: 100 / 250 [ 40%]  (Adaptation)\nChain 1: Iteration: 150 / 250 [ 60%]  (Adaptation)\nChain 1: Iteration: 200 / 250 [ 80%]  (Adaptation)\nChain 1: Iteration: 250 / 250 [100%]  (Adaptation)\nChain 1: Success! Found best value [eta = 0.1].\nChain 1: \nChain 1: Begin stochastic gradient ascent.\nChain 1:   iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes \nChain 1:    100      -248586.032             1.000            1.000\nChain 1:    200      -180460.369             0.689            1.000\nChain 1:    300      -121675.221             0.620            0.483\nChain 1:    400       -87431.017             0.563            0.483\nChain 1:    500      -120999.829             0.506            0.392\nChain 1:    600       -96768.296             0.463            0.392\nChain 1:    700       -93851.607             0.402            0.378\nChain 1:    800       -92494.273             0.353            0.378\nChain 1:    900       -74378.556             0.341            0.277\nChain 1:   1000       -77681.560             0.311            0.277\nChain 1:   1100       -77465.866             0.211            0.250\nChain 1:   1200       -68692.287             0.186            0.244\nChain 1:   1300       -75140.633             0.147            0.128\nChain 1:   1400       -49430.772             0.160            0.128\nChain 1:   1500       -59011.994             0.148            0.128\nChain 1:   1600       -57033.572             0.127            0.086\nChain 1:   1700       -56133.855             0.125            0.086\nChain 1:   1800       -46605.149             0.144            0.128\nChain 1:   1900       -47895.964             0.122            0.086\nChain 1:   2000       -44745.890             0.125            0.086\nChain 1:   2100       -43472.467             0.128            0.086\nChain 1:   2200       -43454.384             0.115            0.070\nChain 1:   2300       -41781.249             0.110            0.040\nChain 1:   2400       -42045.221             0.059            0.035\nChain 1:   2500       -41381.652             0.044            0.029\nChain 1:   2600       -40754.440             0.043            0.027\nChain 1:   2700       -41108.136             0.042            0.027\nChain 1:   2800       -40450.439             0.023            0.016\nChain 1:   2900       -40423.015             0.020            0.016\nChain 1:   3000       -40375.121             0.013            0.015\nChain 1:   3100       -40227.022             0.011            0.009\nChain 1:   3200       -40302.411             0.011            0.009\nChain 1:   3300       -40352.339             0.007            0.006\nChain 1:   3400       -40174.196             0.007            0.004\nChain 1:   3500       -40089.973             0.006            0.004\nChain 1:   3600       -40143.009             0.004            0.002\nChain 1:   3700       -40123.486             0.003            0.002\nChain 1:   3800       -40044.004             0.002            0.002\nChain 1:   3900       -39955.515             0.002            0.002\nChain 1:   4000       -40003.851             0.002            0.002\nChain 1:   4100       -39948.544             0.002            0.002\nChain 1:   4200       -40028.027             0.002            0.002\nChain 1:   4300       -39907.006             0.002            0.002\nChain 1:   4400       -39868.266             0.002            0.002\nChain 1:   4500       -39938.386             0.002            0.002\nChain 1:   4600       -39837.339             0.002            0.002\nChain 1:   4700       -39852.349             0.002            0.002\nChain 1:   4800       -39823.670             0.002            0.002\nChain 1:   4900       -39809.797             0.001            0.001\nChain 1:   5000       -39807.261             0.001            0.001\nChain 1:   5100       -39806.402             0.001            0.001\nChain 1:   5200       -39818.805             0.001            0.001\nChain 1:   5300       -39797.428             0.001            0.001\nChain 1:   5400       -39790.469             0.001            0.000\nChain 1:   5500       -39785.797             0.001            0.000\nChain 1:   5600       -39779.121             0.000            0.000\nChain 1:   5700       -39780.314             0.000            0.000\nChain 1:   5800       -39771.363             0.000            0.000\nChain 1:   5900       -39770.673             0.000            0.000\nChain 1:   6000       -39764.096             0.000            0.000\nChain 1:   6100       -39764.173             0.000            0.000\nChain 1:   6200       -39765.651             0.000            0.000\nChain 1:   6300       -39756.809             0.000            0.000\nChain 1:   6400       -39753.724             0.000            0.000\nChain 1:   6500       -39754.753             0.000            0.000\nChain 1:   6600       -39750.392             0.000            0.000\nChain 1:   6700       -39753.067             0.000            0.000\nChain 1:   6800       -39750.341             0.000            0.000\nChain 1:   6900       -39745.696             0.000            0.000\nChain 1:   7000       -39743.521             0.000            0.000\nChain 1:   7100       -39739.157             0.000            0.000\nChain 1:   7200       -39736.689             0.000            0.000\nChain 1:   7300       -39743.472             0.000            0.000\nChain 1:   7400       -39738.431             0.000            0.000\nChain 1:   7500       -39740.789             0.000            0.000\nChain 1:   7600       -39735.842             0.000            0.000\nChain 1:   7700       -39733.493             0.000            0.000\nChain 1:   7800       -39735.015             0.000            0.000\nChain 1:   7900       -39736.429             0.000            0.000\nChain 1:   8000       -39733.548             0.000            0.000\nChain 1:   8100       -39732.722             0.000            0.000\nChain 1:   8200       -39734.720             0.000            0.000\nChain 1:   8300       -39732.932             0.000            0.000\nChain 1:   8400       -39727.658             0.000            0.000\nChain 1:   8500       -39734.522             0.000            0.000\nChain 1:   8600       -39728.602             0.000            0.000\nChain 1:   8700       -39724.690             0.000            0.000\nChain 1:   8800       -39725.374             0.000            0.000\nChain 1:   8900       -39731.450             0.000            0.000\nChain 1:   9000       -39725.866             0.000            0.000\nChain 1:   9100       -39728.639             0.000            0.000\nChain 1:   9200       -39730.156             0.000            0.000\nChain 1:   9300       -39729.036             0.000            0.000\nChain 1:   9400       -39725.536             0.000            0.000\nChain 1:   9500       -39727.031             0.000            0.000\nChain 1:   9600       -39725.389             0.000            0.000\nChain 1:   9700       -39727.947             0.000            0.000\nChain 1:   9800       -39723.932             0.000            0.000\nChain 1:   9900       -39723.173             0.000            0.000\nChain 1:   10000       -39723.944             0.000            0.000\nChain 1: Informational Message: The maximum number of iterations is reached! The algorithm may not have converged.\nChain 1: This variational approximation is not guaranteed to be meaningful.\nChain 1: \nChain 1: Drawing a sample of size 1000 from the approximate posterior... \nChain 1: COMPLETED.\n\n\nWarning: Pareto k diagnostic value is 2.95. Resampling is disabled. Decreasing\ntol_rel_obj may help if variational algorithm has terminated prematurely.\nOtherwise consider using sampling instead.\n\ntoc()\n\n350.16 sec elapsed\n\nfull_rank_draws <- poststrat_df_60k %>% add_epred_draws(fit_60k_fullrank,\n                                                        ndraws = 1000)\n\nfrvi_points <- full_rank_draws %>% \n                        group_by(state,.draw) %>%\n                        summarize(postrat_draw = sum(.epred*(n/sum(n)))) %>%\n                        mutate(model = \"FR-VI\")\n\ncombined_points_w_frvi <- combined_points_w_lower_tol %>%\n                      bind_rows(frvi_points) %>%\n                      ungroup()\n\ncombined_points_w_frvi %>%\n  mutate(ordered_state = fct_reorder(combined_points_w_frvi$state,\n                                     combined_points_w_frvi$postrat_draw)) %>%\n  ggplot(aes(y = ordered_state,\n             x = postrat_draw,\n             color = model)) +\n     stat_dots(quantiles = 100) +\n     facet_wrap(~model) +\n     theme(legend.position=\"none\") +\n  xlab(\"Should employers be allowed to deny their employees abortion care?\") +\n  ylab(\"State\")\n\n\n\n\nThe first thing to note here is that unlike the mean-field approximation, fitting this model required some tinkering to get it to fit. I ended up needing to set QR = TRUE (ie, use a QR decomposition) to get this to fit at all (unless I set the initialization to 0, at which point the posterior collapsed to nearly a single point).\nUnfortunately, this version has a similar spiky posterior distribution. In terms of uncertainty, it’s clearly worse than the mean-field implementation. The ELBO starts from higher, spends some time actually improving, but also quickly reaches a plateau. It doesn’t seem like this is a way out either."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Andy Timm",
    "section": "",
    "text": "I’m usually teaching myself several new things, and this page is a suggestion from a friend about how to share that learning. If you’re working on similar things, and want to talk about them more or learn together, I’m always excited to chat (especially if your application is socially beneficial)!\nHere are some themes of current personal and work projects I’m particularly excited about. Footnotes expand on resources I’m using to learn more:\n\n\n\nPersonal Projects\n\nScalable Bayesian Inference: What flavors and augmentations of variational inference allow us to most flexibly and reliably scale Bayesian inference to large datasets? Are there other viable tools for scalable Bayes that are less explored? 1.\nModern Survey Experiment Designs: As high quality survey completes grow ever more expensive, can tools like discrete choice models or conjoint experiments more efficiently approximate the same learnings? How can we tell when these methods will and won’t produce the same findings as experiments? 2.\nHow Media Shapes Politics: How do social media and TV as platforms shape US Politics? How real are posited polarization effects and other suggested harms, if at all?3.\n\n\n\n\n(Previous) Work Projects\n\nHierarchical Forecasting for Media Planning: Being able to accurately forecast the likely reach of a media plan at a variety of granularities enables better planning; what are the most reliable and accurate forecasting models for our applications? 4.\nProductizing HTE Estimation: Exploring which heterogeneous treatment effect models perform best in our industry in a variety of contexts, with a fully productized solution coming online as of early 20235.\nBetter Polling Methods: Improving our political survey methodology choices to improve our resulting model quality in the 2022 midterm elections and beyond6.\n\n\n\n\nI also want to start building out a broader list of topics that I’ve spent time with or want to do a deeper dive in. For example, in 2021 I spent a ton of time exploring Potential Outcome vs DAG approaches to causal inference. In grad school I was particularly focused on the effects of educational polarization in the US, etc. This is both a personal thing (to track what I’ve been interested in over the years), and a social one (I love sharing resources and working with others to learn these topics).\n\n\n\n\nFootnotes\n\n\nThere are lots of applications in which I’d prefer a Bayesian model, but where scaling them is simply impractical or unreliable. I started learning more here this out by working through Depth First Learning’s Variational Inference with Normalizing Flows curriculum. To explore more tools in this area, I’m writing a blog post series where I try to get reasonable variational approximations posterior distributions for a basic MRP model, exploring the literature on various flavors and augmentations of VI and diagnostic techniques along the way. I’m particularly excited about normalizing flows-based approaches based on past successes I’ve had in other work. I am also absolutely fascinated by work like Hoffman and Ma (2020) that show Black Box VI actually follows a similar gradient flow to some forms of MCMC, with the resulting algorithmic suggestion that many short chains averaged may be another reasonable formula for scalable Bayes; in practice, I haven’t seen this outperform VI, but would love to see any counter examples!↩︎\nI’m inspired here by Data for Progress/Priorities USA’s testing around replacing in-survey RCTs with a MaxDiff/Best-Worst Scaling deisgn on the discrete choice side. Commonsense Solidarity is a similar inspiration in terms of conjoint experiments that can provide huge amounts of information efficiently. To learn more here, several of the chapters in Advances in Experimental Political Science have been a fantastic starting point for me. Working in advertising where discrete choice modelers commonly rely on Sawtooth, I’ve been thinking about strange contradictions of how it implements MaxDiff ; Jim Savage’s notes and Stan Forum comments here have been super helpful, and I’m working on building Stan implementations of both what Sawtooth chooses to do and the perhaps more reasonable ranked choice random coefficients implementation for the bwsTools R package to get more hands on here.↩︎\nThe rough intuition I have here as someone working in TV is that social media gets a lot of hype about how it negatively shapes politics, but if anything, TV is much more likely to be the more negatively impactful platform. Robert Putnam famously theorizes that television is and was responsible for 20% of the recent decay in American social capital; how plausible is that? To see if that intuition is supported by research, I’m working through reading the papers and books mentioned in Chris Bail’s Social Media and Political Dysfunction Collaborative Review, and working on finding similar books and papers that ask the same questions of television.↩︎\nBy forecasting reach and impressions of a proposed media plan at any level from coarse network-daypart combinations all the way down to individual household likelihoods of viewing a placed ad, our clients can adjust their plans to meet their goals. Of course, different granularities of forecast are orders of magnitude apart in difficulty and resulting accuracy expectations; how do we combine them into a stronger, coherent picture? Without getting too close to IP, our solution relies on Bayesian time series models that are then reconciled using hierarchical forecasting technqiues to maximize accuracy at all levels.↩︎\nThere are a ton of ideas to explore here, but some of the most promising are double/debiased ML estimators, Meta Learners, and other ideas explored in recent ACIC competitions.↩︎\nNone of these links will get at any IP, but broadly I’m synthesizing a lot of what I learned from AAPOR 2022 (this thread is a good starting point) and examining how some decent ideas would’ve changed our recent predictions.↩︎"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Most of the cool stuff I get to build these days isn’t public unfortunately. Hoping to expand out my public stuff eventually, but I do have an R package that gets some use:"
  },
  {
    "objectID": "software.html#retrodesign",
    "href": "software.html#retrodesign",
    "title": "Software",
    "section": "retrodesign",
    "text": "retrodesign\nretrodesign provides tools for working with Type S (Sign) and Type M (Magnitude) errors, as proposed in Gelman and Tuerlinckx (2000) and Gelman & Carlin (2014). In addition to simply calculating the probability of Type S/M error, the package includes functions for calculating these errors across a variety of effect sizes for comparison, and recommended sample size given “tolerances” for Type S/M errors. To improve the speed of these calculations, closed forms solutions for the probability of a Type S/M error from Lu, Qiu, and Deng (2018) are implemented. The broader goal of this project was to make it easier for researchers to understand these issues in their work, and it’s gratifying the package has been able to do that.\n\n        \n        Website\n     \n        \n        Github\n     \n        \n        Package"
  },
  {
    "objectID": "posts/Variational MRP Pt3/variational_mrp_3.html",
    "href": "posts/Variational MRP Pt3/variational_mrp_3.html",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "",
    "text": "Note: I’ve gotten a lot more pessimistic about how generally useful the alternatives to simple KL-Divergence are on their own since writing this post. I still think these are really useful ideas to think to build intuition about VI, and techniques like CHIVI are useful for some lower dimensional problems or as part of an ensemble of techniques for high dimensional ones. However, this paper from Dhaka et al. is very convincing that CHIVI and currently available similar algorithms are in practice very hard to optimize for high dimensional, and that some of the intuitive benefits shown about CHIVI below in low dimensions don’t really generalize the way we’d expect to higher dimensions.\nThis is section 3 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.\nIn the last post we threw caution to the wind, and tried out some simple variational inference implementations, to build up some intuition about what bad VI might look like. Just pulling a simple variational inference implementation off the shelf and whacking run perhaps unsurprisingly produced dubious models, so in this post we’ll bring in long overdue theory to understand why VI is so difficult, and what we can do about it.\nThe general structure for the next couple of posts will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in the next three posts will go a long way towards more robust variational inference. I’ll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.\nThe rough plan for the series is as follows:"
  },
  {
    "objectID": "posts/Variational MRP Pt3/variational_mrp_3.html#chi2-variational-inference-chivi-and-the-cubo-bound",
    "href": "posts/Variational MRP Pt3/variational_mrp_3.html#chi2-variational-inference-chivi-and-the-cubo-bound",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "\\chi^{2} Variational Inference (CHIVI) and the CUBO bound",
    "text": "\\chi^{2} Variational Inference (CHIVI) and the CUBO bound\nThe \\chi^{2}-divergence has form:\n\nD_{\\chi^2}(p||q) = \\mathbb{E}_{q(z;\\lambda)}[(\\frac{p(z|x)}{q(z;\\lambda)})^2 -1]\n For simplicity and comparability, I’m switching here to using Dieng et Al. (2017)’s notation here- they use q(z;\\lambda) to refer to the variational family we’re using, indexed by parameters \\lambda.\nThis divergence has the properties we wanted when we tried to use Inclusive KL Divergence- it tends to be mean seeking instead of mode seeking.\nLike with the ELBO, we need to show that we have a bound here independent of logp(x), and that we have a way to estimate that bound efficiently.\nLet’s first move around a few pieces of the first term above:\n\n\\begin{align}\n\\mathbb{E}_{q(z;\\lambda)}[(\\frac{p(z|x)}{q(z;\\lambda)})^2& = 1 + D_{\\chi^2}(p(z|x)|q(z;\\lambda)) \\\\\n&= p(x)^2[1 + D_{\\chi^2}(p(z|x)|q(z;\\lambda))]\n\\end{align}\n Then we can take the log of both sides of the equation, which gives us:\n\n\\frac{1}{2}log(1 + D_{\\chi^2}(p(z|x)|q(z;\\lambda))) = -logp(x) + \\frac{1}{2}log\\mathbb{E}_{q(z;\\lambda)}[(\\frac{p(z|x)}{q(z;\\lambda)})^2]  \n …and this is starting to feel a lot like the ELBO derivation. Log is monotonic, and the -logp(x) term is constant as we optimize q, so we’ve found something that we’re close to able to minimize:\n\nCUBO_{2}(\\lambda) = \\frac{1}{2}log\\mathbb{E}_{q(z;\\lambda)}[(\\frac{p(z|x)}{q(z;\\lambda)})^2]\n Since this new divergence is non-negative as well, this is a upper bound of the model evidence. This is thus named \\chi upper bound (CUBO)4."
  },
  {
    "objectID": "posts/Variational MRP Pt3/variational_mrp_3.html#but-can-we-estimate-it",
    "href": "posts/Variational MRP Pt3/variational_mrp_3.html#but-can-we-estimate-it",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "… But can we estimate it?",
    "text": "… But can we estimate it?\nOne other issue here: how do we estimate this? The CUBO objective got rid of the logp(x) we were worried about, but it seems like that expectation is going to be difficult to estimate in general.\nYour first idea might be to Monte Carlo (not MCMC) estimate it roughly like this:\n\nCUBO_2(\\lambda) = \\frac{1}{2}log\\frac{1}{S}\\sum_{s=1}^{S}[(\\frac{p(x,z^{s})}{q(z^{s};\\lambda)})^2]\n Unfortunately, the log transform here means our Monte Carlo estimator will be biased: we can see this by applying Jensen’s inequality to the above. To make this stably act as an upper bound, we can apply a clever transformation:\n\n\\bf{L} = exp(n* CUBO_2(\\lambda))\n\nSince exp is monotonic, this has the same objective as the CUBO, but we can Monte Carlo estimate it unbiasedly. Is that the last problem to solve?"
  },
  {
    "objectID": "posts/Variational MRP Pt3/variational_mrp_3.html#but-can-we-calculate-gradients-efficiently",
    "href": "posts/Variational MRP Pt3/variational_mrp_3.html#but-can-we-calculate-gradients-efficiently",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "… But can we calculate gradients efficiently?",
    "text": "… But can we calculate gradients efficiently?\nWait, wait no. Sorry to keep saying there’s one more step here, but there’s a lot that goes into making a full, convenient, general use algorithm here. The last step (for real this time) is that we need to figure out how to get gradients for the estimate of \\bf{L} above, \\bf{\\hat{L}}. The issue is that we don’t have any guarantee that a unbiased Monte Carlo estimator of \\bf{\\hat{L}} gets us a Monte Carlo way to estimate \\nabla_\\lambda\\bf{\\hat{L}}- we can’t guarantee that the gradient of the expectation is equal to the expectation of the gradient.\nFor this, we need to pull out a trick from the variational autoencoder literature. This is usually referred to as the “Reparameterization Trick”5, but the original CHIVI paper refers to them as “reparmeterization gradients”. We will assume we can rewrite the generative process of our model as z = g(\\lambda,\\epsilon), where \\epsilon \\sim p(\\epsilon) and g being a deterministic function. Then we have a new estimator for both \\bf{\\hat{L}} and it’s gradient:\n\n\\begin{align}\n\\bf{\\hat{L}} &= \\frac{1}{B}\\sum_{b=1}^B(\\frac{p(x,g(\\lambda,\\epsilon^{(b)}))}{q(g(\\lambda,\\epsilon^{(b)};\\lambda))})^{2} \\\\\n\\nabla_\\lambda\\bf{\\hat{L}}  &= \\frac{2}{B}\\sum_{b=1}^B(\\frac{p(x,g(\\lambda,\\epsilon^{(b)}))}{q(g(\\lambda,\\epsilon^{(b)};\\lambda))})^2 \\nabla_\\lambda log(\\frac{p(x,g(\\lambda,\\epsilon^{(b)}))}{q(g(\\lambda,\\epsilon^{(b)};\\lambda))})\n\\end{align}\n There are one or two more neat computational tricks in the paper I won’t explain here (essentially: how do we extend this to work in minibatch fashion, and how do we avoid numerical underflow issues), but this is now essentially functional. The whole algorithm, which they dubbed CHIVI is below:"
  },
  {
    "objectID": "posts/Variational MRP Pt4/variational_mrp_pt4.html",
    "href": "posts/Variational MRP Pt4/variational_mrp_pt4.html",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "",
    "text": "This is section 4 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.\nThe general structure for this post and the ones after it will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in these four posts will go a long way towards more robust variational inference. I’ll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.\nIn the last post we took a look at how our ELBO objective requires specific version of KL Divergence (the “Exclusive” formulation of KLD), and saw that it encoded a preference for a certain type of solution to the VI problem. Then we looked at CUBO and CHIVI, an alternative bound and algorithm that avoid this problem, often leading to a more useful posterior distribution by pursuing a more “inclusive” solution.\nIn this post, we’ll leverage importance sampling to make the most of the samples we do have, emphasizing the parts of our q(x) that look like p(x) and de-emphasizing the parts that do not.\nThe rough plan for the series is as follows:"
  },
  {
    "objectID": "posts/Variational MRP Pt4/variational_mrp_pt4.html#importance-weighted-variational-inference",
    "href": "posts/Variational MRP Pt4/variational_mrp_pt4.html#importance-weighted-variational-inference",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "Importance Weighted Variational Inference",
    "text": "Importance Weighted Variational Inference\nImportance Weighting for VI in it’s simplest form is pretty intuitive (draw samples from an already trained q(x), weight them…), so let’s derive the new Importance Weighted Variational Inference (IWVI) estimator first since some nice intuition will come with it.\nI want to emphasize something that wasn’t clear to me for a good while- these two ideas are not equivalent. While both are useful tools, the “train time”, objective-modifying IWVI estimator is a distinct approach from the “test time” importance sampling approach that takes draws from a fixed q(x) and reweights them as best it can.\nWe’ll aim to show that we can get a tighter ELBO by using importance sampling. This type of tighter ELBO was first shown by Burda et Al. (2015) in the context of Variational Autoencoders after which is was fairly clear this could apply to variational inference, but Domke and Sheldon (2018) fleshed out some details of that extension- I’ll be explaining some of the latter group’s main results first.\nTo start, imagine a random variable R, such that \\mathbb{E}{R} = p(x), which we’ll think of as a estimator of p(x). Then by Jensen’s Inequality:\n\nlogp(x) = \\mathbb{E}logR + \\mathbb{E}log\\frac{p(x)}{R}\n\nThe first term is the bound, which will be tighter if R is highly concentrated.\nThis is a more general form of the ELBO; we can make it quite familiar looking by having our R above be:\n\nR = \\frac{p(z,x)}{q(z)}, z \\sim q\n\nThe reason for pointing out this fairly simple generalization is helpful is that it frames how to tighten our ELBO on logp(x) via alternative estimators R.\nBy drawing M samples and averaging them as in importance sampling, we get:\n\nR_M = \\frac{1}{M}\\sum_{m=1}^{M}\\frac{p(z_m,x)}{q(z_m)}, z_m \\sim q\n From there, we can derive a tighter bound on logp(x), referred to as the IW-ELBO:\n\nIW-ELBO_M[q(z)||p(z,x)] := \\mathbb{E}_{q(z_{1:M})}log\\frac{1}{M} \\sum_{m=1}^{M}\\frac{p(z_m,x)}{q(z_m)}\n Where we’re using the 1:M as a shorthand for q(z_{1:M}) = q(z_1)...q(z_M).\nIt’s worth noting that the last few lines don’t specify a particular form of importance sampling- we’re getting the tighter theoretical bounding behavior from the averaging of samples from q. We’ll see a particularly good form of importance sampling with desirable practical properties in a moment.\n\nHow does IW-ELBO change the VI problem conceptually?\nThe tighter bound is nice, but importance sampling also has the side effect (done right, side benefit) of modifying our incentives in choosing a variational family. To see what I mean, we can re-use the example distributions from last post we used to build intuition for KL Divergence, where red was the true distribution, and green were our potential approximations. If we’re not going to draw multiple samples and weight them, it makes sense to choose something like the first plot below. Every draw in the middle of the two target modes is expensive per our ELBO objective, so better to choose a mode.\n\n\n\n\n\n\n\nrkl_plot <- mixture %>% ggplot(aes(x = normals)) +\n  geom_density(aes(x = normals), color = \"red\") +\n  geom_density(aes(x = mode_seeking_kl), color = \"green\") + ggtitle(\"Without weighting, we prefer to capture a mode\") +\n  xlab(\"\")\n\nfkl_plot <- mixture %>% ggplot(aes(x = normals)) +\n  geom_density(aes(x = normals), color = \"red\") +\n  geom_density(aes(x = mean_seeking_kl), color = \"green\") + ggtitle(\"With importance sampling, weights allow us to prefer coverage\") +\n  xlab(\"\")\n\ngrid.arrange(rkl_plot,fkl_plot)\n\n\n\n\nIf we can use importance sampling though, quite the opposite is be true! Note that we’re still using the ELBO, a reverse-KL based metric- that hasn’t changed. What has changed is our ability to mitigate the objective costs of those samples between the two extremes. Via this “train time” implementation of IS, points outside the two target modes will get lower importance weights, and points within the modes will get higher ones, so as long as we’re covering the modes with some reasonable amount of probability mass, and drawing enough samples we can actually do better with the distribution centered between the modes.\nTo further drive home the point about how a “train time” and “test time” implementations of IS differ, could “test time” IS do this? Not really- because the ability to better minimize the ELBO via sampling requires the IW-ELBO variant and associated training process. If we hard-coded q(x) as the green N(9,4) shown above, “test time” IS could weight the right samples up to better approximate p(x), but it doesn’t fundamentally alter our optimization problem the way the IWVI objective does.\nWe can also imagine how varying the number of samples might effect optimization. Between S=1 and “enough draws to get all the benefits of IS”, we can imagine there’s a slow transition from “just stick with 1 mode” and “go with IS”. So it seems like we should be worried about getting the number of samples right, but fortunately as we’ll see in the next section there are great rules of thumb in some variants of IS. We’ll still need to bear the cost of sampling (which gets higher as q(x) becomes “further” from p(x), as we’ll need more samples to weight into a good approximation), but the cost of sampling for most VI implementations will often be pretty manageable if our proposal distribution is somewhat close to p(x).\nAnother way to think about how importance sampling changes our task with variational inference is to think about what sorts of distributions make sense to have as our variational family, and even which objective might be better given IS. On choice of a variational family, if we’re aiming for coverage, moving towards thicker-tailed distributions like t distributions makes a lot of sense. While we explored the IW-ELBO above to build intuition, there’s no reason not to apply IW to the CUBO and thus CHIVI- this also naturally produces nicely overdispersed distributions which can be importance sampled closer to the true p(x). This idea of aiming for a wide proposal to sample from is referred to in the importance sampling literature (eg Owen, 2013) as “defensive sampling”, with Domke and Sheldon (2018) exploring the VI connection more fully. For intuition, by ensuring most of p(x) is covered by some reasonable mass makes it easier to efficiently get draws that can be weighted into a final posterior, even if the unweighted posterior might be too wide."
  },
  {
    "objectID": "posts/Variational MRP Pt4/variational_mrp_pt4.html#solving-our-is-problems-with-pareto-smoothed-importance-sampling",
    "href": "posts/Variational MRP Pt4/variational_mrp_pt4.html#solving-our-is-problems-with-pareto-smoothed-importance-sampling",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "Solving our IS problems with Pareto-Smoothed Importance Sampling",
    "text": "Solving our IS problems with Pareto-Smoothed Importance Sampling\nAs we’ve been talking about importance sampling, we’ve been leaving some of the messier details aside (how many samples to draw, how to deal with the cases when some of the weights get huge, how to know when our proposal distribution is “close” enough).\nWhile the Importance Sampling Literature is huge and there are a lot of possible solutions here, I’ll next introduce Vehtari et Al. (2015)’s Pareto-Smoothed Importance Sampling. I’m a huge fan of this paper- it’s a really elegant and powerful tool, derived from taking Bayesian principles seriously.\nAbove, I described a common failure mode for IS estimators, where some weights are orders of magnitude larger than others, with this long right tail of ratios dominating the weighted average and blowing up the variance of the estimator. Pareto-Smoothed Importance Sampling proposes to model those tail values as coming from a Generalized Pareto Distribution, a distribution for describing extreme values, and replace the most extreme weights with modeled (and more stable) values.\nFor concreteness, let’s introduce a simple 1-D example. We’ll aim to use importance sampling to approximate distributions \\mathcal{T}(\\mu = 0,\\sigma = 1,t =5) and \\mathcal{C}(x_0= 0,\\gamma = 10) with a \\mathcal{N}(\\mu = 0,\\sigma = 1) distribution. If that sounds like the opposite of preferring wide tails on q(x)’s I described above, you’re right, but using a poor choice here will illustrate some useful properties.\n\nsimulated_data <- tibble(\nq_x = rnorm(100000),\nmanageable_p_x = rt(100000,5),\nunmanageable_p_x = rcauchy(100000),\nmanageable_ratios = dt(q_x,5)/dnorm(q_x),\nunmanageable_ratios = dcauchy(q_x,0,10)/dnorm(q_x)\n)\n\nsimulated_data %>%\n            pivot_longer(c(q_x,manageable_p_x,unmanageable_p_x),\n                         values_to = \"draws\",\n                         names_to = \"distributions\") %>%\n            ggplot(aes(x = draws, color = distributions)) +\n            geom_density() +\n            # If you wanted to show the full reach of the Cauchy, it'd be\n            # hard to see the shape of the T vs N; it's that wide.\n            # Hence the 6k values removed\n            xlim(-10,10) +\n            ggtitle(\"Visualizing the distributions in question\") +\n            theme(legend.position=\"none\")\n\nWarning: Removed 6245 rows containing non-finite values (stat_density).\n\n\n\n\n\nThe tails on that Cauchy distribution are super, super wide compared to our normal, so the samples far, far out in the tails of the normal will need massive weights to approximate the cauchy. The t-distribution is wider too, so we’ll need some higher weights, but not nearly as many. As a way to visualize this, you can see that just a handful of draws have weights away from ~1, but these weights are as much as 5000x higher than the mean ratio, and will dominate any average we make of them.\n\nsimulated_data %>%\n  arrange(unmanageable_ratios) %>%\n  mutate(n = seq(1,100000)) %>%\n  ggplot(aes(x = n,y = unmanageable_ratios)) +\n  geom_point() +\n  ggtitle(\"A pretty typical 'unsaveable' set of importance ratios\")\n\n\n\n\nThe t-distribution ratio plot would look similar, but with a much smaller y-scale. The max weight would still be much larger than the average, but more than an order of magnitude or so less large:\n\nmean_t <- mean(simulated_data$manageable_ratios)\nmax_t <- max(simulated_data$manageable_ratios)\n\nmean_c <- mean(simulated_data$unmanageable_ratios)\nmax_c <-max(simulated_data$unmanageable_ratios)\n\nprint(paste0(\"the mean of the t is: \",mean_t,\" compared to a max of \",max_t,\";\",\n             \"The cauchy cause is more extreme- the mean of the cauchy is: \",mean_c,\" compared to a max of \",max_c))\n\n[1] \"the mean of the t is: 0.995904306317625 compared to a max of 164.448932152079;The cauchy cause is more extreme- the mean of the cauchy is: 0.283655933763642 compared to a max of 1428.22324178729\"\n\n\nSo let’s bring this back to Pareto smoothing here. We want to model and smooth that long tail of the ratio distribution. It turns out there’s plenty of study of the distribution of extreme events, and there’s some classical limit results showing:\n\nr(\\theta) | r(\\theta) > \\tau \\rightarrow GPD(\\tau,\\sigma,k), \\tau \\rightarrow \\infty\n\nwhere \\tau is a lower bound parameter, which in our case defines how many ratios from the tail we’ll actually model. \\sigma is a scale parameter, and k is a unconstrained shape parameter. Without getting too far into the weeds, we can implicitly define \\tau via using a well-supported role of thumb suggesting to use the M largest ratios, M = min(0.2S,3\\sqrt{S})3. From there, the \\hat{k} and \\hat{\\sigma} have easy and efficient estimators. The Generalized Pareto Distribution has form:\n\n\\frac{1}{\\sigma} \\left(1 + k\\frac{r - \\tau}{\\sigma} \\right)^{-1/k-1}\n\nand we can replace our M biggest ratios with estimated values calculated via the CDF of the Generalized Pareto Distribution.\nOne of the best things about PSIS is it comes with a built in diagnostic via \\hat{k}. To see how this works, it’s useful to know that importance sampling depends on how many moments r(\\theta) has- for example, if at least two moments exist, the vanilla IS estimator has finite variance (which is obviously required, but no guarantee of performance since it might be finite but massive). The GPD has k^{-1} finite fractional moments when k > 0.\nVehtari et al. show that the replacement of the largest M ratios above changes PSIS to have finite variance and an error distribution converging to normal when k \\in (.5,1). Intuitively, k > .5 implies the raw ratios have infinite variance, but PSIS trades a little bias to make the variance finite again.\nWhat about actually practical to work with variance? This is the really cool bit- k < .7 turns out to be a remarkably robust indicator of when we can expect PSIS to work in a ton of different simulation studies and practical examples.\nWhy is this true? 1.4 fractional moments seems awful arbitrary, right? Let’s ask an alternative question, and kill two birds with one stone: what sample size do we need for PSIS to work? Chaterjee and Draconis (2018) showed that for a given accuracy, how big S needs to be for importance sampling more broadly depends on how close q(x) is to p(x) in KL distance- we need to satisfy log(S) \\geq \\mathbb{E}_{\\theta \\sim q(x)}[r(\\theta)log(r(\\theta))] to get accuracy.\nWell, we don’t know the distribution of r, we should have some pretty good intuition that the important part (read: that explosive, variance ruining tail) is Pareto. If we take r as exactly Pareto, you can trace out S for different \\hat{k}4, and to give a few example points-\n\nfor given \\hat{k}, roughly what S is needed if r is exactly Pareto\n\n\n\n\n\n\n\\hat{k}\nS needed\n\n\n\n\n.5\n~1,000\n\n\n.7\n~140,000\n\n\n.8\n1,000,000,000,000\n\n\n.9\nplease stop you’re making your compute sad.\n\n\n\nWhile we of course know r isn’t Pareto exactly exactly, hopefully this helps with intuition around \\hat{k} telling us when we’re getting into “sampling forever to have any chance at all to control the variance” land.\nNeat! So what does that look like for our Cauchy and T distribution example?\n\nmanageable_psis <- psis(log(simulated_data$manageable_ratios),\n                       r_eff = NA)\n\nunmanageable_psis <- psis(log(simulated_data$unmanageable_ratios),\n                          r_eff = NA)\n\nmanageable_psis$diagnostics\n\n$pareto_k\n[1] 0.5963495\n\n$n_eff\n[1] 62963.93\n\nunmanageable_psis$diagnostics\n\n$pareto_k\n[1] 0.8533127\n\n$n_eff\n[1] 249.2052\n\n\nAs we expected, the the Normal proposal distribution isn’t ideal for the T distribution, but it’s manageable. On the other hand, we’d need somewhere between a trillion and “oh god no :(” samples to make the normal proposal work out for the Cauchy.\nBringing the discussion back to variational inference, PSIS is super helpful- importance sampling more generally broadens the class of q(x)es that are close enough to p(x) for variational inference to work, and PSIS considerably widens that basin of feasibility. The extensive theoretical and simulation framework around the method also give us a solid way to realize when importance sampling isn’t feasible via the \\hat{k} diagnostic, and tells us how roughly samples we need to draw. Super, super cool.\nOne more great thing PSIS does for variational inference- \\hat{k} serves as a powerful diagnostic for variational inference itself! I’ll save most of this discussion for the post on diagnostics, but to sketch out the logic- \\hat{k} tells us when q(x) is too far from p(x) for importance sampling to work, which is a function of KL Divergence from q(x) to p(x)- if that distance is too great for importance sampling to allow us to bridge, that implies we aren’t close enough to trust our base variational approximation either!"
  },
  {
    "objectID": "posts/Variational MRP Pt4/variational_mrp_pt4.html#multiple-proposal-distributions-with-multiple-importance-sampling",
    "href": "posts/Variational MRP Pt4/variational_mrp_pt4.html#multiple-proposal-distributions-with-multiple-importance-sampling",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "Multiple Proposal Distributions with Multiple Importance Sampling",
    "text": "Multiple Proposal Distributions with Multiple Importance Sampling\nWhy stop at just one proposal distribution? This is basically the jumping off point for Multiple Importance sampling, or MIS. If we have several different q(x), and each does a somewhat better job of handling a certain region of the target posterior, then we can efficiently combine them using MIS into an overall better final estimate, and this will work out to be pretty obviously more optimal than just fitting a bunch of VI approximations and averaging them.\nIf we can suddenly have multiple different q(x) working together, this naturally explodes the search space for a good VI strategy. I’d refer the more interested reader to Elvira et al. (2019) which lays out a framework for thinking about all the decision space of MIS more comprehensively, but for the purposes of improving VI specifically, I’ll cover:\n\nHow do we weight the proposals together?\nWhich proposals make sense to include in a MIS framework?\nHow practical is fitting multiple proposals?\n\n\nHow do MIS weights work?\nHow do we generalize a notion of importance weights like the one introduced above:\n\nw_i = \\frac{p(x_i)}{q(x_i)}\n\nto multiple proposals? While there are some obviously not good properties we want to avoid (it’d be pretty silly to give up our unbiasedness), there are a ton of apparent degrees of freedom in MIS weighting. we’ll relax this assumption in a bit, but let’s start by assuming we don’t have any prior information about which proposals might be better, and that we’ll draw the same number of samples from each proposal.\nWhile I won’t work through as extensive of an example as in the last section, let’s fix an example where we’ll have J = 3 different proposals, q_1(x)), q_2(x), and q_3(x)).\nA first question is how to choose the denominator in the weight. One simple and efficient option is to simply use the density of a sample from j to make a weight, for example weighting a draw from q_3(x)) as:\n\nw_{i} = \\frac{p(x_i)}{\\textcolor{pink}{q_3(x_i)}}\n This works, and is pretty common in MIS applications, but we’re not really using all the information we have from having several proposals. We can get a provably lower variance estimator by defining the mixture of the densities \\psi(x) as:\n\n\\psi(x) = \\frac{1}{J} \\sum\\limits_{J = 1}\\limits^{J} q_j(x)\n\nand using that as the denominator. So for the example above, this’d be:\n\nw_{i} = \\frac{p(x_i)}{\\frac{1}{3}(\\textcolor{cyan}{q_1(x)} + \\textcolor{purple}{q_2(x_i)} + \\textcolor{pink}{q_3(x_i))}}\n By defining this mixture and and incorporating it into our weighting, we intuitively should have more efficient exchange of information between the different q(x). By this, I mean that we no longer just are weighting each sample from a proposal using information from that one proposal; we’re now using everything at hand.\nThis feels like it should be pretty solidly better than just using a single proposal density, and indeed Elvira et al. have a result showing that the variance of the mixture based weighting scheme is under pretty general conditions lesser than or equal to that of the single proposal density one5.\nVeach and Guibas (1995), the paper to introduce MIS, called this weighting scheme the balance heuristic, since the weighting scheme is unique in that each sample value at particular x is the same regardless of which distribution produced it. They also prove a bound on the variance of this estimator, showing that there isn’t a lot of room to improve on it, even in the most ideal circumstances. Without getting into the weeds, their result suggests that there isn’t a massively better general-case weighting scheme, which is a helpful guide to practical use.\nWhen can we do (a bit) better than the weighting scheme above? The answer is essentially in cases where we know some of our J proposals are much better than others. In these situations, the variance can often be lowered by pushing weights towards the extremes, making low weights closer to zero, and high weights closer to 1. Their cutoff heuristic suggests an estimator where you pick some bound \\alpha, below which low weights are reassigned to zero (and the rest of the distribution is adjusted back to sum correctly). Their also propose the power heuristic-\n\nw_i = \\frac{p_i^\\beta}{\\sum\\limits_{j}p_j^\\beta}\n which raises the weights to a power \\beta, and normalizes. For intuition, notice that if \\beta = 1, then this is the balance heuristic again, and as \\beta \\rightarrow \\infty, this moves towards only selecting the best proposal at each point.\nAs a final note, we can also Pareto Smooth any of these types of weights once we have them, and this sparks joy, as we can add begin to envision model setups with glorious abbreviations like IW-ELBO/IW-CUBO-PSIS-MIS-VI.\nSo stepping back, we have some provably efficient, provably hard to beat ways to use MIS to combine variational approximations together. Again, there’s a whole literature on MIS which the Elvira paper above reviews, but fairly intuitive weighting schemes exist that work well in most cases, and there are reasonable things to try in more atypical cases to reduce the variance of the MIS estimator as well.\n\n\nWhat proposals combine best?\nA next natural question is what different proposals should we use? There’s a little less work in this area than I expected, but there are a couple of papers; my favorite is Lopez et al. (2020)6.\nThey find that using VI approximations based on different objectives is quite performant- for example, having all of a vanilla ELBO, IW-ELBO and \\chi^2 divergence based VI approximation works particularly well, and as you’d expect, better than any individual model, just like we’d expect with regular ensembling techniques. They also look at taking some samples directly from our priors, which is moderately surprising to me given how broad weakly-informative priors usually are. Overall though, a core nugget of logic from ensembling more generally applies here too: we want to find proposals that are both good and sufficiently different from one another that combining them adds value.\nIt seems to me there’s a lot of room to explore this search space still; there are a lot of generic ML ensembling tricks that feel like they could work. For example, could we save state several times throughout optimizing a variational approximation, and MIS combine samples from each of those, similar to how people cheaply ensemble for neural networks? Or are there ways to optimize the proposals for use together in this way?\n\n\nHow practical is MIS for VI?\nA last obvious question is whether fitting many variational approximations and combining them is computationally practical. While MIS for VIS certainly trades back some computational cost and time for potential accuracy, the good news is everything feels cheap compared to MCMC.\nFitting J VI approximations instead of 1 roughly scales your compute need for fitting the models by a factor of ~J, and then there’s a small additional cost in the MIS combination stage to evaluate all the models to make each importance sampling weight denominator. Unlike with MCMC, these computational needs are parallelizable.\nLopez et al. (2020) find that using 3 proposals slightly more than triples their compute cost given all the objective based models take around the same time to fit, and in practice slightly more than triples their compute time as well since they didn’t do the work to parallelize their models. On the problems they were working on, this is a pretty small (~30s more) time cost in exchange for a meaningful accuracy improvement in the real world biology application they apply this to.\nDepending on what you’re working on, the answer may well be yes, this can be computationally feasible and well worth it."
  },
  {
    "objectID": "posts/Variational MRP Pt4/variational_mrp_pt4.html#conclusions",
    "href": "posts/Variational MRP Pt4/variational_mrp_pt4.html#conclusions",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "Conclusions",
    "text": "Conclusions\nImportance Sampling is a workhorse of modern computational statistics, and it should be no surprise it brings a lot to variational inference.\nLike with the last post, my overall impression is of decreasing fragility for variational inference and a broader set of tools for increasing performance. With IW-ELBO and similar objectives, we can get a tighter bound than the vanilla ELBO, and introduce some new incentives in training our approximation as well. With importance sampling in general and PSIS especially, we can weight an approximation that is close to the target but not perfect into a much, much better approximation of our posterior, and do some in a principled and theoretically grounded way with built-in diagnostics. With MIS, we can make the most of several approximations at once, if we’re willing to pay that computational cost. Collectively, we’re building up a set of tools that broaden the class of problems for which VI works, provided you’re willing to spend time searching for a combination of tools that works well for your specific application.\nThanks for reading. In the next post, we’ll look at Normalizing Flows, an incredibly powerful and general tool for making maximally flexible variational distributions. All code for this post can be found here."
  },
  {
    "objectID": "scratch/python_plotting_tests.html",
    "href": "scratch/python_plotting_tests.html",
    "title": "Python Plotting Tests",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nxlim = ylim = 4\n\ndef density(z):\n  u = 0.5 * ((torch.norm(z, 2, dim=1) - 2) / 0.4) ** 2\n  u = u - torch.log(\n      torch.exp(-0.5 * ((z[:, 0] - 2) / 0.6) ** 2)\n      + torch.exp(-0.5 * ((z[:, 0] + 2) / 0.6) ** 2)\n  )\n  return u\n\nx = y = np.linspace(-xlim, xlim, 300)\nX, Y = np.meshgrid(x, y)\nshape = X.shape\nX_flatten, Y_flatten = np.reshape(X, (-1, 1)), np.reshape(Y, (-1, 1))\nZ = torch.from_numpy(np.concatenate([X_flatten, Y_flatten], 1))\n\nu = 0.5 * ((torch.norm(Z, 2, dim=1) - 2) / 0.4) ** 2\nu = u - torch.log(\n      torch.exp(-0.5 * ((Z[:, 0] - 2) / 0.6) ** 2)\n      + torch.exp(-0.5 * ((Z[:, 0] + 2) / 0.6) ** 2))\n\nU = torch.exp(-density(Z))\nU = U.reshape(shape)\n\nU.size()\nU\n\nx\n\n\ndef U_1(z):\n  u = 0.5 * ((torch.norm(z, 2, dim=1) - 2) / 0.4) ** 2\n  u = u - torch.log(\n      torch.exp(-0.5 * ((z[:, 0] - 2) / 0.6) ** 2)\n      + torch.exp(-0.5 * ((z[:, 0] + 2) / 0.6) ** 2)\n  )\n  return u\n\ndef plot_density(density, xlim=4, ylim=4, ax=None, cmap=\"Purples\"):\n  x = y = np.linspace(-xlim, xlim, 300)\n  X, Y = np.meshgrid(x, y)\n  shape = X.shape\n  X_flatten, Y_flatten = np.reshape(X, (-1, 1)), np.reshape(Y, (-1, 1))\n  Z = torch.from_numpy(np.concatenate([X_flatten, Y_flatten], 1))\n  U = torch.exp(-density(Z))\n  U = U.reshape(shape)\n  if ax is None:\n      fig = plt.figure(figsize=(7, 7))\n      ax = fig.add_subplot(111)\n  \n  ax.set_xlim(-xlim, xlim)\n  ax.set_ylim(-xlim, xlim)\n  ax.set_aspect(1)\n  \n  ax.pcolormesh(X, Y, U, cmap=cmap, rasterized=True)\n  ax.tick_params(\n      axis=\"both\",\n      left=False,\n      top=False,\n      right=False,\n      bottom=False,\n      labelleft=False,\n      labeltop=False,\n      labelright=False,\n      labelbottom=False,\n  )\n  return ax\n\nplot_density(U_1)\n\n\ndef U_1(z):\n                u = 0.5 * ((torch.norm(z, 2, dim=1) - 2) / 0.4) ** 2\n                u = u - torch.log(\n                    torch.exp(-0.5 * ((z[:, 0] - 2) / 0.6) ** 2)\n                    + torch.exp(-0.5 * ((z[:, 0] + 2) / 0.6) ** 2)\n                )\n                return u\n              \ntest = U_1(torch.randn(128,2, 2))\n\ntest\n\n#torch.randn(2, 2)\n\n\nimport torch\n\ndef interpolate_tensor(tensor, coords):\n    # Get the dimensions of the tensor\n    height, width = tensor.shape\n\n    # Separate x and y coordinates from the input tensor\n    x = coords[:, 0]\n    y = coords[:, 1]\n\n    # Calculate the indices of the four surrounding elements\n    x1 = x.floor().clamp(max=width - 1).long()\n    x2 = x1 + 1\n    y1 = y.floor().clamp(max=height - 1).long()\n    y2 = y1 + 1\n\n    # Calculate the weight for interpolation\n    weight_x2 = x - x1.float()\n    weight_x1 = 1 - weight_x2\n    weight_y2 = y - y1.float()\n    weight_y1 = 1 - weight_y2\n\n    # Perform interpolation\n    value = (\n        tensor[y1.clamp(max=height - 1), x1.clamp(max=width - 1)] * weight_x1 * weight_y1 +\n        tensor[y1.clamp(max=height - 1), x2.clamp(max=width - 1)] * weight_x2 * weight_y1 +\n        tensor[y2.clamp(max=height - 1), x1.clamp(max=width - 1)] * weight_x1 * weight_y2 +\n        tensor[y2.clamp(max=height - 1), x2.clamp(max=width - 1)] * weight_x2 * weight_y2\n    )\n\n    return value\n\n# Create a sample PyTorch tensor\ntensor = torch.tensor([[1, 2, 3],\n                       [4, 5, 6],\n                       [7, 8, 9]])\n\n# Create a tensor with x and y coordinates\ncoords = torch.tensor([[1.5, 2.2],\n                       [0.2, 1.7]])\n\n# Interpolate at continuous coordinates\ninterpolated_values = interpolate_tensor(tensor, coords)\n\nprint(interpolated_values)\n\n\nunique_values, counts = torch.unique(torch_posterior, return_counts=True)\n\n# Print the unique values and their counts\nfor value, count in zip(unique_values, counts):\n    print(f\"Value: {value}, Count: {count}\")\n\n\nfig, ax = plt.subplots()\n\n# Plot the tensor values\nax.imshow(torch_posterior.cpu(), cmap='viridis')\n\n# Show the colorbar\nplt.colorbar()\n\n# Show the plot\nplt.show()\n\n\nbatch = torch.zeros(size=(batch_size, 2)).normal_(mean=0, std=1)\nbatch2 = torch.zeros(size=(batch_size, 2)).normal_(mean=150, std=30)\n\ndef two_moons_density(z):\n  x = z[:, 0]\n  y = z[:, 1]\n  d = torch.sqrt(x**2 + y**2)\n  density = torch.exp(-0.2 * d) * torch.cos(4 * np.pi * d)\n  return density\n\ndef ring_density(z):\n    exp1 = torch.exp(-0.5 * ((z[:, 0] - 2) / 0.8) ** 2)\n    exp2 = torch.exp(-0.5 * ((z[:, 0] + 2) / 0.8) ** 2)\n    u = 0.5 * ((torch.norm(z, 2, dim=1) - 4) / 0.4) ** 2\n    u = u - torch.log(exp1 + exp2 + 1e-6)\n    return u\n\ntwo_moons_density(batch).min()\nring_density(batch).min()\nring_density(batch).max()\n\n\ndef interpolate_tensor(tensor, z):\n    # Get the dimensions of the tensor\n    height, width = tensor.shape[:2]\n\n    # Scale and shift the normal draws to match the image coordinates\n    x = (z[:, 0] * 150 + 150).clamp(0, width - 1).long()\n    y = (z[:, 1] * 150 + 150).clamp(0, height - 1).long()\n\n    # Calculate the indices of the four surrounding elements\n    x1 = x.floor()\n    x2 = x1 + 1\n    y1 = y.floor()\n    y2 = y1 + 1\n\n    # Calculate the weight for interpolation\n    weight_x2 = x - x1.float()\n    weight_x1 = 1 - weight_x2\n    weight_y2 = y - y1.float()\n    weight_y1 = 1 - weight_y2\n\n    # Perform interpolation\n    value = (\n        tensor[y1.clamp(max=height - 1), x1.clamp(max=width - 1)] * weight_x1 * weight_y1 +\n        tensor[y1.clamp(max=height - 1), x2.clamp(max=width - 1)] * weight_x2 * weight_y1 +\n        tensor[y2.clamp(max=height - 1), x1.clamp(max=width - 1)] * weight_x1 * weight_y2 +\n        tensor[y2.clamp(max=height - 1), x2.clamp(max=width - 1)] * weight_x2 * weight_y2\n    )\n\n    return value\n  \nbatch = torch.zeros(size=(batch_size, 2)).normal_(mean=0, std=1)\ninterpolate_tensor(torch_posterior, batch)\n\n#ring_density(batch)\n\n\n#ring_density(batch)\n#interpolate_tensor(torch_posterior,batch2)\nbatch2\n\n\nfig, ax = plt.subplots()\n\n# Plot the tensor values\nax.imshow(ring_density(batch).cpu(), cmap='viridis')\n\n# Show the colorbar\nplt.colorbar()\n\n# Show the plot\nplt.show()\n\n\ninterpolate_tensor(torch_posterior, torch.tensor([[81,156.5]]))\n\ntorch_posterior[81,157]\n\nindices = torch.nonzero(torch_posterior != 0)\n\nlen(indices)\n\n# Print the pairs of indices\n#for idx in indices:\n#    i, j = idx\n#    print(f\"Pair of indices: ({i}, {j})\")\n\n\ntest = MultivariateNormal(torch.zeros(2), torch.eye(2))\n\ntest.sample()\n\n\nimport os\nimport imageio\n\n\ndef make_gif_from_train_plots(fname: str) -> None:\n  # Hiding the directory when commiting, but easy to infer rihgt path\n    png_dir = \"\"\n    images = []\n    sort = sorted(os.listdir(png_dir))\n    for file_name in sort[1::1]:\n        if file_name.endswith(\".png\"):\n            file_path = os.path.join(png_dir, file_name)\n            images.append(imageio.imread(file_path))\n\n    imageio.mimsave(\"gifs/\" + fname, images, duration=0.05)\n    \nmake_gif_from_train_plots(\"32_layer.gif\")"
  },
  {
    "objectID": "posts/Variational MRP Pt5/variational_mrp_pt5.html",
    "href": "posts/Variational MRP Pt5/variational_mrp_pt5.html",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "",
    "text": "This is section 5 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.\nThe general structure for this post and the posts around it will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in these four posts will go a long way towards more robust variational inference. I’ll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.\nIn the last post we saw a variety of different ways importance sampling can be used to improve VI and make it more robust, from defining a tighter bound to optimize in the importance weighted ELBO, to weighting q(x) samples together efficiently to look more like p(x), to combining entirely different variational approximations together to cover different parts of the posterior with multiple importance sampling.\nIn this post, we’ll tackle the problem of how to define a deeply flexible variational family \\mathscr{Q} that can adapt to each problem while still being easy to sample from. To do this, we’ll draw on normalizing flows, a technique for defining a composition of invertible transformations on top of a simple base distribution like a normal distribution. We’ll build our way up to using increasingly complex neural networks to define those transformations, allowing for for truly complex variational families that are problem adaptive, training as we train our variational model.\nThe rough plan for the series is as follows:"
  },
  {
    "objectID": "posts/Variational MRP Pt5/variational_mrp_pt5.html#normalizing-flows-for-variational-inference-versus-other-applications",
    "href": "posts/Variational MRP Pt5/variational_mrp_pt5.html#normalizing-flows-for-variational-inference-versus-other-applications",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "Normalizing flows for variational inference versus other applications",
    "text": "Normalizing flows for variational inference versus other applications\nOne source of confusion when I was learning about normalizing flows for variational inference was that variational inference makes up a fairly small proportion of use cases, and thus the academic literature and online discussion. More common applications include density estimation, image generation, representation learning, and reinforcement learning. In addition to making specifically applicable discussions harder to find, often resources will make strong claims about properties of a given flow structure, that really only holding in some subset of the above applications4.\nBy taking a second to explain this crisply and compare different application’s needs, hopefully I can save you some confusion and make engaging with the broader literature easier.\nTo start, consider the relevant operations we’ve introduced so far:\n\ncomputing f, that is pushing a sample through the transformations\ncomputing g, f’s inverse which undoes the manipulations\ncomputing the (log) determinant of the Jacobian\n\n1 and 3 definitely need to be efficient for our use case, since we need to be able to sample and push through using the formula above efficiently to calculate an ELBO and train our model. 2 is where things get more subtle: we definitely need f to be invertible, since our formulas above are dependent on a property of Jacobians of invertible functions. But we don’t actually really need to explicitly compute g for variational inference. Even knowing the inverse exists but not having a formula might be fine for us!\nContrast this with density estimation, where the goal would not to sample from the distribution, but instead to estimate the density. In this case, most of the time would be spent going in the opposite direction, so that they can evaluate the log-likliehood of the data, and maximize it to improve the model5. The need for an expressive transformation of densities unite these two cases, but the goal is quite different!\nThis level of goal disagreement also shows it face in what direction papers choose to call forward: Most papers outside of variational inference applications consider forward to be the opposite of what I do here. For them, “forward” is the direction towards the base density, the normalizing direction.\nFor our use, hopefully this short digression has clarified which operations we need to be fast versus just exist. If you dive deeper into further work on normalizing flows, hopefully recognizing there are two different ways to consider forward helps you more quickly orient yourself to how other work describes flows."
  },
  {
    "objectID": "posts/Variational MRP Pt6/variational_mrp_pt6.html",
    "href": "posts/Variational MRP Pt6/variational_mrp_pt6.html",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "",
    "text": "This is section 6 in my series on using Variational Inference to speed up relatively complex Bayesian models like Multilevel Regression and Poststratification without the approximation being of disastrously poor quality.\nThe general structure for this post and the posts around it will be to describe a problem with VI, and then describe how that problem can be fixed to some degree. Collectively, all the small improvements in these four posts will go a long way towards more robust variational inference. I’ll also have a grab bag at the end of other interesting ideas from the literature I think are cool, but maybe not as important or interesting to me as the 3 below.\nIn the last post we looked at normalizing flows, a way to leverage neural networks to learn significantly more expressive variational families in a way that adapt to specific problems.\nIn this post, we’ll explore different diagnostics for variational inference, ranging from simple statistics that are easy to calculate as we fit our approximation to solving the problem in parallel with MCMC to compare and contrast. Some recurring themes will be aiming to be precise about what constitutes failure under each diagnostic tool, and providing intuition building examples where each diagnostic will fail to do anything useful. While no single diagnostic provides strong guarantees of variational inference’s correctness on their own, taken together the tools in this post broaden our ability to know when our models fall short.\nThe rough plan for the series is as follows:"
  },
  {
    "objectID": "posts/Variational MRP Pt6/variational_mrp_pt6.html#problem-case-1-importance-sampling-neq-direct-variational-inference",
    "href": "posts/Variational MRP Pt6/variational_mrp_pt6.html#problem-case-1-importance-sampling-neq-direct-variational-inference",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "Problem Case 1: Importance sampling \\neq direct variational inference",
    "text": "Problem Case 1: Importance sampling \\neq direct variational inference\nWe should keep in mind that \\hat{k} is ultimately a diagnostic tool for importance sampling, and in cases where the needs of importance sampling and simple variational inference diverge, \\hat{k} can give a misleading answer.\nLet’s re-use an example from the importance sampling post to illustrate this. What happens if we approximate the red distribution below with the green one?\n\n\n\n\n\n\n\nmixture %>% ggplot(aes(x = normals)) +\n  geom_density(aes(x = normals), color = \"red\") +\n  geom_density(aes(x = mean_seeking_kl), color = \"green\") + ggtitle(\"The green approxmiation is great for IS, terrible on it's own\") +\n  xlab(\"\")\n\n\n\n\nThe green distribution here is a prime candidate to importance sample to approximate the red one- it coves all the needed mass, and we can massively down weight the irrelevant points in the center. On the other hand, this’d be a really, really bad variational approximation to use raw, since it has a ton of mass between the two modes which will blow up our loss. Because the needs of PSIS-based estimators and unadjusted VI diverge, \\hat{k} is low, but the approximation would be pretty bad:\n\nimportance_ratios <- tibble(\nq_x = rnorm(200000,9,4),\np_x = c(rnorm(100000,3,1),rnorm(100000,15,2)),\nratios = (.5*(dnorm(q_x,3,1)) + .5*(dnorm(q_x,15,2)))/dnorm(q_x,9,4))\n\npsis_result <- psis(log(importance_ratios$ratios),\n                       r_eff = NA)\n\npsis_result$diagnostics$pareto_k\n\n[1] -1.737515\n\n\nSo our \\hat{k} says everything is beautiful, but in reality it’s really only a happy time for PSIS, not the raw VI estimator. This ultimately isn’t the most concerning failure mode: if you do the work to calculate \\hat{k}, you’re pretty much ready to use PSIS to improve your variational inference anyway. That said, this should provide intuition that \\hat{k} isn’t in general super well equipped to tell you much about non-IS augmented VI."
  },
  {
    "objectID": "posts/Variational MRP Pt6/variational_mrp_pt6.html#problem-case-2-hatk-is-a-local-diagnostic",
    "href": "posts/Variational MRP Pt6/variational_mrp_pt6.html#problem-case-2-hatk-is-a-local-diagnostic",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "Problem Case 2: \\hat{k} is a local diagnostic",
    "text": "Problem Case 2: \\hat{k} is a local diagnostic\n\\hat{k} inherits a common issue with most KL Divergence adjacent metrics: it’s ultimately something we evaluate locally, so if there’s a part of the posterior totally unknown to our q(x), it won’t be able to tell you what you’re missing.\nWe already used 1 example from the importance sampling post, so let’s keep that moving. What do you think will happen with \\hat{k} with the green approximation below that misses a whole mode?\n\nmixture %>% ggplot(aes(x = normals)) +\n  geom_density(aes(x = normals), color = \"red\") +\n  geom_density(aes(x = mode_seeking_kl), color = \"green\") + ggtitle(\"We're missing a whole mode here\") +\n  xlab(\"\")\n\n\n\n\nIf you guessed \\hat{k} will say everything is perfect when it’s not, you’re correct:\n\nsecond_importance_ratios <- tibble(\nq_x = rnorm(200000,3.5,1),\np_x = c(rnorm(100000,3,1),rnorm(100000,15,2)),\n# Notice: these density calls are at the points defined by q(x)!\nratios = (.5*(dnorm(q_x,3,1)) + .5*(dnorm(q_x,15,2)))/dnorm(q_x,3.5,1))\n\npsis_result_2 <- psis(log(second_importance_ratios$ratios),\n                       r_eff = NA)\n\npsis_result_2$diagnostics$pareto_k\n\n[1] 0.07343881\n\n\nThat’s… not great. Since we evaluate the importance ratio and thus eventually \\hat{k} at the collection of values in q(x), the diagnostic has no real way to know we’re missing an entire mode, and unlike in the above case there’s no easy fix here.\nAnother interesting question this example raises is what happens in high dimensions, where it’s much less intuitive what “missing one or several modes” looks like. Just by increasing the sd of the normal q(x) a little in the example, we see a sudden, large increase in \\hat{k};\n\nthird_importance_ratios <- tibble(\nq_x = rnorm(200000,3.5,2),\np_x = c(rnorm(100000,3,1),rnorm(100000,15,2)),\nratios = (.5*(dnorm(q_x,3,1)) + .5*(dnorm(q_x,15,2)))/dnorm(q_x,3.5,2))\n\npsis_result_3 <- psis(log(third_importance_ratios$ratios),\n                       r_eff = NA)\n\nWarning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.\n\npsis_result_3$diagnostics$pareto_k\n\n[1] 3.70381\n\n\nsimilar sudden shifts in \\hat{k} can frequently occur as you increase the dimension of a posterior you’re approximating- intuitively, the mass you do and don’t know about becomes much harder to keep track of in high dimensions and for complex posteriors. This can lead to \\hat{k} being a bit less stable than you’d like over different initializations or other slight modifications of a VI model, with this pattern being common both in my own applications and documented in several papers like Wang et al. (2023)’s testing."
  },
  {
    "objectID": "posts/Variational MRP Pt6/variational_mrp_pt6.html#problem-case-3-hatk-is-a-joint-posterior-level-tool",
    "href": "posts/Variational MRP Pt6/variational_mrp_pt6.html#problem-case-3-hatk-is-a-joint-posterior-level-tool",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "Problem Case 3: \\hat{k} is a joint posterior level tool",
    "text": "Problem Case 3: \\hat{k} is a joint posterior level tool\nA final, more conceptual problem with \\hat{k} that Yao et al. (2018) point out is that it’s ultimately a diagnostic of the joint posterior, not the specific marginal or summary statistic you may ultimately care about.\nVariational inference is hard: we often know that the overall posterior approximation is deeply flawed, but it may be up to the task of representing some metrics we care about correctly enough. For example, in the MRP example I introduced earlier in the series, the mean-field variational inference fit was reasonable at representing the state-level means, but garbage at pretty much anything related to uncertainty. The \\hat{k} from that model was greater than 2, so we clearly know the broader posterior approximation was poor, but \\hat{k} might be a false positive sign if what you really care about was just the means. For the most complicated posteriors, we should expect to spend a lot of time in this feeling of “some parts of the posterior may be good enough”, so this is a useful trap to know about.\n…Let’s step back for a second. Since I introduced \\hat{k} as a diagnostic with a bunch of cases where it falls short in surprising ways, I do want to emphasize it is a very useful heuristic diagnostic tool in general. Large \\hat{k} tells you something is very likely wrong with your joint posterior, and that’s generally practically helpful information. Where we need to be cautious is in inferring whether the wrongness \\hat{k} picks up on is something we care about, and also in remembering that low \\hat{k} doesn’t provide guarantees of correctness."
  },
  {
    "objectID": "posts/Variational MRP Pt6/variational_mrp_pt6.html#whats-a-wassterstein",
    "href": "posts/Variational MRP Pt6/variational_mrp_pt6.html#whats-a-wassterstein",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "What’s a Wassterstein?",
    "text": "What’s a Wassterstein?\nThe p-Wasserstein distance between \\xi and \\pi is \n\\mathcal{W}_p(\\xi, p):=\\inf _{\\gamma \\in \\Gamma(\\xi, p)}\\left\\{\\int\\left\\|\\theta-\\theta^{\\prime}\\right\\|_2^p \\gamma\\left(\\mathrm{d} \\theta, \\mathrm{d} \\theta^{\\prime}\\right)\\right\\}^{1 / p}\n where \\Gamma(\\xi, p) is the set of couplings between \\xi and \\pi.\nAs a quick note on notation, I’m overloading p here a bit; given I’ve used p for our target posterior all series, I’m not going to switch that now, and calling it anything other than a p-Wasserstein distance would just be confusing to anyone who’se seen this distance before.\nThis looks a lot more involved than something like the KL Divergence. For example, the fact that we have an infinum over something complicated looking suggests this’ll be a real pain to calculate. As we’ll see in a second, Huggins et al. don’t actually seek to calculate it or approximate it, they seek to bound it 2.\nBefore we get there though, let’s seek to understand the distance and it’s properties a little better.\nA good way to start unpacking this is to consider the optimal transport problem. Given some probability mass \\xi(\\theta) on a space X, we wish to transport it such that it is transformed into the distribution p(\\theta). To provide physical intuition, this is often formulated as a problem of moving an equal amount of dirt/earth in pile \\xi(\\theta) to make pile \\pi(\\theta)- hence the name commonly used in several disciplines, the Earthmovers Distance.\nLet’s say we have some non-negative cost function for moving mass from \\theta to \\theta^{\\prime}, c(\\theta,\\theta^{\\prime}). A single transport plan for moving from \\xi(\\theta) to p(\\theta^{\\prime}) is a function \\gamma(\\xi, \\pi) which describes the amount of mass to move at each point. If we assume \\gamma is a valid joint probability mass with marginals \\xi\\theta) and \\pi(\\theta^{\\prime}) 3, then the infinitesimal mass we transport from \\theta to \\theta{\\prime} is \\gamma(\\theta, \\theta^{\\prime}) d\\theta d\\theta^{\\prime}, with cost\n\n\\int \\int c(\\theta,\\theta^{\\prime}) \\gamma(\\theta, \\theta^{\\prime}) d\\theta d\\theta^{\\prime} = \\int c(\\theta,\\theta^{\\prime}) d \\gamma(\\theta,\\theta^{\\prime})\n\nFinally getting close to something that looks like our Wasserstein distance. There are many such plans, but the one we want, the solution to the optimal transport problem, is the one with minimal cost out of all such plans.\nOne last point to cover to define this: what’s our cost? If the cost here is the p-distance between our \\thetas, then this is the p-Wassterstein distance.\nWhat are some properties of this distance? I already mentioned a major downside (this looks nasty to estimate in general, and indeed it is). What are the upsides of this?\nUnlike the KL or \\chi^2 divergences we’ve looked at before, the Wasserstein distance takes into account the metric on the underlying space! Let’s unpack that by again drawing on the optimal transport problem for intuition. The Wasserstein distance takes into account not only the differences in the values or probabilities assigned to different points in the distributions but also the actual “spatial”4 arrangement of those points.\nThis is a incredibly useful property because the summaries of the posterior we care about in general also rely on the underlying metric. This is basically how the arbitrarily poor mean and variance examples above work; they exploit the lack of use of an underlying metric. That allows Huggins et al. to derive one of the key results of the paper\n\n\n\n\n\n\nTheorem 3.4. If \\mathcal{W}_1(q, p) \\leq \\varepsilon or \\mathcal{W}_2(q, p) \\leq \\varepsilon, then \\left\\|m_{q}-m_p\\right\\|_2 \\leq \\varepsilon and \\max _i\\left|\\mathrm{MAD}_{q, i}-\\mathrm{MAD}_{p, i}\\right| \\leq 2 \\varepsilon.\n\n\nIf \\mathcal{W}_2(q, p) \\leq \\varepsilon, then for S := \\sqrt{min (\\left\\|\\Sigma_{q}\\right\\|_2, \\left\\|\\Sigma_p\\right\\|_2)}, \\max _i\\left|\\sigma_{q, i}-\\sigma_{p, i}\\right| \\leq \\varepsilon and \\left\\|\\Sigma_{q}-\\Sigma_p\\right\\|_2<2 \\varepsilon(S+\\varepsilon).\n\n\n\nA similar type of result holds for the difference between expectations of any smooth function, so this result is somewhat extensible with additional work.\nThis is a nice improvement over the KL or \\chi^2 divergences as far as an a diagnostic, since we have some guarantees of correctness where we had literally none. I’ll return to how tight these are bounds in practice in a bit, since that’s entangled with how we can actually estimate them in the variational inference use case."
  },
  {
    "objectID": "posts/Variational MRP Pt6/variational_mrp_pt6.html#bounds-of-bounds-via-bounds",
    "href": "posts/Variational MRP Pt6/variational_mrp_pt6.html#bounds-of-bounds-via-bounds",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "Bounds of Bounds via… Bounds!",
    "text": "Bounds of Bounds via… Bounds!\nOne contribution of the Huggins at al. paper is the above result, but where things get even more impressive is that they find a reasonable and practical way to bound these quantities. It’s certainly not simple, but it works.\nHere’s the plan to get real bounds on our posterior summaries in full:\n\nUse the ELBO and CUBO to to bound the KL and \\chi^2 divergences.\nUse tail properties of the distribution q to get bounds on the Wasserstein distance through the KL and \\chi^2 divergences.\nFinally, bound posterior summaries using the Wasserstein bounds.\n\nThat’s a lot of layers of bounding, and it’s reasonable to wonder why this is needed and whether the bounds are usefully tight after such transformations. One key reason this type of bounding is so involved is that we’re using a set of scale-invariant distances to bound a scale-dependent one- we need to incorporate some notion of scale into the bounding process to make it work.\nTo do this, define the moment constants C_p^{\\mathrm{PI}}(\\xi) and C_p^{\\mathrm{EI}}(\\xi). For p \\geq 1, \\xi is p-polynomially integrable if \nC_p^{\\mathrm{PI}}(\\xi):=2 \\inf _{\\theta_0}\\left\\{\\int\\left\\|\\theta-\\theta_0\\right\\|_2^p \\xi(\\mathrm{d} \\theta)\\right\\}^{\\frac{1}{p}}<\\infty\n and that \\xi is p-exponentially integrable if \nC_p^{\\mathrm{EI}}(\\xi):=2 \\inf _{\\theta_0, \\epsilon>0}\\left[\\frac{1}{\\epsilon}\\left\\{\\frac{3}{2}+\\log \\int e^{\\epsilon\\left\\|\\theta-\\theta_0\\right\\|_2^p} \\xi(\\mathrm{d} \\theta)\\right\\}\\right]^{\\frac{1}{p}}<\\infty\n\nNext, with the assumption that the variational approximation q has polynomial (respectively, exponential) tails, our next result provides a bound on the p-Wasserstein distance using the \\chi^2-divergence (respectively, the KL divergence).\nThis is saying we require at least polynomial, and ideally exponential moments for q and p, which isn’t that strenuous of a requirement. Then:\n\n\n\n\n\n\nProposition 4.2. If p is absolutely continuous w.r.t. to q then \n\\mathcal{W}_p(q, p) \\leq C_{2 p}^{\\mathrm{PI}}(q)\\left[\\exp \\left\\{\\chi_2(p \\mid q)\\right\\}-1\\right]^{\\frac{1}{2 p}}\n and \n\\mathcal{W}_p(q, p) \\leq C_p^{\\mathrm{EI}}(q)\\left[\\mathrm{KL}(p \\mid q)^{\\frac{1}{p}}+\\{\\mathrm{KL}(p \\mid q) / 2\\}^{\\frac{1}{2 p}}\\right]\n\n\n\n\nA reasonable question here: does using the KL and \\chi^2 as part of building the bounds inherit KL/\\chi^2’s arbitrarily poor posterior summaries? Nope! I won’t reproduce here, but the counter examples shown above for these divergences on their own no longer work to make our estimates arbitrarily wrong.\nNext step: how do we use the ELBO and CUBO to bound the KL and \\chi^2 terms in the proposition above above?\nWe first define for any distribution \\eta:\n\n\\mathrm{H}_\\alpha(\\xi, \\eta):=\\frac{\\alpha}{\\alpha-1}\\left\\{\\operatorname{CUBO}_\\alpha(\\xi)-\\operatorname{ELBO}(\\eta)\\right\\}\n\nThen we get:\n\n\n\n\n\n\nLemma 4.5. For any distribution \\eta such that p is absolutely continuous w.r.t. \\eta- \n\\mathrm{KL}(p \\mid q) \\leq \\chi_\\alpha(p \\mid q) \\leq \\mathrm{H}_\\alpha(q, \\eta)\n\n\n\n\nBy combining the lemma above and proposition from 4.2 earlier, we can bound the Wasserstein distance finally! To do this, we need all of C_p^{\\mathrm{PI}}(\\xi), C_p^{\\mathrm{EI}}(\\xi), CUBO, and ELBO. All of these are efficiently calculable much of the time (we’ll get to when it’s not soon), and largely result from things we were already calculating as promised. Great! Our combined result:\n\nTheorem 4.6. For any p \\geq 1 and any distribution \\eta, if p is absolutely continuous w.r.t. q, then \n\\mathcal{W}_p(q, p) \\leq C_{2 p}^{\\mathrm{PI}}(q)\\left[\\exp \\left\\{\\mathrm{H}_2(q, \\eta)\\right\\}-1\\right]^{\\frac{1}{2 p}}\n and \n\\mathcal{W}_p(q, p) \\leq C_p^{\\mathrm{EI}}(q)\\left[\\mathrm{H}_2(q, \\eta)^{\\frac{1}{p}}+\\left\\{\\mathrm{H}_2(q, \\eta) / 2\\right\\}^{\\frac{1}{2 p}}\\right] .\n\n\nThis basically completes the process, other than some discussion of how to actually compute the quantities needed for the bounds above. The ELBO and CUBO we can compute as introduced previously in the series.\nThey have a good practical suggestion for a workflow given their bounds rely on CUBO, and thus on Monte Carlo estimation5. Before proceeding any further with a VI approximation, they suggest using \\hat{k} < .7 as an initial diagnostic of the approximation, before calculating their bounds or leveraging importance sampling or PSIS to improve the estimate. Their point is that if \\hat{k} is high, the Monte Carlo work involved in generating their bounds will be unreliable at reasonable sample sizes, and thus you won’t be able to gaurantee the bounds are useful.\nA final computational point you may be wondering about is how to calculate the moment constants, C_p^{\\mathrm{PI}}(\\xi) and C_p^{\\mathrm{EI}}(\\xi). They provide a helpful example showing how to do this when q is a T distribution, and so the moments used are analytically calculable. What about when this isn’t possible? They suggest one can reasonably do this by fixing \\epsilon and \\theta_0 (for example, setting \\theta_0 at the mean of the distribution), and sampling from q, which seems reasonable on a bit of reflection. The main reason this is worth bringing up: you don’t need a q with easy to calculate moments to make this work, which was a worry when I first saw the moment constants.\nSo the final, combined workflow6 they suggest is:\n\nThis was a very, very long derivation, but hopefully walking through why we would want a distance with a sense of scale and how to calculate it helped build your intuition around variational inference."
  },
  {
    "objectID": "posts/Variational MRP Pt6/variational_mrp_pt6.html#so-whats-the-bad-news-about-this-diagnostic",
    "href": "posts/Variational MRP Pt6/variational_mrp_pt6.html#so-whats-the-bad-news-about-this-diagnostic",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "So what’s the bad news about this diagnostic?",
    "text": "So what’s the bad news about this diagnostic?\nWhile these bounds are genuinely useful, let’s talk through some caveats and limitations of this diagnostic tool.\nFirst, the bound really only is trustworthy when we can reliably estimate the CUBO, as I discussed above. Fortunately, as Higgins et al. note, we have an affordable way to check this in \\hat{k}. Of course, we then take on the responsibility of finding a solution where we can trust \\hat{k}, one which doesn’t fall into any of the blind spots the algorithm has that I discussed above. If we want to leverage the bounds, we’re also sort of forced to find a variational family that works well with the CUBO bound, even if an ELBO-based solution might work better. None of this is insurmountable, and much of the time my experience is that you probably need to change your variational family anyway if the \\hat{k} for the CUBO optimized approximation is greater than .7.\nThe second issue here is tightness of the bounds. As you might expect, the whole “bounds via bounds of bounds” thing can result in fairly loose bounding behavior. Wang et al. (2023) show some informative examples where the bounds are anywhere from 10-1000x (!) too conservative. For example, here’s an example of theirs over Neal’s funnel:\n\nThe W^2 based bounds (stars) here are 10-100x times too large compared to the true values (dashed lines). In my experience, this example and others from the paper aren’t pathological examples- the bounds are frequently this loose, especially for high dimensional and complex posteriors. Exactly what drives the achieved tightness of the bounds is fairly opaque to me; there are several stages of bounds, and it’s not really been possible to pinpoint the source of problems when the bounds are particularly loose.\nHowever, this isn’t to say the bounds aren’t useful, far from it. In practice, especially on variance or covariance parameters, when things go off the rails, they often really go off the rails. If your variational family is nowhere near up to fitting a model, knowing you’re not within a few OOMs of reasonable values can actually be pretty helpful7. Also, these bounds provide some rigorous sense of approximation error where we previously had none, so in that sense this is a big step forward, even if they are loose."
  },
  {
    "objectID": "posts/Variational MRP Pt6/variational_mrp_pt6.html#mcmc-can-be-practically-useful-even-when-its-slow",
    "href": "posts/Variational MRP Pt6/variational_mrp_pt6.html#mcmc-can-be-practically-useful-even-when-its-slow",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "MCMC can be practically useful even when it’s slow",
    "text": "MCMC can be practically useful even when it’s slow\nProbably the most obvious way to use MCMC to sanity check variational inference is to not sanity check every model, just the occasional one. For example, say I was going to fit something like our MRP model to a running poll, and re-run the inferences weekly or daily. We can pretty reasonably make the leap from assuming if the variational approximation compares favorably to MCMC in the first such fit, the subsequent ones will also fit alright given the underlying data does fundamentally change in some way.\nThis is a pretty practical way to get the benefits of VI with the comfort MCMC gives you, as long as MCMC fits in a manageable time window. It’s just important to make sure to set up a realistic process where you spot check the occasional fit along the way, or perhaps retest with MCMC for other questions or shifts in the respondent pool that might plausibly break your model. This is the best strategy I’ve found for validating variational inference so far; even if an MCMC run can take a week or more, as long as it only has to happen once in a while that’s a totally reasonable price of admission."
  },
  {
    "objectID": "posts/Variational MRP Pt6/variational_mrp_pt6.html#giving-mcmc-an-environment-to-succeed",
    "href": "posts/Variational MRP Pt6/variational_mrp_pt6.html#giving-mcmc-an-environment-to-succeed",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "Giving MCMC an environment to succeed",
    "text": "Giving MCMC an environment to succeed\n…But what if you’re using variational inference because MCMC will not finish at all? This is totally possible if you have millions of data points, and/or a complex model to estimate. In this case, you may need to give MCMC a good environment to succeed in.\nA simple way to do this is to subsample the full data if that’s the choking point. For example, if I have a model that I want to use variational inference to fit on millions of rows, I can reasonably infer most of the time that 100k observations will still tell me at least something useful. By fitting the subsampled data with both MCMC and variational inference, we can make sure that at least at that scale the fits align.\nAnother, more challenging, but perhaps more efficient way to test when MCMC is unworkably slow on the full data is through data simulation. By simulating data that contains some of the core features I’m hoping my model will understand, and only simulating a moderate amount of it, I can see if VI can capture those features the same way MCMC can. For example, if I think understanding immigration survey question responses is a complex interaction of race, education, and location, I can simulate data which has the patterns I believe exist, and see if VI does meaningfully worse than VI for that covariance structure in the model. This approach or something close to it is something mentioned by David Shor in several of his talks about Blue Rose Research’s Bayesian models which they scale to hundreds of millions of observations."
  },
  {
    "objectID": "posts/Variational MRP Pt6/variational_mrp_pt6.html#taddaa",
    "href": "posts/Variational MRP Pt6/variational_mrp_pt6.html#taddaa",
    "title": "Variational Inference for MRP with Reliable Posterior Distributions",
    "section": "TADDAA",
    "text": "TADDAA\nA final newer and more efficient way to gut check VI with MCMC is through Wang et al. (2023)’s TArgeted Diagnostic for Distribution Approximation Accuracy algorithm, or TADDAA8. TADDAA provides a relatively compute efficient way to bound the error of VI via MCMC. They have two main motivations for this paper: first, that many existing VI diagnostics penalize approximations that are bad in any way, not necessarily just the ways you care about most (hence, TArgeted). Realistically, for complex models, it’s not a question of if a variational approximations are worse than MCMC, it’s a question of how. Second, they note that the Wasserstein bounds we discussed earlier are often so loose as to be impractical. Again, true.\nThese are both sensible points, but both are really properties of MCMC, not their algorithm, so the real juice in the paper is how they bound the error efficiently. Their strategy is to fit a variational inference model, draw values, and then start many short chains of MCMC at those points. If the variational approximation to the target distribution is good, we shouldn’t expect the MCMC running for a while to change much- the points should already be in highly plausible parts of parameter space. If the approximation is less good, then if the MCMC is setup well, the chains should move towards more correct values. Even if the chains don’t reach stationarity, this can be used to provide a lower bound on the amount of approximation error a variational approximation has.\nBefore I discuss a few technical details, visually the idea is:\n\nIf things aren’t too poorly setup for MCMC, we can reasonably assume that the blue distribution (MCMC modified) will be between the red one (original VI posterior) and the black (true posterior). Neat!\nIn practice, this can work pretty well as you’d expect, and provides much tighter bounds than Wasserstein bounds given it’s a flavor of MCMC. Repeating the plot example I used earlier, notice how much closer the solid lines from the TADDAA bounds are to the dashed ones (ground truth) than the stars (denoting Wasserstein bounds):\n\nI won’t wade too, too far into implementation details here as there a lot of them, but I do want to give enough information to discuss compute cost legibly. A first point worth raising is that “how many chains/iterations do I need” is shown by the paper to be a function of the accuracy you want on the bound- this adaptivity and ability to calculate what number of iterations you need ahead of time is convenient. Second, they’re using a lot of fairly sophisticated techniques in MCMC all together to make this efficient- strong sampling algorithms like MALA/HMC/Barker, preconditioning, and inter-chain adaptation (INCA) to adapt proposal parameters across the chains together. That’s both a great thing (this paper taught me about several new techniques around MCMC I didn’t previously know), and a bad one (to implement this for broader use there is a LOT of work to implement TADDAA efficiently9).\nIn their tests, all of this heavy machinery buys them some fairly impressive speed: implementing TADDAA takes from 2-18% of the gradient evaluations that the actual variational approximation takes. It’s a little opaque to me how that translates into wall time- on the one hand the many little chains are parallizeable, on the other if the number of the iterations is large that’d be the major driver of actual time this takes to run. Still, given it’s never several times VI’s compute needs like more generic MCMC would be in their tests, this seems pretty promising.\nThis paper is only a few months old, so I should be clear I’ve only had a bit of time to digest and play with the algorithm. If a more robust implementation became available, and the computational efficiencies they suggest are real for complex posteriors too, then this will be a fantastic new tool. They are absolutely right on the point that targeted diagnostics are valuable, and it seems like this is a way forward to getting bounds on the most relevant posterior summaries efficiently. As is, the time to set this up isn’t worth the effort versus letting simpler MCMC comparisons run for longer."
  },
  {
    "objectID": "posts/NYSOPM_talk_regularized_raking/NYOSPM_talk.html",
    "href": "posts/NYSOPM_talk_regularized_raking/NYOSPM_talk.html",
    "title": "My talk on Regularized Raking at NYOSP",
    "section": "",
    "text": "I recently gave a talk on Regularized Raking at the New York Open Statistical Programming Meetup.\nHere is the abstract:\n\nRaking is among the most common algorithms for producing survey weights, but it is often opaque what qualities of the resulting weights set are prioritized by the method. This is especially true when practitioners turn to heuristic methods like trimming to improve weights. After reviewing the basic raking algorithm and showing some examples in R, I’ll show that survey weighting can also be understood as an optimization problem, one which allows for explicit regularization. In addition to providing a conceptually crisp view of what (vanilla) raking optimizes for, I’ll show that this regularized raking (implemented via the rsw python package) can allow for more fine-grained control over weights distributions, and ultimately more accurate weighted estimates. Examples will be drawn from US elections surveys.\n\nThe slides and reproduction materials can be found here: https://github.com/andytimm/Regularized-Raking. It looks like the presentation on stream froze for a bit in the middle part of the talk, so you may want to pop the slides open to follow along.\nFor anyone else in the New York area, the meetup is a great group of smart folks working in a bunch of interesting industries- come join us sometime.\nThe recording is below:\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{timm2023,\n  author = {Timm, Andy},\n  title = {My Talk on {Regularized} {Raking} at {NYOSP}},\n  date = {2023-12-05},\n  url = {https://andytimm.github.io/posts/NYSOPM_talk_regularized_raking/NYOSPM_talk.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTimm, Andy. 2023. “My Talk on Regularized Raking at NYOSP.”\nDecember 5, 2023. https://andytimm.github.io/posts/NYSOPM_talk_regularized_raking/NYOSPM_talk.html."
  }
]