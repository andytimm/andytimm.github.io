[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Not Yet Updated Blog",
    "section": "",
    "text": "website\n\n\n\n\nI continue my long search for a way to generate a nicely formatted website with publication list based on adding publication information to a single source of truth without re-remembering how all the formatting works each time.\n\n\n\n\n\n\nMay 11, 2022\n\n\nDrew Dimmery\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "bookshelf.html",
    "href": "bookshelf.html",
    "title": "bookshelf",
    "section": "",
    "text": "![](assets/sept-2022-bookshelf.jpg)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andy Timm",
    "section": "",
    "text": "I’m currently a Manager of Data Science at 605, where I work on problems of observational inference and persuadability modeling, helping our corporate and political clients to better understand the impact of their advertising campaigns and optimize their targeting. Previously, I was a grad student at NYU Gallatin, where I was lucky to have the freedom to study a wide variety of topics in political science, statistics, and data science.\nMethodologically, I am passionate about quantifying uncertainty in observational inference; when we work on the many essential political and business questions that cannot be approached with experiments, how do we rigorously understand the uncertainty and limitations that come with observational methods? Some of my substantive interests include the increasing prominence of white identity in American politics, and the extent to which voting is habit forming.\nBefore grad school, I was a field and data staffer up and down the ballot in Minnesota. When not thinking about politics, I can most often be found running, reading, or playing chess.\nMy aim with this space is to show off some of my larger side projects, especially those that would benefit from some expository writing. I’ll also include occasional career updates.\nThis new website is still very much a WIP. Thanks to Drew Dimmery for starting point for this and post on how to make a quick Quarto website!\n\n    \n\n\n\n    \n\n\n\n    \n\n\n\n    \n\n\n\n\n\n\n\nMy Bookshelf as of September 2022. Hopefully more useful than squinting on zoom!"
  },
  {
    "objectID": "posts/quarto-website/index.html",
    "href": "posts/quarto-website/index.html",
    "title": "Quarto for an Academic Website",
    "section": "",
    "text": "I’ve never been good at keeping my website updated. I always go through two different phases of maintenance:\n\nRushing around creating a new website with bells and whistles using whatever the flavor of the month is\nNever updating an existing website\n\nI’m hoping to break out of this cycle, but am currently solidly within Phase 1.\n\nA highlight from my time in Phase 2 was when I forgot to update my DNS and I totally lost control of drewdimmery.com (don’t go there, it has a squatter). I think my website at that time was some Octopress monstrosity. There are a few reasons I think Quarto might help with my vicious circle.\n\nServing static HTML pages is about as easy as it gets\nVery little Quarto-specific syntax to recall (e.g. CLI commands or abstruse markup)\nLots of flexibility (Python / R) in how to generate that static content\nFull programmability means that generation can be based on arbitrary data structures of my choosing\n\nI previously used Hugo Academic for building my website, which was much better than just editing the content directly, but I never remembered the right way to generate a new publication definition (there was a CLI, but I never remembered the syntax). Each publication got its own file describing its details, and I found this quite clunky. I wanted something extremely lightweight: there isn’t much reason for my individual publications to get pages of their own, and I really don’t need a lot of information on each of them. I just want some basic information about each and a set of appropriate links to more details.\nThis post will detail how I’ve set up Quarto to accomplish this task. I’ve nearly completely separated the two main concerns around maintaining an academic website / CV, which to me are data on publications and software from the design elements of how to display them. It’s entirely possible that my particular issues are unique and this post won’t be useful to anyone else. Luckily, the marginal cost of words on the internet is essentially zero (and maybe the marginal value is, too)."
  },
  {
    "objectID": "posts/quarto-website/index.html#data",
    "href": "posts/quarto-website/index.html#data",
    "title": "Quarto for an Academic Website",
    "section": "Data",
    "text": "Data\nI put data about each publication in a basic YAML format:\n\n\nSee example data\n\nsoftblock:\n  title: Efficient Balanced Treatment Assignments for Experimentation\n  authors:\n    - David Arbour\n    - me\n    - Anup Rao\n  year: 2021\n  venue: AISTATS\n  preprint: https://arxiv.org/abs/2010.11332\n  published_url: https://proceedings.mlr.press/v130/arbour21a.html\n  github: https://github.com/ddimmery/softblock\n\nThis is basically like a simplified bibtex entry with more URLs so I can annotate where to find replication materials for a given paper, as well as distinguish between preprints (always freely accessible) versus published versions (not always open access). A convenience that I add in the markup here is referring to myself as me in the author list (which is an ordered list). This allows me to add in extra post-processing to highlight where I sit in the author list.\nSome additional things I considered adding but chose to ignore for a first version:\n\nAn abstract\nA suggested bibtex entry\n\nBoth of these would be easy to add, but I chose to start simpler. I don’t love YAML for entering long blocks of text, which both of these are."
  },
  {
    "objectID": "posts/quarto-website/index.html#formatting",
    "href": "posts/quarto-website/index.html#formatting",
    "title": "Quarto for an Academic Website",
    "section": "Formatting",
    "text": "Formatting\nSince I can write the generation logic for page in Python, this puts me on comfortable ground to hack something together. To knit the above publication data into HTML, I just literally bind together the programmatically generated raw HTML and print it onto the page.\nI do a couple additional useful things in this process: - Separate out working papers or non-archival papers from published work (I make this distinction based on whether I include a published_url field or not). - Order and categorize papers by year - Provide nice Bootstrappy buttons for external links (e.g. to Preprints / Code / etc)\n\n\nSee research.qmd fragment\n\nimport yaml\nfrom IPython.display import display, Markdown, HTML\n\ndef readable_list(_s):\n  if len(_s) < 3:\n    return ' and '.join(map(str, _s))\n  *a, b = _s\n  return f\"{', '.join(map(str, a))}, and {b}\"\n\ndef button(url, str, icon):\n    icon_base = icon[:2]\n    return f\"\"\"<a class=\"btn btn-outline-dark btn-sm\", href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\">\n        <i class=\"{icon_base} {icon}\" role='img' aria-label='{str}'></i>\n        {str}\n    </a>\"\"\"\n\nyaml_data = yaml.safe_load(open(\"papers.yaml\"))\npub_strs = {\"pubs\": {}, \"wps\": {}}\nfor _, data in yaml_data.items():\n    title_str = data[\"title\"]\n    authors = data.get(\"authors\", [\"me\"])\n    authors = [\n        aut if aut != \"me\" else \"<strong>Drew Dimmery</strong>\" for aut in authors\n    ]\n    author_str = readable_list(authors)\n    year_str = data[\"year\"]\n\n    buttons = []\n    preprint = data.get(\"preprint\")\n    if preprint is not None:\n        buttons.append(button(preprint, \"Preprint\", \"bi-file-earmark-pdf\"))\n\n    github = data.get(\"github\")\n    if github is not None:\n        buttons.append(button(github, \"Github\", \"bi-github\"))\n\n    pub_url = data.get(\"published_url\")\n    venue = data.get(\"venue\")\n    working_paper = pub_url is None\n    \n    pub_str = f'{author_str}. ({year_str}) \"{title_str}.\"'\n\n    if venue is not None:\n        pub_str += f\" <em>{venue}</em>\"\n\n    if working_paper:\n        if year_str not in pub_strs[\"wps\"]:\n            pub_strs[\"wps\"][year_str] = []\n        pub_strs[\"wps\"][year_str].append(\n            \"<li class='list-group-item'>\" + pub_str + \"<br>\" + \" \".join(buttons) + \"</li>\"\n        )\n    else:\n        if year_str not in pub_strs[\"pubs\"]:\n            pub_strs[\"pubs\"][year_str] = []\n        buttons.append(button(pub_url, \"Published\", \"ai-archive\"))\n        pub_strs[\"pubs\"][year_str].append(\n            \"<li class='list-group-item'>\" + pub_str + \"<br>\" + \" \".join(buttons) + \"</li>\"\n        )\n\nI then print this out using the display functions from the IPython module and using the asis chunk option:\n\n\nSee research.qmd fragment\n\nfor year in sorted(pub_strs[\"pubs\"].keys(), reverse=True):\n    display(Markdown(f\"### {year}\" + \"{#\" + f\"published-{year}\" + \"}\"))\n    display(HTML(\n        \"<ul class='list-group list-group-flush'>\" + '\\n'.join(pub_strs[\"pubs\"][year]) + \"</ul>\"\n    ))\n\nThe full code is on GitHub.\nIt’s worth noting that to get the years to show up in the Table of Contents its necessary to be careful exactly how the content is stuck onto the page. If you don’t use the asis chunk option, you can still get all the right content to show up, but it won’t necessarily appear in the ToC. I also found it necessary to include section-divs: false in the header, or else the output would get wrapped in additional div tags which made it harder to get the right classes in the right divs. There are probably more elegant ways to do all of this.\nI use the same basic setup to populate the Software page, albeit with simpler logic.\n\nAdditions\nI debated adding an abstract that expands out on click (like the code folding above in this post). This would actually be more or less trivial to add using a <details> HTML tag if I wanted to provide the data in the YAML. I’m ignoring this for now because I want to minimize data entry for my future self (and it’s anyway just a click away at the Preprint link)."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Andy Timm",
    "section": "",
    "text": "Here are some current work and personal projects I’m particularly excited about:\n\n\n\nPersonal Projects\n\nUpskilling on Variational Inference: with a particular focus on techniques like normalizing flows which improve our ability to correctly quantify uncertainty 1.\nStudying Declining US Social Capital: Building a better knowledge base on declining social capital in the US, and the implications of this, especially for politics2.\nLearning more about Effective Altruism: especially longtermism, and evaluating how interested I am becoming more involved in the community3.\n\n\n\n\nWork Projects\n\nBetter Polling Methods: Improving our political survey methodology choices to improve our resulting model quality in the 2022 midterm elections4.\nProductizing HTE Estimation: Exploring which Heterogeneous Treatment Effect models perform best in our industry in a variety of contexts, with an eye towards a fully productized solution in early 20235.\nBetter CI’s when Identification is Weak: Improving our uncertainty quantification when using observational methods with weaker identification strategies6.\n\n\nI also want to start building out a broader list of topics that I’ve spent time with or want to do a deeper dive in. For example, last winter I spent a ton of time exploring Potential Outcome vs DAG approaches to causal inference. In grad school I was particularly focused on the effects of educational polarization in the US, etc. As a future example, I want to do a broader dive into how social media effects culture and politics. This is both a personal thing (to track what I’ve been interested in over the years), and a social one (I love sharing resources and working with others to learn these topics).\n\n\n\n\n\nFootnotes\n\n\nI started this out by working through Depth First Learning’s Variational Inference with Normalizing Flows curriculum. Next, I plan to implement a couple of MRP models with various flavors of VI, and see how it holds up to a MCMC version.↩︎\nFor this, I’m reading some of Putnam’s work since Bowling Alone, and a bunch of papers in the vein of this Nature paper.↩︎\nHaving read WWOTF, I’m now reading The Precipice and plan to read Doing Good Better. In addition, reading a bunch of the EA Forum. Since I already think about maximizing my social impact broadly, there are a lot of appealing ideas here. I have two cruxes here: first, I’m not sure if I trust EA’s commitment to being non-political- as laid out here and here. Second, I’m not yet sure I buy the math behind the heavy focus on existential risk causes.↩︎\nNone of these links will get at any IP, but broadly I’m synthesizing a lot of what I learned from AAPOR 2022 (this thread is a good starting point) and examining how some decent ideas would’ve changed our recent predictions.↩︎\nThere are a ton of ideas to explore here, but some of the most promising are double/debiased ML estimators, Meta Learners, and other ideas explored in recent ACIC competitions.↩︎\nBroadly, if we’re trying to estimate a causal effect without an experiment or believable instrument, and have to rely on covariate adjustment-esque strategies, how can we quantify our uncertainty? Can we reason about plausible sizes of unobserved confounders and in so doing move the discussion past “you needed to control for U”? If we can make such claims, can we use them to then define uncertainty intervals which reflect our various levels of plausibility for such concerns?↩︎"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Most of the cool stuff I get to build these days isn’t public unfortunately. Hoping to expand out my public stuff eventually, but I do have an R package that gets some use:"
  },
  {
    "objectID": "software.html#retrodesign",
    "href": "software.html#retrodesign",
    "title": "Software",
    "section": "retrodesign",
    "text": "retrodesign\nretrodesign provides tools for working with Type S (Sign) and Type M (Magnitude) errors, as proposed in Gelman and Tuerlinckx (2000) and Gelman & Carlin (2014). In addition to simply calculating the probability of Type S/M error, the package includes functions for calculating these errors across a variety of effect sizes for comparison, and recommended sample size given “tolerances” for Type S/M errors. To improve the speed of these calculations, closed forms solutions for the probability of a Type S/M error from Lu, Qiu, and Deng (2018) are implemented. The broader goal of this project was to make it easier for researchers to understand these issues in their work, and it’s gratifying the package has been able to do that.\n\n        \n        Website\n     \n        \n        Github\n     \n        \n        Package"
  }
]