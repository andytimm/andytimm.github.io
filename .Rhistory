tensor[y2.clamp(max=height - 1), x1.clamp(max=width - 1)] * weight_x1 * weight_y2 +
tensor[y2.clamp(max=height - 1), x2.clamp(max=width - 1)] * weight_x2 * weight_y2
)
return value
batch = torch.zeros(size=(batch_size, 2)).normal_(mean=0, std=1)
batch
interpolate_tensor(torch_posterior,batch)
reticulate::repl_python()
#| echo: false
import torch
import numpy as np
import torch.nn as nn
from torch.distributions import Uniform
from torch.distributions import MultivariateNormal
from torch import Tensor
from PIL import Image
from typing import Tuple
from typing import Callable
import matplotlib.pyplot as plt
torch.set_default_device('cuda')
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
raw_img = Image.open("images/hard_to_draw_posterior.png")
# Sum the 3 color channels
greyscale_img = np.array(raw_img).sum(axis = 2)
# Replace white values (1020), with 0, so density is all at letters
#indices = greyscale_img == 1020
# Replace the selected values with 50
#greyscale_img[indices] = 50
# Normalize values to help with fitting
normalized_image  = (greyscale_img - greyscale_img.min()) / (greyscale_img.max() - greyscale_img.min())
# Make a torch tensor of the target to use
torch_posterior = torch.from_numpy(normalized_image).to(device).to(torch.float32)
#300 x 300 px, normalized grayscale representation of the above image
torch_posterior.sum()
#| echo: False
# helper function to get a value for continuous density of my pixelated image
# There's definitely a better way to do this, this is just quick
def interpolate_tensor(tensor, z):
# Get the dimensions of the tensor
height, width = tensor.shape[:2]
# Scale and shift the normal draws to match the image coordinates
x = (z[:, 0] * 150 + 150).clamp(0, width - 1).long()
y = (z[:, 1] * 150 + 150).clamp(0, height - 1).long()
# Calculate the indices of the four surrounding elements
x1 = x.floor()
x2 = x1 + 1
y1 = y.floor()
y2 = y1 + 1
# Calculate the weight for interpolation
weight_x2 = x - x1.float()
weight_x1 = 1 - weight_x2
weight_y2 = y - y1.float()
weight_y1 = 1 - weight_y2
# Perform interpolation
value = (
tensor[y1.clamp(max=height - 1), x1.clamp(max=width - 1)] * weight_x1 * weight_y1 +
tensor[y1.clamp(max=height - 1), x2.clamp(max=width - 1)] * weight_x2 * weight_y1 +
tensor[y2.clamp(max=height - 1), x1.clamp(max=width - 1)] * weight_x1 * weight_y2 +
tensor[y2.clamp(max=height - 1), x2.clamp(max=width - 1)] * weight_x2 * weight_y2
)
return value
class TargetDistribution:
def __init__(self, name: str):
"""Define target distribution.
Args:
name: The name of the target density to use.
Valid choices: ["U_1", "U_2", "U_3", "U_4", "ring"].
"""
self.func = self.get_target_distribution(name)
def __call__(self, z: Tensor) -> Tensor:
return self.func(z)
@staticmethod
def get_target_distribution(name: str) -> Callable[[Tensor], Tensor]:
w1 = lambda z: torch.sin(2 * np.pi * z[:, 0] / 4)
w2 = lambda z: 3 * torch.exp(-0.5 * ((z[:, 0] - 1) / 0.6) ** 2)
w3 = lambda z: 3 * torch.sigmoid((z[:, 0] - 1) / 0.3)
if name == "U_1":
def U_1(z):
u = 0.5 * ((torch.norm(z, 2, dim=1) - 2) / 0.4) ** 2
u = u - torch.log(
torch.exp(-0.5 * ((z[:, 0] - 2) / 0.6) ** 2)
+ torch.exp(-0.5 * ((z[:, 0] + 2) / 0.6) ** 2)
)
return u
return U_1
elif name == "U_2":
def U_2(z):
u = 0.5 * ((z[:, 1] - w1(z)) / 0.4) ** 2
return u
return U_2
elif name == "U_3":
def U_3(z):
u = -torch.log(
torch.exp(-0.5 * ((z[:, 1] - w1(z)) / 0.35) ** 2)
+ torch.exp(-0.5 * ((z[:, 1] - w1(z) + w2(z)) / 0.35) ** 2)
+ 1e-6
)
return u
return U_3
elif name == "U_4":
def U_4(z):
u = -torch.log(
torch.exp(-0.5 * ((z[:, 1] - w1(z)) / 0.4) ** 2)
+ torch.exp(-0.5 * ((z[:, 1] - w1(z) + w3(z)) / 0.35) ** 2)
+ 1e-6
)
return u
return U_4
elif name == "ring":
def ring_density(z):
exp1 = torch.exp(-0.5 * ((z[:, 0] - 2) / 0.8) ** 2)
exp2 = torch.exp(-0.5 * ((z[:, 0] + 2) / 0.8) ** 2)
u = 0.5 * ((torch.norm(z, 2, dim=1) - 4) / 0.4) ** 2
u = u - torch.log(exp1 + exp2 + 1e-6)
return u
return ring_density
elif name == "hi":
def hi_density(z):
return interpolate_tensor(torch_posterior,z)
return hi_density
elif name == "moons":
def two_moons_density(z):
x = z[:, 0]
y = z[:, 1]
d = torch.sqrt(x**2 + y**2)
density = torch.exp(-0.2 * d) * torch.cos(4 * np.pi * d)
return density
return two_moons_density
# https://github.com/e-hulten/planar-flows/blob/master/loss.py
class VariationalLoss(nn.Module):
def __init__(self,distribution):
super().__init__()
self.distr = distribution
self.base_distr = MultivariateNormal(torch.zeros(2), torch.eye(2))
def forward(self, z0: Tensor, z: Tensor, sum_log_det_J: float) -> float:
base_log_prob = self.base_distr.log_prob(z0)
target_density_log_prob = -self.distr(z)
return (base_log_prob - target_density_log_prob - sum_log_det_J).mean()
# From https://github.com/e-hulten/planar-flows/blob/master/planar_transform.py
class PlanarTransform(nn.Module):
"""Implementation of the invertible transformation used in planar flow:
f(z) = z + u * h(dot(w.T, z) + b)
See Section 4.1 in https://arxiv.org/pdf/1505.05770.pdf.
"""
def __init__(self, dim: int = 2):
"""Initialise weights and bias.
Args:
dim: Dimensionality of the distribution to be estimated.
"""
super().__init__()
self.w = nn.Parameter(torch.randn(1, dim).normal_(0, 0.1))
self.b = nn.Parameter(torch.randn(1).normal_(0, 0.1))
self.u = nn.Parameter(torch.randn(1, dim).normal_(0, 0.1))
def forward(self, z: Tensor) -> Tensor:
if torch.mm(self.u, self.w.T) < -1:
self.get_u_hat()
return z + self.u * nn.Tanh()(torch.mm(z, self.w.T) + self.b)
def log_det_J(self, z: Tensor) -> Tensor:
if torch.mm(self.u, self.w.T) < -1:
self.get_u_hat()
a = torch.mm(z, self.w.T) + self.b
psi = (1 - nn.Tanh()(a) ** 2) * self.w
abs_det = (1 + torch.mm(self.u, psi.T)).abs()
log_det = torch.log(1e-4 + abs_det)
return log_det
def get_u_hat(self) -> None:
"""Enforce w^T u >= -1. When using h(.) = tanh(.), this is a sufficient condition
for invertibility of the transformation f(z). See Appendix A.1.
"""
wtu = torch.mm(self.u, self.w.T)
m_wtu = -1 + torch.log(1 + torch.exp(wtu))
self.u.data = (
self.u + (m_wtu - wtu) * self.w / torch.norm(self.w, p=2, dim=1) ** 2
)
class PlanarFlow(nn.Module):
def __init__(self, dim: int = 2, K: int = 6):
"""Make a planar flow by stacking planar transformations in sequence.
Args:
dim: Dimensionality of the distribution to be estimated.
K: Number of transformations in the flow.
"""
super().__init__()
self.layers = [PlanarTransform(dim) for _ in range(K)]
self.model = nn.Sequential(*self.layers)
def forward(self, z: Tensor) -> Tuple[Tensor, float]:
log_det_J = 0
for layer in self.layers:
log_det_J += layer.log_det_J(z)
z = layer(z)
return z, log_det_J
#| echo: False
# https://github.com/e-hulten/planar-flows/blob/master/utils/plot.py
def plot_density(density, xlim=4, ylim=4, ax=None, cmap="Blues"):
x = y = np.linspace(-xlim, xlim, 300)
X, Y = np.meshgrid(x, y)
shape = X.shape
X_flatten, Y_flatten = np.reshape(X, (-1, 1)), np.reshape(Y, (-1, 1))
Z = torch.from_numpy(np.concatenate([X_flatten, Y_flatten], 1))
U = torch.exp(-density(Z))
U = U.reshape(shape)
if ax is None:
fig = plt.figure(figsize=(7, 7))
ax = fig.add_subplot(111)
ax.set_xlim(-xlim, xlim)
ax.set_ylim(-xlim, xlim)
ax.set_aspect(1)
ax.pcolormesh(X, Y, U, cmap=cmap, rasterized=True)
ax.tick_params(
axis="both",
left=False,
top=False,
right=False,
bottom=False,
labelleft=False,
labeltop=False,
labelright=False,
labelbottom=False,
)
return ax
def plot_samples(z):
nbins = 250
lim = 4
# z = np.exp(-z)
k = gaussian_kde([z[:, 0], z[:, 1]])
xi, yi = np.mgrid[-lim : lim : nbins * 1j, -lim : lim : nbins * 1j]
zi = k(np.vstack([xi.flatten(), yi.flatten()]))
fig = plt.figure(figsize=[7, 7])
ax = fig.add_subplot(111)
ax.set_xlim(-5, 5)
ax.set_aspect(1)
plt.pcolormesh(xi, yi, zi.reshape(xi.shape), cmap="Purples", rasterized=True)
return ax
def plot_transformation(model, n=500, xlim=4, ylim=4, ax=None, cmap="Purples"):
base_distr = torch.distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))
x = torch.linspace(-xlim, xlim, n)
xx, yy = torch.meshgrid(x, x)
zz = torch.stack((xx.flatten(), yy.flatten()), dim=-1).squeeze()
zk, sum_log_jacobians = model(zz)
base_log_prob = base_distr.log_prob(zz)
final_log_prob = base_log_prob - sum_log_jacobians
qk = torch.exp(final_log_prob)
if ax is None:
fig = plt.figure(figsize=[7, 7])
ax = fig.add_subplot(111)
ax.set_xlim(-xlim, xlim)
ax.set_ylim(-ylim, ylim)
ax.set_aspect(1)
ax.pcolormesh(
zk[:, 0].detach().cpu().data.reshape(n, n),
zk[:, 1].detach().cpu().data.reshape(n, n),
qk.detach().cpu().data.reshape(n, n),
cmap=cmap,
rasterized=True,
)
plt.tick_params(
axis="both",
left=False,
top=False,
right=False,
bottom=False,
labelleft=False,
labeltop=False,
labelright=False,
labelbottom=False,
)
if cmap == "Purples":
ax.set_facecolor(plt.cm.Purples(0.0))
elif cmap == "Reds":
ax.set_facecolor(plt.cm.Reds(0.0))
return ax
def plot_training(model, flow_length, batch_num, lr, axlim):
ax = plot_transformation(model, xlim=axlim, ylim=axlim)
ax.text(
0,
axlim - 2,
"Flow length: {}\nDensity of one batch, iteration #{:06d}\nLearning rate: {}".format(
flow_length, batch_num, lr
),
horizontalalignment="center",
)
plt.savefig(
f"training_plots/iteration_{batch_num:06d}.png",
bbox_inches="tight",
pad_inches=0.5,
)
plt.close()
def plot_comparison(model, target_distr, flow_length, dpi=400):
xlim = ylim = 7 if target_distr == "ring" else 5
fig, axes = plt.subplots(
ncols=2, nrows=1, sharex=True, sharey=True, figsize=[10, 5], dpi=dpi
)
axes[0].tick_params(
axis="both",
left=False,
top=False,
right=False,
bottom=False,
labelleft=False,
labeltop=False,
labelright=False,
labelbottom=False,
)
# Plot true density.
density = TargetDistribution(target_distr)
plot_density(density, xlim=xlim, ylim=ylim, ax=axes[0])
axes[0].text(
0,
ylim - 1,
"True density $\exp(-{})$".format(target_distr),
size=14,
horizontalalignment="center",
)
# Plot estimated density.
batch = torch.zeros(500, 2).normal_(mean=0, std=1)
z = model(batch)[0].detach().numpy()
axes[1] = plot_transformation(model, xlim=xlim, ylim=ylim, ax=axes[1], cmap="Reds")
axes[1].text(
0,
ylim - 1,
"Estimated density $\exp(-{})$".format(target_distr),
size=14,
horizontalalignment="center",
)
fig.savefig(
"results/" + target_distr + "_K" + str(flow_length) + "_comparison.pdf",
bbox_inches="tight",
pad_inches=0.1,
)
def plot_available_distributions():
target_distributions = ["U_1", "U_2", "U_3", "U_4", "ring"]
cmaps = ["Reds", "Purples", "Oranges", "Greens", "Blues"]
fig, axes = plt.subplots(1, len(target_distributions), figsize=(20, 5))
for i, distr in enumerate(target_distributions):
axlim = 7 if distr == "ring" else 5
density = TargetDistribution(distr)
plot_density(density, xlim=axlim, ylim=axlim, ax=axes[i], cmap=cmaps[i])
axes[i].set_title(f"Name: '{distr}'", size=16)
plt.setp(axes, xticks=[], yticks=[])
plt.show()
#From https://github.com/e-hulten/planar-flows/blob/master/train.py
target_distr = "ring"  # U_1, U_2, U_3, U_4, ring
flow_length = 32
dim = 2
num_batches = 20000
batch_size = 128
lr = 6e-4
axlim = xlim = ylim = 7  # 5 for U_1 to U_4, 7 for ring
# ------------------------------------
density = TargetDistribution(target_distr)
model = PlanarFlow(dim, K=flow_length)
bound = VariationalLoss(density)
optimiser = torch.optim.Adam(model.parameters(), lr=lr)
# Train model.
for batch_num in range(1, num_batches + 1):
# Get batch from N(0,I).
batch = torch.zeros(size=(batch_size, 2)).normal_(mean=0, std=1)
# Pass batch through flow.
zk, log_jacobians = model(batch)
# Compute loss under target distribution.
loss = bound(batch, zk, log_jacobians)
optimiser.zero_grad()
loss.backward()
optimiser.step()
if batch_num % 100 == 0:
print(f"(batch_num {batch_num:05d}/{num_batches}) loss: {loss}")
#print(log_jacobians)
if batch_num == 1 or batch_num % 100 == 0:
# Save plots during training. Plots are saved to the 'train_plots' folder.
plot_training(model, flow_length, batch_num, lr, axlim)
#From https://github.com/e-hulten/planar-flows/blob/master/train.py
target_distr = "ring"  # U_1, U_2, U_3, U_4, ring
flow_length = 1
dim = 2
num_batches = 20000
batch_size = 128
lr = 6e-4
axlim = xlim = ylim = 7  # 5 for U_1 to U_4, 7 for ring
# ------------------------------------
density = TargetDistribution(target_distr)
model = PlanarFlow(dim, K=flow_length)
bound = VariationalLoss(density)
optimiser = torch.optim.Adam(model.parameters(), lr=lr)
# Train model.
for batch_num in range(1, num_batches + 1):
# Get batch from N(0,I).
batch = torch.zeros(size=(batch_size, 2)).normal_(mean=0, std=1)
# Pass batch through flow.
zk, log_jacobians = model(batch)
# Compute loss under target distribution.
loss = bound(batch, zk, log_jacobians)
optimiser.zero_grad()
loss.backward()
optimiser.step()
if batch_num % 100 == 0:
print(f"(batch_num {batch_num:05d}/{num_batches}) loss: {loss}")
#print(log_jacobians)
if batch_num == 1 or batch_num % 100 == 0:
# Save plots during training. Plots are saved to the 'train_plots' folder.
plot_training(model, flow_length, batch_num, lr, axlim)
import os
import imageio
def make_gif_from_train_plots(fname: str) -> None:
png_dir = "train_plots/"
images = []
sort = sorted(os.listdir(png_dir))
for file_name in sort[1::1]:
if file_name.endswith(".png"):
file_path = os.path.join(png_dir, file_name)
images.append(imageio.imread(file_path))
imageio.mimsave("gifs/" + fname, images, duration=0.05)
import os
import imageio
def make_gif_from_train_plots(fname: str) -> None:
png_dir = "train_plots/"
images = []
sort = sorted(os.listdir(png_dir))
for file_name in sort[1::1]:
if file_name.endswith(".png"):
file_path = os.path.join(png_dir, file_name)
images.append(imageio.imread(file_path))
imageio.mimsave("gifs/" + fname, images, duration=0.05)
import os
import imageio
def make_gif_from_train_plots(fname: str) -> None:
png_dir = "train_plots/"
images = []
sort = sorted(os.listdir(png_dir))
for file_name in sort[1::1]:
if file_name.endswith(".png"):
file_path = os.path.join(png_dir, file_name)
images.append(imageio.imread(file_path))
imageio.mimsave("gifs/" + fname, images, duration=0.05)
make_gif_from_train_plots("1_layer.gif")
import os
import imageio
def make_gif_from_train_plots(fname: str) -> None:
png_dir = "~/personal-quarto-website-2022/posts/Variational MRP Pt5/training_plots"
images = []
sort = sorted(os.listdir(png_dir))
for file_name in sort[1::1]:
if file_name.endswith(".png"):
file_path = os.path.join(png_dir, file_name)
images.append(imageio.imread(file_path))
imageio.mimsave("gifs/" + fname, images, duration=0.05)
make_gif_from_train_plots("1_layer.gif")
import os
import imageio
def make_gif_from_train_plots(fname: str) -> None:
png_dir = "C:/Users/timma/Documents/personal-quarto-website-2022/posts/Variational MRP Pt5/training_plots"
images = []
sort = sorted(os.listdir(png_dir))
for file_name in sort[1::1]:
if file_name.endswith(".png"):
file_path = os.path.join(png_dir, file_name)
images.append(imageio.imread(file_path))
imageio.mimsave("gifs/" + fname, images, duration=0.05)
make_gif_from_train_plots("1_layer.gif")
import os
import imageio
def make_gif_from_train_plots(fname: str) -> None:
png_dir = "C:/Users/timma/Documents/personal-quarto-website-2022/posts/Variational MRP Pt5/training_plots"
images = []
sort = sorted(os.listdir(png_dir))
for file_name in sort[1::1]:
if file_name.endswith(".png"):
file_path = os.path.join(png_dir, file_name)
images.append(imageio.imread(file_path))
imageio.mimsave("gifs/" + fname, images, duration=0.05)
make_gif_from_train_plots("1_layer.gif")
#From https://github.com/e-hulten/planar-flows/blob/master/train.py
target_distr = "ring"  # U_1, U_2, U_3, U_4, ring
flow_length = 32
dim = 2
num_batches = 20000
batch_size = 128
lr = 6e-4
axlim = xlim = ylim = 7  # 5 for U_1 to U_4, 7 for ring
# ------------------------------------
density = TargetDistribution(target_distr)
model = PlanarFlow(dim, K=flow_length)
bound = VariationalLoss(density)
optimiser = torch.optim.Adam(model.parameters(), lr=lr)
# Train model.
for batch_num in range(1, num_batches + 1):
# Get batch from N(0,I).
batch = torch.zeros(size=(batch_size, 2)).normal_(mean=0, std=1)
# Pass batch through flow.
zk, log_jacobians = model(batch)
# Compute loss under target distribution.
loss = bound(batch, zk, log_jacobians)
optimiser.zero_grad()
loss.backward()
optimiser.step()
if batch_num % 100 == 0:
print(f"(batch_num {batch_num:05d}/{num_batches}) loss: {loss}")
#print(log_jacobians)
if batch_num == 1 or batch_num % 100 == 0:
# Save plots during training. Plots are saved to the 'train_plots' folder.
plot_training(model, flow_length, batch_num, lr, axlim)
import os
import imageio
def make_gif_from_train_plots(fname: str) -> None:
# Hiding the directory when commiting, but easy to infer rihgt path
png_dir = "C:/Users/timma/Documents/personal-quarto-website-2022/posts/Variational MRP Pt5/training_plots"
images = []
sort = sorted(os.listdir(png_dir))
for file_name in sort[1::1]:
if file_name.endswith(".png"):
file_path = os.path.join(png_dir, file_name)
images.append(imageio.imread(file_path))
imageio.mimsave("gifs/" + fname, images, duration=0.05)
make_gif_from_train_plots("32_layer.gif")
