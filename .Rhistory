self.u = nn.Parameter(torch.randn(1, dim).normal_(0, 0.1))
def forward(self, z: Tensor) -> Tensor:
if torch.mm(self.u, self.w.T) < -1:
self.get_u_hat()
return z + self.u * nn.Tanh()(torch.mm(z, self.w.T) + self.b)
def log_det_J(self, z: Tensor) -> Tensor:
if torch.mm(self.u, self.w.T) < -1:
self.get_u_hat()
a = torch.mm(z, self.w.T) + self.b
psi = (1 - nn.Tanh()(a) ** 2) * self.w
abs_det = (1 + torch.mm(self.u, psi.T)).abs()
log_det = torch.log(1e-4 + abs_det)
return log_det
def get_u_hat(self) -> None:
"""Enforce w^T u >= -1. When using h(.) = tanh(.), this is a sufficient condition
for invertibility of the transformation f(z). See Appendix A.1.
"""
wtu = torch.mm(self.u, self.w.T)
m_wtu = -1 + torch.log(1 + torch.exp(wtu))
self.u.data = (
self.u + (m_wtu - wtu) * self.w / torch.norm(self.w, p=2, dim=1) ** 2
)
class PlanarFlow(nn.Module):
def __init__(self, dim: int = 2, K: int = 6):
"""Make a planar flow by stacking planar transformations in sequence.
Args:
dim: Dimensionality of the distribution to be estimated.
K: Number of transformations in the flow.
"""
super().__init__()
self.layers = [PlanarTransform(dim) for _ in range(K)]
self.model = nn.Sequential(*self.layers)
def forward(self, z: Tensor) -> Tuple[Tensor, float]:
log_det_J = 0
for layer in self.layers:
log_det_J += layer.log_det_J(z)
z = layer(z)
return z, log_det_J
#| echo: False
# https://github.com/e-hulten/planar-flows/blob/master/utils/plot.py
def plot_density(density, xlim=4, ylim=4, ax=None, cmap="Blues"):
x = y = np.linspace(-xlim, xlim, 300)
X, Y = np.meshgrid(x, y)
shape = X.shape
X_flatten, Y_flatten = np.reshape(X, (-1, 1)), np.reshape(Y, (-1, 1))
Z = torch.from_numpy(np.concatenate([X_flatten, Y_flatten], 1))
U = torch.exp(-density(Z))
U = U.reshape(shape)
if ax is None:
fig = plt.figure(figsize=(7, 7))
ax = fig.add_subplot(111)
ax.set_xlim(-xlim, xlim)
ax.set_ylim(-xlim, xlim)
ax.set_aspect(1)
ax.pcolormesh(X, Y, U, cmap=cmap, rasterized=True)
ax.tick_params(
axis="both",
left=False,
top=False,
right=False,
bottom=False,
labelleft=False,
labeltop=False,
labelright=False,
labelbottom=False,
)
return ax
def plot_samples(z):
nbins = 250
lim = 4
# z = np.exp(-z)
k = gaussian_kde([z[:, 0], z[:, 1]])
xi, yi = np.mgrid[-lim : lim : nbins * 1j, -lim : lim : nbins * 1j]
zi = k(np.vstack([xi.flatten(), yi.flatten()]))
fig = plt.figure(figsize=[7, 7])
ax = fig.add_subplot(111)
ax.set_xlim(-5, 5)
ax.set_aspect(1)
plt.pcolormesh(xi, yi, zi.reshape(xi.shape), cmap="Purples", rasterized=True)
return ax
def plot_transformation(model, n=500, xlim=4, ylim=4, ax=None, cmap="Purples"):
base_distr = torch.distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))
x = torch.linspace(-xlim, xlim, n)
xx, yy = torch.meshgrid(x, x)
zz = torch.stack((xx.flatten(), yy.flatten()), dim=-1).squeeze()
zk, sum_log_jacobians = model(zz)
base_log_prob = base_distr.log_prob(zz)
final_log_prob = base_log_prob - sum_log_jacobians
qk = torch.exp(final_log_prob)
if ax is None:
fig = plt.figure(figsize=[7, 7])
ax = fig.add_subplot(111)
ax.set_xlim(-xlim, xlim)
ax.set_ylim(-ylim, ylim)
ax.set_aspect(1)
ax.pcolormesh(
zk[:, 0].detach().cpu().data.reshape(n, n),
zk[:, 1].detach().cpu().data.reshape(n, n),
qk.detach().cpu().data.reshape(n, n),
cmap=cmap,
rasterized=True,
)
plt.tick_params(
axis="both",
left=False,
top=False,
right=False,
bottom=False,
labelleft=False,
labeltop=False,
labelright=False,
labelbottom=False,
)
if cmap == "Purples":
ax.set_facecolor(plt.cm.Purples(0.0))
elif cmap == "Reds":
ax.set_facecolor(plt.cm.Reds(0.0))
return ax
def plot_training(model, flow_length, batch_num, lr, axlim):
ax = plot_transformation(model, xlim=axlim, ylim=axlim)
ax.text(
0,
axlim - 2,
"Flow length: {}\nDensity of one batch, iteration #{:06d}\nLearning rate: {}".format(
flow_length, batch_num, lr
),
horizontalalignment="center",
)
plt.savefig(
f"training_plots/iteration_{batch_num:06d}.png",
bbox_inches="tight",
pad_inches=0.5,
)
plt.close()
def plot_comparison(model, target_distr, flow_length, dpi=400):
xlim = ylim = 7 if target_distr == "ring" else 5
fig, axes = plt.subplots(
ncols=2, nrows=1, sharex=True, sharey=True, figsize=[10, 5], dpi=dpi
)
axes[0].tick_params(
axis="both",
left=False,
top=False,
right=False,
bottom=False,
labelleft=False,
labeltop=False,
labelright=False,
labelbottom=False,
)
# Plot true density.
density = TargetDistribution(target_distr)
plot_density(density, xlim=xlim, ylim=ylim, ax=axes[0])
axes[0].text(
0,
ylim - 1,
"True density $\exp(-{})$".format(target_distr),
size=14,
horizontalalignment="center",
)
# Plot estimated density.
batch = torch.zeros(500, 2).normal_(mean=0, std=1)
z = model(batch)[0].detach().numpy()
axes[1] = plot_transformation(model, xlim=xlim, ylim=ylim, ax=axes[1], cmap="Reds")
axes[1].text(
0,
ylim - 1,
"Estimated density $\exp(-{})$".format(target_distr),
size=14,
horizontalalignment="center",
)
fig.savefig(
"results/" + target_distr + "_K" + str(flow_length) + "_comparison.pdf",
bbox_inches="tight",
pad_inches=0.1,
)
def plot_available_distributions():
target_distributions = ["U_1", "U_2", "U_3", "U_4", "ring"]
cmaps = ["Reds", "Purples", "Oranges", "Greens", "Blues"]
fig, axes = plt.subplots(1, len(target_distributions), figsize=(20, 5))
for i, distr in enumerate(target_distributions):
axlim = 7 if distr == "ring" else 5
density = TargetDistribution(distr)
plot_density(density, xlim=axlim, ylim=axlim, ax=axes[i], cmap=cmaps[i])
axes[i].set_title(f"Name: '{distr}'", size=16)
plt.setp(axes, xticks=[], yticks=[])
plt.show()
#From https://github.com/e-hulten/planar-flows/blob/master/train.py
target_distr = "ring"  # U_1, U_2, U_3, U_4, ring
flow_length = 32
dim = 2
num_batches = 20000
batch_size = 128
lr = 6e-4
axlim = xlim = ylim = 7  # 5 for U_1 to U_4, 7 for ring
# ------------------------------------
density = TargetDistribution(target_distr)
model = PlanarFlow(dim, K=flow_length)
bound = VariationalLoss(density)
optimiser = torch.optim.Adam(model.parameters(), lr=lr)
# Train model.
for batch_num in range(1, num_batches + 1):
# Get batch from N(0,I).
batch = torch.zeros(size=(batch_size, 2)).normal_(mean=0, std=1)
# Pass batch through flow.
zk, log_jacobians = model(batch)
# Compute loss under target distribution.
loss = bound(batch, zk, log_jacobians)
optimiser.zero_grad()
loss.backward()
optimiser.step()
if batch_num % 100 == 0:
print(f"(batch_num {batch_num:05d}/{num_batches}) loss: {loss}")
#print(log_jacobians)
if batch_num == 1 or batch_num % 100 == 0:
# Save plots during training. Plots are saved to the 'train_plots' folder.
plot_training(model, flow_length, batch_num, lr, axlim)
#From https://github.com/e-hulten/planar-flows/blob/master/train.py
target_distr = "ring"  # U_1, U_2, U_3, U_4, ring
flow_length = 1
dim = 2
num_batches = 20000
batch_size = 128
lr = 6e-4
axlim = xlim = ylim = 7  # 5 for U_1 to U_4, 7 for ring
# ------------------------------------
density = TargetDistribution(target_distr)
model = PlanarFlow(dim, K=flow_length)
bound = VariationalLoss(density)
optimiser = torch.optim.Adam(model.parameters(), lr=lr)
# Train model.
for batch_num in range(1, num_batches + 1):
# Get batch from N(0,I).
batch = torch.zeros(size=(batch_size, 2)).normal_(mean=0, std=1)
# Pass batch through flow.
zk, log_jacobians = model(batch)
# Compute loss under target distribution.
loss = bound(batch, zk, log_jacobians)
optimiser.zero_grad()
loss.backward()
optimiser.step()
if batch_num % 100 == 0:
print(f"(batch_num {batch_num:05d}/{num_batches}) loss: {loss}")
#print(log_jacobians)
if batch_num == 1 or batch_num % 100 == 0:
# Save plots during training. Plots are saved to the 'train_plots' folder.
plot_training(model, flow_length, batch_num, lr, axlim)
import os
import imageio
def make_gif_from_train_plots(fname: str) -> None:
png_dir = "train_plots/"
images = []
sort = sorted(os.listdir(png_dir))
for file_name in sort[1::1]:
if file_name.endswith(".png"):
file_path = os.path.join(png_dir, file_name)
images.append(imageio.imread(file_path))
imageio.mimsave("gifs/" + fname, images, duration=0.05)
import os
import imageio
def make_gif_from_train_plots(fname: str) -> None:
png_dir = "train_plots/"
images = []
sort = sorted(os.listdir(png_dir))
for file_name in sort[1::1]:
if file_name.endswith(".png"):
file_path = os.path.join(png_dir, file_name)
images.append(imageio.imread(file_path))
imageio.mimsave("gifs/" + fname, images, duration=0.05)
import os
import imageio
def make_gif_from_train_plots(fname: str) -> None:
png_dir = "train_plots/"
images = []
sort = sorted(os.listdir(png_dir))
for file_name in sort[1::1]:
if file_name.endswith(".png"):
file_path = os.path.join(png_dir, file_name)
images.append(imageio.imread(file_path))
imageio.mimsave("gifs/" + fname, images, duration=0.05)
make_gif_from_train_plots("1_layer.gif")
import os
import imageio
def make_gif_from_train_plots(fname: str) -> None:
png_dir = "~/personal-quarto-website-2022/posts/Variational MRP Pt5/training_plots"
images = []
sort = sorted(os.listdir(png_dir))
for file_name in sort[1::1]:
if file_name.endswith(".png"):
file_path = os.path.join(png_dir, file_name)
images.append(imageio.imread(file_path))
imageio.mimsave("gifs/" + fname, images, duration=0.05)
make_gif_from_train_plots("1_layer.gif")
import os
import imageio
def make_gif_from_train_plots(fname: str) -> None:
png_dir = "C:/Users/timma/Documents/personal-quarto-website-2022/posts/Variational MRP Pt5/training_plots"
images = []
sort = sorted(os.listdir(png_dir))
for file_name in sort[1::1]:
if file_name.endswith(".png"):
file_path = os.path.join(png_dir, file_name)
images.append(imageio.imread(file_path))
imageio.mimsave("gifs/" + fname, images, duration=0.05)
make_gif_from_train_plots("1_layer.gif")
import os
import imageio
def make_gif_from_train_plots(fname: str) -> None:
png_dir = "C:/Users/timma/Documents/personal-quarto-website-2022/posts/Variational MRP Pt5/training_plots"
images = []
sort = sorted(os.listdir(png_dir))
for file_name in sort[1::1]:
if file_name.endswith(".png"):
file_path = os.path.join(png_dir, file_name)
images.append(imageio.imread(file_path))
imageio.mimsave("gifs/" + fname, images, duration=0.05)
make_gif_from_train_plots("1_layer.gif")
#From https://github.com/e-hulten/planar-flows/blob/master/train.py
target_distr = "ring"  # U_1, U_2, U_3, U_4, ring
flow_length = 32
dim = 2
num_batches = 20000
batch_size = 128
lr = 6e-4
axlim = xlim = ylim = 7  # 5 for U_1 to U_4, 7 for ring
# ------------------------------------
density = TargetDistribution(target_distr)
model = PlanarFlow(dim, K=flow_length)
bound = VariationalLoss(density)
optimiser = torch.optim.Adam(model.parameters(), lr=lr)
# Train model.
for batch_num in range(1, num_batches + 1):
# Get batch from N(0,I).
batch = torch.zeros(size=(batch_size, 2)).normal_(mean=0, std=1)
# Pass batch through flow.
zk, log_jacobians = model(batch)
# Compute loss under target distribution.
loss = bound(batch, zk, log_jacobians)
optimiser.zero_grad()
loss.backward()
optimiser.step()
if batch_num % 100 == 0:
print(f"(batch_num {batch_num:05d}/{num_batches}) loss: {loss}")
#print(log_jacobians)
if batch_num == 1 or batch_num % 100 == 0:
# Save plots during training. Plots are saved to the 'train_plots' folder.
plot_training(model, flow_length, batch_num, lr, axlim)
import os
import imageio
def make_gif_from_train_plots(fname: str) -> None:
# Hiding the directory when commiting, but easy to infer rihgt path
png_dir = "C:/Users/timma/Documents/personal-quarto-website-2022/posts/Variational MRP Pt5/training_plots"
images = []
sort = sorted(os.listdir(png_dir))
for file_name in sort[1::1]:
if file_name.endswith(".png"):
file_path = os.path.join(png_dir, file_name)
images.append(imageio.imread(file_path))
imageio.mimsave("gifs/" + fname, images, duration=0.05)
make_gif_from_train_plots("32_layer.gif")
quarto preview
reticulate::repl_python()
import pystan
import vistan
# installation: pip install vistan
import vistan
import matplotlib.pyplot as plt
import numpy as np
import scipy
plt.style.use("ggplot")
code = """
data {
}
parameters {
int<lower=0> N;
int<lower=0, upper=1> x[N];
}
}
model {
real<lower=0, upper=1> p;
pbeta (1,1);
X bernoulli (p);
}
"""
data = {"N":5, "x":[0,1,0,0,0]}
for r in ['meanfield', 'flows']:
algo = vistan.recipe(r) # runs Meanfield VI by default
posterior = algo (code, data)
samples = posterior.sample (100000)
plt.hist(samples ['p'], 200, density = True, histtype = 'step', label = r, linewidth = 1.5)
points = np.arange (0, 1, .01)
plt.plot(points, scipy.stats.beta(2,5).pdf (points), label='True Posterior', linewidth
plt.legend()
plt.show()
# installation: pip install vistan
import vistan
import matplotlib.pyplot as plt
import numpy as np
import scipy
plt.style.use("ggplot")
code = """
data {
int<lower=0> N;
int<lower=0, upper=1> x[N];
}
parameters {
real<lower=0,upper=1>> p;
}
model {
p ~ beta(1,1);
x ~ bernoulli(p);
}
"""
data = {"N":5, "x":[0,1,0,0,0]}
for r in ['meanfield', 'flows']:
algo = vistan.recipe(r) # runs Meanfield VI by default
posterior = algo (code, data)
samples = posterior.sample (100000)
plt.hist(samples ['p'], 200, density = True, histtype = 'step', label = r, linewidth = 1.5)
points = np.arange (0, 1, .01)
plt.plot(points, scipy.stats.beta(2,5).pdf (points), label='True Posterior', linewidth
plt.legend()
plt.show()
import matplotlib
reticulate::repl_python()
import pystan
import vistan
import matplotlib
# installation: pip install vistan
import vistan
import numpy as np
import scipy
code = """
data {
int<lower=0> N;
int<lower=0, upper=1> x[N];
}
parameters {
real<lower=0,upper=1>> p;
}
model {
p ~ beta(1,1);
x ~ bernoulli(p);
}
"""
data = {"N":5, "x":[0,1,0,0,0]}
for r in ['meanfield', 'flows']:
algo = vistan.recipe(r) # runs Meanfield VI by default
posterior = algo (code, data)
samples = posterior.sample(100000)
points = np.arange (0, 1, .01)
# installation: pip install vistan
import vistan
import numpy as np
import scipy
code = """
data {
int<lower=0> N;
int<lower=0, upper=1> x[N];
}
parameters {
real<lower=0,upper=1>> p;
}
model {
p ~ beta(1,1);
x ~ bernoulli(p);
}
"""
data = {"N":5, "x":[0,1,0,0,0]}
for r in ['meanfield', 'flows']:
algo = vistan.recipe(r) # runs Meanfield VI by default
posterior = algo (code, data)
samples = posterior.sample(100000)
points = np.arange (0, 1, .01)
>>> model_code = 'parameters {real y;} model {y ~ normal(0,1);}'
>>> model = pystan.StanModel(model_code=model_code)
>>> y = model.sampling().extract()['y']
>>> y.mean()
model_code = 'parameters {real y;} model {y ~ normal(0,1);}'
model = pystan.StanModel(model_code=model_code)
y = model.sampling().extract()['y']
y.mean()
import pystan
model_code = 'parameters {real y;} model {y ~ normal(0,1);}'
model = pystan.StanModel(model_code=model_code)
reticulate::repl_python()
import pystan
import vistan
reticulate::repl_python()
import pystan
import vistan
model_code = 'parameters {real y;} model {y ~ normal(0,1);}'
model = pystan.StanModel(model_code=model_code,verbose = True)
y = model.sampling().extract()['y']
y.mean()
library(rstanarm)
mcmc_fit <- readRDS("posts/Variational MRP Pt7/data/fit_60k_meanfield.rds")
mcmc_fit$stanfit@stanmodel
mcmc_fit$stanfit
mcmc_fit <- readRDS("posts/Variational MRP Pt7/data/fit_60k_mcmc.rds")
mcmc_fit <- readRDS("posts/Variational MRP Pt7/data/fit_60k_mcmc.rds")
mcmc_fit$stanfit@stanmodel
mcmc_fit$data
quarto preview
mean(abs(rnorm(2000)))
mean((rnorm(2000))
mean((rnorm(2000)))
mean((rnorm(2000)))
mean((rnorm(2000)))
mean((rnorm(2000)))
mean((rnorm(2000)))
mean(abs(rnorm(2000)))
