---
page-layout: full
---

I always learn a ton about people's interests by hearing about what projects they're excited about. If you're working on similar things, and want to talk about them more or learn together, I'm always excited to chat (especially if you're doing the work for social-good)!

Here are some current work and personal projects I'm particularly excited about:

::: columns
::: {.column width="50%"}
# Personal Projects

1.  **Upskilling on Variational Inference:** with a particular focus on techniques like normalizing flows which improve our ability to correctly quantify uncertainty [^1].
2.  **Studying Declining US Social Capital:** Building a better knowledge base on declining social capital in the US, and the implications of this, especially for politics[^2].
3.  **Learning more about Effective Altruism:** especially longtermism, and evaluating how interested I am becoming more involved in the community[^3].
:::

::: {.column width="50%"}
# Work Projects

1.  **Better Polling Methods:** Improving our political survey methodology choices to improve our resulting model quality in the 2022 midterm elections[^4].
2.  **Productizing HTE Estimation:** Exploring which Heterogeneous Treatment Effect models perform best in our industry in a variety of contexts, with an eye towards a fully productized solution in early 2023[^5].
3.  **Better CI's when Identification is Weak:** Improving our uncertainty quantification when using observational methods with weaker identification strategies[^6].
:::

I also want to start building out a broader list of topics that I've spent time with or want to do a deeper dive in. For example, last winter I spent a ton of time exploring [Potential Outcome vs DAG approaches to causal inference](https://www.aeaweb.org/articles?id=10.1257/jel.20191597). In grad school I was particularly focused on the effects of educational polarization in the US, etc. As a future example, I want to do a broader dive into how social media effects culture and politics. This is both a personal thing (to track what I've been interested in over the years), and a social one (I love sharing resources and working with others to learn these topics).
:::

[^1]: I started this out by working through Depth First Learning's [Variational Inference with Normalizing Flows](https://www.depthfirstlearning.com/2021/VI-with-NFs) curriculum. Next, I plan to implement a couple of [MRP](https://en.wikipedia.org/wiki/Multilevel_regression_with_poststratification) models with various flavors of VI, and see how it holds up to a MCMC version.

[^2]: For this, I'm reading some of Putnam's work since [Bowling Alone](https://en.wikipedia.org/wiki/Bowling_Alone), and a bunch of papers in the vein of [this Nature paper](https://www.nature.com/articles/s41586-022-04996-4).

[^3]: Having read [WWOTF](https://whatweowethefuture.com/), I'm now reading [The Precipice](https://theprecipice.com/) and plan to read [Doing Good Better](https://en.wikipedia.org/wiki/Doing_Good_Better). In addition, reading a bunch of the [EA Forum](https://forum.effectivealtruism.org/). Since I already think about maximizing my social impact, there are a lot of appealing ideas here. I have two cruxes here: first, I'm not sure if I trust EA's commitment to being non-political- as laid out [here](https://jacobin.com/2015/08/peter-singer-charity-effective-altruism/) and [here](https://academic.oup.com/book/32430/chapter-abstract/268752370?redirectedFrom=fulltext). Second, I'm not yet sure I buy the math behind the heavy focus on existential risk causes.

[^4]: None of these links will get at any IP, but broadly I'm synthesizing a lot of what I learned from AAPOR 2022 ([this thread](https://mobile.twitter.com/kwcollins/status/1525162193104392194) is a good starting point) and examining how some decent ideas would've changed our recent predictions.

[^5]: There are a ton of ideas to explore here, but some of the most promising are [double/debiased ML](https://arxiv.org/abs/1608.00060) estimators, [Meta Learners](https://arxiv.org/abs/1706.03461), and [other ideas](https://arxiv.org/abs/1707.02641) explored in recent ACIC competitions.

[^6]: If we're trying to estimate a causal effect without an experiment or believable instrument, and have to rely on covariate adjustment-esque strategies, how can we quantify our uncertainty? Can we reason about [plausible sizes](https://onlinelibrary.wiley.com/doi/full/10.1002/sim.6973) of [unobserved confounders](https://www.researchgate.net/publication/322509816_Making_Sense_of_Sensitivity_Extending_Omitted_Variable_Bias) and in so doing move the discussion past "you needed to control for U"? If we can make such claims, can we use them to then define uncertainty intervals which reflect our various levels of plausibility for such concerns?
